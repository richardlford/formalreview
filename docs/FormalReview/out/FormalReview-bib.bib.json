{
  "_preamble": "",
  "_comments": "",
  "cassez_verification_2021": {
    "id": "cassez_verification_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Cassez",
        "given": "Franck"
      }
    ],
    "title": "Verification of the Incremental Merkle Tree Algorithm with Dafny",
    "container-title": "arXiv:2105.06009 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "1"
        ]
      ]
    },
    "abstract": "The Deposit Smart Contract (DSC) is an instrumental component of the Ethereum 2.0 Phase 0 infrastructure. We have developed the first machine-checkable version of the incremental Merkle tree algorithm used in the DSC. We present our new and original correctness proof of the algorithm along with the Dafny machine-checkable version. The main results are: 1) a new proof of total correctness; 2) a software artefact with the proof in the form of the complete Dafny code base and 3) new provably correct optimisations of the algorithm.",
    "keywords": "Computer Science - Logic in Computer Science, D.2.4, F.3",
    "URLtext": "2105.06009",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.06009",
    "URL": "http://arxiv.org/abs/2105.06009",
    "_line": "Blockchain.bib:2"
  },
  "liu_survey_nodate": {
    "id": "liu_survey_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Liu",
        "given": "Chao"
      },
      {
        "family": "Zhang",
        "given": "Xiaoshuai"
      },
      {
        "family": "Chai",
        "given": "Kok Koeng"
      },
      {
        "family": "Loo",
        "given": "Jonathan"
      },
      {
        "family": "Chen",
        "given": "Yue"
      }
    ],
    "title": "A survey on blockchain-enabled smart grids: Advances, applications and challenges",
    "container-title": "IET Smart Cities",
    "container-title-short": "A survey on blockchain-enabled smart grids",
    "title-short": "A survey on blockchain-enabled smart grids",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "27"
        ]
      ]
    },
    "issn": "2631-7680",
    "abstract": "Electric power grid infrastructure has revolutionized our world and changed the way of living. So has blockchain technology. The hierarchical electric power grid has been shifting from a centralized structure to a decentralized structure to achieve higher flexibility and stability, and blockchain technology has been widely adopted in the energy sector to deal with grid management, billing, metering, and so on, because of its nature of decentralization. Here, the aim is to provide a multi-dimensional review on the technological advances of the blockchain in smart grids. Its corresponding applications based on these advances, including company projects and use cases, are summarized. Furthermore, the security threat issues in smart grids, Ethereum Virtual Machine (i.e. the operating environment of consensus mechanisms), and smart contracts are analysed, with a brief conclusion to manifest the prior tasks in building secure blockchain-based infrastructures in smart grids. As such, the challenges and features of different protocols and their applicability in each use case are identified to provide an insightful guide for future research studies.",
    "URL": "https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/smc2.12010",
    "DOI": "10.1049/smc2.12010",
    "volume": "n/a",
    "note": "&underscore;eprint: https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/smc2.12010",
    "language": "en-US",
    "_line": "Blockchain.bib:16"
  },
  "crain_red_2021": {
    "id": "crain_red_2021",
    "type": "book",
    "author": [
      {
        "family": "Crain",
        "given": "Tyler"
      },
      {
        "family": "Natoli",
        "given": "Christopher"
      },
      {
        "family": "Gramoli",
        "given": "Vincent"
      }
    ],
    "title": "Red Belly: A Secure, Fair and Scalable Open Blockchain",
    "container-title-short": "Red Belly",
    "title-short": "Red Belly",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "18"
        ]
      ]
    },
    "abstract": "Blockchain has found applications to track ownership of digital assets. Yet, several blockchains were shown vulnerable to network attacks. It is thus crucial for companies to adopt secure blockchains before moving them to production. In this paper, we present Red Belly Blockchain (RBBC), the first secure blockchain whose throughput scales to hundreds of geodistributed consensus participants. To this end, we drastically revisited Byzantine Fault Tolerant (BFT) blockchains through three contributions: (i) defining the Set Byzantine Consensus problem of agreeing on a superblock of all proposed blocks instead of a single block; (ii) adopting a fair leaderless design to offer censorship-resistance guaranteeing the commit of correctly requested transactions; (iii) introducing sharded verification to limit the number of signature verifications without hampering security. We evaluate RBBC on up to 1000 VMs of 3 different types, spread across 4 continents, and under attacks. Although its performance is affected by attacks, RBBC scales in that its throughput increases to hundreds of consensus nodes and achieves 30k TPS throughput and 3 second latency on 1000 VMs, hence improving by 3x both the latency and the throughput of its closest competitor.",
    "DOI": "10.1109/SP40001.2021.00087",
    "_line": "Blockchain.bib:33"
  },
  "fahmideh_software_2021": {
    "id": "fahmideh_software_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Fahmideh",
        "given": "Mahdi"
      },
      {
        "family": "Grundy",
        "given": "John"
      },
      {
        "family": "Ahmed",
        "given": "Aakash"
      },
      {
        "family": "Shen",
        "given": "Jun"
      },
      {
        "family": "Yan",
        "given": "Jun"
      },
      {
        "family": "Mougouei",
        "given": "Davoud"
      },
      {
        "family": "Wang",
        "given": "Peng"
      },
      {
        "family": "Ghose",
        "given": "Aditya"
      },
      {
        "family": "Gunawardana",
        "given": "Anuradha"
      },
      {
        "family": "Aickelin",
        "given": "Uwe"
      },
      {
        "family": "Abedin",
        "given": "Babak"
      }
    ],
    "title": "Software Engineering for Blockchain Based Software Systems: Foundations, Survey, and Future Directions",
    "container-title": "arXiv e-prints",
    "container-title-short": "Software Engineering for Blockchain Based Software Systems",
    "title-short": "Software Engineering for Blockchain Based Software Systems",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "12"
        ]
      ]
    },
    "abstract": "Many scientific and practical areas have shown increasing interest in \nreaping the benefits of blockchain technology to empower software\nsystems. However, the unique characteristics and requirements associated\nwith Blockchain Based Software (BBS) systems raise new challenges across\nthe development lifecycle that entail an extensive improvement of\nconventional software engineering. This article presents a systematic\nliterature review of the state-of-the-art in BBS engineering research\nfrom a software engineering perspective. We characterize BBS engineering\nfrom the theoretical foundations, processes, models, and roles and\ndiscuss a rich repertoire of key development activities, principles,\nchallenges, and techniques. The focus and depth of this survey not only\ngives software engineering practitioners and researchers a consolidated\nbody of knowledge about current BBS development but also underpins a\nstarting point for further research in this field.",
    "keywords": "Computer Science - Software Engineering",
    "URL": "http://adsabs.harvard.edu/abs/2021arXiv210501881F",
    "page": "arXiv:2105.01881",
    "page-first": "arXiv:2105.01881",
    "volume": "2105",
    "_line": "Blockchain.bib:42"
  },
  "losa_formal_2020": {
    "id": "losa_formal_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Losa",
        "given": "Giuliano"
      },
      {
        "family": "Dodds",
        "given": "Mike"
      }
    ],
    "title": "On the Formal Verification of the Stellar Consensus Protocol",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "abstract": "The Stellar Consensus Protocol (SCP) is a quorum-based BFT consensus protocol. However, instead of using threshold-based quorums, SCP is permissionless and its quorum system emerges from participants’ self-declared trust relationships. In this paper, we describe the methodology we deploy to formally verify the safety and liveness of SCP for arbitrary but ﬁxed conﬁgurations.",
    "page": "9",
    "page-first": "9",
    "language": "en-US",
    "_line": "Blockchain.bib:71"
  },
  "annenkov_extracting_2021": {
    "id": "annenkov_extracting_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Annenkov",
        "given": "Danil"
      },
      {
        "family": "Milo",
        "given": "Mikkel"
      },
      {
        "family": "Nielsen",
        "given": "Jakob Botsch"
      },
      {
        "family": "Spitters",
        "given": "Bas"
      }
    ],
    "title": "Extracting smart contracts tested and verified in Coq",
    "container-title": "Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "collection-title": "CPP 2021",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "25"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8299-1",
    "abstract": "We implement extraction of Coq programs to functional languages based on MetaCoq's certified erasure. As part of this, we implement an optimisation pass removing unused arguments. We prove the pass correct wrt. a conventional call-by-value operational semantics of functional languages. We apply this to two functional smart contract languages, Liquidity and Midlang, and to the functional language Elm. Our development is done in the context of the ConCert framework that enables smart contract verification. We contribute a verified boardroom voting smart contract featuring maximum voter privacy such that each vote is kept private except under collusion of all other parties. We also integrate property-based testing into ConCert using QuickChick and our development is the first to support testing properties of interacting smart contracts. We test several complex contracts such as a DAO-like contract, an escrow contract, an implementation of a Decentralized Finance (DeFi) contract which includes a custom token standard (Tezos FA2), and more. In total, this gives us a way to write dependent programs in Coq, test them semi-automatically, verify, and then extract to functional smart contract languages, while retaining a small trusted computing base of only MetaCoq and the pretty-printers into these languages.",
    "keywords": "formal verification, Coq, proof assistants, blockchain, certified programming, code extraction, property-based testing, smart contracts, software correctness",
    "URL": "https://doi.org/10.1145/3437992.3439934",
    "DOI": "10.1145/3437992.3439934",
    "publisher-place": "New York, NY, USA",
    "page": "105-121",
    "page-first": "105",
    "_line": "FormalBib.bib:665"
  },
  "wan_smart_2021": {
    "id": "wan_smart_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Wan",
        "given": "Zhiyuan"
      },
      {
        "family": "Xia",
        "given": "Xin"
      },
      {
        "family": "Lo",
        "given": "David"
      },
      {
        "family": "Chen",
        "given": "Jiachi"
      },
      {
        "family": "Luo",
        "given": "Xiapu"
      },
      {
        "family": "Yang",
        "given": "Xiaohu"
      }
    ],
    "title": "Smart Contract Security: a Practitioners' Perspective",
    "container-title": "arXiv:2102.10963 \\[cs\\]",
    "container-title-short": "Smart Contract Security",
    "title-short": "Smart Contract Security",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "1"
        ]
      ]
    },
    "abstract": "Smart contracts have been plagued by security incidents, which resulted in substantial financial losses. Given numerous research efforts in addressing the security issues of smart contracts, we wondered how software practitioners build security into smart contracts in practice. We performed a mixture of qualitative and quantitative studies with 13 interviewees and 156 survey respondents from 35 countries across six continents to understand practitioners' perceptions and practices on smart contract security. Our study uncovers practitioners' motivations and deterrents of smart contract security, as well as how security efforts and strategies fit into the development lifecycle. We also find that blockchain platforms have a statistically significant impact on practitioners' security perceptions and practices of smart contract development. Based on our findings, we highlight future research directions and provide recommendations for practitioners.",
    "keywords": "Computer Science - Software Engineering",
    "URLtext": "2102.10963",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2102.10963",
    "URL": "http://arxiv.org/abs/2102.10963",
    "_line": "Blockchain.bib:95"
  },
  "cassez_formal_2021": {
    "id": "cassez_formal_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Cassez",
        "given": "Franck"
      },
      {
        "family": "Fuller",
        "given": "Joanne"
      },
      {
        "family": "Asgaonkar",
        "given": "Aditya"
      }
    ],
    "title": "Formal Verification of the Ethereum 2.0 Beacon Chain",
    "container-title": "arXiv:2110.12909 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "21"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "1"
        ]
      ]
    },
    "abstract": "We report our experience in the formal verification of the reference implementation of the Beacon Chain. The Beacon Chain is the backbone component of the new Proof-of-Stake Ethereum 2.0 network: it is in charge of tracking information about the validators, their stakes, their attestations (votes) and if some validators are found to be dishonest, to slash them (they lose some of their stakes). The Beacon Chain is mission-critical and any bug in it could compromise the whole network. The Beacon Chain reference implementation developed by the Ethereum Foundation is written in Python, and provides a detailed operational description of the state machine each Beacon Chain's network participant (node) must implement. We have formally specified and verified the absence of runtime errors in (a large and critical part of) the Beacon Chain reference implementation using the verification-friendly language Dafny. During the course of this work, we have uncovered several issues, proposed verified fixes. We have also synthesised functional correctness specifications that enable us to provide guarantees beyond runtime errors. Our software artefact is available at https://github.com/ConsenSys/eth2.0-dafny.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science, F.2.2, F.4.2",
    "URLtext": "2110.12909",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2110.12909",
    "URL": "http://arxiv.org/abs/2110.12909",
    "_line": "Blockchain.bib:110"
  },
  "gabbay_money_2021": {
    "id": "gabbay_money_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Gabbay",
        "given": "Murdoch"
      },
      {
        "family": "Jakobsson",
        "given": "Arvid"
      },
      {
        "family": "Sojakova",
        "given": "Kristina"
      }
    ],
    "title": "Money grows on (proof-)trees: the formal FA1.2 ledger standard",
    "container-title": "arXiv:2109.09451 \\[cs\\]",
    "container-title-short": "Money grows on (proof-)trees",
    "title-short": "Money grows on (proof-)trees",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "26"
        ]
      ]
    },
    "abstract": "Once you have invented digital money, you may need a ledger to track who owns what &ndash; and an interface to that ledger so that users of your money can transact. On the Tezos blockchain this implies: a smart contract (distributed program), storing in its state a ledger to map owner addresses to token quantities, and standardised entrypoints to transact on accounts. A bank does a similar job &ndash; it maps account numbers to account quantities and permits users to transact &ndash; but in return the bank demands trust, it incurs expense to maintain a centralised server and staff, it uses a proprietary interface ... and it may speculate using your money and/or display rent-seeking behaviour. A blockchain ledger is by design decentralised, inexpensive, open, and it won't just bet your tokens on risky derivatives (unless you ask). The FA1.2 standard is an open standard for ledger-keeping smart contracts on the Tezos blockchain. Several FA1.2 implementations already exist. Or do they? Is the standard sensible and complete? Are the implementations correct? And what are they implementations &bslash;emph&lcurly;of&rcurly;? The FA1.2 standard is written in English, a specification language favoured by wet human brains but notorious for its incompleteness and ambiguity when rendered into dry and unforgiving code. In this paper we report on a formalisation of the FA1.2 standard as a Coq specification, and on a formal verification of three FA1.2-compliant smart contracts with respect to that specification. Errors were found and ambiguities were resolved; but also, there now exists a &bslash;emph&lcurly;mathematically precise&rcurly; and battle-tested specification of the FA1.2 ledger standard. We will describe FA1.2 itself, outline the structure of the Coq theories &ndash; which in itself captures some non-trivial and novel design decisions of the development &ndash; and review the detailed verification of the implementations.",
    "keywords": "Computer Science - Logic in Computer Science, D.2.4, F.3.1, K.4.4, K.7.3",
    "URLtext": "2109.09451",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2109.09451",
    "URL": "http://arxiv.org/abs/2109.09451",
    "_line": "Blockchain.bib:124"
  },
  "boston_veried_nodate": {
    "id": "boston_veried_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Boston",
        "given": "Brett"
      },
      {
        "family": "Breese",
        "given": "Samuel"
      },
      {
        "family": "Dodds",
        "given": "Joey"
      },
      {
        "family": "Dodds",
        "given": "Mike"
      },
      {
        "family": "Huﬀman",
        "given": "Brian"
      },
      {
        "family": "Petcher",
        "given": "Adam"
      },
      {
        "family": "Stefanescu",
        "given": "Andrei"
      }
    ],
    "title": "Veriﬁed Cryptographic Code for Everybody",
    "abstract": "We have completed machine-assisted proofs of two highlyoptimized cryptographic primitives, AES-256-GCM and SHA-384. We have veriﬁed that the implementations of these primitives, written in a mix of C and x86 assembly, are memory safe and functionally correct, by which we mean input-output equivalent to their algorithmic speciﬁcations. Our proofs were completed using SAW, a bounded cryptographic veriﬁcation tool which we have extended to handle embedded x86. The code we have veriﬁed comes from AWS LibCrypto. This code is identical to BoringSSL and very similar to OpenSSL, from which it ultimately derives. We believe we are the ﬁrst to formally verify these implementations, which protect the security of nearly everybody on the internet.",
    "page": "23",
    "page-first": "23",
    "language": "en-US",
    "_line": "Cryptography.bib:2"
  },
  "barthe_structured_2021": {
    "id": "barthe_structured_2021",
    "type": "report",
    "author": [
      {
        "family": "Barthe",
        "given": "Gilles"
      },
      {
        "family": "Gregoire",
        "given": "Benjamin"
      },
      {
        "family": "Laporte",
        "given": "Vincent"
      },
      {
        "family": "Priya",
        "given": "Swarn"
      }
    ],
    "title": "Structured Leakage and Applications to Cryptographic Constant-Time and Cost",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "31"
        ]
      ]
    },
    "abstract": "Many security properties of interest are captured by instrumented semantics that model the functional behavior and the leakage of programs. For several important properties, including cryptographic constant-time (CCT), leakage models are sufficiently abstract that one can define instrumented semantics for high-level and low-level programs. One important goal is then to relate leakage of source programs and leakage of their compilation&mdash;this can be used, e.g.&bslash;, to prove preservation of CCT. To simplify this task, we put forward the idea of structured leakage. In contrast to the usual modeling of leakage as a sequence of observations, structured leakage is tightly coupled with the operational semantics of programs. This coupling greatly simplifies the definition of leakage transformers that map the leakage of source programs to leakage of their compilation and yields more precise statements about the preservation of security properties. We illustrate our methods on the Jasmin compiler and prove preservation results for two policies of interest: CCT and cost.",
    "keywords": "Cost, Cryptographic Constant-Time, foundations, Secure Compilation",
    "URL": "http://eprint.iacr.org/2021/650",
    "number": "650",
    "_line": "Cryptography.bib:11"
  },
  "hood_trusted_2016": {
    "id": "hood_trusted_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Hood",
        "given": "Jonathan"
      }
    ],
    "title": "Trusted, Third-Party Authenticated, Quantum Key Distribution",
    "issued": {
      "date-parts": [
        [
          "2016",
          "8",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "16"
        ]
      ]
    },
    "abstract": "This dissertation presents an algorithm that provides a way of establishing trust and authentication. The protocol negotiates a key using extensions to QKD algorithms that include non-repudiation and endpoint verification through a trusted third-party. The new algorithm proves the viability of implementing a trusted third-party in a QKD scheme. \n \nDue to the capacity of quantum algorithms, the complexity of the new method is not meaningful to calculate using traditional big O methods. However, the Kolmogorov complexity calculation can be used to determine a form of the algorithm's complexity by measuring the operations it takes the algorithm to reach a successful state of entropy. Additional padding and negotiation with the third party yields a longer entropy calculation than QKD-only algorithms. \n \nA reference implementation for the presented algorithm is provided. To test the reference implementation, a simulated quantum environment is created. The quantum simulation model and its correctness in implementing the newly created algorithm are validated for using standard model verification techniques. \n \nExperimentation is set up as a \"pass\" or \"fail\" scenario. If any party is unable to unpad or decrypt a message, the algorithm is deemed a failure. If a party runs out of negotiated qubits, an entropy error is recorded and up to three retries are attempted. Experimentation on key sizes of at least 100 bits results in successful trusted key negotiation with 99.9999999987&perc; confidence. \n \nThe results of the experiment culminate in a new algorithm, dubbed HHUYS16, which can be implemented using current technology. This could particularly be useful to government systems that require a quantum network and its assets to be secured. Implementation guidance is provided in the form of a QKD Security Implementation Technical Guideline (STIG); however, DoD implementation requires further coordination among organizations. Further improvements and clarifications can be made with the National Institute of Standards and Technology's (NIST) proper identification of quantum-resistant encryption algorithms.",
    "URL": "https://etd.auburn.edu//handle/10415/5373",
    "note": "Accepted: 2016-08-05T15:54:45Z",
    "language": "en-US",
    "_line": "Cryptography.bib:23"
  },
  "dougherty_tutorial-style_2021": {
    "id": "dougherty_tutorial-style_2021",
    "type": "chapter",
    "author": [
      {
        "family": "Bhargavan",
        "given": "Karthikeyan"
      },
      {
        "family": "Bichhawat",
        "given": "Abhishek"
      },
      {
        "family": "Do",
        "given": "Quoc Huy"
      },
      {
        "family": "Hosseyni",
        "given": "Pedram"
      },
      {
        "family": "Küsters",
        "given": "Ralf"
      },
      {
        "family": "Schmitz",
        "given": "Guido"
      },
      {
        "family": "Würtele",
        "given": "Tim"
      }
    ],
    "editor": [
      {
        "family": "Dougherty",
        "given": "Daniel"
      },
      {
        "family": "Meseguer",
        "given": "José"
      },
      {
        "family": "Mödersheim",
        "given": "Sebastian Alexander"
      },
      {
        "family": "Rowe",
        "given": "Paul"
      }
    ],
    "title": "A Tutorial-Style Introduction to DY\\*",
    "container-title": "Protocols, Strands, and Logic",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "26"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-91630-5 978-3-030-91631-2",
    "abstract": "DY is a recently proposed formal veriﬁcation framework for the symbolic security analysis of cryptographic protocol code written in the F programming language. Unlike automated symbolic provers, DY accounts for advanced protocol features like unbounded loops and mutable recursive data structures as well as low-level implementation details like protocol state machines and message formats, which are often at the root of real-world attacks. Protocols modeled in DY can be executed, and hence, tested, and they can even interoperate with real-world counterparts. DY extends a long line of research on using dependent type systems but takes a fundamentally new approach by explicitly modeling the global trace-based semantics within the framework, hence bridging the gap between trace-based and type-based protocol analyses. With this, one can uniformly, precisely, and soundly model, for the ﬁrst time using dependent types, long-lived mutable protocol state, equational theories, ﬁne-grained dynamic corruption, and trace-based security properties like forward secrecy and post-compromise security. In this paper, we provide a tutorial-style introduction to DY : We illustrate how to model and prove the security of the ISO-DH protocol, a simple key exchange protocol based on Diﬃe-Hellman.",
    "URL": "https://link.springer.com/10.1007/978-3-030-91631-2_4",
    "DOI": "10.1007/978-3-030-91631-2_4",
    "publisher-place": "Cham",
    "page": "77-97",
    "page-first": "77",
    "volume": "13066",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "Cryptography.bib:43"
  },
  "bhargavan_handshake_nodate": {
    "id": "bhargavan_handshake_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Bhargavan",
        "given": "Karthikeyan"
      },
      {
        "family": "Cheval",
        "given": "Vincent"
      },
      {
        "family": "Wood",
        "given": "Christopher"
      }
    ],
    "title": "Handshake Privacy for TLS 1.3 - Technical report",
    "abstract": "TLS 1.3, the newest version of the Transport Layer Security (TLS) protocol, provides stronger authentication and confidentiality guarantees than prior TLS version. Despite additional encryption of handshake messages, some parts of the TLS 1.3 handshake, including the ClientHello, are still in the clear. For example, the protocol reveals the identity of the target server to network attackers, allowing the passive surveillance and active censorship of TLS connections. A recent privacy extension called Encrypted Client Hello (ECH, previously called ESNI) addresses this problem and offers more comprehensive handshake encryption and privacy for TLS 1.3. Surprisingly however, although the security of the TLS 1.3 handshake has been comprehensively analyzed in a variety of formal models, the privacy guarantees of handshake encryption have never been formally studied. This gap has resulted in several mis-steps: several of the initial designs for ECH were found to be vulnerable to passive and active network attacks.",
    "page": "53",
    "page-first": "53",
    "language": "en-US",
    "_line": "Cryptography.bib:63"
  },
  "rise4fun:dafny": {
    "id": "Rise4fun:dafny",
    "type": "no-type",
    "author": [
      {
        "family": "Leino",
        "given": "K. Rustan M."
      }
    ],
    "title": "Try Dafny In Your Browser",
    "note": "Available at <http://rise4fun.com/Dafny>",
    "_line": "DafnyRef.bib:1"
  },
  "msr:dafny:main": {
    "id": "MSR:dafny:main",
    "type": "no-type",
    "author": [
      {
        "family": "Leino",
        "given": "K. Rustan M."
      }
    ],
    "title": "Main Microsoft Research Dafny Web page",
    "note": "Available at <http://research.microsoft.com/en-us/projects/dafny>",
    "_line": "DafnyRef.bib:6"
  },
  "msr:dafny:source": {
    "id": "MSR:dafny:source",
    "type": "no-type",
    "author": [
      {
        "family": "",
        "given": "K. Rustan M. Leino",
        "dropping-particle": "et al"
      }
    ],
    "title": "Dafny Source Code",
    "note": "Available at <http://dafny.codeplex.com>",
    "_line": "DafnyRef.bib:11"
  },
  "msr:dafny:quickref": {
    "id": "MSR:dafny:quickref",
    "type": "no-type",
    "author": [
      {
        "family": "Leino",
        "given": "K. Rustan M."
      }
    ],
    "title": "Dafny Quick Reference",
    "note": "Available at <http://research.microsoft.com/en-us/projects/dafny/reference.aspx>",
    "_line": "DafnyRef.bib:16"
  },
  "linz:coco": {
    "id": "Linz:Coco",
    "type": "no-type",
    "author": [
      {
        "family": "M&ouml;ssenb&ouml;ck",
        "given": "Hanspeter"
      },
      {
        "family": "L&ouml;berbauer",
        "given": "Markus"
      },
      {
        "family": "W&ouml;&szlig;",
        "given": "Albrecht"
      }
    ],
    "title": "The Compiler Generator Coco/R",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "publisher": "Open source from University of Linz",
    "note": "Available at <http://www.ssw.uni-linz.ac.at/Research/Projects/Coco/>",
    "_line": "DafnyRef.bib:21"
  },
  "leino:dafny:calc": {
    "id": "LEINO:Dafny:Calc",
    "type": "no-type",
    "author": [
      {
        "family": "Leino",
        "given": "K. Rustan M."
      },
      {
        "family": "Polikarpova",
        "given": "Nadia"
      }
    ],
    "title": "Verified Calculations",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "publisher": "Manuscript KRML 231",
    "note": "Available at <http://research.microsoft.com/en-us/um/people/leino/papers/krml231.pdf>",
    "_line": "DafnyRef.bib:28"
  },
  "leino:dafny:coinduction": {
    "id": "LEINO:Dafny:Coinduction",
    "type": "no-type",
    "author": [
      {
        "family": "Leino",
        "given": "K. Rustan M."
      },
      {
        "family": "Moskal",
        "given": "Michal"
      }
    ],
    "title": "Co-induction Simply: Automatic Co-inductive Proofs in a Program Verifier",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "publisher": "Manuscript KRML 230",
    "note": "Available at <http://research.microsoft.com/en-us/um/people/leino/papers/krml230.pdf>",
    "_line": "DafnyRef.bib:35"
  },
  "leino:dafny:dynamicframes": {
    "id": "LEINO:Dafny:DynamicFrames",
    "type": "no-type",
    "author": [
      {
        "family": "Leino",
        "given": "K. Rustan M."
      }
    ],
    "title": "Dynamic-frame specifications in Dafny",
    "issued": {
      "date-parts": [
        [
          "2009"
        ]
      ]
    },
    "publisher": "JML seminar, Dagstuhl, Germany",
    "note": "Available at <http://research.microsoft.com/en-us/um/people/leino/papers/dafny-jml-dagstuhl-2009.pptx>",
    "_line": "DafnyRef.bib:42"
  },
  "spector-zabusky_dont_2021": {
    "id": "spector-zabusky_dont_2021",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Spector-Zabusky",
        "given": "Antal"
      }
    ],
    "title": "DON’T MIND THE FORMALIZATION GAP: THE DESIGN AND USAGE OF HS-TO-COQ",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "University of Pennsylvania",
    "number-of-pages": "240",
    "abstract": "Using proof assistants to perform formal, mechanical software verification is a\npowerful technique for producing correct software. However, the verification is timeconsuming and limited to software written in the language of the proof assistant. As\nan approach to mitigating this tradeoff, this dissertation presents hs-to-coq, a tool\nfor translating programs written in the Haskell programming language into the Coq\nproof assistant, along with its applications and a general methodology for using it to\nverify programs. By introducing edit files containing programmatic descriptions of\ncode transformations, we provide the ability to flexibly adapt our verification goals to\nexist anywhere on the spectrum between “increased confidence” and “full functional\ncorrectness”.",
    "URL": "https://www.cis.upenn.edu/~sweirich/papers/spector-zabusky-thesis.pdf",
    "language": "en-US",
    "_line": "FormalBib.bib:2"
  },
  "abate_extended_2021": {
    "id": "abate_extended_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Abate",
        "given": "Carmine"
      },
      {
        "family": "Blanco",
        "given": "Roberto"
      },
      {
        "family": "Ciobâcă",
        "given": "Tefan"
      },
      {
        "family": "Durier",
        "given": "Adrien"
      },
      {
        "family": "Garg",
        "given": "Deepak"
      },
      {
        "family": "Hrit",
        "given": "Cătălin"
      },
      {
        "family": "Patrignani",
        "given": "Marco"
      },
      {
        "family": "Tanter",
        "given": "Éric"
      },
      {
        "family": "Thibault",
        "given": "Jérémy"
      }
    ],
    "title": "An Extended Account of Trace-Relating Compiler Correctness and Secure Compilation",
    "container-title": "TOPLAS",
    "container-title-short": "TOPLAS",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "URL": "https://people.mpi-sws.org/~dg/papers/toplas21-diff.pdf",
    "page": "48",
    "page-first": "48",
    "volume": "To Appear",
    "language": "en-US",
    "_line": "FormalBib.bib:23"
  },
  "ye_type-directed_2016": {
    "id": "ye_type-directed_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Ye",
        "given": "Wenjia"
      }
    ],
    "title": "Type-Directed Operational Semantics for Gradual Typing",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "abstract": "The semantics of gradually typed languages is typically given indirectly via an elaboration into a cast calculus. This contrasts with more conventional formulations of programming language semantics, where the semantics of a language is given directly using, for instance, an operational semantics. This paper presents a new approach to give the semantics of gradually typed languages directly. We use a recently proposed variant of small-step operational semantics called type-directed operational semantics (TDOS). In TDOS type annotations become operationally relevant and can affect the result of a program. In the context of a gradually typed language, such type annotations are used to trigger type-based conversions on values. We illustrate how to employ TDOS on gradually typed languages using two calculi. The first calculus, called λBg, is inspired by the semantics of the blame calculus, but it has implicit type conversions, enabling it to be used as a gradually typed language. The second calculus, called λBr, explores a different design space in the semantics of gradually typed languages. It uses a so-called blame recovery semantics, which enables eliminating some false positives where blame is raised but normal computation could succeed. For both calculi, type safety is proved. Furthermore we show that the semantics of λBg is sound with respect to the semantics of the blame calculus, and that λBr comes with a gradual guarantee. All the results have been mechanically formalized in the Coq theorem prover.",
    "page": "29",
    "page-first": "29",
    "language": "en-US",
    "_line": "FormalBib.bib:36"
  },
  "sinkarovs_choosing_2021": {
    "id": "sinkarovs_choosing_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Šinkarovs",
        "given": "Artjoms"
      },
      {
        "family": "Cockx",
        "given": "Jesper"
      }
    ],
    "title": "Choosing is Losing: How to combine the benefits of shallow and deep embeddings through reflection",
    "container-title": "arXiv:2105.10819 \\[cs\\]",
    "container-title-short": "Choosing is Losing",
    "title-short": "Choosing is Losing",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "31"
        ]
      ]
    },
    "abstract": "Dependently-typed host languages empower users to verify a wide range of properties of embedded languages and programs written in them. Designers of such embedded languages are faced with a difficult choice between using a shallow or a deep embedding. The former is easier to use because the entire infrastructure of the host langauge is immediately available. Meanwhile, the latter gives full access to the structure of embedded programs, but is difficult to use in practice, especially when the embedded language is itself dependently typed. The main insight presented in this paper is that the choice between shallow and deep embedding can be eliminated by working in a host language with reflection capabilities: we start from a shallow embedding that can use all libraries and tools of the host language, and later use reflection to expose the deep structure of the embedded programs. Concretely, we apply this technique to embed three programming languages &ndash; Kaleidoscope, SaC, and (a subset of) APL &ndash; into the dependently typed theorem prover Agda, using dependent types to statically enforce several properties of interest. We then use Agda's reflection capabilities to extract the embedded programs back into the original language, so that the existing toolchain can be leveraged. In this process, statically verified properties of the host language are mapped onto runtime checks in the target language, allowing extracted programs to interact safely with existing code. Finally, we demonstrate the feasibility of our approach with the implementation and extraction of a convolutional neural network in our embedding of APL.&bslash;@",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2105.10819",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.10819",
    "URL": "http://arxiv.org/abs/2105.10819",
    "_line": "FormalBib.bib:46"
  },
  "pimpalkhare_medleysolver_nodate": {
    "id": "pimpalkhare_medleysolver_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Pimpalkhare",
        "given": "Nikhil"
      },
      {
        "family": "Mora",
        "given": "Federico"
      },
      {
        "family": "Polgreen",
        "given": "Elizabeth"
      },
      {
        "family": "Seshia",
        "given": "Sanjit A"
      }
    ],
    "title": "MedleySolver: Online SMT Algorithm Selection",
    "abstract": "Satisﬁability modulo theories (SMT) solvers implement a wide range of optimizations that are often tailored to a particular class of problems, and that diﬀer signiﬁcantly between solvers. As a result, one solver may solve a query quickly while another might be ﬂummoxed completely. Predicting the performance of a given solver is diﬃcult for users of SMT-driven applications, particularly when the problems they have to solve do not fall neatly into a well-understood category. In this paper, we propose an online algorithm selection framework for SMT called MedleySolver that predicts the relative performances of a set of SMT solvers on a given query, distributes time amongst the solvers, and deploys the solvers in sequence until a solution is obtained. We evaluate MedleySolver against the best available alternative, an oﬄine learning technique, in terms of pure performance and practical usability for a typical SMT user. We ﬁnd that with no prior training, MedleySolver solves 93.9&perc; of the queries solved by the virtual best solver selector achieving 59.8&perc; of the par-2 score of the most successful individual solver, which solves 87.3&perc;. For comparison, the best available alternative takes longer to train than MedleySolver takes to solve our entire set of 2000 queries.",
    "page": "18",
    "page-first": "18",
    "language": "en-US",
    "_line": "FormalBib.bib:61"
  },
  "milner_models_1973": {
    "id": "milner_models_1973",
    "type": "report",
    "author": [
      {
        "family": "Milner",
        "given": "Robin"
      }
    ],
    "title": "Models of LCF",
    "issued": {
      "date-parts": [
        [
          "1973",
          "1"
        ]
      ]
    },
    "publisher": "Stanford University",
    "URL": "http://i.stanford.edu/pub/cstr/reports/cs/tr/73/332/CS-TR-73-332.pdf",
    "publisher-place": "Stanford University",
    "page": "19",
    "page-first": "19",
    "number": "{STAN}-{CS}-73-332, Memo {AIM}-186",
    "_line": "FormalBib.bib:70"
  },
  "iosif_encoding_nodate": {
    "id": "iosif_encoding_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Iosif",
        "given": "Radu"
      },
      {
        "family": "Serban",
        "given": "Cristina"
      },
      {
        "family": "Reynolds",
        "given": "Andrew"
      },
      {
        "family": "Sighireanu",
        "given": "Mihaela"
      }
    ],
    "title": "Encoding Separation Logic in SMT-LIB v2.5",
    "abstract": "We propose an encoding of Separation Logic using SMT-LIB v2.5. This format is currently supported by SMT solvers (CVC4) and inductive prooftheoretic solvers (SLIDE and SPEN). Moreover, we provide a library of benchmarks written using this format, which stems from the set of benchmarks used in SL-COMP’14 \\[7\\].",
    "page": "8",
    "page-first": "8",
    "language": "en-US",
    "_line": "FormalBib.bib:82"
  },
  "li_memory_2021": {
    "id": "li_memory_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "Shaofeng"
      },
      {
        "family": "Qiao",
        "given": "Lei"
      },
      {
        "family": "Yang",
        "given": "Mengfei"
      }
    ],
    "title": "Memory State Verification Based on Inductive and Deductive Reasoning",
    "container-title": "IEEE Transactions on Reliability",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "issn": "1558-1721",
    "abstract": "Memory allocation and deallocation are the fundamental operations of embedded operating systems, which have been extensively used in many safety critical systems. The correctness of the operations is of paramount importance because their failure could incur severe consequences. While the system is running, the memory state can easily grow to a gigantic amount, which means that it is impossible to verify the huge memory states one by one. Therefore, it is a challenge how to verify the correctness of running memory state of the system. In this article, we propose a novel memory state verification method based on inductive and deductive reasoning. First, we abstract the memory state as a list of memory blocks, which will transform in memory operations. Second, we construct the generic model based on the transition function of the memory management and summarize the invariant properties of the memory state. Third, we use the inductive method to calculate the changes between the memory states, and verify that the memory state of the system always satisfy the global properties. All the proofs are implemented in the interactive theorem prover Coq. On the basis of our proposed model, we verify the correctness of a two-level segregated fit (TLSF) algorithm through some extensions, and we also apply this method to verify the correctness of the memory state of the embedded system at runtime.",
    "keywords": "Safety, Deductive, Embedded systems, formal verification, Indexes, inductive, Kernel, memory management, Memory management, OS kernels, Resource management, Runtime",
    "DOI": "10.1109/TR.2021.3074709",
    "page": "1-14",
    "page-first": "1",
    "note": "Conference Name: IEEE Transactions on Reliability",
    "_line": "FormalBib.bib:91"
  },
  "nipkow_functional_nodate": {
    "id": "nipkow_functional_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Nipkow",
        "given": "Tobias"
      },
      {
        "family": "Blanchette",
        "given": "Jasmin"
      },
      {
        "family": "Eberl",
        "given": "Manuel"
      },
      {
        "family": "Gómez-Londoño",
        "given": "Alejandro"
      },
      {
        "family": "Lammich",
        "given": "Peter"
      },
      {
        "family": "Sternagel",
        "given": "Christian"
      },
      {
        "family": "Wimmer",
        "given": "Simon"
      },
      {
        "family": "Zhan",
        "given": "Bohua"
      }
    ],
    "title": "Functional Algorithms, Veriﬁed!",
    "page": "276",
    "page-first": "276",
    "language": "en-US",
    "_line": "FormalBib.bib:105"
  },
  "mitsch_implicit_nodate": {
    "id": "mitsch_implicit_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Mitsch",
        "given": "Stefan"
      }
    ],
    "title": "Implicit and Explicit Proof Management in KeYmaera X",
    "page": "15",
    "page-first": "15",
    "language": "en-US",
    "_line": "FormalBib.bib:113"
  },
  "morshtein_verifying_nodate": {
    "id": "morshtein_verifying_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Morshtein",
        "given": "Shiri"
      },
      {
        "family": "Ettinger",
        "given": "Ran"
      }
    ],
    "title": "Verifying Time Complexity of Binary Search using Dafny",
    "abstract": "Formal software veriﬁcation techniques are widely used to specify and prove the functional correctness of programs. However, nonfunctional properties such as time complexity are usually carried out with pen and paper. Ineﬃcient code in terms of time complexity may cause massive performance problems in large-scale complex systems. We present a proof of concept for using the Dafny veriﬁcation tool to specify and verify the worst-case time complexity of binary search. This approach can also be used for academic purposes as a new way to teach algorithms and complexity.",
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "FormalBib.bib:121"
  },
  "rakotomalala_verifying_2021": {
    "id": "rakotomalala_verifying_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Rakotomalala",
        "given": "Lucien"
      },
      {
        "family": "Roux",
        "given": "Pierre"
      },
      {
        "family": "Boyer",
        "given": "Marc"
      }
    ],
    "title": "Verifying min-plus Computations with Coq (extended version with appendix)",
    "container-title": "13th NASA Formal Methods Symposium (NFM 2021)",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "26"
        ]
      ]
    },
    "abstract": "Network-calculus is a theory that bounds delays in embedded networks such as AFDX networks used in modern airplanes. Effective computations rely on operators from the min-plus algebra on real functions. Algorithms on specific subsets can be found in the literature. Such algorithms and related implementations are however complicated. Instead of redeveloping a provably correct implementation, we take an existing implementation as an oracle and propose a Coq based verifier.",
    "keywords": "Coq, functions on real numbers, min-plus computations, network-calculus",
    "URL": "https://hal.archives-ouvertes.fr/hal-03176024",
    "publisher-place": "virtual, United States",
    "_line": "FormalBib.bib:130"
  },
  "chen_initial_2020": {
    "id": "chen_initial_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Chen",
        "given": "Xiaohong"
      },
      {
        "family": "Lucanu",
        "given": "Dorel"
      },
      {
        "family": "Roşu",
        "given": "Grigore"
      }
    ],
    "title": "Initial Algebra Semantics in Matching Logic",
    "issued": {
      "date-parts": [
        [
          "2020",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "25"
        ]
      ]
    },
    "abstract": "Matching logic is a unifying foundational logic for defining formal programming language semantics, which adopts a minimalist design with few primitive constructs that are enough to express all properties within a variety of logical systems, including FOL, separation logic, (dependent) type systems, modal mu-logic, and more. In this paper, we consider initial algebra semantics and show how to capture it by matching logic specifications. Formally, given an algebraic specification E that defines a set of sorts (of data) and a set of operations whose behaviors are defined by a set of equational axioms, we define a corresponding matching logic specification, denoted INITIALALGEBRA(E), whose models are exactly the initial algebras of E. Thus, we reduce initial E-algebra semantics to the matching logic specifications INITIALALGEBRA(E), and reduce extrinsic initial E-algebra reasoning, which includes inductive reasoning, to generic, intrinsic matching logic reasoning.",
    "URL": "https://www.ideals.illinois.edu/handle/2142/107781",
    "note": "Accepted: 2020-07-19T19:02:15Z",
    "language": "en-US",
    "_line": "FormalBib.bib:143"
  },
  "chen_general_2020": {
    "id": "chen_general_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Chen",
        "given": "Xiaohong"
      },
      {
        "family": "Roşu",
        "given": "Grigore"
      }
    ],
    "title": "A general approach to define binders using matching logic",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2020",
          "8",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "25"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "We propose a novel definition of binders using matching logic, where the binding behavior of object-level binders is directly inherited from the built-in exists binder of matching logic. We show that the behavior of binders in various logical systems such as lambda-calculus, System F, pi-calculus, pure type systems, can be axiomatically defined in matching logic as notations and logical theories. We show the correctness of our definitions by proving conservative extension theorems, which state that a sequent/judgment is provable in the original system if and only if it is provable in matching logic, in the corresponding theory. Our matching logic definition of binders also yields models to all binders, which are deductively complete with respect to formal reasoning in the original systems. For lambda-calculus, we further show that the yielded models are representationally complete, a desired property that is not enjoyed by many existing lambda-calculus semantics. This work is part of a larger effort to develop a logical foundation for the programming language semantics framework K (http://kframework.org).",
    "URL": "https://dl.acm.org/doi/10.1145/3408970",
    "DOI": "10.1145/3408970",
    "page": "1-32",
    "page-first": "1",
    "volume": "4",
    "language": "en-US",
    "_line": "FormalBib.bib:155"
  },
  "rosu_matching_2017": {
    "id": "rosu_matching_2017",
    "type": "webpage",
    "author": [
      {
        "family": "Rosu",
        "given": "Grigore"
      }
    ],
    "title": "Matching logic · Formal Systems Laboratory",
    "issued": {
      "date-parts": [
        [
          "2017",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "24"
        ]
      ]
    },
    "abstract": "This paper presents matching logic , a first-order logic (FOL) variant for spec-\nifying and reasoning about structure by means of patterns and pattern matching. Its\nsentences, the patterns , are constructed using variables , symbols , connectives and quan-\ntifiers , but no difference is made between function and predicate symbols. In models, a\npattern evaluates into a power-set domain (the set of values that match it), in contrast to\nFOL where functions and predicates map into a regular domain. Matching logic uniformly\ngeneralizes several logical frameworks important for program analysis, such as: proposi-\ntional logic, algebraic specification, FOL with equality, modal logic, and separation logic.\nPatterns can specify separation requirements at any level in any program configuration,\nnot only in the heaps or stores, without any special logical constructs for that: the very\nnature of pattern matching is that if two structures are matched as part of a pattern, then\nthey can only be spatially separated. Like FOL, matching logic can also be translated\ninto pure predicate logic with equality, at the same time admitting its own sound and\ncomplete proof system. A practical aspect of matching logic is that FOL reasoning with\nequality remains sound, so off-the-shelf provers and SMT solvers can be used for matching\nlogic reasoning. Matching logic is particularly well-suited for reasoning about programs in\nprogramming languages that have an operational semantics, but it is not limited to this.",
    "URL": "https://fsl.cs.illinois.edu/publications/rosu-2017-lmcs.html",
    "_line": "FormalBib.bib:173"
  },
  "tusil_executable_2017": {
    "id": "tusil_executable_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Tušil",
        "given": "Jan"
      }
    ],
    "title": "An Executable Formal Semantics of C++",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "page": "87",
    "page-first": "87",
    "language": "en-US",
    "_line": "FormalBib.bib:199"
  },
  "thakur_posthat_2013": {
    "id": "thakur_posthat_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Thakur",
        "given": "A"
      },
      {
        "family": "Lal",
        "given": "A"
      },
      {
        "family": "Lim",
        "given": "J"
      },
      {
        "family": "Reps",
        "given": "T"
      }
    ],
    "title": "PostHat and All That: Automating Abstract Interpretation",
    "container-title": "Electronic Notes in Theoretical Computer Science",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "abstract": "Abstract interpretation provides an elegant formalism for performing program analysis. Unfortunately, designing and implementing a sound, precise, scalable, and extensible abstract interpreter is diﬃcult. In this paper, we describe an approach to creating correct-by-construction abstract interpreters that also attain the fundamental limits on precision that abstract-interpretation theory establishes. Our approach requires the analysis designer to implement only a small number of operations. In particular, we describe a systematic method for implementing an abstract interpreter that solves the following problem: Given program P , and an abstract domain A, ﬁnd the most-precise inductive A-invariant for P .",
    "page": "20",
    "page-first": "20",
    "language": "en-US",
    "_line": "FormalBib.bib:208"
  },
  "bugariu_identifying_2021": {
    "id": "bugariu_identifying_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bugariu",
        "given": "Alexandra"
      },
      {
        "family": "Ter-Gabrielyan",
        "given": "Arshavir"
      },
      {
        "family": "Müller",
        "given": "Peter"
      }
    ],
    "title": "Identifying Overly Restrictive Matching Patterns in SMT-based Program Verifiers",
    "container-title": "arXiv:2105.04385 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "19"
        ]
      ]
    },
    "abstract": "Universal quantifiers occur frequently in proof obligations produced by program verifiers, for instance, to axiomatize uninterpreted functions and to express properties of arrays. SMT-based verifiers typically reason about them via E-matching, an SMT algorithm that requires syntactic matching patterns to guide the quantifier instantiations. Devising good matching patterns is challenging. In particular, overly restrictive patterns may lead to spurious verification errors if the quantifiers needed for a proof are not instantiated; they may also conceal unsoundness caused by inconsistent axiomatizations. In this paper, we present the first technique that identifies and helps the users remedy the effects of overly restrictive matching patterns. We designed a novel algorithm to synthesize missing triggering terms required to complete a proof. Tool developers can use this information to refine their matching patterns and prevent similar verification errors, or to fix a detected unsoundness.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2105.04385",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.04385",
    "URL": "http://arxiv.org/abs/2105.04385",
    "_line": "FormalBib.bib:219"
  },
  "foster_formally_2021": {
    "id": "foster_formally_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Foster",
        "given": "Simon"
      },
      {
        "family": "Hur",
        "given": "Chung-Kil"
      },
      {
        "family": "Woodcock",
        "given": "Jim"
      }
    ],
    "title": "Formally Verified Simulations of State-Rich Processes using Interaction Trees in Isabelle/HOL",
    "container-title": "arXiv:2105.05133 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "19"
        ]
      ]
    },
    "abstract": "Simulation and formal verification are important complementary techniques necessary in high assurance model-based systems development. In order to support coherent results, it is necessary to provide unifying semantics and automation for both activities. In this paper we apply Interaction Trees in Isabelle/HOL to produce a verification and simulation framework for state-rich process languages. We develop the core theory and verification techniques for Interaction Trees, use them to give a semantics to the CSP and Circus languages, and formally link our new semantics with the failures-divergences semantic model. We also show how the Isabelle code generator can be used to generate verified executable simulations for reactive and concurrent programs.",
    "keywords": "Computer Science - Logic in Computer Science",
    "URLtext": "2105.05133",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.05133",
    "URL": "http://arxiv.org/abs/2105.05133",
    "_line": "FormalBib.bib:233"
  },
  "steinberg_computable_nodate": {
    "id": "steinberg_computable_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Steinberg",
        "given": "Florian"
      },
      {
        "family": "Théry",
        "given": "Laurent"
      },
      {
        "family": "Thies",
        "given": "Holger"
      }
    ],
    "title": "Computable analysis and notions of continuity in Coq",
    "abstract": "We give a number of formal proofs of theorems from computable analysis. Many of our results specify executable algorithms that work on inﬁnite inputs by means of operating on ﬁnite approximations. The proofs that these algorithms are correct in the sense of computable analysis are veriﬁed in the proof assistant Coq heavily relying on the Incone library for information theoretic continuity. This library is developed by one of the authors and the paper can be used as an introduction to it. Incone formulates the continuity-theoretic aspects of computable analysis. It is designed in such a way that it can be combined with Coq’s Type/Prop distinction to provide a general purpose interface for algorithmic reasoning on continuous structures and many of our results provide complete computational content.",
    "page": "43",
    "page-first": "43",
    "volume": "17",
    "language": "en-US",
    "_line": "FormalBib.bib:247"
  },
  "ferrando_toward_2021": {
    "id": "ferrando_toward_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Ferrando",
        "given": "Angelo"
      },
      {
        "family": "Dennis",
        "given": "Louise A."
      },
      {
        "family": "Cardoso",
        "given": "Rafael C."
      },
      {
        "family": "Fisher",
        "given": "Michael"
      },
      {
        "family": "Ancona",
        "given": "Davide"
      },
      {
        "family": "Mascardi",
        "given": "Viviana"
      }
    ],
    "title": "Toward a Holistic Approach to Verification and Validation of Autonomous Cognitive Systems",
    "container-title": "ACM Transactions on Software Engineering and Methodology",
    "container-title-short": "ACM Trans. Softw. Eng. Methodol.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "17"
        ]
      ]
    },
    "issn": "1049-331X",
    "abstract": "When applying formal verification to a system that interacts with the real world, we must use a model of the environment. This model represents an abstraction of the actual environment, so it is necessarily incomplete and hence presents an issue for system verification. If the actual environment matches the model, then the verification is correct; however, if the environment falls outside the abstraction captured by the model, then we cannot guarantee that the system is well behaved. A solution to this problem consists in exploiting the model of the environment used for statically verifying the system’s behaviour and, if the verification succeeds, using it also for validating the model against the real environment via runtime verification. The article discusses this approach and demonstrates its feasibility by presenting its implementation on top of a framework integrating the Agent Java PathFinder model checker. A high-level Domain Specific Language is used to model the environment in a user-friendly way; the latter is then compiled to trace expressions for both static formal verification and runtime verification. To evaluate our approach, we apply it to two different case studies: an autonomous cruise control system and a simulation of the Mars Curiosity rover.",
    "keywords": "autonomous systems, MCAPL, model checking, Runtime verification, trace expressions",
    "URL": "https://doi.org/10.1145/3447246",
    "DOI": "10.1145/3447246",
    "page": "43:1-43:43",
    "page-first": "43",
    "volume": "30",
    "issue": "4",
    "_line": "FormalBib.bib:257"
  },
  "kimura_decidability_2021": {
    "id": "kimura_decidability_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Kimura",
        "given": "Daisuke"
      },
      {
        "family": "Tatsuta",
        "given": "Makoto"
      }
    ],
    "title": "Decidability for Entailments of Symbolic Heaps with Arrays",
    "container-title": "Logical Methods in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "11"
        ]
      ]
    },
    "abstract": "This paper presents two decidability results on the validity checking problem for entailments of symbolic heaps in separation logic with Presburger arithmetic and arrays. The ﬁrst result is for a system with arrays and existential quantiﬁers. The correctness of the decision procedure is proved under the condition that sizes of arrays in the succedent are not existentially quantiﬁed. This condition is diﬀerent from that proposed by Brotherston et al. in 2017 and one of them does not imply the other. The main idea is a novel translation from an entailment of symbolic heaps into a formula in Presburger arithmetic. The second result is the decidability for a system with both arrays and lists. The key idea is to extend the unroll collapse technique proposed by Berdine et al. in 2005 to arrays and arithmetic as well as double-linked lists.",
    "DOI": "DOI:10.23638/LMCS-17(2:15)2021",
    "page": "33",
    "page-first": "33",
    "volume": "17",
    "issue": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:275"
  },
  "robles_methodology_nodate": {
    "id": "robles_methodology_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Robles",
        "given": "Virgile"
      },
      {
        "family": "Kosmatov",
        "given": "Nikolai"
      },
      {
        "family": "Prevosto",
        "given": "Virgile"
      },
      {
        "family": "Rilling",
        "given": "Louis"
      },
      {
        "family": "Paris-Saclay",
        "given": "Université"
      }
    ],
    "title": "Methodology for Speciﬁcation and Veriﬁcation of High-Level Requirements with MetAcsl",
    "abstract": "Speciﬁcation and formal veriﬁcation of high-level properties (such as security properties, like data integrity or conﬁdentiality) over a large software product remains an important challenge for the industrial practice. Recent work introduced METACSL, a plugin of the FRAMA-C veriﬁcation platform, that allows the user to specify high-level properties, called HIghLevel ACSL REquirements or HILARE, for C programs and transform them into assertions that can then be veriﬁed by classic deductive veriﬁcation. This paper presents a methodology of speciﬁcation and veriﬁcation of a wide range of high-level properties with METACSL and illustrates it on several examples. The goal is to provide veriﬁcation practitioners with detailed methodological guidelines for common patterns of properties in order to facilitate their everyday work and to avoid some frequent pitfalls. The illustrating examples are inspired by very usual kinds of properties and illustrated on two use cases. One of them—on the real-life code of the bootloader module of the secure storage device Wookey—was fully veriﬁed using the described approach, demonstrating its capacity to scale to real-life code. The other one—on a microkernel of an OS—was added to illustrate other common properties, where the description of the system was intentionally left very generic.",
    "page": "14",
    "page-first": "14",
    "language": "en-US",
    "_line": "FormalBib.bib:289"
  },
  "zhang_compositional_nodate": {
    "id": "zhang_compositional_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Zhang",
        "given": "Weixin"
      },
      {
        "family": "Sun",
        "given": "Yaozhu"
      }
    ],
    "title": "Compositional Programming",
    "abstract": "Modularity is a key concern in programming. However, programming languages remain limited in terms of modularity and extensibility. Small canonical problems, such as the Expression Problem (EP), illustrate some of the basic issues: the dilemma between choosing one kind of extensibility over another one in most programming languages. Other problems, such as how to express dependencies in a modular way, add up to the basic issues and remain a significant challenge. This paper presents a new statically typed modular programming style called Compositional Programming. In Compositional Programming, there is no EP: it is easy to get extensibility in multiple dimensions (i.e. it is easy to add new variants as well as new operations). Compositional Programming offers an alternative way to model data structures that differs from both algebraic datatypes in functional programming and conventional OOP class hierarchies. We introduce four key concepts for Compositional Programming: compositional interfaces, compositional traits, method patterns, and nested trait composition. Altogether these concepts allow us to naturally solve challenges such as the Expression Problem, model attribute-grammar-like programs, and generally deal with modular programs with complex dependencies. We present a language design, called CP, which is proved to be type-safe, together with several examples and three case studies. CCS Concepts: • Software and its engineering → Object oriented languages.",
    "page": "60",
    "page-first": "60",
    "volume": "1",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:298"
  },
  "monniaux_simple_2021": {
    "id": "monniaux_simple_2021",
    "type": "manuscript",
    "author": [
      {
        "family": "Monniaux",
        "given": "David"
      },
      {
        "family": "Six",
        "given": "Cyril"
      }
    ],
    "title": "Simple, Light, Yet Formally Verified, Global Common Subexpression Elimination and Loop-Invariant Code Motion",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "12"
        ]
      ]
    },
    "abstract": "We present an approach for implementing a formally certified loop-invariant code motion optimization by composing an unrolling pass and a formally certified yet efficient global subexpression elimination.\nThis approach is lightweight: each pass comes with a simple and independent proof of correctness.\nExperiments show the approach significantly narrows the performance gap between the CompCert certified compiler and state-of-the-art optimizing compilers.\nOur static analysis employs an efficient yet verified hashed set structure, resulting in fast compilation.",
    "keywords": "Coq, common subexpression elimination, invariants, optimization, verified compilers, verified hashed sets",
    "URL": "https://hal.archives-ouvertes.fr/hal-03212087",
    "_line": "FormalBib.bib:309"
  },
  "stanford_symbolic_2021": {
    "id": "stanford_symbolic_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Stanford",
        "given": "Caleb"
      },
      {
        "family": "Veanes",
        "given": "Margus"
      },
      {
        "family": "Bj",
        "given": "Nikolaj"
      }
    ],
    "title": "Symbolic Boolean Derivatives for Efficiently Solving Extended Regular Expression Constraints",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "The manipulation of raw string data is ubiquitous in securitycritical software, and verification of such software relies on efficiently solving string and regular expression constraints via SMT. However, the typical case of Boolean combinations of regular expression constraints exposes blowup in existing techniques. To address solvability of such constraints, we propose a new theory of derivatives of symbolic extended regular expressions (extended meaning that complement and intersection are incorporated), and show how to apply this theory to obtain more efficient decision procedures. Our implementation of these ideas, built on top of Z3, matches or outperforms state-of-the-art solvers on standard and handwritten benchmarks, showing particular benefits on examples with Boolean combinations.",
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "FormalBib.bib:323"
  },
  "sanan_csim2_2021": {
    "id": "sanan_csim2_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Sanan",
        "given": "David"
      },
      {
        "family": "Zhao",
        "given": "Yongwang"
      },
      {
        "family": "Lin",
        "given": "Shang-Wei"
      },
      {
        "family": "Yang",
        "given": "Liu"
      }
    ],
    "title": "CSim&caret;2: Compositional Top-down Verification of Concurrent Systems using Rely-Guarantee",
    "container-title": "ACM Transactions on Programming Languages and Systems",
    "container-title-short": "**CSim**\\/$^{\\textrm{\\textit{2}}}$",
    "title-short": "**CSim**\\/$^{\\textrm{\\textit{2}}}$",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "issn": "0164-0925",
    "abstract": "To make feasible and scalable the verification of large and complex concurrent systems, it is necessary the use of compositional techniques even at the highest abstraction layers. When focusing on the lowest software abstraction layers, such as the implementation or the machine code, the high level of detail of those layers makes the direct verification of properties very difficult and expensive. It is therefore essential to use techniques allowing to simplify the verification on these layers. One technique to tackle this challenge is top-down verification where by means of simulation properties verified on top layers (representing abstract specifications of a system) are propagated down to the lowest layers (that are an implementation of the top layers). There is no need to say that simulation of concurrent systems implies a greater level of complexity, and having compositional techniques to check simulation between layers is also desirable when seeking for both feasibility and scalability of the refinement verification. In this article, we present CSim2 a (compositional) rely-guarantee-based framework for the top-down verification of complex concurrent systems in the Isabelle/HOL theorem prover. CSim2 uses CSimpl, a language with a high degree of expressiveness designed for the specification of concurrent programs. Thanks to its expressibility, CSimpl is able to model many of the features found in real world programming languages like exceptions, assertions, and procedures. CSim2 provides a framework for the verification of rely-guarantee properties to compositionally reason on CSimpl specifications. Focusing on top-down verification, CSim2 provides a simulation-based framework for the preservation of CSimpl rely-guarantee properties from specifications to implementations. By using the simulation framework, properties proven on the top layers (abstract specifications) are compositionally propagated down to the lowest layers (source or machine code) in each concurrent component of the system. Finally, we show the usability of CSim2 by running a case study over two CSimpl specifications of an Arinc-653 communication service. In this case study, we prove a complex property on a specification, and we use CSim2 to preserve the property on lower abstraction layers.",
    "keywords": "compositional verification, concurrency verification, isabelle/HOL, operating systems verification, Rely-guarantee, simulation and refinement",
    "URL": "https://doi.org/10.1145/3436808",
    "DOI": "10.1145/3436808",
    "page": "2:1-2:46",
    "page-first": "2",
    "volume": "43",
    "issue": "1",
    "_line": "FormalBib.bib:333"
  },
  "khayam_jskel_2021": {
    "id": "khayam_jskel_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Khayam",
        "given": "Adam"
      },
      {
        "family": "Noizet",
        "given": "Louis"
      },
      {
        "family": "Schmitt",
        "given": "Alan"
      }
    ],
    "title": "JSkel: Towards a Formalization of JavaScript’s Semantics",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "We present JSkel, a formalization of the semantics of JavaScript in Skel, the concrete language used to write skeletal semantics. We describe the improvements to Skel we designed and implemented to signiﬁcantly simplify the formalization. We show the formalization is both close to the speciﬁcation and executable.",
    "page": "22",
    "page-first": "22",
    "language": "en-US",
    "_line": "FormalBib.bib:352"
  },
  "wing_specifiers_1990": {
    "id": "wing_specifiers_1990",
    "type": "article-journal",
    "author": [
      {
        "family": "Wing",
        "given": "Jeannette M."
      }
    ],
    "title": "A Specifier's Introduction to Formal Methods",
    "container-title": "IEEE Computer",
    "issued": {
      "date-parts": [
        [
          "1990"
        ]
      ]
    },
    "_line": "FormalBib.bib:362"
  },
  "guttag_larch_1993": {
    "id": "guttag_larch_1993",
    "type": "book",
    "author": [
      {
        "family": "Guttag",
        "given": "John V."
      },
      {
        "family": "Horning",
        "given": "James J."
      },
      {
        "family": "Garland",
        "given": "S. J."
      },
      {
        "family": "Jones",
        "given": "K. D."
      },
      {
        "family": "Modet",
        "given": "A."
      },
      {
        "family": "Wing",
        "given": "J. M."
      }
    ],
    "title": "Larch: Languages and Tools for Formal Specification",
    "container-title-short": "Larch",
    "title-short": "Larch",
    "issued": {
      "date-parts": [
        [
          "1993"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "3"
        ]
      ]
    },
    "publisher": "Springer New York",
    "isbn": "978-1-4612-7636-4 978-1-4612-2704-5",
    "URL": "http://link.springer.com/10.1007/978-1-4612-2704-5",
    "DOI": "10.1007/978-1-4612-2704-5",
    "publisher-place": "New York, NY",
    "language": "en-US",
    "_line": "FormalBib.bib:370"
  },
  "zaliva_helix_2018": {
    "id": "zaliva_helix_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Zaliva",
        "given": "Vadim"
      },
      {
        "family": "Franchetti",
        "given": "Franz"
      }
    ],
    "title": "HELIX: a case study of a formal verification of high performance program generation",
    "container-title": "Proceedings of the 7th ACM SIGPLAN International Workshop on Functional High-Performance Computing",
    "container-title-short": "HELIX",
    "collection-title": "FHPC 2018",
    "title-short": "HELIX",
    "issued": {
      "date-parts": [
        [
          "2018",
          "9",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-5813-2",
    "abstract": "In this paper, we present HELIX, a formally verified operator language and rewriting engine for generation of high-performance implementation for a variety of linear algebra algorithms. Based on the existing SPIRAL system, HELIX adds the rigor of formal verification of its correctness using Coq proof assistant. It formally defines two domain-specific languages: HCOL, which represents a computation data flow and Σ-HCOL, which extends HCOL with iterative computations. A framework for automatically proving semantic preservation of expression rewriting for both languages is presented. The structural properties of the dataflow graph which allow efficient compilation are formalized, and a monadic approach to tracking them and to reasoning about structural correctness of Σ-HCOL expressions is presented.",
    "keywords": "formal verification, Coq, operator language, rule rewriting",
    "URL": "https://doi.org/10.1145/3264738.3264739",
    "DOI": "10.1145/3264738.3264739",
    "publisher-place": "New York, NY, USA",
    "page": "1-9",
    "page-first": "1",
    "_line": "FormalBib.bib:385"
  },
  "zaliva_helix_2021": {
    "id": "zaliva_helix_2021",
    "type": "thesis",
    "genre": "thesis",
    "author": [
      {
        "family": "Zaliva",
        "given": "Vadim"
      }
    ],
    "title": "HELIX: From Math to Verified Code",
    "container-title-short": "HELIX",
    "title-short": "HELIX",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "25"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "Carnegie Mellon University",
    "abstract": "This thesis presents HELIX, a code generation and formal verification system with a focus on the intersection of high-performance and high-assurance numerical computing.This allowed us to build a system that could be ?ne-tuned to generate efficient code for a broad set of computer architectures while providing formal guarantees ofsuch generated code's correctness. The method we used for high-performance code synthesis is the algebraictransformation of vector and matrix computations into a dataow optimized for parallel or vectorized processing on target hardware. The abstraction used to formalize and verify this technique is an operator language used with semantics-preserving term-rewriting. We use sparse vector abstraction to represent partial computations, enabling us to use algebraic reasoning to prove parallel decomposition properties. HELIX provides a formal verification foundation for rewriting-based algebraic code synthesis optimizations, driven by an external oracle. Presently HELIX uses SPIRAL as an oracle deriving the rule application order. The SPIRAL system was developed over the years and successfully applied to generate code for various numeric algorithms. Building on its sound algebraic foundation, we generalize and extend it in the direction of non-linear operators, towards a new theory of partial computations, applying formal language theory and formal verification techniques.HELIX is developed and proven in Coq proof assistant and demonstrated on a real-life example of verified high-performance code generation of the dynamic window safety monitor for a cyber-physical robot system.",
    "URL": "/articles/thesis/HELIX_From_Math_to_Verified_Code/13636808/1",
    "DOI": "10.1184/R1/13636808.v1",
    "language": "en-US",
    "_line": "FormalBib.bib:403"
  },
  "noauthor_carnegie_nodate": {
    "id": "noauthor_carnegie_nodate",
    "type": "webpage",
    "title": "Carnegie Mellon University research repository - Browse",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://kilthub.cmu.edu/etd",
    "_line": "FormalBib.bib:418"
  },
  "bao_haccle_2020": {
    "id": "bao_haccle_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Bao",
        "given": "Yuyan"
      },
      {
        "family": "Sundararajah",
        "given": "Kirshanthan"
      },
      {
        "family": "Malik",
        "given": "Raghav"
      },
      {
        "family": "Ye",
        "given": "Qianchuan"
      },
      {
        "family": "Wagner",
        "given": "Christopher"
      },
      {
        "family": "Wang",
        "given": "Fei"
      },
      {
        "family": "Ameri",
        "given": "Mohammad Hassan"
      },
      {
        "family": "Lu",
        "given": "Donghang"
      },
      {
        "family": "Seto",
        "given": "Alexander"
      },
      {
        "family": "Delaware",
        "given": "Benjamin"
      },
      {
        "family": "Samanta",
        "given": "Roopsha"
      },
      {
        "family": "Kate",
        "given": "Aniket"
      },
      {
        "family": "Garman",
        "given": "Christina"
      },
      {
        "family": "Blocki",
        "given": "Jeremiah"
      },
      {
        "family": "Letourneau",
        "given": "Pierre-David"
      },
      {
        "family": "Meister",
        "given": "Benoit"
      },
      {
        "family": "Springer",
        "given": "Jonathan"
      },
      {
        "family": "Rompf",
        "given": "Tiark"
      },
      {
        "family": "Kulkarni",
        "given": "Milind"
      }
    ],
    "title": "HACCLE: An Ecosystem for Building Secure Multi-Party Computations",
    "container-title": "arXiv:2009.01489 \\[cs\\]",
    "container-title-short": "HACCLE",
    "title-short": "HACCLE",
    "issued": {
      "date-parts": [
        [
          "2020",
          "9",
          "3"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Cryptographic techniques have the potential to enable distrusting parties to collaborate in fundamentally new ways, but their practical implementation poses numerous challenges. An important class of such cryptographic techniques is known as secure multi-party computation (MPC). In an effort to provide an ecosystem for building secure MPC applications using higher degrees of automation, we present the HACCLE (High Assurance Compositional Cryptography: Languages and Environments) toolchain. The HACCLE toolchain contains an embedded domain-specific language (Harpoon) for software developers without cryptographic expertise to write MPC-based programs. Harpoon programs are compiled into acyclic circuits represented in HACCLE's Intermediate Representation (HIR) that serves as an abstraction for implementing a computation using different cryptographic protocols such as secret sharing, homomorphic encryption, or garbled circuits. Implementations of different cryptographic protocols serve as different backends of our toolchain. The extensible design of HIR allows cryptographic experts to plug in new primitives and protocols to realize computations.We have implemented HACCLE, and used it to program interesting algorithms and applications (e.g., secure auction, matrix-vector multiplication, and merge sort). We show that the performance is improved by using our optimization strategies and heuristics.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2009.01489",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2009.01489",
    "URL": "http://arxiv.org/abs/2009.01489",
    "_line": "FormalBib.bib:425"
  },
  "dickerson_rhle_2020": {
    "id": "dickerson_rhle_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Dickerson",
        "given": "Robert"
      },
      {
        "family": "Ye",
        "given": "Qianchuan"
      },
      {
        "family": "Delaware",
        "given": "Benjamin"
      }
    ],
    "title": "RHLE: Modular Deductive Verification of Relational &dollar;&bslash;forall&bslash;exists&dollar; Properties",
    "container-title": "arXiv:2002.02904 \\[cs\\]",
    "container-title-short": "RHLE",
    "title-short": "RHLE",
    "issued": {
      "date-parts": [
        [
          "2020",
          "11",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Relational program logics are used to prove that a desired relationship holds between the execution of multiple programs. Existing relational program logics have focused on verifying that all runs of a collection of programs do not fall outside a desired set of behaviors. Several important relational properties, including refinement and noninterference, do not fit into this category, as they require the existence of specific desirable executions. This paper presents RHLE, a logic for verifying a class of relational properties which we term &dollar;&bslash;forall&bslash;exists&dollar; properties. &dollar;&bslash;forall&bslash;exists&dollar; properties assert that for all executions of a collection of programs, there exist executions of another set of programs exhibiting some intended behavior. Importantly, RHLE can reason modularly about programs which make library calls, ensuring that &dollar;&bslash;forall&bslash;exists&dollar; properties are preserved when the programs are linked with any valid implementation of the library. To achieve this, we develop a novel form of function specification that requires the existence of certain behaviors in valid implementations. We have built a tool based on RHLE which we use to verify a diverse set of relational properties drawn from the literature, including refinement and generalized noninterference.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science, D.2.4, F.3.1",
    "URLtext": "2002.02904",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2002.02904",
    "URL": "http://arxiv.org/abs/2002.02904",
    "_line": "FormalBib.bib:440"
  },
  "sokar_self-attention_2021": {
    "id": "sokar_self-attention_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Sokar",
        "given": "Ghada"
      },
      {
        "family": "Mocanu",
        "given": "Decebal Constantin"
      },
      {
        "family": "Pechenizkiy",
        "given": "Mykola"
      }
    ],
    "title": "Self-Attention Meta-Learner for Continual Learning",
    "container-title": "arXiv:2101.12136 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Continual learning aims to provide intelligent agents capable of learning multiple tasks sequentially with neural networks. One of its main challenging, catastrophic forgetting, is caused by the neural networks non-optimal ability to learn in non-stationary distributions. In most settings of the current approaches, the agent starts from randomly initialized parameters and is optimized to master the current task regardless of the usefulness of the learned representation for future tasks. Moreover, each of the future tasks uses all the previously learned knowledge although parts of this knowledge might not be helpful for its learning. These cause interference among tasks, especially when the data of previous tasks is not accessible. In this paper, we propose a new method, named Self-Attention Meta-Learner (SAM), which learns a prior knowledge for continual learning that permits learning a sequence of tasks, while avoiding catastrophic forgetting. SAM incorporates an attention mechanism that learns to select the particular relevant representation for each future task. Each task builds a specific representation branch on top of the selected knowledge, avoiding the interference between tasks. We evaluate the proposed method on the Split CIFAR-10/100 and Split MNIST benchmarks in the task agnostic inference. We empirically show that we can achieve a better performance than several state-of-the-art methods for continual learning by building on the top of selected representation learned by SAM. We also show the role of the meta-attention mechanism in boosting informative features corresponding to the input data and identifying the correct target in the task agnostic inference. Finally, we demonstrate that popular existing continual learning methods gain a performance boost when they adopt SAM as a starting point.",
    "keywords": "Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition",
    "URLtext": "2101.12136",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2101.12136",
    "URL": "http://arxiv.org/abs/2101.12136",
    "_line": "FormalBib.bib:455"
  },
  "hofmann_type-based_2021": {
    "id": "hofmann_type-based_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Hofmann",
        "given": "Martin"
      },
      {
        "family": "Leutgeb",
        "given": "Lorenz"
      },
      {
        "family": "Moser",
        "given": "Georg"
      },
      {
        "family": "Obwaller",
        "given": "David"
      },
      {
        "family": "Zuleger",
        "given": "Florian"
      }
    ],
    "title": "Type-Based Analysis of Logarithmic Amortised Complexity",
    "container-title": "arXiv:2101.12029 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "We introduce a novel amortised resource analysis couched in a type-and-effect system. Our analysis is formulated in terms of the physicist's method of amortised analysis, and is potential-based. The type system makes use of logarithmic potential functions and is the first such system to exhibit \\*logarithmic amortised complexity\\*. With our approach we target the automated analysis of self-adjusting data structures, like splay trees, which so far have only manually been analysed in the literature. In particular, we have implemented a semi-automated prototype, which successfully analyses the zig-zig case of \\*splaying\\*, once the type annotations are fixed.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science, F.3.2",
    "URLtext": "2101.12029",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2101.12029",
    "URL": "http://arxiv.org/abs/2101.12029",
    "_line": "FormalBib.bib:469"
  },
  "xu_-ide_2021": {
    "id": "xu_-ide_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Xu",
        "given": "Frank F."
      },
      {
        "family": "Vasilescu",
        "given": "Bogdan"
      },
      {
        "family": "Neubig",
        "given": "Graham"
      }
    ],
    "title": "In-IDE Code Generation from Natural Language: Promise and Challenges",
    "container-title": "arXiv:2101.11149 \\[cs\\]",
    "container-title-short": "In-IDE Code Generation from Natural Language",
    "title-short": "In-IDE Code Generation from Natural Language",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning concept into code, especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from natural language queries, but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer-written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. We perform the first comprehensive investigation of the promise and challenges of using such technology inside the IDE, asking \"at the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?\" We first develop a plugin for the IDE that implements a hybrid of code generation and code retrieval functionality, and orchestrate virtual environments to enable collection of many user events. We ask developers with various backgrounds to complete 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Analysis identifies several pain points that could improve the effectiveness of future machine learning based code generation/retrieval developer assistants, and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies and development of better models.",
    "keywords": "Computer Science - Software Engineering",
    "URLtext": "2101.11149",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2101.11149",
    "URL": "http://arxiv.org/abs/2101.11149",
    "_line": "FormalBib.bib:483"
  },
  "malecha_towards_2020": {
    "id": "malecha_towards_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Malecha",
        "given": "Gregory"
      },
      {
        "family": "Anand",
        "given": "Abhishek"
      },
      {
        "family": "Stewart",
        "given": "Gordon"
      }
    ],
    "title": "Towards an Axiomatic Basis for C++",
    "container-title": "The Coq Workshop 2020",
    "issued": {
      "date-parts": [
        [
          "2020",
          "7",
          "6"
        ]
      ]
    },
    "page": "3",
    "page-first": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:498"
  },
  "mitsch_modelplex_2016": {
    "id": "mitsch_modelplex_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Mitsch",
        "given": "Stefan"
      },
      {
        "family": "Platzer",
        "given": "André"
      }
    ],
    "title": "ModelPlex: verified runtime validation of verified cyber-physical system models",
    "container-title": "Formal Methods in System Design",
    "container-title-short": "ModelPlex",
    "title-short": "ModelPlex",
    "issued": {
      "date-parts": [
        [
          "2016",
          "10",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "27"
        ]
      ]
    },
    "issn": "1572-8102",
    "abstract": "Formal verification and validation play a crucial role in making cyber-physical systems (CPS) safe. Formal methods make strong guarantees about the system behavior if accurate models of the system can be obtained, including models of the controller and of the physical dynamics. In CPS, models are essential; but any model we could possibly build necessarily deviates from the real world. If the real system fits to the model, its behavior is guaranteed to satisfy the correctness properties verified with respect to the model. Otherwise, all bets are off. This article introduces ModelPlex, a method ensuring that verification results about models apply to CPS implementations. ModelPlex provides correctness guarantees for CPS executions at runtime: it combines offline verification of CPS models with runtime validation of system executions for compliance with the model. ModelPlex ensures in a provably correct way that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model. If, at some point, the observed behavior no longer complies with the model so that offline verification results no longer apply, ModelPlex initiates provably safe fallback actions, assuming the system dynamics deviation is bounded. This article, furthermore, develops a systematic technique to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic by a correct-by-construction approach, leading to verifiably correct runtime model validation. Overall, ModelPlex generates provably correct monitor conditions that, if checked to hold at runtime, are provably guaranteed to imply that the offline safety verification results about the CPS model apply to the present run of the actual CPS implementation.",
    "URL": "https://doi.org/10.1007/s10703-016-0241-z",
    "DOI": "10.1007/s10703-016-0241-z",
    "page": "33-74",
    "page-first": "33",
    "volume": "49",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:508"
  },
  "beg_working_2020": {
    "id": "beg_working_2020",
    "type": "book",
    "author": [
      {
        "family": "Beg",
        "given": "Arshad"
      },
      {
        "family": "Butterfield",
        "given": "Andrew"
      }
    ],
    "title": "Working Document for state of the art - formality meets autonomy/robotics",
    "issued": {
      "date-parts": [
        [
          "2020",
          "8",
          "1"
        ]
      ]
    },
    "abstract": "This document summarises the state-of-the-art of our ultimate goal of formal verification of the ROS based robotic systems.",
    "DOI": "10.13140/RG.2.2.30437.22249",
    "_line": "FormalBib.bib:527"
  },
  "chong_code-level_2021": {
    "id": "chong_code-level_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Chong",
        "given": "Nathan"
      },
      {
        "family": "Cook",
        "given": "Byron"
      },
      {
        "family": "Eidelman",
        "given": "Jonathan"
      },
      {
        "family": "Kallas",
        "given": "Konstantinos"
      },
      {
        "family": "Khazem",
        "given": "Kareem"
      },
      {
        "family": "Monteiro",
        "given": "Felipe R."
      },
      {
        "family": "Schwartz‐Narbonne",
        "given": "Daniel"
      },
      {
        "family": "Tasiran",
        "given": "Serdar"
      },
      {
        "family": "Tautschnig",
        "given": "Michael"
      },
      {
        "family": "Tuttle",
        "given": "Mark R."
      }
    ],
    "title": "Code-level model checking in the software development workflow at Amazon Web Services",
    "container-title": "Software: Practice and Experience",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "26"
        ]
      ]
    },
    "issn": "1097-024X",
    "abstract": "This article describes a style of applying symbolic model checking developed over the course of four years at Amazon Web Services (AWS). Lessons learned are drawn from proving properties of numerous C-based systems, for example, custom hypervisors, encryption code, boot loaders, and an IoT operating system. Using our methodology, we find that we can prove the correctness of industrial low-level C-based systems with reasonable effort and predictability. Furthermore, AWS developers are increasingly writing their own formal specifications. As part of this effort, we have developed a CI system that allows integration of the proofs into standard development workflows and extended the proof tools to provide better feedback to users. All proofs discussed in this article are publicly available on GitHub.",
    "keywords": "model checking, continuous integration, memory safety",
    "URL": "https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2949",
    "DOI": "10.1002/spe.2949",
    "volume": "n/a",
    "note": "&underscore;eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2949",
    "language": "en-US",
    "_line": "FormalBib.bib:535"
  },
  "vu_secure_2021": {
    "id": "vu_secure_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Vu",
        "given": "Son Tuan"
      },
      {
        "family": "Cohen",
        "given": "Albert"
      },
      {
        "family": "Heydemann",
        "given": "Karine"
      }
    ],
    "title": "Secure Optimization Through Opaque Observations",
    "container-title": "Preprint PriSC workshop (with POPL 2021)",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "17"
        ]
      ]
    },
    "page": "43",
    "page-first": "43",
    "language": "en-US",
    "_line": "FormalBib.bib:553"
  },
  "hu_formalizing_2021": {
    "id": "hu_formalizing_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hu",
        "given": "Jason Z. S."
      },
      {
        "family": "Carette",
        "given": "Jacques"
      }
    ],
    "title": "Formalizing category theory in Agda",
    "container-title": "Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "collection-title": "CPP 2021",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "26"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8299-1",
    "abstract": "The generality and pervasiveness of category theory in modern mathematics makes it a frequent and useful target of formalization. It is however quite challenging to formalize, for a variety of reasons. Agda currently (i.e. in 2020) does not have a standard, working formalization of category theory. We document our work on solving this dilemma. The formalization revealed a number of potential design choices, and we present, motivate and explain the ones we picked. In particular, we find that alternative definitions or alternative proofs from those found in standard textbooks can be advantageous, as well as \"fit\" Agda's type theory more smoothly. Some definitions regarded as equivalent in standard textbooks turn out to make different \"universe level\" assumptions, with some being more polymorphic than others. We also pay close attention to engineering issues so that the library integrates well with Agda's own standard library, as well as being compatible with as many of supported type theories in Agda as possible.",
    "keywords": "Agda, category theory, formal mathematics",
    "URL": "https://doi.org/10.1145/3437992.3439922",
    "DOI": "10.1145/3437992.3439922",
    "publisher-place": "New York, NY, USA",
    "page": "327-342",
    "page-first": "327",
    "_line": "FormalBib.bib:563"
  },
  "artho_decision_2016": {
    "id": "artho_decision_2016",
    "type": "chapter",
    "author": [
      {
        "family": "Reynolds",
        "given": "Andrew"
      },
      {
        "family": "Iosif",
        "given": "Radu"
      },
      {
        "family": "Serban",
        "given": "Cristina"
      },
      {
        "family": "King",
        "given": "Tim"
      }
    ],
    "editor": [
      {
        "family": "Artho",
        "given": "Cyrille"
      },
      {
        "family": "Legay",
        "given": "Axel"
      },
      {
        "family": "Peled",
        "given": "Doron"
      }
    ],
    "title": "A Decision Procedure for Separation Logic in SMT",
    "container-title": "Automated Technology for Verification and Analysis",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "25"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-46519-7 978-3-319-46520-3",
    "abstract": "This paper presents a complete decision procedure for the entire quantiﬁerfree fragment of Separation Logic (SL) interpreted over heaplets with data elements ranging over a parametric multi-sorted (possibly inﬁnite) domain. The algorithm uses a combination of theories and is used as a specialized solver inside a DPLL(T ) architecture. A prototype was implemented within the CVC4 SMT solver. Preliminary evaluation suggests the possibility of using this procedure as a building block of a more elaborate theorem prover for SL with inductive predicates, or as back-end of a bounded model checker for programs with low-level pointer and data manipulations.",
    "URL": "http://link.springer.com/10.1007/978-3-319-46520-3_16",
    "DOI": "10.1007/978-3-319-46520-3_16",
    "publisher-place": "Cham",
    "page": "244-261",
    "page-first": "244",
    "volume": "9938",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "FormalBib.bib:581"
  },
  "xie_effect_2020": {
    "id": "xie_effect_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Xie",
        "given": "Ningning"
      },
      {
        "family": "Brachthäuser",
        "given": "Jonathan Immanuel"
      },
      {
        "family": "Hillerström",
        "given": "Daniel"
      },
      {
        "family": "Schuster",
        "given": "Philipp"
      },
      {
        "family": "Leijen",
        "given": "Daan"
      }
    ],
    "title": "Effect handlers, evidently",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2020",
          "8",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "25"
        ]
      ]
    },
    "abstract": "Algebraic effect handlers are a powerful way to incorporate effects in a programming language. Sometimes perhaps even &underscore;too&underscore; powerful. In this article we define a restriction of general effect handlers with &underscore;scoped resumptions&underscore;. We argue one can still express all important effects, while improving reasoning about effect handlers. Using the newly gained guarantees, we define a sound and coherent evidence translation for effect handlers, which directly passes the handlers as evidence to each operation. We prove full soundness and coherence of the translation into plain lambda calculus. The evidence in turn enables efficient implementations of effect operations; in particular, we show we can execute tail-resumptive operations &underscore;in place&underscore; (without needing to capture the evaluation context), and how we can replace the runtime search for a handler by indexing with a constant offset.",
    "keywords": "Algebraic Effects, Evidence Passing Translation, Handlers",
    "URL": "https://doi.org/10.1145/3408981",
    "DOI": "10.1145/3408981",
    "page": "99:1-99:29",
    "page-first": "99",
    "volume": "4",
    "_line": "FormalBib.bib:601"
  },
  "xie_effect_2020-1": {
    "id": "xie_effect_2020-1",
    "type": "paper-conference",
    "author": [
      {
        "family": "Xie",
        "given": "Ningning"
      },
      {
        "family": "Leijen",
        "given": "Daan"
      }
    ],
    "title": "Effect handlers in Haskell, evidently",
    "container-title": "Proceedings of the 13th ACM SIGPLAN International Symposium on Haskell",
    "collection-title": "Haskell 2020",
    "issued": {
      "date-parts": [
        [
          "2020",
          "8",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "25"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8050-8",
    "abstract": "Algebraic effect handlers offer an alternative to monads to incorporate effects in Haskell. In recent work Xie &underscore;et al.&underscore; show how to give semantics to effect handlers in terms of plain polymorphic lambda calculus through &underscore;evidence translation&underscore;. Besides giving precise semantics, this translation also allows for potentially more efficient implementations. Here we present the first implementation of this technique as a library for effect handlers in Haskell. We show how the design naturally leads to a concise effect interface and how evidence translation enables evaluating &underscore;tail resumptive&underscore; operations &underscore;in-place&underscore;. We give detailed benchmark results where our library performs well with respect to other approaches.",
    "keywords": "Algebraic Effects, Evidence Passing Translation, Handlers",
    "URL": "https://doi.org/10.1145/3406088.3409022",
    "DOI": "10.1145/3406088.3409022",
    "publisher-place": "New York, NY, USA",
    "page": "95-108",
    "page-first": "95",
    "_line": "FormalBib.bib:618"
  },
  "mit_programming_languages_and_verification_group_fiat-crypto_2021": {
    "id": "mit_programming_languages_and_verification_group_fiat-crypto_2021",
    "type": "book",
    "author": [
      {
        "family": "\\",
        "given": "MIT Programming Languages"
      },
      {
        "family": "Group",
        "given": "&rcurly; Verification"
      }
    ],
    "title": "Fiat-Crypto",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "25"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "25"
        ]
      ]
    },
    "publisher": "Programming Languages and Verification Group at MIT CSAIL",
    "abstract": "Cryptographic Primitive Code Generation by Fiat.",
    "URL": "https://github.com/mit-plv/fiat-crypto",
    "note": "original-date: 2015-09-10T20:29:16Z",
    "_line": "FormalBib.bib:635"
  },
  "tabareau_marriage_2021": {
    "id": "tabareau_marriage_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Tabareau",
        "given": "Nicolas"
      },
      {
        "family": "Tanter",
        "given": "Éric"
      },
      {
        "family": "Sozeau",
        "given": "Matthieu"
      }
    ],
    "title": "The Marriage of Univalence and Parametricity",
    "container-title": "Journal of the ACM",
    "container-title-short": "J. ACM",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "25"
        ]
      ]
    },
    "issn": "0004-5411",
    "abstract": "Reasoning modulo equivalences is natural for everyone, including mathematicians. Unfortunately, in proof assistants based on type theory, which are frequently used to mechanize mathematical results and carry out program verification efforts, equality is appallingly syntactic, and as a result, exploiting equivalences is cumbersome at best. Parametricity and univalence are two major concepts that have been explored in the literature to transport programs and proofs across type equivalences, but they fall short of achieving seamless, automatic transport. This work first clarifies the limitations of these two concepts when considered in isolation and then devises a fruitful marriage between both. The resulting concept, called univalent parametricity, is an extension of parametricity strengthened with univalence that fully realizes programming and proving modulo equivalences. Our approach handles both type and term dependency, as well as type-level computation. In addition to the theory of univalent parametricity, we present a lightweight framework implemented in the Coq proof assistant that allows the user to transparently transfer definitions and theorems for a type to an equivalent one, as if they were equal. For instance, this makes it possible to conveniently switch between an easy-to-reason-about representation and a computationally efficient representation as soon as they are proven equivalent. The combination of parametricity and univalence supports transport à la carte: basic univalent transport, which stems from a type equivalence, can be complemented with additional proofs of equivalences between functions over these types, in order to be able to transport more programs and proofs, as well as to yield more efficient terms. We illustrate the use of univalent parametricity on several examples, including a recent integration of native integers in Coq. This work paves the way to easier-to-use proof assistants by supporting seamless programming and proving modulo equivalences.",
    "keywords": "Coq, parametricity, proof assistants, Type equivalence, univalence",
    "URL": "https://doi.org/10.1145/3429979",
    "DOI": "10.1145/3429979",
    "page": "5:1-5:44",
    "page-first": "5",
    "volume": "68",
    "issue": "1",
    "_line": "FormalBib.bib:647"
  },
  "merigoux_steel_2020": {
    "id": "merigoux_steel_2020",
    "type": "webpage",
    "author": [
      {
        "family": "Merigoux",
        "given": "Denis"
      },
      {
        "family": "Fromherz",
        "given": "Aymeric"
      }
    ],
    "title": "Steel: scaling up memory reasoning for F\\* (ADSL 2020) - POPL 2020",
    "container-title-short": "Steel",
    "title-short": "Steel",
    "issued": {
      "date-parts": [
        [
          "2020",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "22"
        ]
      ]
    },
    "abstract": "We present Steel, a semi-automated separation logic framework for the F\\* proof assistant. Steel is built as a set of verified libraries on top of an existing C-like subset language and memory model, Low\\*. It relies on the careful mixing of separation logic terms describing the heap together with functional specifications to offer an improved proof development experience compared to current Low\\*, already used for large-scale projects going over several thousands lines of code. While manipulation of separation logic terms is ensured by F\\* tactics, the rest of the functional verification conditions is still discharged to the Z3 theorem prover. Bundling fractional permissions and a basic fork-join concurrency model, Steel is designed to automate by tactics the most common patterns of separation logic proofs such as the application of the frame rule in a populated context. While lacking proper evaluation and currently suffering from F\\* tactics performance issue, we hope to demonstrate its usefulness in the coming year.",
    "URL": "https://popl20.sigplan.org/details/adsl-2020-papers/8/Steel-scaling-up-memory-reasoning-for-F-",
    "_line": "FormalBib.bib:683"
  },
  "zuleger_strong-separation_nodate": {
    "id": "zuleger_strong-separation_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Zuleger",
        "given": "Florian"
      },
      {
        "family": "Katelaan",
        "given": "Jens"
      }
    ],
    "title": "Strong-Separation Logic (ADSL 2020) - POPL 2020",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "22"
        ]
      ]
    },
    "abstract": "Most automated verifiers for separation logic target the symbolic-heap fragment, disallowing both the magic-wand operator and the application of classical Boolean operators to spatial formulas. This is not surprising, as support for the magic wand quickly leads to undecidability, especially when combined with inductive predicates for reasoning about data structures.\n\nTo circumvent these undecidability results, we propose to assign a more restrictive semantics to the separating conjunction. We argue that the resulting logic, strong-separation logic, can be used for compositional program verification and bi-abductive static analysis just like \"standard\" separation logic, while remaining decidable even in the presence of both the magic wand and the list-segment predicate—a combination of features that leads to undecidability assuming the standard semantics.",
    "URL": "https://popl20.sigplan.org/details/adsl-2020-papers/9/Strong-Separation-Logic",
    "_line": "FormalBib.bib:694"
  },
  "noauthor_sledge_nodate": {
    "id": "noauthor_sledge_nodate",
    "type": "webpage",
    "title": "SLEdge: Bounded Model Checking in Separation Logic (ADSL 2020) - POPL 2020",
    "container-title-short": "SLEdge",
    "title-short": "SLEdge",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "22"
        ]
      ]
    },
    "abstract": "In recent times, the verification of heap-manipulating programs, and static analyses in particular, has seen substantial success, largely due to the development of ‘Separation Logics’ (SLs). SLs provide embedded support for ‘local reasoning’: reasoning about the resource(s) being modified, instead of the state of the entire system. This form of reasoning is enabled by new syntax (dedicated atomic proposition and separating connectives) and corresponding semantics. Such expressivity comes with the inherent difficulty of automating these logics. Combining this power with induction/recursion  ...",
    "URL": "https://popl20.sigplan.org/details/adsl-2020-papers/1/SLEdge-Bounded-Model-Checking-in-Separation-Logic",
    "_line": "FormalBib.bib:705"
  },
  "rastogi_layered_nodate": {
    "id": "rastogi_layered_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Rastogi",
        "given": "Aseem"
      },
      {
        "family": "Martínez",
        "given": "Guido"
      },
      {
        "family": "Fromherz",
        "given": "Aymeric"
      },
      {
        "family": "Ramananandro",
        "given": "Tahina"
      },
      {
        "family": "Swamy",
        "given": "Nikhil"
      }
    ],
    "title": "Layered Indexed Effects",
    "container-title": "Unpublished",
    "page": "28",
    "page-first": "28",
    "volume": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:714"
  },
  "everestteam_project_2021": {
    "id": "everestteam_project_2021",
    "type": "webpage",
    "author": [
      {
        "family": "EverestTeam"
      }
    ],
    "title": "Project Everest",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "21"
        ]
      ]
    },
    "URL": "https://project-everest.github.io/",
    "_line": "FormalBib.bib:724"
  },
  "leino_dafny_2021": {
    "id": "leino_dafny_2021",
    "type": "book",
    "author": [
      {
        "family": "Leino",
        "given": "Rustan"
      }
    ],
    "title": "Dafny",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "21"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "21"
        ]
      ]
    },
    "publisher": "Dafny",
    "abstract": "Dafny is a verification-aware programming language",
    "URL": "https://github.com/dafny-lang/dafny",
    "note": "original-date: 2016-04-16T20:05:38Z",
    "_line": "FormalBib.bib:733"
  },
  "p4_consortium_p4_16_2020": {
    "id": "p4_consortium_p4_16_2020",
    "type": "webpage",
    "author": [
      {
        "family": "Consortium",
        "given": "P4"
      }
    ],
    "title": "P4&underscore;16 Language Specification",
    "issued": {
      "date-parts": [
        [
          "2020",
          "6",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "5"
        ]
      ]
    },
    "URL": "https://p4.org/p4-spec/docs/P4-16-working-spec.html",
    "_line": "FormalBib.bib:745"
  },
  "peterson_5g_2020": {
    "id": "peterson_5g_2020",
    "type": "webpage",
    "author": [
      {
        "family": "Peterson",
        "given": "Larry"
      },
      {
        "family": "Sunay",
        "given": "Oguz"
      }
    ],
    "title": "5G Mobile Networks: A Systems Approach",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "12",
          "31"
        ]
      ]
    },
    "URL": "https://5g.systemsapproach.org/",
    "_line": "FormalBib.bib:754"
  },
  "peterson_software-defined_2020": {
    "id": "peterson_software-defined_2020",
    "type": "webpage",
    "author": [
      {
        "family": "Peterson",
        "given": "Larry"
      },
      {
        "family": "Davie",
        "given": "Bruce"
      },
      {
        "family": "Cascone",
        "given": "Carmelo"
      },
      {
        "family": "O'Connor",
        "given": "Brian"
      },
      {
        "family": "Vachuska",
        "given": "Thomas"
      }
    ],
    "title": "Software-Defined Networks: A Systems Approach",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "12",
          "31"
        ]
      ]
    },
    "URL": "https://sdn.systemsapproach.org/",
    "_line": "FormalBib.bib:763"
  },
  "peterson_computer_2020": {
    "id": "peterson_computer_2020",
    "type": "webpage",
    "author": [
      {
        "family": "Peterson",
        "given": "Larry"
      },
      {
        "family": "Davie",
        "given": "Bruce"
      }
    ],
    "title": "Computer Networks: A Systems Approach — Computer Networks: A Systems Approach Version 6.2-dev documentation",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "12",
          "31"
        ]
      ]
    },
    "URL": "https://book.systemsapproach.org/",
    "_line": "FormalBib.bib:772"
  },
  "vacca_systematic_2021": {
    "id": "vacca_systematic_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Vacca",
        "given": "Anna"
      },
      {
        "family": "Di Sorbo",
        "given": "Andrea"
      },
      {
        "family": "Visaggio",
        "given": "Corrado A."
      },
      {
        "family": "Canfora",
        "given": "Gerardo"
      }
    ],
    "title": "A systematic literature review of blockchain and smart contract development: Techniques, tools, and open challenges",
    "container-title": "Journal of Systems and Software",
    "container-title-short": "A systematic literature review of blockchain and smart contract development",
    "title-short": "A systematic literature review of blockchain and smart contract development",
    "issued": {
      "date-parts": [
        [
          "2021",
          "4",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "issn": "0164-1212",
    "abstract": "Blockchain platforms and languages for writing smart contracts are becoming increasingly popular. However, smart contracts and blockchain applications are developed through non-standard software life-cycles, in which, for instance, delivered applications can hardly be updated or bugs resolved by releasing a new version of the software. Therefore, this systematic literature review oriented to software engineering aims at highlighting current problems and possible solutions concerning smart contracts and blockchain applications development. In this paper, we analyze 96 articles (written from 2016 to 2020) presenting solutions to tackle software engineering-specific challenges related to the development, test, and security assessment of blockchain-oriented software. In particular, we review papers (that appeared in international journals and conferences) relating to six specific topics: smart contract testing, smart contract code analysis, smart contract metrics, smart contract security, Dapp performance, and blockchain applications. Beyond the systematic review of the techniques, tools, and approaches that have been proposed in the literature to address the issues posed by the development of blockchain-based software, for each of the six aforementioned topics, we identify open challenges that require further research.",
    "keywords": "Empirical study, Ethereum, Smart contract, Software engineering for blockchain technologies, Software metrics, Software quality",
    "URL": "http://www.sciencedirect.com/science/article/pii/S0164121220302818",
    "DOI": "10.1016/j.jss.2020.110891",
    "page": "110891",
    "page-first": "110891",
    "volume": "174",
    "language": "en-US",
    "_line": "FormalBib.bib:781"
  },
  "tanaka_coq_2021": {
    "id": "tanaka_coq_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Tanaka",
        "given": "Akira"
      }
    ],
    "title": "Coq to C translation with partial evaluation",
    "container-title": "Proceedings of the 2021 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation",
    "collection-title": "PEPM 2021",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8305-9",
    "abstract": "Coq proof assistant can be used to prove various properties of programs written in the Gallina language. It is also possible to translate Gallina programs to OCaml programs. However, OCaml is not suitable for low-level programs. Therefore, we are developing a Coq plugin for Gallina to C translation. This plugin transforms functions written in Gallina into a form as close to C as possible within Gallina. This transformation includes partial evaluation, which improves execution efficiency and eliminates polymorphism and dependent types. We can easily verify in Coq that this transformation does not change the execution result, and thus it is highly reliable. And Gallina functions after this transformation can be easily translated to C.",
    "keywords": "Coq, C, compiler, Gallina, partial evaluation, tail recursion, translator, verification",
    "URL": "https://doi.org/10.1145/3441296.3441394",
    "DOI": "10.1145/3441296.3441394",
    "publisher-place": "New York, NY, USA",
    "page": "14-31",
    "page-first": "14",
    "_line": "FormalBib.bib:800"
  },
  "noauthor_open_nodate": {
    "id": "noauthor_open_nodate",
    "type": "webpage",
    "title": "Open vSwitch 2.15.90 documentation",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "URL": "https://docs.openvswitch.org/en/latest/",
    "_line": "Networking.bib:594"
  },
  "muller_concise_2020": {
    "id": "muller_concise_2020",
    "type": "report",
    "genre": "Working Paper",
    "author": [
      {
        "family": "Müller",
        "given": "Peter"
      },
      {
        "family": "Wolf",
        "given": "Felix A."
      },
      {
        "family": "Schwerhoff",
        "given": "Malte"
      }
    ],
    "title": "Concise Outlines for a Complex Logic: A Proof Outline Checker for TaDA",
    "container-title-short": "Concise Outlines for a Complex Logic",
    "title-short": "Concise Outlines for a Complex Logic",
    "issued": {
      "date-parts": [
        [
          "2020",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "publisher": "Cornell University",
    "URL": "https://www.research-collection.ethz.ch/handle/20.500.11850/456833",
    "DOI": "10.3929/ethz-b-000456825",
    "page": "2010.07080",
    "page-first": "2010",
    "note": "Accepted: 2020-12-17T12:02:04Z\nPublication Title: arXiv",
    "language": "en-US",
    "_line": "FormalBib.bib:825"
  },
  "noauthor_infersharp_2021": {
    "id": "noauthor_infersharp_2021",
    "type": "book",
    "title": "infersharp",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "publisher": "Microsoft",
    "abstract": "Infer&hash; is an interprocedural and scalable static code analyzer for C&hash;. Via the capabilities of Facebook&amp;&hash;39;s Infer, this tool detects null pointer dereferences and resource leak.",
    "URL": "https://github.com/microsoft/infersharp",
    "note": "original-date: 2020-07-02T19:22:01Z",
    "_line": "FormalBib.bib:843"
  },
  "noauthor_qsharp-language_2021": {
    "id": "noauthor_qsharp-language_2021",
    "type": "book",
    "title": "qsharp-language",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "publisher": "Microsoft",
    "abstract": "Official repository for design of the quantum programming language Q&hash; and its core libraries",
    "URL": "https://github.com/microsoft/qsharp-language",
    "note": "original-date: 2020-09-11T23:31:17Z",
    "_line": "FormalBib.bib:854"
  },
  "noauthor_infer_2020": {
    "id": "noauthor_infer_2020",
    "type": "webpage",
    "title": "Infer&hash;: Interprocedural Memory Safety Analysis For C&hash;..NET Blog",
    "container-title-short": "Infer&hash;",
    "title-short": "Infer&hash;",
    "issued": {
      "date-parts": [
        [
          "2020",
          "12",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "abstract": "Announcing the public release of Infer&hash;, which brings the interprocedural static analysis capabilities of Infer to the .NET community.",
    "URL": "https://devblogs.microsoft.com/dotnet/infer-interprocedural-memory-safety-analysis-for-c/",
    "language": "en-US",
    "_line": "FormalBib.bib:865"
  },
  "ozdemir_unifying_2020": {
    "id": "ozdemir_unifying_2020",
    "type": "report",
    "author": [
      {
        "family": "Ozdemir",
        "given": "Alex"
      },
      {
        "family": "Brown",
        "given": "Fraser"
      },
      {
        "family": "Wahby",
        "given": "Riad S."
      }
    ],
    "title": "Unifying Compilers for SNARKs, SMT, and More",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "abstract": "The programming languages community, the cryptography community, and others rely on translating programs in high-level source languages (e.g., C) to logical constraint representations. Unfortunately, building compilers for this task is difficult and time consuming. In this work, we show that all of these communities can build upon a shared compiler infrastructure, because they all share a common abstraction: stateless, non-deterministic computations that we call existentially quantified circuits, or EQCs.\n\n\n\nTo make our approach concrete we create CirC, an infrastructure for building compilers to EQCs. CirC makes it easy to add support for new EQCs: we build support for two, one used by the PL community and one used by the cryptography community, in &dollar;&bslash;approx&dollar;2000 LOC. It’s also easy to extend CirC to support new source languages: we build a feature complete compiler for a cryptographic language in one week and &dollar;&bslash;approx&dollar;700 LOC, whereas the reference compiler for the same language took years to write, comprises &dollar;&bslash;approx&dollar;24000 LOC, and produces worse-performing output than our compiler. Finally, CirC enables novel applications that combine multiple EQCs. For example, we build the first pipeline that (1) automatically identifies bugs in programs, then (2) automatically constructs cryptographic proofs of the bugs' existence.",
    "keywords": "implementation, zero knowledge",
    "URL": "http://eprint.iacr.org/2020/1586",
    "number": "1586",
    "_line": "FormalBib.bib:877"
  },
  "chhak_towards_2020": {
    "id": "chhak_towards_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Chhak",
        "given": "C. H. R."
      },
      {
        "family": "Tolmach",
        "given": "Andrew"
      },
      {
        "family": "Anderson",
        "given": "Sean"
      }
    ],
    "title": "Towards Formally Verified Compilation of Tag-Based Policy Enforcement",
    "container-title": "arXiv:2012.10313 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2020",
          "12",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "7"
        ]
      ]
    },
    "abstract": "Hardware-assisted reference monitoring is receiving increasing attention as a way to improve the security of existing software. One example is the PIPE architecture extension, which attaches metadata tags to register and memory values and executes tag-based rules at each machine instruction to enforce a software-defined security policy. To use PIPE effectively, engineers should be able to write security policies in terms of source-level concepts like functions, local variables, and structured control operators, which are not visible at machine level. It is the job of the compiler to generate PIPE-aware machine code that enforces these source-level policies. The compiler thus becomes part of the monitored system's trusted computing base &ndash; and hence a prime candidate for verification. To formalize compiler correctness in this setting, we extend the source language semantics with its own form of user-specified tag-based monitoring, and show that the compiler preserves that monitoring behavior. The challenges of compilation include mapping source-level monitoring policies to instruction-level tag rules, preserving fail-stop behaviors, and satisfying the surprisingly complex preconditions for conventional optimizations. In this paper, we describe the design and verification of Tagine, a small prototype compiler that translates a simple tagged WHILE language to a tagged register transfer language and performs simple optimizations. Tagine is based on the RTLgen and Deadcode phases of the CompCert compiler, and hence is written and verified in Coq. This work is a first step toward verification of a full-scale compiler for a realistic tagged source language.",
    "keywords": "Computer Science - Cryptography and Security",
    "URLtext": "2012.10313",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2012.10313",
    "URL": "http://arxiv.org/abs/2012.10313",
    "_line": "Security.bib:136"
  },
  "haslbeck_veried_nodate": {
    "id": "haslbeck_veried_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Haslbeck",
        "given": "Maximilian P L"
      },
      {
        "family": "Lammich",
        "given": "Peter"
      }
    ],
    "title": "Veriﬁed Fine-Grained Algorithm Analysis Down to LLVM",
    "abstract": "We present a framework to verify both, functional correctness and worst-case complexity of practically eﬃcient algorithms. We implemented a stepwise reﬁnement approach, using the novel concept of resource currencies to naturally structure the resource analysis along the reﬁnement chain, and allow a ﬁne-grained analysis of operation counts. Our framework targets the LLVM intermediate representation. We extend its semantics from earlier work with a cost model. As case study, we verify the correctness and O(n log n) worst-case complexity of an implementation of the introsort algorithm, whose performance is on par with the state-of-the-art implementation found in the GNU C++ Library.",
    "page": "28",
    "page-first": "28",
    "language": "en-US",
    "_line": "FormalBib.bib:907"
  },
  "schoolderman_efficient_2020": {
    "id": "schoolderman_efficient_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Schoolderman",
        "given": "Marc"
      },
      {
        "family": "Moerman",
        "given": "Jonathan"
      },
      {
        "family": "Smetsers",
        "given": "Sjaak"
      },
      {
        "family": "Eekelen",
        "given": "Marko",
        "dropping-particle": "van"
      }
    ],
    "title": "Efficient Verification of Optimized Code: Correct High-speed Curve25519",
    "container-title": "arXiv:2012.09919 \\[cs\\]",
    "container-title-short": "Efficient Verification of Optimized Code",
    "title-short": "Efficient Verification of Optimized Code",
    "issued": {
      "date-parts": [
        [
          "2020",
          "12",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "abstract": "Code that is highly optimized poses a problem for program-level verification. Programmers can employ various clever tricks that are non-trivial to reason about. For cryptography on low-power devices, it is nonetheless crucial that implementations be functionally correct, secure, and efficient. These are usually crafted in hand-optimized machine code that eschew conventional control flow as much as possible. We have formally verified such code: a library which implements elliptic curve cryptography on 8-bit AVR microcontrollers. The chosen implementation is the most efficient currently known for this microarchitecture. It consists of over 3000 lines of assembly instructions. Building on earlier work, we use the Why3 platform to model the code and generate verification conditions, which are proven using automated provers. The approach is re-usable and adaptable, and allows for validation. Furthermore, an error in the original implementation was found and corrected, at the same time reducing its memory footprint. This shows that practical verification of cutting-edge code is not only possible, but can in fact add to its efficiency &ndash; and is clearly necessary.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Logic in Computer Science, F.3, E.3",
    "URLtext": "2012.09919",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2012.09919",
    "URL": "http://arxiv.org/abs/2012.09919",
    "_line": "FormalBib.bib:916"
  },
  "martin_mastering_2013": {
    "id": "martin_mastering_2013",
    "type": "book",
    "author": [
      {
        "family": "Martin",
        "given": "Ken"
      },
      {
        "family": "Hoffman",
        "given": "Bill"
      },
      {
        "family": "Cedilnik",
        "given": "Andy"
      }
    ],
    "title": "Mastering CMake: a cross-platform build system ; covers installing and running CMake ; details converting existing build processes to CMake ; create powerful cross-platform build scripts",
    "container-title-short": "Mastering CMake",
    "title-short": "Mastering CMake",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "publisher": "Kitware",
    "number-of-pages": "640",
    "edition": "6. ed",
    "isbn": "978-1-930934-26-9",
    "publisher-place": "Clifton Park, NY",
    "note": "OCLC: 869872480",
    "_line": "FormalBib.bib:931"
  },
  "feng_correct-by-construction_2018": {
    "id": "feng_correct-by-construction_2018",
    "type": "chapter",
    "author": [
      {
        "family": "Zhang",
        "given": "Teng"
      },
      {
        "family": "Wiegley",
        "given": "John"
      },
      {
        "family": "Giannakopoulos",
        "given": "Theophilos"
      },
      {
        "family": "Eakman",
        "given": "Gregory"
      },
      {
        "family": "Pit-Claudel",
        "given": "Clément"
      },
      {
        "family": "Lee",
        "given": "Insup"
      },
      {
        "family": "Sokolsky",
        "given": "Oleg"
      }
    ],
    "editor": [
      {
        "family": "Feng",
        "given": "Xinyu"
      },
      {
        "family": "Müller-Olm",
        "given": "Markus"
      },
      {
        "family": "Yang",
        "given": "Zijiang"
      }
    ],
    "title": "Correct-by-Construction Implementation of Runtime Monitors Using Stepwise Refinement",
    "container-title": "Dependable Software Engineering. Theories, Tools, and Applications",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-99932-6 978-3-319-99933-3",
    "URL": "http://link.springer.com/10.1007/978-3-319-99933-3_3",
    "DOI": "10.1007/978-3-319-99933-3_3",
    "publisher-place": "Cham",
    "page": "31-49",
    "page-first": "31",
    "volume": "10998",
    "_line": "FormalBib.bib:945"
  },
  "pit-claudel_extensible_nodate": {
    "id": "pit-claudel_extensible_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Pit-Claudel",
        "given": "Clément"
      },
      {
        "family": "Wang",
        "given": "Peng"
      },
      {
        "family": "Delaware",
        "given": "Benjamin"
      },
      {
        "family": "Gross",
        "given": "Jason"
      },
      {
        "family": "Chlipala",
        "given": "Adam"
      }
    ],
    "title": "Extensible Extraction of Efﬁcient Imperative Programs with Foreign Functions, Manually Managed Memory, and Proofs",
    "abstract": "We present an original approach to sound program extraction in a proof assistant, using syntax-driven automation to derive correct-by-construction imperative programs from nondeterministic functional source code. Our approach does not require committing to a single inﬂexible compilation strategy and instead makes it straightforward to create domainspeciﬁc code translators. In addition to a small set of core definitions, our framework is a large, user-extensible collection of compilation rules each phrased to handle speciﬁc language constructs, code patterns, or data manipulations. By mixing and matching these pieces of logic, users can easily tailor extraction to their own domains and programs, getting maximum performance and ensuring correctness of the resulting assembly code. Using this approach, we complete the ﬁrst proof-generating pipeline that goes automatically from high-level speciﬁcations to assembly code. In our main case study, the original speciﬁcations are phrased to resemble SQL-style queries, while the ﬁnal assembly code does manual memory management, calls out to foreign data structures and functions, and is suitable to deploy on resource-constrained platforms. The pipeline runs entirely within the Coq proof assistant, leading to ﬁnal, linked assembly code inside Coq with overall full-functional-correctness proofs in separation logic.",
    "URL": "http://pit-claudel.fr/clement/papers/fiat-to-facade.pdf",
    "page": "14",
    "page-first": "14",
    "note": "clement/fiat-to-facade.pdg",
    "language": "en-US",
    "_line": "FormalBib.bib:962"
  },
  "chlipala_formal_2019": {
    "id": "chlipala_formal_2019",
    "type": "book",
    "author": [
      {
        "family": "Chlipala",
        "given": "Adam"
      }
    ],
    "title": "Formal Reasoning About Programs - Github",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "URL": "https://github.com/achlipala/frap",
    "note": "original-date: 2016-02-02T18:43:56Z",
    "_line": "FormalBib.bib:973"
  },
  "chlipala_certied_nodate": {
    "id": "chlipala_certied_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Chlipala",
        "given": "Adam"
      }
    ],
    "title": "Certiﬁed Programming with Dependent Types",
    "page": "369",
    "page-first": "369",
    "language": "en-US",
    "_line": "FormalBib.bib:983"
  },
  "chlipala_certified_2013": {
    "id": "chlipala_certified_2013",
    "type": "book",
    "author": [
      {
        "family": "Chlipala",
        "given": "Adam"
      }
    ],
    "title": "Certified programming with dependent types: a pragmatic introduction to the Coq proof assistant",
    "container-title-short": "Certified programming with dependent types",
    "title-short": "Certified programming with dependent types",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "publisher": "The MIT Press",
    "number-of-pages": "424",
    "isbn": "978-0-262-02665-9",
    "keywords": "Automatic theorem proving, Computer programming, Computer programs, Coq (Electronic resource)",
    "URL": "http://adam.chlipala.net/cpdt/",
    "publisher-place": "Cambridge, MA",
    "_line": "FormalBib.bib:991"
  },
  "chlipala_introduction_nodate": {
    "id": "chlipala_introduction_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Chlipala",
        "given": "Adam"
      }
    ],
    "title": "An Introduction to Programming and Proving with Dependent Types in Coq",
    "container-title": "Journal of Formalized Reasoning",
    "abstract": "Computer proof assistants vary along many dimensions. Among the mature implementations, the Coq system is distinguished by two key features. First, we have support for programming with\ndependent types in the tradition of type theory, based on dependent function types and inductive type families. Second, we have a domain-specific language for coding correct-by-construction proof automation. Though the Coq user community has grown quite large, neither of the aspects\nI highlight is widely used. In this tutorial, I aim to provide a pragmatic introduction to both, showing how they can bring significant improvements in productivity.",
    "page": "93",
    "page-first": "93",
    "volume": "3",
    "note": "chlipala/1978-4445-1-PB.pdf",
    "issue": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:1004"
  },
  "chargueraud_characteristic_2010": {
    "id": "chargueraud_characteristic_2010",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Charguéraud",
        "given": "Arthur"
      }
    ],
    "title": "Characteristic Formulae for Mechanized Program Verification",
    "issued": {
      "date-parts": [
        [
          "2010",
          "12",
          "16"
        ]
      ]
    },
    "publisher": "UNIVERSITÉ PARIS.DIDEROT",
    "number-of-pages": "185",
    "abstract": "This dissertation describes a new approach to program veri cation,\nbased on characteristic formulae. The characteristic formula of a program\nis a higher-order logic formula that describes the behavior of that\nprogram, in the sense that it is sound and complete with respect to\nthe semantics. This formula can be exploited in an interactive theorem\nprover to establish that the program satis es a speci cation expressed\nin the style of Separation Logic, with respect to total correctness.\nThe characteristic formula of a program is automatically generated\nfrom its source code alone. In particular, there is no need to annotate the\nsource code with speci cations or loop invariants, as such information\ncan be given in interactive proof scripts. One key feature of characteristic\nformulae is that they are of linear size and that they can be prettyprinted\nin a way that closely resemble the source code they describe, even\nthough they do not refer to the syntax of the programming language.\nCharacteristic formulae serve as a basis for a tool, called CFML, that\nsupports the veri cation of Caml programs using the Coq proof assistant.\nCFML has been employed to verify about half of the content of\nOkasaki's book on purely functional data structures, and to verify several\nimperative data structures such as mutable lists, sparse arrays and\nunion- nd. CFML also supports reasoning on higher-order imperative\nfunctions, such as functions in CPS form and higher-order iterators",
    "publisher-place": "Paris, France",
    "note": "chargueraud/chargueraud&underscore;thesis&underscore;final.pdf",
    "language": "en-US",
    "_line": "FormalBib.bib:1019"
  },
  "chargueraud_characteristic_2011": {
    "id": "chargueraud_characteristic_2011",
    "type": "paper-conference",
    "author": [
      {
        "family": "Charguéraud",
        "given": "Arthur"
      }
    ],
    "title": "Characteristic Formulae for the Verification of Imperative Programs",
    "container-title": "Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming",
    "collection-title": "ICFP '11",
    "issued": {
      "date-parts": [
        [
          "2011"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-0865-6",
    "abstract": "In previous work, we introduced an approach to program verification based on characteristic formulae. The approach consists of generating a higher-order logic formula from the source code of a program. This characteristic formula is constructed in such a way that it gives a sound and complete description of the semantics of that program. The formula can thus be exploited in an interactive proof assistant to formally verify that the program satisfies a particular specification. This previous work was, however, only concerned with purely-functional programs. In the present paper, we describe the generalization of characteristic formulae to an imperative programming language. In this setting, characteristic formulae involve specifications expressed in the style of Separation Logic. They also integrate the frame rule, which enables local reasoning. We have implemented a tool based on characteristic formulae. This tool, called CFML, supports the verification of imperative Caml programs using the Coq proof assistant. Using CFML, we have formally verified nontrivial imperative algorithms, as well as CPS functions, higher-order iterators, and programs involving higher-order stores.",
    "keywords": "characteristic formula, interactive verification, total correctness",
    "URL": "http://doi.acm.org/10.1145/2034773.2034828",
    "DOI": "10.1145/2034773.2034828",
    "publisher-place": "New York, NY, USA",
    "page": "418-430",
    "page-first": "418",
    "_line": "FormalBib.bib:1053"
  },
  "chargueraud_program_2010": {
    "id": "chargueraud_program_2010",
    "type": "paper-conference",
    "author": [
      {
        "family": "Charguéraud",
        "given": "Arthur"
      }
    ],
    "title": "Program Verification Through Characteristic Formulae",
    "container-title": "Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming",
    "collection-title": "ICFP '10",
    "issued": {
      "date-parts": [
        [
          "2010"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-60558-794-3",
    "abstract": "This paper describes CFML, the first program verification tool based on characteristic formulae. Given the source code of a pure Caml program, this tool generates a logical formula that implies any valid post-condition for that program. One can then prove that the program satisfies a given specification by reasoning interactively about the characteristic formula using a proof assistant such as Coq. Our characteristic formulae improve over Honda et al's total characteristic assertion pairs in that they are expressible in standard higher-order logic, allowing to exploit them in practice to verify programs using existing proof assistants. Our technique has been applied to formally verify more than half of the content of Okasaki's Purely Functional Data Structures reference book",
    "keywords": "characteristic formula, total correctness, functional program",
    "URL": "http://doi.acm.org/10.1145/1863543.1863590",
    "DOI": "10.1145/1863543.1863590",
    "publisher-place": "New York, NY, USA",
    "page": "321-332",
    "page-first": "321",
    "_line": "FormalBib.bib:1071"
  },
  "gu_certified_2018": {
    "id": "gu_certified_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Gu",
        "given": "Ronghui"
      },
      {
        "family": "Shao",
        "given": "Zhong"
      },
      {
        "family": "Kim",
        "given": "Jieung"
      },
      {
        "family": "Wu",
        "given": "Xiongnan (Newman)"
      },
      {
        "family": "Koenig",
        "given": "Jérémie"
      },
      {
        "family": "Sjöberg",
        "given": "Vilhelm"
      },
      {
        "family": "Chen",
        "given": "Hao"
      },
      {
        "family": "Costanzo",
        "given": "David"
      },
      {
        "family": "Ramananandro",
        "given": "Tahina"
      }
    ],
    "title": "Certified Concurrent Abstraction Layers",
    "container-title": "Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation",
    "collection-title": "PLDI 2018",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-5698-5",
    "abstract": "Concurrent abstraction layers are ubiquitous in modern computer systems because of the pervasiveness of multithreaded programming and multicore hardware. Abstraction layers are used to hide the implementation details (e.g., fine-grained synchronization) and reduce the complex dependencies among components at different levels of abstraction. Despite their obvious importance, concurrent abstraction layers have not been treated formally. This severely limits the applicability of layer-based techniques and makes it difficult to scale verification across multiple concurrent layers.   In this paper, we present CCAL&mdash;a fully mechanized programming toolkit developed under the CertiKOS project&mdash;for specifying, composing, compiling, and linking certified concurrent abstraction layers. CCAL consists of three technical novelties: a new game-theoretical, strategy-based compositional semantic model for concurrency (and its associated program verifiers), a set of formal linking theorems for composing multithreaded and multicore concurrent layers, and a new CompCertX compiler that supports certified thread-safe compilation and linking. The CCAL toolkit is implemented in Coq and supports layered concurrent programming in both C and assembly. It has been successfully applied to build a fully certified concurrent OS kernel with fine-grained locking.",
    "keywords": "abstraction layer, certified compilers, certified OS kernels, concurrency, modularity, Verification",
    "URL": "http://doi.acm.org/10.1145/3192366.3192381",
    "DOI": "10.1145/3192366.3192381",
    "publisher-place": "New York, NY, USA",
    "page": "646-661",
    "page-first": "646",
    "_line": "FormalBib.bib:1089"
  },
  "costanzo_end--end_nodate": {
    "id": "costanzo_end--end_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Costanzo",
        "given": "David"
      },
      {
        "family": "Shao",
        "given": "Zhong"
      },
      {
        "family": "Gu",
        "given": "Ronghui"
      }
    ],
    "title": "End-to-End Veriﬁcation of Information-Flow Security for C and Assembly Programs - Tech Report",
    "abstract": "Protecting the conﬁdentiality of information manipulated by a computing system is one of the most important challenges facing today’s cybersecurity community. A promising step toward conquering this challenge is to formally verify that the end-to-end behavior of the computing system really satisﬁes various information-ﬂow policies. Unfortunately, because today’s system software still consists of both C and assembly programs, the end-to-end veriﬁcation necessarily requires that we not only prove the security properties of individual components, but also carefully preserve these properties through compilation and cross-language linking.",
    "URL": "http://flint.cs.yale.edu/certikos/publications/security-tr.pdf",
    "page": "21",
    "page-first": "21",
    "note": "certikos/pldi16-certikos-security-tr.pdf",
    "language": "en-US",
    "_line": "FormalBib.bib:1107"
  },
  "gu_certikos:_2016": {
    "id": "gu_certikos:_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Gu",
        "given": "Ronghui"
      },
      {
        "family": "Shao",
        "given": "Zhong"
      },
      {
        "family": "Chen",
        "given": "Hao"
      },
      {
        "family": "Wu",
        "given": "Xiongnan"
      },
      {
        "family": "Kim",
        "given": "Jieung"
      },
      {
        "family": "Sjöberg",
        "given": "Vilhelm"
      },
      {
        "family": "Costanzo",
        "given": "David"
      }
    ],
    "title": "CertiKOS: An Extensible Architecture for Building Certified Concurrent OS Kernels",
    "container-title": "Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation",
    "container-title-short": "CertiKOS",
    "collection-title": "OSDI'16",
    "title-short": "CertiKOS",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "USENIX Association",
    "isbn": "978-1-931971-33-1",
    "abstract": "Complete formal verification of a non-trivial concurrent OS kernel is widely considered a grand challenge. We present a novel compositional approach for building certified concurrent OS kernels. Concurrency allows interleaved execution of kernel/user modules across different layers of abstraction. Each such layer can have a different set of observable events. We insist on formally specifying these layers and their observable events, and then verifying each kernel module at its proper abstraction level. To support certified linking with other CPUs or threads, we prove a strong contextual refinement property for every kernel function, which states that the implementation of each such function will behave like its specification under any kernel/user context with any valid interleaving. We have successfully developed a practical concurrent OS kernel and verified its (contextual) functional correctness in Coq. Our certified kernel is written in 6500 lines of C and x86 assembly and runs on stock x86 multicore machines. To our knowledge, this is the first proof of functional correctness of a complete, general-purpose concurrent OS kernel with fine-grained locking.",
    "URL": "http://dl.acm.org/citation.cfm?id=3026877.3026928",
    "publisher-place": "Berkeley, CA, USA",
    "page": "653-669",
    "page-first": "653",
    "_line": "FormalBib.bib:1118"
  },
  "herlihy_linearizability:_1990": {
    "id": "herlihy_linearizability:_1990",
    "type": "article-journal",
    "author": [
      {
        "family": "Herlihy",
        "given": "Maurice P."
      },
      {
        "family": "Wing",
        "given": "Jeannette M."
      }
    ],
    "title": "Linearizability: A Correctness Condition for Concurrent Objects",
    "container-title": "ACM Trans. Program. Lang. Syst.",
    "container-title-short": "Linearizability",
    "title-short": "Linearizability",
    "issued": {
      "date-parts": [
        [
          "1990",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "0164-0925",
    "abstract": "A concurrent object is a data object shared by concurrent processes. Linearizability is a correctness condition for concurrent objects that exploits the semantics of abstract data types. It permits a high degree of concurrency, yet it permits programmers to specify and reason about concurrent objects using known techniques from the sequential domain. Linearizability provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object's operations can be given by pre- and post-conditions. This paper defines linearizability, compares it to other correctness conditions, presents and demonstrates a method for proving the correctness of implementations, and shows how to reason about concurrent objects, given they are linearizable.",
    "URL": "http://doi.acm.org/10.1145/78969.78972",
    "DOI": "10.1145/78969.78972",
    "page": "463-492",
    "page-first": "463",
    "volume": "12",
    "issue": "3",
    "_line": "FormalBib.bib:1135"
  },
  "murawski_invitation_2016": {
    "id": "murawski_invitation_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Murawski",
        "given": "Andrzej S."
      },
      {
        "family": "Tzevelekos",
        "given": "Nikos"
      }
    ],
    "title": "An Invitation to Game Semantics",
    "container-title": "ACM SIGLOG News",
    "issued": {
      "date-parts": [
        [
          "2016",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "2372-3491",
    "abstract": "Game semantics is a flexible semantic theory that has led in recent years to an unprecedented number of full abstraction results for various programming paradigms. We present a gentle introduction to the subject, focussing on high-level ideas and examples with a view to providing a bridge to more technical literature.",
    "URL": "http://doi.acm.org/10.1145/2948896.2948902",
    "DOI": "10.1145/2948896.2948902",
    "page": "56-67",
    "page-first": "56",
    "volume": "3",
    "issue": "2",
    "_line": "FormalBib.bib:1152"
  },
  "gu_deep_2015": {
    "id": "gu_deep_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Gu",
        "given": "Ronghui"
      },
      {
        "family": "Koenig",
        "given": "Jérémie"
      },
      {
        "family": "Ramananandro",
        "given": "Tahina"
      },
      {
        "family": "Shao",
        "given": "Zhong"
      },
      {
        "family": "Wu",
        "given": "Xiongnan (Newman)"
      },
      {
        "family": "Weng",
        "given": "Shu-Chun"
      },
      {
        "family": "Zhang",
        "given": "Haozhong"
      },
      {
        "family": "Guo",
        "given": "Yu"
      }
    ],
    "title": "Deep Specifications and Certified Abstraction Layers",
    "container-title": "Proceedings of the 42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "collection-title": "POPL '15",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-3300-9",
    "abstract": "Modern computer systems consist of a multitude of abstraction layers (e.g., OS kernels, hypervisors, device drivers, network protocols), each of which defines an interface that hides the implementation details of a particular set of functionality. Client programs built on top of each layer can be understood solely based on the interface, independent of the layer implementation. Despite their obvious importance, abstraction layers have mostly been treated as a system concept; they have almost never been formally specified or verified. This makes it difficult to establish strong correctness properties, and to scale program verification across multiple layers. In this paper, we present a novel language-based account of abstraction layers and show that they correspond to a strong form of abstraction over a particularly rich class of specifications which we call deep specifications. Just as data abstraction in typed functional languages leads to the important representation independence property, abstraction over deep specification is characterized by an important implementation independence property: any two implementations of the same deep specification must have contextually equivalent behaviors. We present a new layer calculus showing how to formally specify, program, verify, and compose abstraction layers. We show how to instantiate the layer calculus in realistic programming languages such as C and assembly, and how to adapt the CompCert verified compiler to compile certified C layers such that they can be linked with assembly layers. Using these new languages and tools, we have successfully developed multiple certified OS kernels in the Coq proof assistant, the most realistic of which consists of 37 abstraction layers, took less than one person year to develop, and can boot a version of Linux as a guest.",
    "keywords": "abstraction layer, certified compilers, modularity, certified os kernels, deep specification, program verification",
    "URL": "http://doi.acm.org/10.1145/2676726.2676975",
    "DOI": "10.1145/2676726.2676975",
    "publisher-place": "New York, NY, USA",
    "page": "595-608",
    "page-first": "595",
    "_line": "FormalBib.bib:1168"
  },
  "bowman_j1:_nodate": {
    "id": "bowman_j1:_nodate",
    "type": "book",
    "author": [
      {
        "family": "Bowman",
        "given": "James"
      }
    ],
    "title": "J1: a small Forth CPU Core for FPGAs",
    "container-title-short": "J1",
    "title-short": "J1",
    "abstract": "Abstract—This paper describes a 16-bit Forth CPU core, intended for FPGAs. The instruction set closely matches the Forth programming language, simplifying cross-compilation. Because it has higher throughput than comparable CPU cores, it can stream uncompressed video over Ethernet using a simple software loop.The entire system (source Verilog,cross compiler, and TCP/IP networking code) is published under the BSD license. The core is less than 200 lines of Verilog, and operates reliably at 80 MHz in a Xilinx Spartan R○-3E FPGA, delivering approximately 100 ANS Forth MIPS. I.",
    "_line": "FormalBib.bib:1186"
  },
  "boulier_next_2017": {
    "id": "boulier_next_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Boulier",
        "given": "Simon"
      },
      {
        "family": "Pédrot",
        "given": "Pierre-Marie"
      },
      {
        "family": "Tabareau",
        "given": "Nicolas"
      }
    ],
    "title": "The next 700 syntactical models of type theory",
    "event-title": "Certified Programs and Proofs (CPP 2017)",
    "issued": {
      "date-parts": [
        [
          "2017",
          "1",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "A family of syntactic models for the calculus of construction with universes (CC ω) is described, all of them preserving conversion of the calculus definitionally, and thus giving rise directly to a program transformation of CC ω into itself. Those models are based on the remark that negative type constructors (e.g., dependent product, coinductive types or universes) are underspecified in type theory—which leaves some freedom on extra intensional specifications. The model construction can be seen as a compilation phase from a complex type theory into a simpler type theory. Such models can be used to derive (the negative part of) independence results with respect to CC ω , such as functional extensional-ity, propositional extensionality, univalence or the fact that bisimulation on a coinductive type may not coincide with equality. They can also be used to add new principles to the theory, which we illustrate by defining a version of CC ω with ad-hoc polymorphism that shows in particular that para-metricity is not an implicit requirement of type theory. The correctness of some of the models/program transformations have been checked in the COQ proof assistant and have been instrumented as a COQ plugin.",
    "URL": "https://hal.inria.fr/hal-01445835/document",
    "DOI": "10.1145/3018610.3018620",
    "page": "182 - 194",
    "page-first": "182",
    "language": "en-US",
    "_line": "FormalBib.bib:1194"
  },
  "sherman_making_2017": {
    "id": "sherman_making_2017",
    "type": "thesis",
    "genre": "Master of Science",
    "author": [
      {
        "family": "Sherman",
        "given": "Benjamin"
      }
    ],
    "title": "Making Discrete Decisions Based on Continuous Values",
    "issued": {
      "date-parts": [
        [
          "2017",
          "6"
        ]
      ]
    },
    "publisher": "MIT",
    "number-of-pages": "105",
    "abstract": "Many safety-critical software systems are cyber-physical systems that compute with continuous values; confirming their safety requires guaranteeing the accuracy of their computations. It is impossible for these systems to compute (total and deterministic) discrete computations (e.g., decisions) based on connected input spaces such as R. We propose a programming language based on constructive topology, whose types are spaces and programs are executable continuous maps, that facilitates making formal guarantees of accuracy of computed results. We demonstrate that discrete decisions can be made based on continuous values by permitting nondeterminism. This thesis describes variants of the programming language allowing nondeterminism and/or partiality, and introduces two tools for creating nondeterministic programs on spaces. Overlapping pattern matching is a generalization of pattern matching in functional programming, where patterns need not represent decidable predicates and also may overlap, allowing potentially nondeterministic behavior in overlapping regions. Binary covers, which are pairs of predicates such that at least one of them holds, yield a formal logic for constructing approximate decision procedures.",
    "URL": "http://adam.chlipala.net/theses/sherman_sm.pdf",
    "publisher-place": "Cambridge, MA",
    "note": "ben-sherman/sm-thesis.pdf",
    "language": "en-US",
    "_line": "FormalBib.bib:1208"
  },
  "bedford_coqatoo:_nodate": {
    "id": "bedford_coqatoo:_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Bedford",
        "given": "Andrew"
      }
    ],
    "title": "Coqatoo: Generating Natural Language Versions of Coq Proofs - Slides",
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "FormalBib.bib:1223"
  },
  "bedford_coqatoo:_2017": {
    "id": "bedford_coqatoo:_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Bedford",
        "given": "Andrew"
      }
    ],
    "title": "Coqatoo: Generating Natural Language Versions of Coq Proofs",
    "container-title": "arXiv:1712.03894 \\[cs\\]",
    "container-title-short": "Coqatoo",
    "title-short": "Coqatoo",
    "issued": {
      "date-parts": [
        [
          "2017",
          "12",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Due to their numerous advantages, formal proofs and proof assistants, such as Coq, are becoming increasingly popular. However, one disadvantage of using proof assistants is that the resulting proofs can sometimes be hard to read and understand, particularly for less-experienced users. To address this issue, we have implemented a tool capable of generating natural language versions of Coq proofs called Coqatoo, which we present in this paper.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "1712.03894",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1712.03894",
    "URL": "http://arxiv.org/abs/1712.03894",
    "_line": "FormalBib.bib:1231"
  },
  "mine_taking_2016": {
    "id": "mine_taking_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Miné",
        "given": "Antoine"
      },
      {
        "family": "Mauborgne",
        "given": "Laurent"
      },
      {
        "family": "Rival",
        "given": "Xavier"
      },
      {
        "family": "Feret",
        "given": "Jerome"
      },
      {
        "family": "Cousot",
        "given": "Patrick"
      },
      {
        "family": "Kästner",
        "given": "Daniel"
      },
      {
        "family": "Wilhelm",
        "given": "Stephan"
      },
      {
        "family": "Ferdinand",
        "given": "Christian"
      }
    ],
    "title": "Taking Static Analysis to the Next Level: Proving the Absence of Run-Time Errors and Data Races with Astrée",
    "container-title": "8th European Congress on Embedded Real Time Software and Systems (ERTS 2016)",
    "container-title-short": "Taking Static Analysis to the Next Level",
    "title-short": "Taking Static Analysis to the Next Level",
    "issued": {
      "date-parts": [
        [
          "2016",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "We present an extension of Astrée to concurrent C software. Astrée is a sound static analyzer for run-time errors previously limited to sequential C software. Our extension employs a scalable abstraction which covers all possible thread interleavings, and soundly reports all run-time errors and data races: when the analyzer does not report any alarm, the program is proven free from those classes of errors. We show how this extension is able to support a variety of operating systems (such as POSIX threads, ARINC 653, OSEK/AUTOSAR) and report on experimental results obtained on concurrent software from different domains, including large industrial software.",
    "URL": "https://hal.archives-ouvertes.fr/hal-01271552",
    "publisher-place": "Toulouse, France",
    "_line": "FormalBib.bib:1246"
  },
  "kastner_astree:_nodate": {
    "id": "kastner_astree:_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Kästner",
        "given": "D"
      },
      {
        "family": "Wilhelm",
        "given": "S"
      },
      {
        "family": "Nenova",
        "given": "S"
      },
      {
        "family": "Miné",
        "given": "A"
      },
      {
        "family": "Rival",
        "given": "X"
      },
      {
        "family": "Mauborgne",
        "given": "L"
      },
      {
        "family": "Feret",
        "given": "J"
      },
      {
        "family": "Cousot",
        "given": "P"
      },
      {
        "family": "Cousot",
        "given": "R"
      }
    ],
    "title": "Astree: Proving the Absence of Runtime Errors",
    "abstract": "Safety-critical embedded software has to satisfy stringent quality requirements. Testing and validation consumes a large – and growing – fraction of development cost. The last years have seen the emergence of semantics-based static analysis tools in various application areas, from runtime error analysis to worst-case execution time prediction. Their appeal is that they have the potential to reduce testing eﬀort while providing 100&perc; coverage, thus enhancing safety. Static runtime error analysis is applicable to large industryscale projects and produces a list of deﬁnite runtime errors and of potential runtime errors which might be true errors or false alarms. In the past, often only the deﬁnite errors were ﬁxed because manually inspecting each alarm was too time-consuming due to a large number of false alarms. Therefore no proof of the absence of runtime errors could be given. In this article the parameterizable static analyzer Astr´ee is presented. By specialization and parameterization Astr´ee can be adapted to the software under analysis. This enables Astr´ee to eﬃciently compute precise results. Astr´ee has successfully been used to analyze large-scale safety-critical avionics software with zero false alarms.",
    "URL": "https://www.di.ens.fr/~rival/papers/erts10.pdf",
    "page": "9",
    "page-first": "9",
    "note": "astree/astee-proving-absence-rte.pdf",
    "language": "en-US",
    "_line": "FormalBib.bib:1259"
  },
  "monniaux_parallel_2005": {
    "id": "monniaux_parallel_2005",
    "type": "article-journal",
    "author": [
      {
        "family": "Monniaux",
        "given": "David"
      }
    ],
    "title": "The parallel implementation of the Astr&bslash;'&lcurly;e&rcurly;e static analyzer",
    "container-title": "arXiv:cs/0701191",
    "issued": {
      "date-parts": [
        [
          "2005"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "The Astr&bslash;'&lcurly;e&rcurly;e static analyzer is a specialized tool that can prove the absence of runtime errors, including arithmetic overflows, in large critical programs. Keeping analysis times reasonable for industrial use is one of the design objectives. In this paper, we discuss the parallel implementation of the analysis.",
    "keywords": "Computer Science - Programming Languages, D.2.4, Computer Science - Performance",
    "URLtext": "cs/0701191",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "cs/0701191",
    "URL": "http://arxiv.org/abs/cs/0701191",
    "DOI": "10.1007/11575467_7",
    "page": "86-96",
    "page-first": "86",
    "volume": "3780",
    "_line": "FormalBib.bib:1270"
  },
  "kastner_program_2015": {
    "id": "kastner_program_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Kästner",
        "given": "Daniel"
      },
      {
        "family": "Pohland",
        "given": "Jan"
      }
    ],
    "editor": [
      {
        "family": "Roy",
        "given": "Matthieu"
      }
    ],
    "title": "Program Analysis on Evolving Software",
    "container-title": "CARS 2015 - Critical Automotive applications: Robustness &amp; Safety",
    "issued": {
      "date-parts": [
        [
          "2015",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Static analysis is well-suited for continuous verification during the software development stage since it only works on the source code and does not require a running system for testing. However, applying the program analysis during software development means that the analysis has to cope with evolving software and evolving analyzer configurations, especially in a model-based development process. In this article we present a unique history-aware concept for program analysis that has been developed for the static analyzer Astrée. It not only provides the ability to backtrack and access previous versions of the analysis configuration, it can also automatically determine the differences between two analysis configurations and relate them to the correct source code versions. Users can explicitly create a revision, i.e. a snapshot of the analysis project; changes of the source code, analysis options, analysis directives and results in different revisions are automatically detected and highlighted. The analyzer provides automatic correctness checks for all specified analysis directives, e.g., to tune the precision of the analyzer or provide information about the environment. This makes software verification applicable during the implementation stage, significantly reduces the effort to adapt the analyzer configuration to new source code versions, and makes analysis results on previous software versions easily reproducible.",
    "URL": "https://hal.archives-ouvertes.fr/hal-01192985",
    "publisher-place": "Paris, France",
    "_line": "FormalBib.bib:1287"
  },
  "appel_verified_2012": {
    "id": "appel_verified_2012",
    "type": "paper-conference",
    "author": [
      {
        "family": "Appel",
        "given": "Andrew W."
      }
    ],
    "title": "Verified Software Toolchain",
    "container-title": "Proceedings of the 4th International Conference on NASA Formal Methods",
    "collection-title": "NFM'12",
    "issued": {
      "date-parts": [
        [
          "2012"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer-Verlag",
    "isbn": "978-3-642-28890-6",
    "abstract": "The software toolchain includes static analyzers to check assertions about programs; optimizing compilers to translate programs to machine language; operating systems and libraries to supply context for programs. Our Verified Software Toolchain verifies with machine-checked proofs that the assertions claimed at the top of the toolchain really hold in the machine-language program, running in the operating-system context, on a weakly-consistent-shared-memory machine. Our verification approach is modular, in that proofs about operating systems or concurrency libraries are oblivious of the programming language or machine language, proofs about compilers are oblivious of the program logic used to verify static analyzers, and so on. The approach is scalable, in that each component is verified in the semantic idiom most natural for that component. Finally, the verification is foundational: the trusted base for proofs of observable properties of the machine-language program includes only the operational semantics of the machine language, not the source language, the compiler, the program logic, or any other part of the toolchain&ndash;even when these proofs are carried out by source-level static analyzers. In this paper I explain the construction of a a verified toolchain, using the Coq proof assistant. I will illustrate with shape analysis for C programs based on separation logic.",
    "URL": "http://dx.doi.org/10.1007/978-3-642-28891-3_2",
    "DOI": "10.1007/978-3-642-28891-3_2",
    "publisher-place": "Berlin, Heidelberg",
    "page": "2-2",
    "page-first": "2",
    "_line": "FormalBib.bib:1300"
  },
  "stewart_verified_2012": {
    "id": "stewart_verified_2012",
    "type": "paper-conference",
    "author": [
      {
        "family": "Stewart",
        "given": "Gordon"
      },
      {
        "family": "Beringer",
        "given": "Lennart"
      },
      {
        "family": "Appel",
        "given": "Andrew W."
      }
    ],
    "title": "Verified Heap Theorem Prover by Paramodulation",
    "container-title": "Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming",
    "collection-title": "ICFP '12",
    "issued": {
      "date-parts": [
        [
          "2012"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-1054-3",
    "abstract": "We present VeriStar, a verified theorem prover for a decidable subset of separation logic. Together with VeriSmall \\[3\\], a proved-sound Smallfoot-style program analysis for C minor, VeriStar demonstrates that fully machine-checked static analyses equipped with efficient theorem provers are now within the reach of formal methods. As a pair, VeriStar and VeriSmall represent the first application of the Verified Software Toolchain \\[4\\], a tightly integrated collection of machine-verified program logics and compilers giving foundational correctness guarantees. VeriStar is (1) purely functional, (2) machine-checked, (3) end-to-end, (4) efficient and (5) modular. By purely functional, we mean it is implemented in Gallina, the pure functional programming language embedded in the Coq theorem prover. By machine-checked, we mean it has a proof in Coq that when the prover says \"valid\", the checked entailment holds in a proved-sound separation logic for C minor. By end-to-end, we mean that when the static analysis+theorem prover says a C minor program is safe, the program will be compiled to a semantically equivalent assembly program that runs on real hardware. By efficient, we mean that the prover implements a state-of-the-art algorithm for deciding heap entailments and uses highly tuned verified functional data structures. By modular, we mean that VeriStar can be retrofitted to other static analyses as a plug-compatible entailment checker and its soundness proof can easily be ported to other separation logics.",
    "keywords": "theorem proving, paramodulation, separation logic",
    "URL": "http://doi.acm.org/10.1145/2364527.2364531",
    "DOI": "10.1145/2364527.2364531",
    "publisher-place": "New York, NY, USA",
    "page": "3-14",
    "page-first": "3",
    "_line": "FormalBib.bib:1317"
  },
  "jouannaud_verismall:_2011": {
    "id": "jouannaud_verismall:_2011",
    "type": "chapter",
    "author": [
      {
        "family": "Appel",
        "given": "Andrew W."
      }
    ],
    "editor": [
      {
        "family": "Jouannaud",
        "given": "Jean-Pierre"
      },
      {
        "family": "Shao",
        "given": "Zhong"
      }
    ],
    "title": "VeriSmall: Verified Smallfoot Shape Analysis",
    "container-title": "Certified Programs and Proofs",
    "container-title-short": "VeriSmall",
    "title-short": "VeriSmall",
    "issued": {
      "date-parts": [
        [
          "2011"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-25378-2 978-3-642-25379-9",
    "URL": "http://link.springer.com/10.1007/978-3-642-25379-9_18",
    "DOI": "10.1007/978-3-642-25379-9_18",
    "publisher-place": "Berlin, Heidelberg",
    "page": "231-246",
    "page-first": "231",
    "volume": "7086",
    "_line": "FormalBib.bib:1335"
  },
  "appel_verification_2015": {
    "id": "appel_verification_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Appel",
        "given": "Andrew W."
      }
    ],
    "title": "Verification of a Cryptographic Primitive: SHA-256",
    "container-title": "ACM Trans. Program. Lang. Syst.",
    "container-title-short": "Verification of a Cryptographic Primitive",
    "title-short": "Verification of a Cryptographic Primitive",
    "issued": {
      "date-parts": [
        [
          "2015",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "0164-0925",
    "abstract": "This article presents a full formal machine-checked verification of a C program: the OpenSSL implementation of SHA-256. This is an interactive proof of functional correctness in the Coq proof assistant, using the Verifiable C program logic. Verifiable C is a separation logic for the C language, proved sound with respect to the operational semantics for C, connected to the CompCert verified optimizing C compiler.",
    "keywords": "Cryptography",
    "URL": "http://doi.acm.org/10.1145/2701415",
    "DOI": "10.1145/2701415",
    "page": "7:1-7:31",
    "page-first": "7",
    "volume": "37",
    "issue": "2",
    "_line": "FormalBib.bib:1353"
  },
  "appel_verifiabble_2014": {
    "id": "appel_verifiabble_2014",
    "type": "book",
    "author": [
      {
        "family": "Appel",
        "given": "Andrew W."
      },
      {
        "family": "Dockins",
        "given": "Robert"
      },
      {
        "family": "Hobor",
        "given": "Aquinas"
      },
      {
        "family": "Beringer",
        "given": "Lennart"
      },
      {
        "family": "Dodds",
        "given": "Josiah"
      },
      {
        "family": "Stewart",
        "given": "Gordon"
      },
      {
        "family": "Blazy",
        "given": "Sandrine"
      },
      {
        "family": "Leroy",
        "given": "Xavier"
      }
    ],
    "title": "Verifiabble C, Version 2.2",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Cambridge University Press",
    "isbn": "978-1-107-25655-2",
    "URL": "http://ebooks.cambridge.org/ref/id/CBO9781107256552",
    "DOI": "10.1017/CBO9781107256552",
    "publisher-place": "Cambridge",
    "language": "en-US",
    "_line": "FormalBib.bib:1371"
  },
  "hutchison_verified_2014": {
    "id": "hutchison_verified_2014",
    "type": "chapter",
    "author": [
      {
        "family": "Beringer",
        "given": "Lennart"
      },
      {
        "family": "Stewart",
        "given": "Gordon"
      },
      {
        "family": "Dockins",
        "given": "Robert"
      },
      {
        "family": "Appel",
        "given": "Andrew W."
      }
    ],
    "editor": [
      {
        "family": "Shao",
        "given": "Zhong"
      }
    ],
    "title": "Verified Compilation for Shared-Memory C",
    "container-title": "Programming Languages and Systems",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-54832-1 978-3-642-54833-8",
    "URL": "http://link.springer.com/10.1007/978-3-642-54833-8_7",
    "DOI": "10.1007/978-3-642-54833-8_7,",
    "publisher-place": "Berlin, Heidelberg",
    "page": "107-127",
    "page-first": "107",
    "volume": "8410",
    "note": "appel/shmemc.pdf is a preview",
    "_line": "FormalBib.bib:1385"
  },
  "cao_vst-floyd:_2018": {
    "id": "cao_vst-floyd:_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Cao",
        "given": "Qinxiang"
      },
      {
        "family": "Beringer",
        "given": "Lennart"
      },
      {
        "family": "Gruetter",
        "given": "Samuel"
      },
      {
        "family": "Dodds",
        "given": "Josiah"
      },
      {
        "family": "Appel",
        "given": "Andrew W."
      }
    ],
    "title": "VST-Floyd: A Separation Logic Tool to Verify Correctness of C Programs",
    "container-title": "J. Autom. Reason.",
    "container-title-short": "VST-Floyd",
    "title-short": "VST-Floyd",
    "issued": {
      "date-parts": [
        [
          "2018",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "0168-7433",
    "abstract": "The Verified Software Toolchain builds foundational machine-checked proofs of the functional correctness of C programs. Its program logic, Verifiable C, is a shallowly embedded higher-order separation Hoare logic which is proved sound in Coq with respect to the operational semantics of CompCert Clight. This paper introduces VST-Floyd, a verification assistant which offers a set of semiautomatic tactics helping users build functional correctness proofs for C programs using Verifiable C.",
    "keywords": "Program verification, Proof automation, Separation logic, Symbolic execution",
    "URL": "https://doi.org/10.1007/s10817-018-9457-5",
    "DOI": "10.1007/s10817-018-9457-5",
    "page": "367-422",
    "page-first": "367",
    "volume": "61",
    "issue": "1",
    "_line": "FormalBib.bib:1405"
  },
  "hobor_theory_2010": {
    "id": "hobor_theory_2010",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hobor",
        "given": "Aquinas"
      },
      {
        "family": "Dockins",
        "given": "Robert"
      },
      {
        "family": "Appel",
        "given": "Andrew W."
      }
    ],
    "title": "A Theory of Indirection via Approximation",
    "container-title": "Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "collection-title": "POPL '10",
    "issued": {
      "date-parts": [
        [
          "2010"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-60558-479-9",
    "abstract": "Building semantic models that account for various kinds of indirect reference has traditionally been a difficult problem. Indirect reference can appear in many guises, such as heap pointers, higher-order functions, object references, and shared-memory mutexes. We give a general method to construct models containing indirect reference by presenting a \"theory of indirection\". Our method can be applied in a wide variety of settings and uses only simple, elementary mathematics. In addition to various forms of indirect reference, the resulting models support powerful features such as impredicative quantification and equirecursion; moreover they are compatible with the kind of powerful substructural accounting required to model (higher-order) separation logic. In contrast to previous work, our model is easy to apply to new settings and has a simple axiomatization, which is complete in the sense that all models of it are isomorphic. Our proofs are machine-checked in Coq.",
    "keywords": "indirection theory, step-indexed models",
    "URL": "http://doi.acm.org/10.1145/1706299.1706322",
    "DOI": "10.1145/1706299.1706322",
    "publisher-place": "New York, NY, USA",
    "page": "171-184",
    "page-first": "171",
    "_line": "FormalBib.bib:1423"
  },
  "conchon_increasing_2016": {
    "id": "conchon_increasing_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Conchon",
        "given": "Sylvain"
      },
      {
        "family": "Iguernlala",
        "given": "Mohamed"
      }
    ],
    "editor": [
      {
        "family": "Lecomte",
        "given": "Thierry"
      },
      {
        "family": "Pinger",
        "given": "Ralf"
      },
      {
        "family": "Romanovsky",
        "given": "Alexander"
      }
    ],
    "title": "Increasing Proofs Automation Rate of Atelier-B Thanks to Alt-Ergo",
    "container-title": "Reliability, Safety, and Security of Railway Systems. Modelling, Analysis, Verification, and Certification",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-33951-1",
    "abstract": "In this paper, we report on our recent improvements in the Alt-Ergo SMT solver to make it effective in discharging proof obligations (POs) translated from the Atelier-B framework. In particular, we made important modifications in its internal data structures to boost performances of its core decision procedures, we improved quantifiers instantiation heuristics, and enhanced the interaction between the SAT solver and the decision procedures. We also introduced a new plugin architecture to facilitate experiments with different SAT engines, and implemented a profiling plugin to track and identify “bottlenecks” when a formula requires a long time to be discharged, or makes the solver timeout. Experiments made with more than 10,000 POs generated from real industrial B projects show significant improvements compared to both previous versions of Alt-Ergo and Atelier-B’s automatic main prover.",
    "keywords": "B method, B proof obligations, SMT solvers",
    "page": "243-253",
    "page-first": "243",
    "note": "alt-ergo/Alt-Ergo&ndash;Atelier-B&ndash;RSSR-2016.pdf",
    "language": "en-US",
    "_line": "FormalBib.bib:1441"
  },
  "conchon_alt-ergo_2018": {
    "id": "conchon_alt-ergo_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Conchon",
        "given": "Sylvain"
      },
      {
        "family": "Coquereau",
        "given": "Albin"
      },
      {
        "family": "Iguernlala",
        "given": "Mohamed"
      },
      {
        "family": "Mebsout",
        "given": "Alain"
      }
    ],
    "title": "Alt-Ergo 2.2",
    "container-title": "SMT Workshop: International Workshop on Satisfiability Modulo Theories",
    "issued": {
      "date-parts": [
        [
          "2018",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Alt-Ergo is an SMT solver jointly developed by Université Paris-Sud and the OCamlPro company. The first version was released in 2006. Since then, its architecture has been continuously adapted for proving formulas generated by software development frameworks. As type systems with polymorphism arise naturally is such platforms, the design of Alt-Ergo has been guided (and constrained) by a native-and non SMT-LIB compliant-input language for a polymorphic first-order logic. In this paper, we present the last version of Alt-Ergo, its architecture and main features. The main recent work is a support for a conservative polymorphic extension of the SMT-LIB 2 standard. We measure Alt-Ergo's performances with this new frontend on a set of benchmarks coming from the deductive program verification systems Frama-C, SPARK 2014, Why3 and Atelier-B, as well as from the SMT-LIB benchmarks library.",
    "URL": "https://hal.inria.fr/hal-01960203",
    "publisher-place": "Oxford, United Kingdom",
    "note": "alt-ergo/Alt-Ergo-2.2&ndash;SMT-Workshop-2018.pdf",
    "_line": "FormalBib.bib:1458"
  },
  "altenkirch_quotient_2018": {
    "id": "altenkirch_quotient_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Altenkirch",
        "given": "Thorsten"
      },
      {
        "family": "Capriotti",
        "given": "Paolo"
      },
      {
        "family": "Dijkstra",
        "given": "Gabe"
      },
      {
        "family": "Kraus",
        "given": "Nicolai"
      },
      {
        "family": "Forsberg",
        "given": "Fredrik Nordvall"
      }
    ],
    "title": "Quotient inductive-inductive types",
    "container-title": "arXiv:1612.02346 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Higher inductive types (HITs) in Homotopy Type Theory (HoTT) allow the definition of datatypes which have constructors for equalities over the defined type. HITs generalise quotient types and allow to define types which are not sets in the sense of HoTT (i.e. do not satisfy uniqueness of equality proofs) such as spheres, suspensions and the torus. However, there are also interesting uses of HITs to define sets, such as the Cauchy reals, the partiality monad, and the internal, total syntax of type theory. In each of these examples we define several types that depend on each other mutually, i.e. they are inductive-inductive definitions. We call those HITs quotient inductive-inductive types (QIITs). Although there has been recent progress on the general theory of HITs, there isn't yet a theoretical foundation of the combination of equality constructors and induction-induction, despite having many interesting applications. In the present paper we present a first step towards a semantic definition of QIITs. In particular, we give an initial-algebra semantics and show that this is equivalent to the section induction principle, which justifies the intuitively expected elimination rules.",
    "keywords": "Computer Science - Logic in Computer Science, 03B15 (Primary) 18C10 (Secondary)",
    "URLtext": "1612.02346,",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1612.02346,",
    "URL": "http://arxiv.org/abs/1612.02346",
    "DOI": "10.1007/978-3-319-89366-2_16",
    "page": "293-310",
    "page-first": "293",
    "volume": "10803",
    "note": "altenkirch/Quotient&underscore;inductive-inductive&underscore;types.pdf",
    "_line": "FormalBib.bib:1471"
  },
  "hritcu_micro-policies:_2015": {
    "id": "hritcu_micro-policies:_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hriţcu",
        "given": "Cǎtǎlin"
      }
    ],
    "title": "Micro-Policies: Formally Verified, Tag-Based Security Monitors",
    "container-title": "Proceedings of the 10th ACM Workshop on Programming Languages and Analysis for Security - PLAS'15",
    "container-title-short": "Micro-Policies",
    "title-short": "Micro-Policies",
    "event-title": "the 10th ACM Workshop",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "ACM Press",
    "isbn": "978-1-4503-3661-1",
    "URL": "http://dl.acm.org/citation.cfm?doid=2786558.2786560",
    "DOI": "10.1145/2786558.2786560",
    "publisher-place": "Prague, Czech Republic",
    "page": "1-1",
    "page-first": "1",
    "note": "amorim/nicro-policies.pdf",
    "language": "en-US",
    "_line": "FormalBib.bib:1489"
  },
  "qureshi_formal_nodate": {
    "id": "qureshi_formal_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Qureshi",
        "given": "Zahid H"
      }
    ],
    "title": "Formal Modelling and Analysis of Mission-Critical Software in Military Avionics Systems",
    "container-title": "11th Australian Workshop on Safety Related Programmable Systems (SCS’06)",
    "abstract": "A typical avionics mission system of a military aircraft is a complex real-time system consisting of a mission control computer, different kinds of sensors, navigation and communication subsystems, and various displays and stores; all interconnected by a number of serial data buses. The mission capability is increasingly implemented in the mission-critical software and the robustness of this software is vital for mission success. The complexity and real-time requirements of mission systems represent major challenges to the Australian Defence Force during new acquisitions, upgrades and maintenance. This paper describes the experiences on a joint research project between the University of South Australia and Australia’s Defence Science and Technology Organisation into the modelling and analysis of avionics mission systems. The paper provides a summary of the key aspects of our previous research work on the modelling of a generic mission system using Coloured Petri Nets and the analysis of task scheduling on the mission computer. Finally, the paper briefly discusses the extension of the generic model to obtain a formal model of the mission system of the AP3C Orion maritime surveillance aircraft..",
    "URL": "http://crpit.com/confpapers/CRPITV69Qureshi.pdf",
    "page": "11",
    "page-first": "11",
    "language": "en-US",
    "_line": "FormalBib.bib:1507"
  },
  "ohearn_categorical_2015": {
    "id": "ohearn_categorical_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "O'Hearn",
        "given": "Peter"
      }
    ],
    "title": "From Categorical Logic to Facebook Engineering",
    "container-title": "Proceedings of the 2015 30th Annual ACM/IEEE Symposium on Logic in Computer Science (LICS)",
    "collection-title": "LICS '15",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "IEEE Computer Society",
    "isbn": "978-1-4799-8875-4",
    "URL": "https://doi.org/10.1109/LICS.2015.11",
    "DOI": "10.1109/LICS.2015.11",
    "publisher-place": "Washington, DC, USA",
    "page": "17-20",
    "page-first": "17",
    "_line": "FormalBib.bib:1518"
  },
  "brookes_semantics_2007": {
    "id": "brookes_semantics_2007",
    "type": "article-journal",
    "author": [
      {
        "family": "Brookes",
        "given": "Stephen"
      }
    ],
    "title": "A semantics for concurrent separation logic",
    "container-title": "Theoretical Computer Science",
    "issued": {
      "date-parts": [
        [
          "2007",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "03043975",
    "URL": "https://linkinghub.elsevier.com/retrieve/pii/S0304397506009248",
    "DOI": "10.1016/j.tcs.2006.12.034",
    "page": "227-270",
    "page-first": "227",
    "volume": "375",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:1534"
  },
  "brookes_concurrent_2016": {
    "id": "brookes_concurrent_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Brookes",
        "given": "Stephen"
      },
      {
        "family": "O'Hearn",
        "given": "Peter W."
      }
    ],
    "title": "Concurrent Separation Logic",
    "container-title": "ACM SIGLOG News",
    "issued": {
      "date-parts": [
        [
          "2016",
          "8"
        ]
      ]
    },
    "issn": "2372-3491",
    "URL": "http://doi.acm.org/10.1145/2984450.2984457",
    "DOI": "10.1145/2984450.2984457",
    "page": "47-65",
    "page-first": "47",
    "volume": "3",
    "issue": "3",
    "_line": "FormalBib.bib:1550"
  },
  "ohearn_continuous_2018": {
    "id": "ohearn_continuous_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "O'Hearn",
        "given": "Peter W."
      }
    ],
    "title": "Continuous Reasoning: Scaling the impact of formal methods",
    "container-title": "Proceedings of the 33rd Annual ACM/IEEE Symposium on Logic in Computer Science  - LICS '18",
    "container-title-short": "Continuous Reasoning",
    "title-short": "Continuous Reasoning",
    "event-title": "the 33rd Annual ACM/IEEE Symposium",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "ACM Press",
    "isbn": "978-1-4503-5583-4",
    "URL": "http://dl.acm.org/citation.cfm?doid=3209108.3209109",
    "DOI": "10.1145/3209108.3209109",
    "publisher-place": "Oxford, United Kingdom",
    "page": "13-25",
    "page-first": "13",
    "language": "en-US",
    "_line": "FormalBib.bib:1563"
  },
  "gorogiannis_true_2019": {
    "id": "gorogiannis_true_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Gorogiannis",
        "given": "Nikos"
      },
      {
        "family": "O'Hearn",
        "given": "Peter W."
      },
      {
        "family": "Sergey",
        "given": "Ilya"
      }
    ],
    "title": "A true positives theorem for a static race detector",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "24751421",
    "URL": "http://dl.acm.org/citation.cfm?doid=3302515.3290370",
    "DOI": "10.1145/3290370",
    "page": "1-29",
    "page-first": "1",
    "volume": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:1581"
  },
  "jung_rustbelt:_2017": {
    "id": "jung_rustbelt:_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Jung",
        "given": "Ralf"
      },
      {
        "family": "Jourdan",
        "given": "Jacques-Henri"
      },
      {
        "family": "Krebbers",
        "given": "Robbert"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      }
    ],
    "title": "RustBelt: securing the foundations of the rust programming language",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "RustBelt",
    "title-short": "RustBelt",
    "issued": {
      "date-parts": [
        [
          "2017",
          "12",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "24751421",
    "URL": "http://dl.acm.org/citation.cfm?doid=3177123.3158154",
    "DOI": "10.1145/3158154",
    "page": "1-34",
    "page-first": "1",
    "volume": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:1597"
  },
  "krishnan_modelling_2018": {
    "id": "krishnan_modelling_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Krishnan",
        "given": "Ranjani"
      },
      {
        "family": "Lalithambika",
        "given": "V R"
      }
    ],
    "title": "Modelling and validating 1553B protocol using the SPIN model checker",
    "container-title": "2018 10th International Conference on Communication Systems &amp; Networks (COMSNETS)",
    "event-title": "2018 10th International Conference on Communication Systems &amp; Networks (COMSNETS)",
    "issued": {
      "date-parts": [
        [
          "2018",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-5386-1182-1",
    "URL": "http://ieeexplore.ieee.org/document/8328247/",
    "DOI": "10.1109/COMSNETS.2018.8328247",
    "publisher-place": "Bengaluru",
    "page": "472-475",
    "page-first": "472",
    "note": "1553B/08328247.pdf",
    "_line": "FormalBib.bib:1614"
  },
  "calcagno_compositional_2011": {
    "id": "calcagno_compositional_2011",
    "type": "article-journal",
    "author": [
      {
        "family": "Calcagno",
        "given": "Cristiano"
      },
      {
        "family": "Distefano",
        "given": "Dino"
      },
      {
        "family": "O’Hearn",
        "given": "Peter W."
      },
      {
        "family": "Yang",
        "given": "Hongseok"
      }
    ],
    "title": "Compositional Shape Analysis by Means of Bi-Abduction",
    "container-title": "Journal of the ACM",
    "container-title-short": "J. ACM",
    "issued": {
      "date-parts": [
        [
          "2011",
          "12",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "26"
        ]
      ]
    },
    "issn": "0004-5411",
    "abstract": "The accurate and efficient treatment of mutable data structures is one of the outstanding problem areas in automatic program verification and analysis. Shape analysis is a form of program analysis that attempts to infer descriptions of the data structures in a program, and to prove that these structures are not misused or corrupted. It is one of the more challenging and expensive forms of program analysis, due to the complexity of aliasing and the need to look arbitrarily deeply into the program heap. This article describes a method of boosting shape analyses by defining a compositional method, where each procedure is analyzed independently of its callers. The analysis algorithm uses a restricted fragment of separation logic, and assigns a collection of Hoare triples to each procedure; the triples provide an over-approximation of data structure usage. Our method brings the usual benefits of compositionality&mdash;increased potential to scale, ability to deal with incomplete programs, graceful way to deal with imprecision&mdash;to shape analysis, for the first time. The analysis rests on a generalized form of abduction (inference of explanatory hypotheses), which we call bi-abduction. Bi-abduction displays abduction as a kind of inverse to the frame problem: it jointly infers anti-frames (missing portions of state) and frames (portions of state not touched by an operation), and is the basis of a new analysis algorithm. We have implemented our analysis and we report case studies on smaller programs to evaluate the quality of discovered specifications, and larger code bases (e.g., sendmail, an imap server, a Linux distribution) to illustrate the level of automation and scalability that we obtain from our compositional method. This article makes number of specific technical contributions on proof procedures and analysis algorithms, but in a sense its more important contribution is holistic: the explanation and demonstration of how a massive increase in automation is possible using abductive inference.",
    "keywords": "static analysis, separation logic, Abstract interpretation, compositionality, program proving",
    "URL": "https://doi.org/10.1145/2049697.2049700",
    "DOI": "10.1145/2049697.2049700",
    "page": "26:1-26:66",
    "page-first": "26",
    "volume": "58",
    "issue": "6",
    "_line": "LanguageTools.bib:744"
  },
  "ishtiaq_bi_2011": {
    "id": "ishtiaq_bi_2011",
    "type": "article-journal",
    "author": [
      {
        "family": "Ishtiaq",
        "given": "Samin"
      },
      {
        "family": "O'Hearn",
        "given": "Peter W."
      }
    ],
    "title": "BI As an Assertion Language for Mutable Data Structures",
    "container-title": "SIGPLAN Not.",
    "issued": {
      "date-parts": [
        [
          "2011",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "0362-1340",
    "abstract": "Reynolds has developed a logic for reasoning about mutable data structures in which the pre- and postconditions are written in an intuitionistic logic enriched with a spatial form of conjunction. We investigate the approach from the point of view of the logic BI of bunched implications of O'Hearn and Pym. We begin by giving a model in which the law of the excluded middle holds, thus showing that the approach is compatible with classical logic. The relationship between the intuitionistic and classical versions of the system is established by a translation, analogous to a translation from intuitionistic logic into the modal logic S4. We also consider the question of completeness of the axioms. BI's spatial implication is used to express weakest preconditions for object-component assignments, and an axiom for allocating a cons cell is shown to be complete under an interpretation of triples that allows a command to be applied to states with dangling pointers. We make this latter a feature, by incorporating an operation, and axiom, for disposing of memory. Finally, we describe a local character enjoyed by specifications in the logic, and show how this enables a class of frame axioms, which say what parts of the heap don't change, to be inferred automatically.",
    "URL": "http://doi.acm.org/10.1145/1988042.1988050",
    "DOI": "10.1145/1988042.1988050",
    "page": "84-96",
    "page-first": "84",
    "volume": "46",
    "issue": "4",
    "_line": "FormalBib.bib:1646"
  },
  "wenzel_isabelle/isar_2018": {
    "id": "wenzel_isabelle/isar_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Wenzel",
        "given": "Markus"
      }
    ],
    "title": "The Isabelle/Isar Reference Manual",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Intelligible semi-automated reasoning (Isar) is a generic approach to readable formal proof documents. It sets out to bridge the semantic gap between any internal notions of proof based on primitive inferences and tactics, and an appropriate level of abstraction for user-level work. The Isar formal proof language has been designed to satisfy quite contradictory requirements, being both &amp;quot;declarative&amp;quot; and immediately &amp;quot;executable&amp;quot;, by virtue of the Isar/VM  interpreter. The Isabelle/Isar system provides an interpreter for the Isar formal proof language. The input may consist either of proper document constructors, or improper auxiliary commands (for diagnostics, exploration etc.). Proof texts consisting of proper elements only admit a purely static reading, thus being intelligible later without requiring dynamic replay that is so typical for traditional proof scripts. Any of the Isabelle/Isar commands may be executed in single-steps, so basically the interpreter has a proof text debugger ..",
    "URL": "https://core.ac.uk/display/22830292",
    "language": "en-US",
    "_line": "FormalBib.bib:1662"
  },
  "paulson_foundation_2000": {
    "id": "paulson_foundation_2000",
    "type": "article-journal",
    "author": [
      {
        "family": "Paulson",
        "given": "Lawrence C."
      }
    ],
    "title": "The Foundation of a Generic Theorem Prover",
    "container-title": "arXiv:cs/9301105",
    "issued": {
      "date-parts": [
        [
          "2000",
          "10",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Isabelle is an interactive theorem prover that supports a variety of logics. It represents rules as propositions (not as functions) and builds proofs by combining rules. These operations constitute a meta-logic (or 'logical framework') in which the object-logics are formalized. Isabelle is now based on higher-order logic &ndash; a precise and well-understood foundation. Examples illustrate use of this meta-logic to formalize logics and proofs. Axioms for first-order logic are shown sound and complete. Backwards proof is formalized by meta-reasoning about object-level entailment. Higher-order logic has several practical advantages over other meta-logics. Many proof techniques are known, such as Huet's higher-order unification procedure.",
    "keywords": "Computer Science - Logic in Computer Science, F.3.1, F.4.1",
    "URLtext": "cs/9301105",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "cs/9301105",
    "URL": "http://arxiv.org/abs/cs/9301105",
    "_line": "FormalBib.bib:1673"
  },
  "jung_iris_2018": {
    "id": "jung_iris_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Jung",
        "given": "Ralf"
      },
      {
        "family": "Krebbers",
        "given": "Robbert"
      },
      {
        "family": "Jourdan",
        "given": "Jacques-Henri"
      },
      {
        "family": "Bizjak",
        "given": "Aleš"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      }
    ],
    "title": "Iris from the ground up: A modular foundation for higher-order concurrent separation logic",
    "container-title": "Journal of Functional Programming",
    "container-title-short": "Iris from the ground up",
    "title-short": "Iris from the ground up",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "0956-7968, 1469-7653",
    "abstract": "Iris is a framework for higher-order concurrent separation logic, which has been implemented in the Coq proof assistant and deployed very effectively in a wide variety of verification projects. Iris was designed with the express goal of simplifying and consolidating the foundations of modern separation logics, but it has evolved over time, and the design and semantic foundations of Iris itself have yet to be fully written down and explained together properly in one place. Here, we attempt to fill this gap, presenting a reasonably complete picture of the latest version of Iris (version 3.1), from first principles and in one coherent narrative.",
    "URL": "https://www.cambridge.org/core/journals/journal-of-functional-programming/article/iris-from-the-ground-up-a-modular-foundation-for-higherorder-concurrent-separation-logic/26301B518CE2C52796BFA12B8BAB5B5F",
    "DOI": "10.1017/S0956796818000151",
    "volume": "28",
    "language": "en-US",
    "_line": "FormalBib.bib:1687"
  },
  "nelson_hyperkernel:_2017": {
    "id": "nelson_hyperkernel:_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Nelson",
        "given": "Luke"
      },
      {
        "family": "Sigurbjarnarson",
        "given": "Helgi"
      },
      {
        "family": "Zhang",
        "given": "Kaiyuan"
      },
      {
        "family": "Johnson",
        "given": "Dylan"
      },
      {
        "family": "Bornholt",
        "given": "James"
      },
      {
        "family": "Torlak",
        "given": "Emina"
      },
      {
        "family": "Wang",
        "given": "Xi"
      }
    ],
    "title": "Hyperkernel: Push-Button Verification of an OS Kernel - Slides",
    "container-title": "Proceedings of the 26th Symposium on Operating Systems Principles  - SOSP '17",
    "container-title-short": "Hyperkernel",
    "title-short": "Hyperkernel",
    "event-title": "the 26th Symposium",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM Press",
    "isbn": "978-1-4503-5085-3",
    "URL": "http://dl.acm.org/citation.cfm?doid=3132747.3132748",
    "DOI": "10.1145/3132747.3132748",
    "publisher-place": "Shanghai, China",
    "page": "252-269",
    "page-first": "252",
    "language": "en-US",
    "_line": "FormalBib.bib:1703"
  },
  "nelson_hyperkernel:_2017-1": {
    "id": "nelson_hyperkernel:_2017-1",
    "type": "paper-conference",
    "author": [
      {
        "family": "Nelson",
        "given": "Luke"
      },
      {
        "family": "Sigurbjarnarson",
        "given": "Helgi"
      },
      {
        "family": "Zhang",
        "given": "Kaiyuan"
      },
      {
        "family": "Johnson",
        "given": "Dylan"
      },
      {
        "family": "Bornholt",
        "given": "James"
      },
      {
        "family": "Torlak",
        "given": "Emina"
      },
      {
        "family": "Wang",
        "given": "Xi"
      }
    ],
    "title": "Hyperkernel: Push-Button Verification of an OS Kernel",
    "container-title": "Proceedings of the 26th Symposium on Operating Systems Principles",
    "container-title-short": "Hyperkernel",
    "collection-title": "SOSP '17",
    "title-short": "Hyperkernel",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-5085-3",
    "abstract": "This paper describes an approach to designing, implementing, and formally verifying the functional correctness of an OS kernel, named Hyperkernel, with a high degree of proof automation and low proof burden. We base the design of Hyperkernel's interface on xv6, a Unix-like teaching operating system. Hyperkernel introduces three key ideas to achieve proof automation: it finitizes the kernel interface to avoid unbounded loops or recursion; it separates kernel and user address spaces to simplify reasoning about virtual memory; and it performs verification at the LLVM intermediate representation level to avoid modeling complicated C semantics. We have verified the implementation of Hyperkernel with the Z3 SMT solver, checking a total of 50 system calls and other trap handlers. Experience shows that Hyperkernel can avoid bugs similar to those found in xv6, and that the verification of Hyperkernel can be achieved with a low proof burden.",
    "URL": "http://doi.acm.org/10.1145/3132747.3132748",
    "DOI": "10.1145/3132747.3132748",
    "publisher-place": "New York, NY, USA",
    "page": "252-269",
    "page-first": "252",
    "_line": "FormalBib.bib:1721"
  },
  "adams_common_2015": {
    "id": "adams_common_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Adams",
        "given": "Mark"
      }
    ],
    "title": "The Common HOL Platform",
    "container-title": "Electronic Proceedings in Theoretical Computer Science",
    "issued": {
      "date-parts": [
        [
          "2015",
          "7",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "2075-2180",
    "abstract": "The Common HOL project aims to facilitate porting source code and proofs between members of the HOL family of theorem provers. At the heart of the project is the Common HOL Platform, which defines a standard HOL theory and API that aims to be compatible with all HOL systems. So far, HOL Light and hol90 have been adapted for conformance, and HOL Zero was originally developed to conform. In this paper we provide motivation for a platform, give an overview of the Common HOL Platform's theory and API components, and show how to adapt legacy systems. We also report on the platform's successful application in the hand-translation of a few thousand lines of source code from HOL Light to HOL Zero.",
    "keywords": "Computer Science - Logic in Computer Science, Computer Science - Digital Libraries",
    "URLtext": "1507.08718",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1507.08718",
    "URL": "http://arxiv.org/abs/1507.08718",
    "DOI": "10.4204/EPTCS.186.6",
    "page": "42-56",
    "page-first": "42",
    "volume": "186",
    "_line": "FormalBib.bib:1739"
  },
  "lahiri_symdiff:_2012": {
    "id": "lahiri_symdiff:_2012",
    "type": "paper-conference",
    "author": [
      {
        "family": "Lahiri",
        "given": "Shuvendu K."
      },
      {
        "family": "Hawblitzel",
        "given": "Chris"
      },
      {
        "family": "Kawaguchi",
        "given": "Ming"
      },
      {
        "family": "Rebêlo",
        "given": "Henrique"
      }
    ],
    "editor": [
      {
        "family": "Madhusudan",
        "given": "P."
      },
      {
        "family": "Seshia",
        "given": "Sanjit A."
      }
    ],
    "title": "SYMDIFF: A Language-Agnostic Semantic Diff Tool for Imperative Programs",
    "container-title": "Computer Aided Verification",
    "container-title-short": "SYMDIFF",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "SYMDIFF",
    "issued": {
      "date-parts": [
        [
          "2012"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-31424-7",
    "abstract": "In this paper, we describe SymDiff, a language-agnostic tool for equivalence checking and displaying semantic (behavioral) differences over imperative programs. The tool operates on an intermediate verification language Boogie, for which translations exist from various source languages such as C, C&hash; and x86. We discuss the tool and the front-end interface to target various source languages. Finally, we provide a brief description of the front-end for C programs.",
    "keywords": "Equivalence Check, Imperative Language, Imperative Program, Source Language, Symbolic Execution",
    "page": "712-717",
    "page-first": "712",
    "language": "en-US",
    "_line": "FormalBib.bib:1757"
  },
  "yang_safe_2011": {
    "id": "yang_safe_2011",
    "type": "article-journal",
    "author": [
      {
        "family": "Yang",
        "given": "Jean"
      },
      {
        "family": "Hawblitzel",
        "given": "Chris"
      }
    ],
    "title": "Safe to the last instruction: automated verification of a type-safe operating system",
    "container-title": "Communications of the ACM",
    "container-title-short": "Safe to the last instruction",
    "title-short": "Safe to the last instruction",
    "issued": {
      "date-parts": [
        [
          "2011",
          "12",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "00010782",
    "URL": "http://dl.acm.org/citation.cfm?doid=2043174.2043197",
    "DOI": "10.1145/2043174.2043197",
    "page": "123",
    "page-first": "123",
    "volume": "54",
    "issue": "12",
    "language": "en-US",
    "_line": "FormalBib.bib:1774"
  },
  "lahiri_automatic_2015": {
    "id": "lahiri_automatic_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Lahiri",
        "given": "Shuvendu K."
      },
      {
        "family": "Sinha",
        "given": "Rohit"
      },
      {
        "family": "Hawblitzel",
        "given": "Chris"
      }
    ],
    "editor": [
      {
        "family": "Kroening",
        "given": "Daniel"
      },
      {
        "family": "Păsăreanu",
        "given": "Corina S."
      }
    ],
    "title": "Automatic Rootcausing for Program Equivalence Failures in Binaries",
    "container-title": "Computer Aided Verification",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-21690-4",
    "abstract": "Equivalence checking of imperative programs has several applications including compiler validation and cross-version verification. Debugging equivalence failures can be tedious for large examples, especially for low-level binary programs. In this paper, we formalize a simple yet precise notion of verifiable rootcause for equivalence failures that leverages semantic similarity between two programs. Unlike existing works on program repair, our definition of rootcause avoids the need for a template of fixes or the need for a complete repair to ensure equivalence. We show progressively weaker checks for detecting rootcauses that can be applicable even when multiple fixes are required to make the two programs equivalent. We provide optimizations based on Maximum Satisfiability (MAXSAT) and binary search to prune the search space of such rootcauses. We have implemented the techniques in SymDiff and provide an evaluation on a set of real-world compiler validation binary benchmarks.",
    "page": "362-379",
    "page-first": "362",
    "language": "en-US",
    "_line": "FormalBib.bib:1791"
  },
  "hawblitzel_automated_2015": {
    "id": "hawblitzel_automated_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hawblitzel",
        "given": "Chris"
      },
      {
        "family": "Petrank",
        "given": "Erez"
      },
      {
        "family": "Qadeer",
        "given": "Shaz"
      },
      {
        "family": "Tasiran",
        "given": "Serdar"
      }
    ],
    "title": "Automated and Modular Refinement Reasoning for Concurrent Programs",
    "container-title": "Computer Aided Verification",
    "event-title": "International Conference on Computer Aided Verification",
    "issued": {
      "date-parts": [
        [
          "2015",
          "7",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "Springer, Cham",
    "abstract": "We present civl, a language and verifier for concurrent programs based on automated and modular refinement reasoning. civlsupports reasoning about a concurrent program at many levels of abstraction....",
    "URL": "https://link.springer.com/chapter/10.1007/978-3-319-21668-3_26",
    "DOI": "10.1007/978-3-319-21668-3_26",
    "page": "449-465",
    "page-first": "449",
    "language": "en-US",
    "_line": "FormalBib.bib:1806"
  },
  "hawblitzel_ironfleet:_2015": {
    "id": "hawblitzel_ironfleet:_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hawblitzel",
        "given": "Chris"
      },
      {
        "family": "Howell",
        "given": "Jon"
      },
      {
        "family": "Kapritsos",
        "given": "Manos"
      },
      {
        "family": "Lorch",
        "given": "Jacob R."
      },
      {
        "family": "Parno",
        "given": "Bryan"
      },
      {
        "family": "Roberts",
        "given": "Michael L."
      },
      {
        "family": "Setty",
        "given": "Srinath"
      },
      {
        "family": "Zill",
        "given": "Brian"
      }
    ],
    "title": "IronFleet: Proving Practical Distributed Systems Correct",
    "container-title": "Proceedings of the 25th Symposium on Operating Systems Principles",
    "container-title-short": "IronFleet",
    "collection-title": "SOSP '15",
    "title-short": "IronFleet",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-3834-9",
    "abstract": "Distributed systems are notorious for harboring subtle bugs. Verification can, in principle, eliminate these bugs a priori, but verification has historically been difficult to apply at full-program scale, much less distributed-system scale. We describe a methodology for building practical and provably correct distributed systems based on a unique blend of TLA-style state-machine refinement and Hoare-logic verification. We demonstrate the methodology on a complex implementation of a Paxos-based replicated state machine library and a lease-based sharded key-value store. We prove that each obeys a concise safety specification, as well as desirable liveness requirements. Each implementation achieves performance competitive with a reference system. With our methodology and lessons learned, we aim to raise the standard for distributed systems from \"tested\" to \"correct.\"",
    "URL": "http://doi.acm.org/10.1145/2815400.2815428",
    "DOI": "10.1145/2815400.2815428",
    "publisher-place": "New York, NY, USA",
    "page": "1-17",
    "page-first": "1",
    "_line": "FormalBib.bib:1822"
  },
  "hawblitzel_ironclad_nodate": {
    "id": "hawblitzel_ironclad_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Hawblitzel",
        "given": "Chris"
      },
      {
        "family": "Howell",
        "given": "Jon"
      },
      {
        "family": "Lorch",
        "given": "Jacob R"
      },
      {
        "family": "Narayan",
        "given": "Arjun"
      },
      {
        "family": "Parno",
        "given": "Bryan"
      },
      {
        "family": "Zhang",
        "given": "Danfeng"
      },
      {
        "family": "Zill",
        "given": "Brian"
      }
    ],
    "title": "Ironclad Apps: End-to-End Security via Automated Full-System Veriﬁcation",
    "abstract": "An Ironclad App lets a user securely transmit her data to a remote machine with the guarantee that every instruction executed on that machine adheres to a formal abstract speciﬁcation of the app’s behavior. This does more than eliminate implementation vulnerabilities such as buffer overﬂows, parsing errors, or data leaks; it tells the user exactly how the app will behave at all times. We provide these guarantees via complete, low-level software veriﬁcation. We then use cryptography and secure hardware to enable secure channels from the veriﬁed software to remote users. To achieve such complete veriﬁcation, we developed a set of new and modiﬁed tools, a collection of techniques and engineering disciplines, and a methodology focused on rapid development of veriﬁed systems software. We describe our methodology, formal results, and lessons we learned from building a full stack of veriﬁed software. That software includes a veriﬁed kernel; veriﬁed drivers; veriﬁed system and crypto libraries including SHA, HMAC, and RSA; and four Ironclad Apps.",
    "page": "18",
    "page-first": "18",
    "language": "en-US",
    "_line": "FormalBib.bib:1840"
  },
  "fournet_deploying_nodate": {
    "id": "fournet_deploying_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Fournet",
        "given": "Cedric"
      },
      {
        "family": "Hawblitzel",
        "given": "Chris"
      },
      {
        "family": "Parno",
        "given": "Bryan"
      },
      {
        "family": "Swamy",
        "given": "Nikhil"
      }
    ],
    "title": "Deploying a Veriﬁed Secure Implementation of the HTTPS Ecosystem",
    "page": "10",
    "page-first": "10",
    "language": "en-US",
    "_line": "FormalBib.bib:1849"
  },
  "hutchison_fresh_2009": {
    "id": "hutchison_fresh_2009",
    "type": "chapter",
    "author": [
      {
        "family": "Dockins",
        "given": "Robert"
      },
      {
        "family": "Hobor",
        "given": "Aquinas"
      },
      {
        "family": "Appel",
        "given": "Andrew W."
      }
    ],
    "editor": [
      {
        "family": "Hu",
        "given": "Zhenjiang"
      }
    ],
    "title": "A Fresh Look at Separation Algebras and Share Accounting",
    "container-title": "Programming Languages and Systems",
    "issued": {
      "date-parts": [
        [
          "2009"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-10671-2 978-3-642-10672-9",
    "abstract": "Separation Algebras serve as models of Separation Logics; Share Accounting allows reasoning about concurrent-read/exclusive-write resources in Separation Logic. In designing a Concurrent Separation Logic and in mechanizing proofs of its soundness, we found previous axiomatizations of separation algebras and previous systems of share accounting to be useful but ﬂawed. We adjust the axioms of separation algebras; we demonstrate an operator calculus for constructing new separation algebras; we present a more powerful system of share accounting with a new, simple model; and we provide a reusable Coq development.",
    "URL": "http://link.springer.com/10.1007/978-3-642-10672-9_13",
    "DOI": "10.1007/978-3-642-10672-9_13",
    "publisher-place": "Berlin, Heidelberg",
    "page": "161-177",
    "page-first": "161",
    "volume": "5904",
    "language": "en-US",
    "_line": "FormalBib.bib:1857"
  },
  "spector-zabusky_total_2018": {
    "id": "spector-zabusky_total_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Spector-Zabusky",
        "given": "Antal"
      },
      {
        "family": "Breitner",
        "given": "Joachim"
      },
      {
        "family": "Rizkallah",
        "given": "Christine"
      },
      {
        "family": "Weirich",
        "given": "Stephanie"
      }
    ],
    "title": "Total Haskell is Reasonable Coq",
    "container-title": "Proceedings of the 7th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "collection-title": "CPP 2018",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-5586-5",
    "abstract": "We would like to use the Coq proof assistant to mechanically verify properties of Haskell programs. To that end, we present a tool, named &lt;tt&gt;hs-to-coq&lt;/tt&gt;, that translates total Haskell programs into Coq programs via a shallow embedding. We apply our tool in three case studies – a lawful &lt;tt&gt;Monad&lt;/tt&gt; instance, “Hutton’s razor”, and an existing data structure library – and prove their correctness. These examples show that this approach is viable: both that &lt;tt&gt;hs-to-coq&lt;/tt&gt; applies to existing Haskell code, and that the output it produces is amenable to verification.",
    "keywords": "Coq, verification, Haskell",
    "URL": "http://doi.acm.org/10.1145/3167092",
    "DOI": "10.1145/3167092",
    "publisher-place": "New York, NY, USA",
    "page": "14-27",
    "page-first": "14",
    "_line": "FormalBib.bib:1878"
  },
  "harrison_hol_2013": {
    "id": "harrison_hol_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Harrison",
        "given": "John"
      }
    ],
    "title": "The HOL Light Theory of Euclidean Space",
    "container-title": "Journal of Automated Reasoning",
    "issued": {
      "date-parts": [
        [
          "2013",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "0168-7433, 1573-0670",
    "URL": "http://link.springer.com/10.1007/s10817-012-9250-9",
    "DOI": "10.1007/s10817-012-9250-9",
    "page": "173-190",
    "page-first": "173",
    "volume": "50",
    "issue": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:1896"
  },
  "harper_framework_1993": {
    "id": "harper_framework_1993",
    "type": "article-journal",
    "author": [
      {
        "family": "Harper",
        "given": "Robert"
      },
      {
        "family": "Honsell",
        "given": "Furio"
      },
      {
        "family": "Plotkin",
        "given": "Gordon"
      }
    ],
    "title": "A Framework for Defining Logics",
    "container-title": "J. ACM",
    "issued": {
      "date-parts": [
        [
          "1993",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "0004-5411",
    "abstract": "The Edinburgh Logical Framework (LF) provides a means to define (or present) logics. It is based on a general treatment of syntax, rules, and proofs by means of a typed &amp;lgr;-calculus with dependent types. Syntax is treated in a style similar to, but more general than, Martin-Lo¨f's system of arities. The treatment of rules and proofs focuses on his notion of a judgment. Logics are represented in LF via a new principle, the judgments as types principle, whereby each judgment is identified with the type of its proofs. This allows for a smooth treatment of discharge and variable occurence conditions and leads to a uniform treatment of rules and proofs whereby rules are viewed as proofs of higher-order judgments and proof checking is reduced to type checking. The practical benefit of our treatment of formal systems is that logic-independent tools, such as proof editors and proof checkers, can be constructed.",
    "keywords": "formal systems, interactive theorem proving, proof checking, typed lambda calculus",
    "URL": "http://doi.acm.org/10.1145/138027.138060",
    "DOI": "10.1145/138027.138060",
    "page": "143-184",
    "page-first": "143",
    "volume": "40",
    "issue": "1",
    "_line": "FormalBib.bib:1912"
  },
  "ramsey_applicative_2006": {
    "id": "ramsey_applicative_2006",
    "type": "article-journal",
    "author": [
      {
        "family": "Ramsey",
        "given": "Norman"
      },
      {
        "family": "Dias",
        "given": "João"
      }
    ],
    "title": "An Applicative Control-Flow Graph Based on Huet's Zipper",
    "container-title": "Electronic Notes in Theoretical Computer Science",
    "container-title-short": "Electronic Notes in Theoretical Computer Science",
    "collection-title": "Proceedings of the ACM-SIGPLAN Workshop on ML (ML 2005)",
    "issued": {
      "date-parts": [
        [
          "2006",
          "3",
          "24"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "1571-0661",
    "abstract": "We are using ML to build a compiler that does low-level optimization. To support optimizations in classic imperative style, we built a control-flow graph using mutable pointers and other mutable state in the nodes. This decision proved unfortunate: the mutable flow graph was big and complex, and it led to many bugs. We have replaced it by a smaller, simpler, applicative flow graph based on Huet's \\[Huet, Gérard, 1997. The Zipper. Journal of Functional Programming, 7(5):549–554. Functional Pearl\\] zipper. The new flow graph is a success; this paper presents its design and shows how it leads to a gratifyingly simple implementation of the dataflow framework developed by \\[Lerner, Sorin, David Grove, and Craig Chambers. 2002. Composing dataflow analyses and transformations. Conference Record of the 29th Annual ACM Symposium on Principles of Programming Languages, in SIGPLAN Notices, 31(1):270–282\\].",
    "keywords": "optimization, applicative data structures, compilers, control-flow graphs, dataflow analysis",
    "URL": "http://www.sciencedirect.com/science/article/pii/S1571066106001289",
    "DOI": "10.1016/j.entcs.2005.11.042",
    "page": "105-126",
    "page-first": "105",
    "volume": "148",
    "issue": "2",
    "_line": "FormalBib.bib:1929"
  },
  "mokhov_algebraic_2017": {
    "id": "mokhov_algebraic_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Mokhov",
        "given": "Andrey"
      }
    ],
    "title": "Algebraic Graphs with Class (Functional Pearl)",
    "container-title": "Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell",
    "collection-title": "Haskell 2017",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-5182-9",
    "abstract": "The paper presents a minimalistic and elegant approach to working with graphs in Haskell. It is built on a rigorous mathematical foundation &mdash; an algebra of graphs &mdash; that allows us to apply equational reasoning for proving the correctness of graph transformation algorithms. Algebraic graphs let us avoid partial functions typically caused by 'malformed graphs' that contain an edge referring to a non-existent vertex. This helps to liberate APIs of existing graph libraries from partial functions.   The algebra of graphs can represent directed, undirected, reflexive and transitive graphs, as well as hypergraphs, by appropriately choosing the set of underlying axioms. The flexibility of the approach is demonstrated by developing a library for constructing and transforming polymorphic graphs.",
    "keywords": "Haskell, algebra, graph theory",
    "URL": "http://doi.acm.org/10.1145/3122955.3122956",
    "DOI": "10.1145/3122955.3122956",
    "publisher-place": "New York, NY, USA",
    "page": "2-13",
    "page-first": "2",
    "_line": "FormalBib.bib:1948"
  },
  "gonthier_how_2011": {
    "id": "gonthier_how_2011",
    "type": "paper-conference",
    "author": [
      {
        "family": "Gonthier",
        "given": "Georges"
      },
      {
        "family": "Ziliani",
        "given": "Beta"
      },
      {
        "family": "Nanevski",
        "given": "Aleksandar"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      }
    ],
    "title": "How to Make Ad Hoc Proof Automation Less Ad Hoc",
    "container-title": "Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming",
    "collection-title": "ICFP '11",
    "issued": {
      "date-parts": [
        [
          "2011"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-0865-6",
    "abstract": "Most interactive theorem provers provide support for some form of user-customizable proof automation. In a number of popular systems, such as Coq and Isabelle, this automation is achieved primarily through tactics, which are programmed in a separate language from that of the prover's base logic. While tactics are clearly useful in practice, they can be difficult to maintain and compose because, unlike lemmas, their behavior cannot be specified within the expressive type system of the prover itself. We propose a novel approach to proof automation in Coq that allows the user to specify the behavior of custom automated routines in terms of Coq's own type system. Our approach involves a sophisticated application of Coq's canonical structures, which generalize Haskell type classes and facilitate a flexible style of dependently-typed logic programming. Specifically, just as Haskell type classes are used to infer the canonical implementation of an overloaded term at a given type, canonical structures can be used to infer the canonical proof of an overloaded lemma for a given instantiation of its parameters. We present a series of design patterns for canonical structure programming that enable one to carefully and predictably coax Coq's type inference engine into triggering the execution of user-supplied algorithms during unification, and we illustrate these patterns through several realistic examples drawn from Hoare Type Theory. We assume no prior knowledge of Coq and describe the relevant aspects of Coq type inference from first principles.",
    "keywords": "interactive theorem proving, canonical structures, coq, custom proof automation, hoare type theory, tactics, type classes",
    "URL": "http://doi.acm.org/10.1145/2034773.2034798",
    "DOI": "10.1145/2034773.2034798",
    "publisher-place": "New York, NY, USA",
    "page": "163-175",
    "page-first": "163",
    "_line": "FormalBib.bib:1966"
  },
  "gonthier_introduction_2010": {
    "id": "gonthier_introduction_2010",
    "type": "article-journal",
    "author": [
      {
        "family": "Gonthier",
        "given": "Georges"
      },
      {
        "family": "Mahboubi",
        "given": "Assia"
      }
    ],
    "title": "An introduction to small scale reflection in Coq",
    "container-title": "Journal of Formalized Reasoning",
    "issued": {
      "date-parts": [
        [
          "2010"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "1972-5787",
    "abstract": "This tutorial presents the SSReflect extension to the Coq system. This extension consists of an extension to the Coq language of script, and of a set of libraries, originating from the formal proof of the Four Color theorem. This tutorial proposes a guided tour in some of the basic libraries distributed in the SSReflect package. It focuses on the application of the small scale reflection methodology to the formalization of finite objects in intuitionistic type theory.",
    "URL": "https://jfr.unibo.it/article/view/1979",
    "DOI": "10.6092/issn.1972-5787/1979",
    "page": "95-152",
    "page-first": "95",
    "volume": "3",
    "issue": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:1984"
  },
  "filinski_representing_1999": {
    "id": "filinski_representing_1999",
    "type": "paper-conference",
    "author": [
      {
        "family": "Filinski",
        "given": "Andrzej"
      }
    ],
    "title": "Representing Layered Monads",
    "container-title": "Proceedings of the 26th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "collection-title": "POPL '99",
    "issued": {
      "date-parts": [
        [
          "1999"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-58113-095-9",
    "abstract": "There has already been considerable research on constructing modular, monad-based specifications of computational effects (state, exceptions, nondeterminism, etc.) in programming languages. We present a simple framework in this tradition, based on a Church-style effect-typing system for an ML-like language. The semantics of this language is formally defined by a series of monadic translations, each one expanding away a layer of effects. Such a layered specification is easy to reason about, but its direct implementation (whether by parameterized interpretation or by actual translation) is often prohibitively inefficient.By exploiting deeper semantic properties of monads, however, it is also possible to derive a vastly more efficient implementation: we show that each layer of effects can be uniformly simulated by continuation-passing, and further that multiple such layers can themselves be simulated by a standard semantics for call/cc and mutable state. Thus, even multi-effect programs can be executed in Scheme or SML/NJ at full native speed, generalizing an earlier single-effect result. As an example, we show how a simple resumption-based semantics of concurrency allows us to directly simulate a shared-state program across all possible dynamic interleavings of execution threads.",
    "URL": "http://doi.acm.org/10.1145/292540.292557",
    "DOI": "10.1145/292540.292557",
    "publisher-place": "New York, NY, USA",
    "page": "175-188",
    "page-first": "175",
    "_line": "FormalBib.bib:2002"
  },
  "filinski_representing_1994": {
    "id": "filinski_representing_1994",
    "type": "paper-conference",
    "author": [
      {
        "family": "Filinski",
        "given": "Andrzej"
      }
    ],
    "title": "Representing Monads",
    "container-title": "Proceedings of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "collection-title": "POPL '94",
    "issued": {
      "date-parts": [
        [
          "1994"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-0-89791-636-3",
    "abstract": "We show that any monad whose unit and extension operations are expressible as purely functional terms can be embedded in a call-by-value language with “composable continuations”. As part of the development, we extend Meyer and Wand's characterization of the relationship between continuation-passing and direct style to one for continuation-passing vs. general “monadic” style. We further show that the composable-continuations construct can itself be represented using ordinary, non-composable first-class continuations and a single piece of state. Thus, in the presence of two specific computational effects - storage and escapes - any expressible monadic structure (e.g., nondeterminism as represented by the list monad) can be added as a purely definitional extension, without requiring a reinterpretation of the whole language. The paper includes an implementation of the construction (in Standard ML with some New Jersey extensions) and several examples.",
    "URL": "http://doi.acm.org/10.1145/174675.178047",
    "DOI": "10.1145/174675.178047",
    "publisher-place": "New York, NY, USA",
    "page": "446-457",
    "page-first": "446",
    "_line": "FormalBib.bib:2019"
  },
  "ahman_recalling_2017": {
    "id": "ahman_recalling_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Ahman",
        "given": "Danel"
      },
      {
        "family": "Fournet",
        "given": "Cédric"
      },
      {
        "family": "Hriţcu",
        "given": "Cătălin"
      },
      {
        "family": "Maillard",
        "given": "Kenji"
      },
      {
        "family": "Rastogi",
        "given": "Aseem"
      },
      {
        "family": "Swamy",
        "given": "Nikhil"
      }
    ],
    "title": "Recalling a Witness: Foundations and Applications of Monotonic State",
    "container-title": "Proc. ACM Program. Lang.",
    "container-title-short": "Recalling a Witness",
    "title-short": "Recalling a Witness",
    "issued": {
      "date-parts": [
        [
          "2017",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "We provide a way to ease the verification of programs whose state evolves monotonically. The main idea is that a property witnessed in a prior state can be soundly recalled in the current state, provided (1) state evolves according to a given preorder, and (2) the property is preserved by this preorder. In many scenarios, such monotonic reasoning yields concise modular proofs, saving the need for explicit program invariants. We distill our approach into the monotonic-state monad, a general yet compact interface for Hoare-style reasoning about monotonic state in a dependently typed language. We prove the soundness of the monotonic-state monad and use it as a unified foundation for reasoning about monotonic state in the F⋆ verification system. Based on this foundation, we build libraries for various mutable data structures like monotonic references and apply these libraries at scale to the verification of several distributed applications.",
    "keywords": "Formal Foundations, Hoare Logic, Modular Reasoning, Monotonic References, Monotonic-State Monad, Program Verification, Secure File Transfer, State Continuity",
    "URL": "http://doi.acm.org/10.1145/3158153",
    "DOI": "10.1145/3158153",
    "page": "65:1-65:30",
    "page-first": "65",
    "volume": "2",
    "_line": "FormalBib.bib:2036"
  },
  "hritcu_quest_nodate": {
    "id": "hritcu_quest_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Hriţcu",
        "given": "Cătălin"
      }
    ],
    "title": "The Quest for Formally Secure Compartmentalizing Compilation",
    "abstract": "Severe low-level vulnerabilities abound in today’s computer systems, allowing cyber-attackers to remotely gain full control. This happens in big part because our programming languages, compilation chains, and architectures too often trade o security for e ciency. The semantics of mainstream low-level languages like C is inherently insecure, and even for safer languages, all guarantees are lost when interacting with low-level code, for instance when using low-level libraries. This habilitation presents my ongoing quest to build formally secure compartmentalizing compilation chains that defend against such attacks. In particular, we propose several formal de nitions that characterize what it means for a compartmentalizing compilation chain to be secure, both in the case of safe and of unsafe source languages.",
    "page": "96",
    "page-first": "96",
    "language": "en-US",
    "_line": "FormalBib.bib:2054"
  },
  "ahman_dijkstra_2017": {
    "id": "ahman_dijkstra_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Ahman",
        "given": "Danel"
      },
      {
        "family": "Hriţcu",
        "given": "Cătălin"
      },
      {
        "family": "Maillard",
        "given": "Kenji"
      },
      {
        "family": "Martínez",
        "given": "Guido"
      },
      {
        "family": "Plotkin",
        "given": "Gordon"
      },
      {
        "family": "Protzenko",
        "given": "Jonathan"
      },
      {
        "family": "Rastogi",
        "given": "Aseem"
      },
      {
        "family": "Swamy",
        "given": "Nikhil"
      }
    ],
    "title": "Dijkstra Monads for Free",
    "container-title": "Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages",
    "collection-title": "POPL 2017",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-4660-3",
    "abstract": "Dijkstra monads enable a dependent type theory to be enhanced with support for specifying and verifying effectful code via weakest preconditions. Together with their closely related counterparts, Hoare monads, they provide the basis on which verification tools like F\\*, Hoare Type Theory (HTT), and Ynot are built. We show that Dijkstra monads can be derived \"for free\" by applying a continuation-passing style (CPS) translation to the standard monadic definitions of the underlying computational effects. Automatically deriving Dijkstra monads in this way provides a correct-by-construction and efficient way of reasoning about user-defined effects in dependent type theories. We demonstrate these ideas in EMF\\*, a new dependently typed calculus, validating it via both formal proof and a prototype implementation within F\\*. Besides equipping F\\* with a more uniform and extensible effect system, EMF\\* enables a novel mixture of intrinsic and extrinsic proofs within F\\*.",
    "keywords": "proof assistants, verification, dependent types, effectful programming",
    "URL": "http://doi.acm.org/10.1145/3009837.3009878",
    "DOI": "10.1145/3009837.3009878",
    "publisher-place": "New York, NY, USA",
    "page": "515-529",
    "page-first": "515",
    "_line": "FormalBib.bib:2063"
  },
  "swamy_verifying_2013": {
    "id": "swamy_verifying_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Swamy",
        "given": "Nikhil"
      },
      {
        "family": "Chen",
        "given": "Juan"
      },
      {
        "family": "Livshits",
        "given": "Ben"
      }
    ],
    "title": "Verifying Higher-order Programs with the Dijkstra Monad",
    "issued": {
      "date-parts": [
        [
          "2013",
          "6",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Modern programming languages, ranging from Haskell and ML, to JavaScript, C&hash; and Java, all make extensive use of higher-order state. This paper advocates a new verification methodology for higher-order stateful programs, based on a new monad of predicate transformers called the Dijkstra monad. Using the Dijkstra monad has a number of benefits. First, the monad …",
    "URL": "https://www.microsoft.com/en-us/research/publication/verifying-higher-order-programs-with-the-dijkstra-monad/",
    "language": "en-US",
    "_line": "FormalBib.bib:2081"
  },
  "dijkstra_guarded_1975": {
    "id": "dijkstra_guarded_1975",
    "type": "article-journal",
    "author": [
      {
        "family": "Dijkstra",
        "given": "Edsger W."
      }
    ],
    "title": "Guarded Commands, Nondeterminacy and Formal Derivation of Programs",
    "container-title": "Commun. ACM",
    "issued": {
      "date-parts": [
        [
          "1975",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "0001-0782",
    "abstract": "So-called “guarded commands” are introduced as a building block for alternative and repetitive constructs that allow nondeterministic program components for which at least the activity evoked, but possibly even the final state, is not necessarily uniquely determined by the initial state. For the formal derivation of programs expressed in terms of these constructs, a calculus will be be shown.",
    "keywords": "case-construction, correctness proof, derivation of programs, nondeterminancy, program semantics, programming language semantics, programming languages, programming methodology, repetition, sequencing primitives, termination",
    "URL": "http://doi.acm.org/10.1145/360933.360975",
    "DOI": "10.1145/360933.360975",
    "page": "453-457",
    "page-first": "453",
    "volume": "18",
    "issue": "8",
    "_line": "FormalBib.bib:2092"
  },
  "blatter_static_2018": {
    "id": "blatter_static_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Blatter",
        "given": "Lionel"
      },
      {
        "family": "Kosmatov",
        "given": "Nikolai"
      },
      {
        "family": "Le Gall",
        "given": "Pascale"
      },
      {
        "family": "Prevosto",
        "given": "Virgile"
      },
      {
        "family": "Petiot",
        "given": "Guillaume"
      }
    ],
    "editor": [
      {
        "family": "Dubois",
        "given": "Catherine"
      },
      {
        "family": "Wolff",
        "given": "Burkhart"
      }
    ],
    "title": "Static and Dynamic Verification of Relational Properties on Self-composed C Code",
    "container-title": "Tests and Proofs",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-92994-1",
    "abstract": "Function contracts are a well-established way of formally specifying the intended behavior of a function. However, they usually only describe what should happen during a single call. Relational properties, on the other hand, link several function calls. They include such properties as non-interference, continuity and monotonicity. Other examples relate sequences of function calls, for instance, to show that decrypting an encrypted message with the appropriate key gives back the original message. Such properties cannot be expressed directly in the traditional setting of modular deductive verification, but are amenable to verification through self-composition. This paper presents a verification technique dedicated to relational properties in C programs and its implementation in the form of a Frama-C plugin called RPP and based on self-composition. It supports functions with side effects and recursive functions. The proposed approach makes it possible to prove a relational property, to check it at runtime, to generate a counterexample using testing and to use it as a hypothesis in the subsequent verification. Our initial experiments on existing benchmarks confirm that the proposed technique is helpful for static and dynamic analysis of relational properties.",
    "keywords": "Deductive verification, Dynamic verification, Frama-C, Relational properties, Self-composition, Specification",
    "page": "44-62",
    "page-first": "44",
    "language": "en-US",
    "_line": "FormalBib.bib:2109"
  },
  "petiot_your_2015": {
    "id": "petiot_your_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Petiot",
        "given": "Guillaume"
      },
      {
        "family": "Kosmatov",
        "given": "Nikolai"
      },
      {
        "family": "Botella",
        "given": "Bernard"
      },
      {
        "family": "Giorgetti",
        "given": "Alain"
      },
      {
        "family": "Julliand",
        "given": "Jacques"
      }
    ],
    "title": "Your Proof Fails? Testing Helps to Find the Reason",
    "container-title": "arXiv:1508.01691 \\[cs\\]",
    "container-title-short": "Your Proof Fails?",
    "title-short": "Your Proof Fails?",
    "issued": {
      "date-parts": [
        [
          "2015",
          "8",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Applying deductive verification to formally prove that a program respects its formal specification is a very complex and time-consuming task due in particular to the lack of feedback in case of proof failures. Along with a non-compliance between the code and its specification (due to an error in at least one of them), possible reasons of a proof failure include a missing or too weak specification for a called function or a loop, and lack of time or simply incapacity of the prover to finish a particular proof. This work proposes a new methodology where test generation helps to identify the reason of a proof failure and to exhibit a counter-example clearly illustrating the issue. We describe how to transform an annotated C program into C code suitable for testing and illustrate the benefits of the method on comprehensive examples. The method has been implemented in STADY, a plugin of the software analysis platform FRAMA-C. Initial experiments show that detecting non-compliances and contract weaknesses allows to precisely diagnose most proof failures.",
    "keywords": "D.2.4, Computer Science - Software Engineering, D.2.5",
    "URLtext": "1508.01691",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1508.01691",
    "URL": "http://arxiv.org/abs/1508.01691",
    "_line": "FormalBib.bib:2125"
  },
  "petiot_how_2018": {
    "id": "petiot_how_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Petiot",
        "given": "Guillaume"
      },
      {
        "family": "Kosmatov",
        "given": "Nikolai"
      },
      {
        "family": "Botella",
        "given": "Bernard"
      },
      {
        "family": "Giorgetti",
        "given": "Alain"
      },
      {
        "family": "Julliand",
        "given": "Jacques"
      }
    ],
    "title": "How testing helps to diagnose proof failures",
    "container-title": "Formal Aspects of Computing",
    "container-title-short": "Form Asp Comp",
    "issued": {
      "date-parts": [
        [
          "2018",
          "11",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "1433-299X",
    "abstract": "Applying deductive verification to formally prove that a program respects its formal specification is a very complex and time-consuming task due in particular to the lack of feedback in case of proof failures. Along with a non-compliance between the code and its specification (due to an error in at least one of them), possible reasons of a proof failure include a missing or too weak specification for a called function or a loop, and lack of time or simply incapacity of the prover to finish a particular proof. This work proposes a methodology where test generation helps to identify the reason of a proof failure and to exhibit a counterexample clearly illustrating the issue. We define the categories of proof failures, introduce two subcategories of contract weaknesses (single and global ones), and examine their properties. We describe how to transform a C program formally specified in an executable specification language into C code suitable for testing, and illustrate the benefits of the method on comprehensive examples. The method has been implemented in StaDy, a plugin of the software analysis platform Frama-C. Initial experiments show that detecting non-compliances and contract weaknesses allows to precisely diagnose most proof failures.",
    "keywords": "Deductive verification, Frama-C, Specification, Proof debugging, Test generation",
    "URL": "https://doi.org/10.1007/s00165-018-0456-4",
    "DOI": "10.1007/s00165-018-0456-4",
    "page": "629-657",
    "page-first": "629",
    "volume": "30",
    "issue": "6",
    "language": "en-US",
    "_line": "FormalBib.bib:2140"
  },
  "blanchard_concurrent_2017": {
    "id": "blanchard_concurrent_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Blanchard",
        "given": "Allan"
      },
      {
        "family": "Loulergue",
        "given": "Frédéric"
      },
      {
        "family": "Kosmatov",
        "given": "Nikolai"
      }
    ],
    "title": "From Concurrent Programs to Simulating Sequential Programs: Correctness of a Transformation",
    "container-title": "Electronic Proceedings in Theoretical Computer Science",
    "container-title-short": "From Concurrent Programs to Simulating Sequential Programs",
    "title-short": "From Concurrent Programs to Simulating Sequential Programs",
    "issued": {
      "date-parts": [
        [
          "2017",
          "8",
          "23"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "2075-2180",
    "abstract": "Frama-C is a software analysis framework that provides a common infrastructure and a common behavioral specification language to plugins that implement various static and dynamic analyses of C programs. Most plugins do not support concurrency. We have proposed Conc2Seq, a Frama-C plugin based on program transformation, capable to leverage the existing huge code base of plugins and to handle concurrent C programs. In this paper we formalize and sketch the proof of correctness of the program transformation principle behind Conc2Seq, and present an effort towards the full mechanization of both the formalization and proofs with the proof assistant Coq.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "1708.07226",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1708.07226",
    "URL": "http://arxiv.org/abs/1708.07226",
    "DOI": "10.4204/EPTCS.253.9",
    "page": "109-123",
    "page-first": "109",
    "volume": "253",
    "_line": "FormalBib.bib:2159"
  },
  "brahmi_formalise_nodate": {
    "id": "brahmi_formalise_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Brahmi",
        "given": "Abderrahmane"
      },
      {
        "family": "Delmas",
        "given": "David"
      },
      {
        "family": "Essoussi",
        "given": "Mohamed Habib"
      },
      {
        "family": "Randimbivololona",
        "given": "Famantanantsoa"
      },
      {
        "family": "Informatics",
        "given": "CEPRESY"
      },
      {
        "family": "Nauzere",
        "given": "La"
      },
      {
        "family": "Atki",
        "given": "Abdellatif"
      },
      {
        "family": "Marie",
        "given": "Thomas"
      }
    ],
    "title": "Formalise to automate: deployment of a safe and cost-efﬁcient process for avionics software -Extended",
    "abstract": "For over a decade, Airbus have been introducing formal techniques into the veriﬁcation processes of some of their avionics software products, to cope with the steady increase of the size and complexity of related avionics systems. These techniques have come of age for large-scale industrial deployment. All design and veriﬁcation processes are currently being revised to take maximum advantage from them, i.e. improve industrial efﬁciency while maintaining the safety and reliability of avionics systems.",
    "page": "17",
    "page-first": "17",
    "language": "en-US",
    "_line": "FormalBib.bib:2178"
  },
  "brahmi_formalise_2018": {
    "id": "brahmi_formalise_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Brahmi",
        "given": "Abderrahmane"
      },
      {
        "family": "Delmas",
        "given": "David"
      },
      {
        "family": "Essoussi",
        "given": "Mohamed Habib"
      },
      {
        "family": "Randimbivololona",
        "given": "Famantanantsoa"
      },
      {
        "family": "Atki",
        "given": "Abdellatif"
      },
      {
        "family": "Marie",
        "given": "Thomas"
      }
    ],
    "title": "Formalise to automate: deployment of a safe and cost-efficient process for avionics software",
    "container-title": "9th European Congress on Embedded Real Time Software and Systems (ERTS 2018)",
    "container-title-short": "Formalise to automate",
    "title-short": "Formalise to automate",
    "issued": {
      "date-parts": [
        [
          "2018",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "For over a decade, Airbus have been introducing formal techniques into the verification processes of some of their avionics software products, to cope with the steady increase of the size and complexity of related avionics systems. These techniques have come of age for large-scale industrial deployment. All design and verification processes are currently being revised to take maximum advantage from them, i.e. improve industrial efficiency while maintaining the safety and reliability of avionics systems. To achieve this goal, all human-engineered design artefacts are being formalised using languages with well-defined syntaxes and semantics, in order to allow for the automatic generation of all subsequent, computable design or verification artefacts, and the preparation of the input data for non computable activities. To this aim, several domain-specific languages and related compilers have been developed internally, which cover all design activities, and bridge the gaps to integrate external tools into the overall development processes, e.g. sound, semantics-based, static analysis tools. For instance, the formalisation of detailed designs in the form of function contracts expressed in a first-order logic-based language allows for a hybrid approach to unit verification. Designs may be compiled down to ACSL \\[5\\] contracts, allowing for program proof with Frama-C \\[22\\], or they may be compiled down to test contracts, allowing for semi-automatic unit tests.",
    "keywords": "static analysis, domain-specific languages, avionics software, compilation, design, development process, DO-178C, formal methods, formalisation, industrial application",
    "URL": "https://hal.archives-ouvertes.fr/hal-01708332",
    "publisher-place": "Toulouse, France",
    "_line": "FormalBib.bib:2187"
  },
  "delignat-lavaud_implementing_2017": {
    "id": "delignat-lavaud_implementing_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Delignat-Lavaud",
        "given": "Antoine"
      },
      {
        "family": "Fournet",
        "given": "Cédric"
      },
      {
        "family": "Kohlweiss",
        "given": "Markulf"
      },
      {
        "family": "Protzenko",
        "given": "Jonathan"
      },
      {
        "family": "Rastogi",
        "given": "Aseem"
      },
      {
        "family": "Swamy",
        "given": "Nikhil"
      },
      {
        "family": "Zanella-Beguelin",
        "given": "Santiago"
      },
      {
        "family": "Bhargavan",
        "given": "Karthikeyan"
      },
      {
        "family": "Pan",
        "given": "Jianyang"
      },
      {
        "family": "Zinzindohoue",
        "given": "Jean Karim"
      }
    ],
    "title": "Implementing and Proving the TLS 1.3 Record Layer",
    "issued": {
      "date-parts": [
        [
          "2017",
          "8",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "The record layer is the main bridge between TLS applications and internal sub-protocols. Its core functionality is an elaborate form of authenticated encryption: streams of messages for each sub-protocol (handshake, alert, and application data) are fragmented, multiplexed, and encrypted with optional padding to hide their lengths. Conversely, the sub-protocols may provide fresh keys or signal …",
    "URL": "https://www.microsoft.com/en-us/research/publication/implementing-proving-tls-1-3-record-layer/",
    "language": "en-US",
    "_line": "FormalBib.bib:2201"
  },
  "parigot_tactic_2000": {
    "id": "parigot_tactic_2000",
    "type": "chapter",
    "author": [
      {
        "family": "Delahaye",
        "given": "David"
      }
    ],
    "editor": [
      {
        "family": "Parigot",
        "given": "Michel"
      },
      {
        "family": "Voronkov",
        "given": "Andrei"
      }
    ],
    "title": "A Tactic Language for the System Coq",
    "container-title": "Logic for Programming and Automated Reasoning",
    "issued": {
      "date-parts": [
        [
          "2000"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-540-41285-4",
    "URL": "http://link.springer.com/10.1007/3-540-44404-1_7",
    "DOI": "10.1007/3-540-44404-1_7",
    "publisher-place": "Berlin, Heidelberg",
    "page": "85-95",
    "page-first": "85",
    "volume": "1955",
    "language": "en-US",
    "_line": "FormalBib.bib:2212"
  },
  "ekici_smtcoq:_2017": {
    "id": "ekici_smtcoq:_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Ekici",
        "given": "Burak"
      },
      {
        "family": "Mebsout",
        "given": "Alain"
      },
      {
        "family": "Tinelli",
        "given": "Cesare"
      },
      {
        "family": "Keller",
        "given": "Chantal"
      },
      {
        "family": "Katz",
        "given": "Guy"
      },
      {
        "family": "Reynolds",
        "given": "Andrew"
      },
      {
        "family": "Barrett",
        "given": "Clark"
      }
    ],
    "editor": [
      {
        "family": "Majumdar",
        "given": "Rupak"
      },
      {
        "family": "Kunčak",
        "given": "Viktor"
      }
    ],
    "title": "SMTCoq: A Plug-In for Integrating SMT Solvers into Coq",
    "container-title": "Computer Aided Verification",
    "container-title-short": "SMTCoq",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "SMTCoq",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-63390-9",
    "abstract": "This paper describes SMTCoq, a plug-in for the integration of external solvers into the Coq proof assistant. Based on a checker for generic first-order proof certificates fully implemented and proved correct in Coq, SMTCoq offers facilities to check answers from external SAT and SMT solvers and to increase Coq’s automation using such solvers, all in a safe way. The current version supports proof certificates produced by the SAT solver ZChaff, for propositional logic, and the SMT solvers veriT and CVC4, for the quantifier-free fragment of the combined theory of fixed-size bit vectors, functional arrays with extensionality, linear integer arithmetic, and uninterpreted function symbols.",
    "page": "126-133",
    "page-first": "126",
    "language": "en-US",
    "_line": "FormalBib.bib:2230"
  },
  "hunt_warren_a._industrial_2017": {
    "id": "hunt_warren_a._industrial_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Hunt Warren A."
      },
      {
        "family": "Kaufmann Matt"
      },
      {
        "family": "Moore J Strother"
      },
      {
        "family": "Slobodova Anna"
      }
    ],
    "title": "Industrial hardware and software verification with ACL2",
    "container-title": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "container-title-short": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "issued": {
      "date-parts": [
        [
          "2017",
          "10",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "The ACL2 theorem prover has seen sustained industrial use since the mid-1990s. Companies that have used ACL2 regularly include AMD, Centaur Technology, IBM, Intel, Kestrel Institute, Motorola/Freescale, Oracle and Rockwell Collins. This paper introduces ACL2 and focuses on how and why ACL2 is used in industry. ACL2 is well-suited to its industrial application to numerous software and hardware systems, because it is an integrated programming/proof environment supporting a subset of the ANSI standard Common Lisp programming language. As a programming language ACL2 permits the coding of efficient and robust programs; as a prover ACL2 can be fully automatic but provides many features permitting domain-specific human-supplied guidance at various levels of abstraction. ACL2 specifications and models often serve as efficient execution engines for the modelled artefacts while permitting formal analysis and proof of properties. Crucially, ACL2 also provides support for the development and verification of other formal analysis tools. However, ACL2 did not find its way into industrial use merely because of its technical features. The core ACL2 user/development community has a shared vision of making mechanized verification routine when appropriate and has been committed to this vision for the quarter century since the Computational Logic, Inc., Verified Stack. The community has focused on demonstrating the viability of the tool by taking on industrial projects (often at the expense of not being able to publish much).This article is part of the themed issue ‘Verified trustworthy software systems’.",
    "URL": "https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0399",
    "DOI": "10.1098/rsta.2015.0399",
    "page": "20150399",
    "page-first": "20150399",
    "volume": "375",
    "issue": "2104",
    "_line": "FormalBib.bib:2246"
  },
  "white_neil_formal_2017": {
    "id": "white_neil_formal_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "White Neil"
      },
      {
        "family": "Matthews Stuart"
      },
      {
        "family": "Chapman Roderick"
      }
    ],
    "title": "Formal verification: will the seedling ever flower?",
    "container-title": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "container-title-short": "Formal verification",
    "title-short": "Formal verification",
    "issued": {
      "date-parts": [
        [
          "2017",
          "10",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "In one sense, formal specification and verification have been highly successful: techniques have been developed in pioneering academic research, transferred to software companies through training and partnerships, and successfully deployed in systems with national significance. Altran UK has been in the vanguard of this movement. This paper summarizes some of our key deployments of formal techniques over the past 20 years, including both security- and safety-critical systems. The impact of formal techniques, however, remains within an industrial niche, and while government and suppliers across industry search for solutions to the problems of poor-quality software, the wider software industry remains resistant to adoption of this proven solution. We conclude by reflecting on some of the challenges we face as a community in ensuring that formal techniques achieve their true potential impact on society.This article is part of the themed issue ‘Verified trustworthy software systems’.",
    "URL": "https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0402",
    "DOI": "10.1098/rsta.2015.0402",
    "page": "20150402",
    "page-first": "20150402",
    "volume": "375",
    "issue": "2104",
    "_line": "FormalBib.bib:2262"
  },
  "david_cristina_program_2017": {
    "id": "david_cristina_program_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "David Cristina"
      },
      {
        "family": "Kroening Daniel"
      }
    ],
    "title": "Program synthesis: challenges and opportunities",
    "container-title": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "container-title-short": "Program synthesis",
    "title-short": "Program synthesis",
    "issued": {
      "date-parts": [
        [
          "2017",
          "10",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Program synthesis is the mechanized construction of software, dubbed ‘self-writing code’. Synthesis tools relieve the programmer from thinking about how the problem is to be solved; instead, the programmer only provides a description of what is to be achieved. Given a specification of what the program should do, the synthesizer generates an implementation that provably satisfies this specification. From a logical point of view, a program synthesizer is a solver for second-order existential logic. Owing to the expressiveness of second-order logic, program synthesis has an extremely broad range of applications. We survey some of these applications as well as recent trends in the algorithms that solve the program synthesis problem. In particular, we focus on an approach that has raised the profile of program synthesis and ushered in a generation of new synthesis tools, namely counter-example-guided inductive synthesis (CEGIS). We provide a description of the CEGIS architecture, followed by recent algorithmic improvements. We conjecture that the capacity of program synthesis engines will see further step change, in a manner that is transparent to the applications, which will open up an even broader range of use-cases.This article is part of the themed issue ‘Verified trustworthy software systems’.",
    "URL": "https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0403",
    "DOI": "10.1098/rsta.2015.0403",
    "page": "20150403",
    "page-first": "20150403",
    "volume": "375",
    "issue": "2104",
    "_line": "FormalBib.bib:2279"
  },
  "appel_andrew_w._position_2017": {
    "id": "appel_andrew_w._position_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Appel Andrew W."
      },
      {
        "family": "Beringer Lennart"
      },
      {
        "family": "Chlipala Adam"
      },
      {
        "family": "Pierce Benjamin C."
      },
      {
        "family": "Shao Zhong"
      },
      {
        "family": "Weirich Stephanie"
      },
      {
        "family": "Zdancewic Steve"
      }
    ],
    "title": "Position paper: the science of deep specification",
    "container-title": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "container-title-short": "Position paper",
    "title-short": "Position paper",
    "issued": {
      "date-parts": [
        [
          "2017",
          "10",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "We introduce our efforts within the project ‘The science of deep specification’ to work out the key formal underpinnings of industrial-scale formal specifications of software and hardware components, anticipating a world where large verified systems are routinely built out of smaller verified components that are also used by many other projects. We identify an important class of specification that has already been used in a few experiments that connect strong component-correctness theorems across the work of different teams. To help popularize the unique advantages of that style, we dub it deep specification, and we say that it encompasses specifications that are rich, two-sided, formal and live (terms that we define in the article). Our core team is developing a proof-of-concept system (based on the Coq proof assistant) whose specification and verification work is divided across largely decoupled subteams at our four institutions, encompassing hardware microarchitecture, compilers, operating systems and applications, along with cross-cutting principles and tools for effective specification. We also aim to catalyse interest in the approach, not just by basic researchers but also by users in industry.This article is part of the themed issue ‘Verified trustworthy software systems’.",
    "URL": "https://royalsocietypublishing.org/doi/10.1098/rsta.2016.0331",
    "DOI": "10.1098/rsta.2016.0331",
    "page": "20160331",
    "page-first": "20160331",
    "volume": "375",
    "issue": "2104",
    "_line": "FormalBib.bib:2296"
  },
  "batty_mark_compositional_2017": {
    "id": "batty_mark_compositional_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Batty Mark"
      }
    ],
    "title": "Compositional relaxed concurrency",
    "container-title": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "container-title-short": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "issued": {
      "date-parts": [
        [
          "2017",
          "10",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "There is a broad design space for concurrent computer processors: they can be optimized for low power, low latency or high throughput. This freedom to tune each processor design to its niche has led to an increasing diversity of machines, from powerful pocketable devices to those responsible for complex and critical tasks, such as car guidance systems. Given this context, academic concurrency research sounds notes of both caution and optimism. Caution because recent work has uncovered flaws in the way we explain the subtle memory behaviour of concurrent systems: specifications have been shown to be incorrect, leading to bugs throughout the many layers of the system. And optimism because our tools and methods for verifying the correctness of concurrent code—although built above an idealized model of concurrency—are becoming more mature. This paper looks at the way we specify the memory behaviour of concurrent systems and suggests a new direction. Currently, there is a siloed approach, with each processor and programming language specified separately in an incomparable way. But this does not match the structure of our programs, which may use multiple processors and languages together. Instead we propose a compositional approach, where program components carry with them a description of the sort of concurrency they rely on, and there is a mechanism for composing these. This will support not only components written for the multiple varied processors found in a modern system but also those that use idealized models of concurrency, providing a sound footing for mature verification techniques.This article is part of the themed issue ‘Verified trustworthy software systems’.",
    "URL": "https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0406",
    "DOI": "10.1098/rsta.2015.0406",
    "page": "20150406",
    "page-first": "20150406",
    "volume": "375",
    "issue": "2104",
    "_line": "FormalBib.bib:2313"
  },
  "klein_gerwin_provably_2017": {
    "id": "klein_gerwin_provably_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Klein Gerwin"
      },
      {
        "family": "Andronick June"
      },
      {
        "family": "Keller Gabriele"
      },
      {
        "family": "Matichuk Daniel"
      },
      {
        "family": "Murray Toby"
      },
      {
        "family": "O'Connor Liam"
      }
    ],
    "title": "Provably trustworthy systems",
    "container-title": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "container-title-short": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "issued": {
      "date-parts": [
        [
          "2017",
          "10",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "We present recent work on building and scaling trustworthy systems with formal, machine-checkable proof from the ground up, including the operating system kernel, at the level of binary machine code. We first give a brief overview of the seL4 microkernel verification and how it can be used to build verified systems. We then show two complementary techniques for scaling these methods to larger systems: proof engineering, to estimate verification effort; and code/proof co-generation, for scalable development of provably trustworthy applications.This article is part of the themed issue ‘Verified trustworthy software systems’.",
    "URL": "https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0404",
    "DOI": "10.1098/rsta.2015.0404",
    "page": "20150404",
    "page-first": "20150404",
    "volume": "375",
    "issue": "2104",
    "_line": "FormalBib.bib:2329"
  },
  "delaware_narcissus:_2018": {
    "id": "delaware_narcissus:_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Delaware",
        "given": "Benjamin"
      },
      {
        "family": "Suriyakarn",
        "given": "Sorawit"
      },
      {
        "family": "Pit&ndash;Claudel",
        "given": "Clément"
      },
      {
        "family": "Ye",
        "given": "Qianchuan"
      },
      {
        "family": "Chlipala",
        "given": "Adam"
      }
    ],
    "title": "Narcissus: Deriving Correct-By-Construction Decoders and Encoders from Binary Formats",
    "container-title-short": "Narcissus",
    "title-short": "Narcissus",
    "issued": {
      "date-parts": [
        [
          "2018",
          "3",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "It is a neat result from functional programming that libraries of parser\ncombinators can support rapid construction of decoders for quite a range of\nformats. With a little more work, the same combinator program can denote both a\ndecoder and an encoder. Unfortunately, the real world is full of gnarly\nformats, as with the packet formats that make up the standard Internet protocol\nstack. Most past parser-combinator approaches cannot handle these formats, and\nthe few exceptions require redundancy &ndash; one part of the natural grammar needs\nto be hand-translated into hints in multiple parts of a parser program. We show\nhow to recover very natural and nonredundant format specifications, covering\nall popular network packet formats and generating both decoders and encoders\nautomatically. The catch is that we use the Coq proof assistant to derive both\nkinds of artifacts using tactics, automatically, in a way that guarantees that\nthey form inverses of each other. We used our approach to reimplement packet\nprocessing for a full Internet protocol stack, inserting our replacement into\nthe OCaml-based MirageOS unikernel, resulting in minimal performance\ndegradation.",
    "URL": "https://arxiv.org/abs/1803.04870v2",
    "language": "en-US",
    "_line": "FormalBib.bib:2345"
  },
  "choi_kami:_2017": {
    "id": "choi_kami:_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Choi",
        "given": "Joonwon"
      },
      {
        "family": "Vijayaraghavan",
        "given": "Muralidaran"
      },
      {
        "family": "Sherman",
        "given": "Benjamin"
      },
      {
        "family": "Chlipala",
        "given": "Adam"
      },
      {
        "family": "Arvind"
      }
    ],
    "title": "Kami: A Platform for High-level Parametric Hardware Specification and Its Modular Verification",
    "container-title": "Proc. ACM Program. Lang.",
    "container-title-short": "Kami",
    "title-short": "Kami",
    "issued": {
      "date-parts": [
        [
          "2017",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "It has become fairly standard in the programming-languages research world to verify functional programs in proof assistants using induction, algebraic simplification, and rewriting. In this paper, we introduce Kami, a Coq library that enables similar expressive and modular reasoning for hardware designs expressed in the style of the Bluespec language. We can specify, implement, and verify realistic designs entirely within Coq, ending with automatic extraction into a pipeline that bottoms out in FPGAs. Our methodology, using labeled transition systems, has been evaluated in a case study verifying an infinite family of multicore systems, with cache-coherent shared memory and pipelined cores implementing (the base integer subset of) the RISC-V instruction set.",
    "keywords": "formal verification, proof assistants, hardware",
    "URL": "http://doi.acm.org/10.1145/3110268",
    "DOI": "10.1145/3110268",
    "page": "24:1-24:30",
    "page-first": "24",
    "volume": "1",
    "_line": "FormalBib.bib:2372"
  },
  "fisher_kathleen_hacms_2017": {
    "id": "fisher_kathleen_hacms_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Fisher Kathleen"
      },
      {
        "family": "Launchbury John"
      },
      {
        "family": "Richards Raymond"
      }
    ],
    "title": "The HACMS program: using formal methods to eliminate exploitable bugs",
    "container-title": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "container-title-short": "The HACMS program",
    "title-short": "The HACMS program",
    "issued": {
      "date-parts": [
        [
          "2017",
          "10",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "For decades, formal methods have offered the promise of verified software that does not have exploitable bugs. Until recently, however, it has not been possible to verify software of sufficient complexity to be useful. Recently, that situation has changed. SeL4 is an open-source operating system microkernel efficient enough to be used in a wide range of practical applications. Its designers proved it to be fully functionally correct, ensuring the absence of buffer overflows, null pointer exceptions, use-after-free errors, etc., and guaranteeing integrity and confidentiality. The CompCert Verifying C Compiler maps source C programs to provably equivalent assembly language, ensuring the absence of exploitable bugs in the compiler. A number of factors have enabled this revolution, including faster processors, increased automation, more extensive infrastructure, specialized logics and the decision to co-develop code and correctness proofs rather than verify existing artefacts. In this paper, we explore the promise and limitations of current formal-methods techniques. We discuss these issues in the context of DARPA’s HACMS program, which had as its goal the creation of high-assurance software for vehicles, including quadcopters, helicopters and automobiles.This article is part of the themed issue ‘Verified trustworthy software systems’.",
    "URL": "https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0401",
    "DOI": "10.1098/rsta.2015.0401",
    "page": "20150401",
    "page-first": "20150401",
    "volume": "375",
    "issue": "2104",
    "_line": "FormalBib.bib:2390"
  },
  "petcher_foundational_2015": {
    "id": "petcher_foundational_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Petcher",
        "given": "Adam"
      },
      {
        "family": "Morrisett",
        "given": "Greg"
      }
    ],
    "editor": [
      {
        "family": "Focardi",
        "given": "Riccardo"
      },
      {
        "family": "Myers",
        "given": "Andrew"
      }
    ],
    "title": "The Foundational Cryptography Framework",
    "container-title": "Principles of Security and Trust",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-662-46666-7",
    "abstract": "We present the Foundational Cryptography Framework (FCF) for developing and checking complete proofs of security for cryptographic schemes within a proof assistant. This is a general-purpose framework that is capable of modeling and reasoning about a wide range of cryptographic schemes, security definitions, and assumptions. Security is proven in the computational model, and the proof provides concrete bounds as well as asymptotic conclusions. FCF provides a language for probabilistic programs, a theory that is used to reason about programs, and a library of tactics and definitions that are useful in proofs about cryptography. The framework is designed to leverage fully the existing theory and capabilities of the Coq proof assistant in order to reduce the effort required to develop proofs.",
    "keywords": "Coq, Cryptography, Proof Assistant, Protocol Verification",
    "URL": "http://www.cs.cornell.edu/~jgm/papers/FCF.pdf",
    "page": "53-72",
    "page-first": "53",
    "language": "en-US",
    "_line": "FormalBib.bib:2407"
  },
  "harrison_formal_2008": {
    "id": "harrison_formal_2008",
    "type": "article-journal",
    "author": [
      {
        "family": "Harrison",
        "given": "John"
      }
    ],
    "title": "Formal Proof—Theory and Practice",
    "issued": {
      "date-parts": [
        [
          "2008"
        ]
      ]
    },
    "page": "12",
    "page-first": "12",
    "volume": "55",
    "issue": "11",
    "language": "en-US",
    "_line": "FormalBib.bib:2424"
  },
  "wiedijk_formal_2008": {
    "id": "wiedijk_formal_2008",
    "type": "article-journal",
    "author": [
      {
        "family": "Wiedijk",
        "given": "Freek"
      }
    ],
    "title": "Formal Proof—Getting Started",
    "issued": {
      "date-parts": [
        [
          "2008"
        ]
      ]
    },
    "page": "7",
    "page-first": "7",
    "volume": "55",
    "issue": "11",
    "language": "en-US",
    "_line": "FormalBib.bib:2435"
  },
  "gonthier_formal_2008": {
    "id": "gonthier_formal_2008",
    "type": "article-journal",
    "author": [
      {
        "family": "Gonthier",
        "given": "Georges"
      }
    ],
    "title": "Formal Proof—The Four- Color Theorem",
    "issued": {
      "date-parts": [
        [
          "2008"
        ]
      ]
    },
    "page": "12",
    "page-first": "12",
    "volume": "55",
    "issue": "11",
    "language": "en-US",
    "_line": "FormalBib.bib:2446"
  },
  "chlipala_end_nodate": {
    "id": "chlipala_end_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Chlipala",
        "given": "Adam"
      },
      {
        "family": "Delaware",
        "given": "Benjamin"
      },
      {
        "family": "Duchovni",
        "given": "Samuel"
      },
      {
        "family": "Gross",
        "given": "Jason"
      },
      {
        "family": "Pit-Claudel",
        "given": "Clément"
      },
      {
        "family": "Suriyakarn",
        "given": "Sorawit"
      },
      {
        "family": "Wang",
        "given": "Peng"
      },
      {
        "family": "ye",
        "given": "Katherine"
      }
    ],
    "title": "THE END OF HISTORY? USING A PROOF ASSISTANT TO REPLACE LANGUAGE DESIGN WITH LIBRARY DESIGN",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Functionality of software systems has exploded in part because of advances in programming-language support for packaging reusable functionality as libraries. Developers benefit from the uniformity that comes of exposing many interfaces in the same language, as opposed to stringing together hodgepodges of command-line tools. Domain-specific languages may be viewed as an evolution of the power of reusable interfaces, when those interfaces become so flexible as to deserve to be called programming languages. However, common approaches to domain-specific languages give up many of the hard-won advantages of library-building in a rich common language, and even the traditional approach poses significant challenges in learning new APIs. We suggest that instead of continuing to develop new domain-specific languages, our community should embrace library-based ecosystems within very expressive languages that mix programming and theorem proving. Our prototype framework Fiat, a library for the Coq proof assistant, turns languages into easily comprehensible libraries via the key idea of modularizing functionality and performance away from each other, the former via macros that desugar into higher-order logic and the latter via optimization scripts that derive efficient code from logical programs.",
    "URL": "https://snapl.org/2017/abstracts/Chlipala.html",
    "_line": "FormalBib.bib:2457"
  },
  "delaware_fiat:_2015": {
    "id": "delaware_fiat:_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Delaware",
        "given": "Benjamin"
      },
      {
        "family": "Pit-Claudel",
        "given": "Clément"
      },
      {
        "family": "Gross",
        "given": "Jason"
      },
      {
        "family": "Chlipala",
        "given": "Adam"
      }
    ],
    "title": "Fiat: Deductive Synthesis of Abstract Data Types in a Proof Assistant",
    "container-title": "Proceedings of the 42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "container-title-short": "Fiat",
    "collection-title": "POPL '15",
    "title-short": "Fiat",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-3300-9",
    "abstract": "We present Fiat, a library for the Coq proof assistant supporting refinement of declarative specifications into efficient functional programs with a high degree of automation. Each refinement process leaves a proof trail, checkable by the normal Coq kernel, justifying its soundness. We focus on the synthesis of abstract data types that package methods with private data. We demonstrate the utility of our framework by applying it to the synthesis of query structures &ndash; abstract data types with SQL-like query and insert operations. Fiat includes a library for writing specifications of query structures in SQL-inspired notation, expressing operations over relations (tables) in terms of mathematical sets. This library includes a suite of tactics for automating the refinement of specifications into efficient, correct-by-construction OCaml code. Using these tactics, a programmer can generate such an implementation completely automatically by only specifying the equivalent of SQL indexes, data structures capturing useful views of the abstract data. Throughout we speculate on the new programming modularity possibilities enabled by an automated refinement system with proved-correct rules.",
    "keywords": "deductive synthesis, mechanized derivation of abstract data types",
    "URL": "http://doi.acm.org/10.1145/2676726.2677006",
    "DOI": "10.1145/2676726.2677006",
    "publisher-place": "New York, NY, USA",
    "page": "689-700",
    "page-first": "689",
    "_line": "FormalBib.bib:2466"
  },
  "sozeau_equations:_2010": {
    "id": "sozeau_equations:_2010",
    "type": "paper-conference",
    "author": [
      {
        "family": "Sozeau",
        "given": "Matthieu"
      }
    ],
    "editor": [
      {
        "family": "Kaufmann",
        "given": "Matt"
      },
      {
        "family": "Paulson",
        "given": "Lawrence C."
      }
    ],
    "title": "Equations: A Dependent Pattern-Matching Compiler",
    "container-title": "Interactive Theorem Proving",
    "container-title-short": "Equations",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "Equations",
    "issued": {
      "date-parts": [
        [
          "2010"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-14052-5",
    "abstract": "We present a compiler for definitions made by pattern matching on inductive families in the Coq system. It allows to write structured, recursive dependently-typed functions as a set of equations, automatically find their realization in the core type theory and generate proofs to ease reasoning on them. It provides a complete package to define and reason on functions in the proof assistant, substantially reducing the boilerplate code and proofs one usually has to write, also hiding the intricacies related to the use of dependent types and complex recursion schemes.",
    "keywords": "Proof Assistant, Recursive Call, Split Node, Type Theory, User Node",
    "page": "419-434",
    "page-first": "419",
    "language": "en-US",
    "_line": "FormalBib.bib:2485"
  },
  "chen_project_nodate": {
    "id": "chen_project_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Chen",
        "given": "Yixuan"
      }
    ],
    "title": "Project Report on DeepSpecDB",
    "abstract": "Recent years have witnessed a rapid development of mainmemory database systems thanks to the growingly aﬀordable memory. DeepSpecDB is another main-memory database management system implemented in C with deep speciﬁcation and end-to-end veriﬁcation guaranteeing the correctness of the system.",
    "page": "35",
    "page-first": "35",
    "language": "en-US",
    "_line": "FormalBib.bib:2502"
  },
  "barriere_vst_nodate": {
    "id": "barriere_vst_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Barriere",
        "given": "Aurele"
      },
      {
        "family": "Appel",
        "given": "Andrew"
      }
    ],
    "title": "VST Veriﬁcation of B+Trees with Cursors",
    "abstract": "The DeepSpecDB project aims to deﬁne, specify and verify a high-performance concurrent in-memory database system. Based on MassTree, it uses B+Trees, a well-studied key-value data structure. Our sequential B+Trees library uses cursors, introduced in the database engine SQLite. Such cursors reduce the complexity of operations when dealing with partially sorted data. We deﬁne a Coq formal model for such trees, then use it to specify and prove the correctness of the C implementation using the Veriﬁed Software Toolchain.",
    "page": "19",
    "page-first": "19",
    "language": "en-US",
    "_line": "FormalBib.bib:2511"
  },
  "anand_typed_nodate": {
    "id": "anand_typed_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Anand",
        "given": "Abhishek"
      },
      {
        "family": "Tabareau",
        "given": "Simon Boulier Nicolas"
      },
      {
        "family": "Sozeau",
        "given": "Matthieu"
      }
    ],
    "title": "Typed Template Coq",
    "abstract": "Template-Coq1 is a plugin for Coq, originally implemented by Malecha \\[7\\], which provides a reifier for Coq terms and global declarations, as represented in the Coq kernel, as well as a denotation command. Initially, it was developed for the purpose of writing functions on Coq’s AST in Gallina. Recently, its use was extended for the needs of the CertiCoq certified compiler project \\[2\\], which uses it as its front-end language and to derive parametricity properties \\[1\\], and the work of \\[5\\] on extracting Coq terms to a CBV λ-calculus. However, the syntax currently lacks semantics, be it typing semantics or operational semantics, which should reflect, as formal specifications in Coq, the semantics of Coq itself. This is an issue for CertiCoq where both a non-deterministic small step semantics and a deterministic call-by-value big step semantics had to be defined and preserved, without an “official” reference specification to refer to. Our hope with this work is to remedy this situation and provide a formal semantics of Coq’s implemented type theory, that can independently be refined and studied. By implementing a (partial) independent checker in Coq, we can also help formalize certified translations from Coq to Coq (Section 3).",
    "page": "2",
    "page-first": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:2520"
  },
  "leino_well-founded_2016": {
    "id": "leino_well-founded_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Leino",
        "given": "Rustan"
      }
    ],
    "title": "Well-Founded Functions and Extreme Predicates in Dafny: A Tutorial",
    "container-title-short": "Well-Founded Functions and Extreme Predicates in Dafny",
    "title-short": "Well-Founded Functions and Extreme Predicates in Dafny",
    "issued": {
      "date-parts": [
        [
          "2016",
          "12",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "A recursive function is well defined if its every recursive call corresponds a decrease in some well-founded order. Such well-founded functions are useful for example in computer programs when computing a value from some input. A boolean function can also be defined as an extreme solution to a recurrence relation, that is, as a least …",
    "URL": "https://www.microsoft.com/en-us/research/publication/well-founded-functions-extreme-predicates-dafny-tutorial/",
    "volume": "40",
    "language": "en-US",
    "_line": "FormalBib.bib:2529"
  },
  "leino_verified_2016": {
    "id": "leino_verified_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Leino",
        "given": "Rustan"
      },
      {
        "family": "Polikarpova",
        "given": "Nadia"
      }
    ],
    "title": "Verified Calculations",
    "issued": {
      "date-parts": [
        [
          "2016",
          "12",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Calculational proofs—proofs by stepwise formula manipulation—are praised for their rigor, readability, and elegance. It seems desirable to reuse this style, often employed on paper, in the context of mechanized reasoning, and in particular, program verification. This work leverages the power of SMT solvers to machine-check calculational proofs at the level of detail they are usually …",
    "URL": "https://www.microsoft.com/en-us/research/publication/verified-calculations/",
    "language": "en-US",
    "_line": "FormalBib.bib:2542"
  },
  "parkinson_relationship_nodate": {
    "id": "parkinson_relationship_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Parkinson",
        "given": "Matthew J"
      },
      {
        "family": "Summers",
        "given": "Alexander J"
      }
    ],
    "title": "The Relationship between Separation Logic and Implicit Dynamic Frames",
    "container-title": "LNCS",
    "abstract": "Separation logic is a concise method for specifying programs that manipulate dynamically allocated storage. Partially inspired by separation logic, Implicit Dynamic Frames has recently been proposed, aiming at ﬁrst-order tool support. In this paper, we provide a total heap semantics for a standard separation logic, and prove it equivalent to the standard model. With small adaptations, we then show how to give a direct semantics to implicit dynamic frames and show this semantics correctly captures the existing deﬁnitions. This precisely connects the two logics. As a consequence of this connection, we show that a fragment of separation logic can be faithfully encoded in a ﬁrst-order automatic veriﬁcation tool (Chalice).",
    "page": "439-458",
    "page-first": "439",
    "volume": "6602",
    "note": "ESOP 2011",
    "language": "en-US",
    "_line": "FormalBib.bib:2553"
  },
  "leino_compiling_2016": {
    "id": "leino_compiling_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Leino",
        "given": "Rustan"
      }
    ],
    "title": "Compiling Hilbert's epsilon Operator",
    "container-title": "LPAR-20. 20th International Conferences on Logic for Programming, Artificial Intelligence and Reasoning",
    "issued": {
      "date-parts": [
        [
          "2016",
          "12",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Hilbert’s epsilon (ϵ) operator is a binder that picks an arbitrary element from a nonempty set. The operator is typically used in logics and proof engines. This paper contributes a discussion of considerations in supporting this operator in a programming language. More specifically, the paper presents the design choices made around supporting this operator in …",
    "URL": "https://www.microsoft.com/en-us/research/publication/compiling-hilberts-%cf%b5-operator/",
    "volume": "35",
    "language": "en-US",
    "_line": "FormalBib.bib:2565"
  },
  "koenig_programming_2016": {
    "id": "koenig_programming_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Koenig",
        "given": "Jason"
      },
      {
        "family": "Leino",
        "given": "Rustan"
      }
    ],
    "title": "Programming Language Features for Refinement",
    "issued": {
      "date-parts": [
        [
          "2016",
          "12",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Algorithmic and data refinement are well studied topics that provide a mathematically rigorous approach to gradually introducing details in the implementation of software. Program refinements are performed in the context of some programming language, but mainstream languages lack features for recording the sequence of refinement steps in the program text. To experiment with the combination …",
    "URL": "https://www.microsoft.com/en-us/research/publication/programming-language-features-refinement/",
    "language": "en-US",
    "_line": "FormalBib.bib:2578"
  },
  "leino_fine-grained_2016": {
    "id": "leino_fine-grained_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Leino",
        "given": "Rustan"
      },
      {
        "family": "Wüstholz",
        "given": "Valentin"
      }
    ],
    "title": "Fine-grained Caching of Verification Results",
    "issued": {
      "date-parts": [
        [
          "2016",
          "12",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Developing provably correct programs is an incremental process that often involves a series of interactions with a program verifier. To increase the responsiveness of the program verifier during such interactions, we designed a system for fine-grained caching of verification results. The caching system uses the program’s call graph and control-flow graph to focus the verification …",
    "URL": "https://www.microsoft.com/en-us/research/publication/fine-grained-caching-verification-results/",
    "volume": "9206",
    "language": "en-US",
    "_line": "FormalBib.bib:2589"
  },
  "leino_stepwise_2016": {
    "id": "leino_stepwise_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Leino",
        "given": "Rustan"
      },
      {
        "family": "Yessenov",
        "given": "Kuat"
      }
    ],
    "title": "Stepwise Refinement of Heap-Manipulating Code in Chalice",
    "issued": {
      "date-parts": [
        [
          "2016",
          "12",
          "29"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Stepwise refinement is a well-studied technique for developing a program from an abstract description to a concrete implementation. This paper describes a system with automated tool support for refinement, powered by a stateof-the-art verification engine that uses an SMT solver. Unlike previous refinement systems, users of the presented system interact only via declarations in the …",
    "URL": "https://www.microsoft.com/en-us/research/publication/stepwise-refinement-heap-manipulating-code-chalice/",
    "language": "en-US",
    "_line": "FormalBib.bib:2601"
  },
  "leino_verification_2016": {
    "id": "leino_verification_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Leino",
        "given": "Rustan"
      },
      {
        "family": "Müller",
        "given": "Peter"
      },
      {
        "family": "Smans",
        "given": "Jan"
      }
    ],
    "title": "Verification of Concurrent Programs with Chalice",
    "issued": {
      "date-parts": [
        [
          "2016",
          "12",
          "29"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "A program verifier is a tool that allows developers to prove that their code satisfies its specification for every possible input and every thread schedule. These lecture notes describe a verifier for concurrent programs called Chalice. Chalice’s verification methodology centers around permissions and permission transfer. In particular, a memory location may be accessed by a …",
    "URL": "https://www.microsoft.com/en-us/research/publication/verification-concurrent-programs-chalice/",
    "language": "en-US",
    "_line": "FormalBib.bib:2612"
  },
  "hatcliff_behavioral_2012": {
    "id": "hatcliff_behavioral_2012",
    "type": "article-journal",
    "author": [
      {
        "family": "Hatcliff",
        "given": "John"
      },
      {
        "family": "Leavens",
        "given": "Gary T."
      },
      {
        "family": "Leino",
        "given": "K. Rustan M."
      },
      {
        "family": "Müller",
        "given": "Peter"
      },
      {
        "family": "Parkinson",
        "given": "Matthew"
      }
    ],
    "title": "Behavioral Interface Specification Languages",
    "container-title": "ACM Comput. Surv.",
    "issued": {
      "date-parts": [
        [
          "2012",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "0360-0300",
    "abstract": "Behavioral interface specification languages provide formal code-level annotations, such as preconditions, postconditions, invariants, and assertions that allow programmers to express the intended behavior of program modules. Such specifications are useful for precisely documenting program behavior, for guiding implementation, and for facilitating agreement between teams of programmers in modular development of software. When used in conjunction with automated analysis and program verification tools, such specifications can support detection of common code vulnerabilities, capture of light-weight application-specific semantic properties, generation of test cases and test oracles, and full formal program verification. This article surveys behavioral interface specification languages with a focus toward automatic program verification and with a view towards aiding the Verified Software Initiative—a fifteen-year, cooperative, international project directed at the scientific challenges of large-scale software verification.",
    "keywords": "separation logic, Abstraction, assertion, behavioral subtyping, frame conditions, interface specification language, invariant, JML, postcondition, precondition, SPARK, Spec&hash;",
    "URL": "http://doi.acm.org/10.1145/2187671.2187678",
    "DOI": "10.1145/2187671.2187678",
    "page": "16:1-16:58",
    "page-first": "16",
    "volume": "44",
    "issue": "3",
    "_line": "FormalBib.bib:2623"
  },
  "amin_computing_2016": {
    "id": "amin_computing_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Amin",
        "given": "Nada"
      },
      {
        "family": "Leino",
        "given": "Rustan"
      },
      {
        "family": "Rompf",
        "given": "Tiark"
      }
    ],
    "title": "Computing with an SMT Solver",
    "issued": {
      "date-parts": [
        [
          "2016",
          "12",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Satisfiability modulo theories (SMT) solvers that support quantifier instantiations via matching triggers can be programmed to give practical support for user-defined theories. Care must be taken to avoid so-called matching loops, which may prevent termination of the solver. By design, such avoidance limits the extent to which the SMT solver is able to apply the …",
    "URL": "https://www.microsoft.com/en-us/research/publication/computing-smt-solver/",
    "volume": "8570",
    "language": "en-US",
    "_line": "FormalBib.bib:2640"
  },
  "leino_co-induction_2013": {
    "id": "leino_co-induction_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Leino",
        "given": "Rustan"
      },
      {
        "family": "Moskal",
        "given": "Michal"
      }
    ],
    "title": "Co-Induction Simply: Automatic Co-Inductive Proofs in a Program Verifier",
    "container-title-short": "Co-Induction Simply",
    "title-short": "Co-Induction Simply",
    "issued": {
      "date-parts": [
        [
          "2013",
          "7",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Program verification relies heavily on induction, which has received decades of attention in mechanical verification tools. When program correctness is best described by infinite structures, program verification is usefully aided also by co-induction, which has not benefited from the same degree of tool support. Co-induction is complicated to work with in interactive proof assistants and …",
    "URL": "https://www.microsoft.com/en-us/research/publication/co-induction-simply-automatic-co-inductive-proofs-in-a-program-verifier/",
    "language": "en-US",
    "_line": "FormalBib.bib:2652"
  },
  "christakis_collaborative_2012": {
    "id": "christakis_collaborative_2012",
    "type": "paper-conference",
    "author": [
      {
        "family": "Christakis",
        "given": "Maria"
      },
      {
        "family": "Müller",
        "given": "Peter"
      },
      {
        "family": "Wüstholz",
        "given": "Valentin"
      }
    ],
    "editor": [
      {
        "family": "Giannakopoulou",
        "given": "Dimitra"
      },
      {
        "family": "Méry",
        "given": "Dominique"
      }
    ],
    "title": "Collaborative Verification and Testing with Explicit Assumptions",
    "container-title": "FM 2012: Formal Methods",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2012"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-32759-9",
    "abstract": "Many mainstream static code checkers make a number of compromises to improve automation, performance, and accuracy. These compromises include not checking certain program properties as well as making implicit, unsound assumptions. Consequently, the results of such static checkers do not provide definite guarantees about program correctness, which makes it unclear which properties remain to be tested. We propose a technique for collaborative verification and testing that makes compromises of static checkers explicit such that they can be compensated for by complementary checkers or testing. Our experiments suggest that our technique finds more errors and proves more properties than static checking alone, testing alone, and combinations that do not explicitly document the compromises made by static checkers. Our technique is also useful to obtain small test suites for partially-verified programs.",
    "keywords": "Symbolic Execution, Static Checker, Test Case Generation, Testing Tool, Tool Chain",
    "page": "132-146",
    "page-first": "132",
    "language": "en-US",
    "_line": "FormalBib.bib:2664"
  },
  "leino_assertional_2015": {
    "id": "leino_assertional_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Leino",
        "given": "K. Rustan M."
      },
      {
        "family": "Lucio",
        "given": "Paqui"
      }
    ],
    "title": "An Assertional Proof of the Stability and Correctness of Natural Mergesort",
    "container-title": "ACM Trans. Comput. Logic",
    "issued": {
      "date-parts": [
        [
          "2015",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "1529-3785",
    "abstract": "We present a mechanically verified implementation of the sorting algorithm Natural Mergesort that consists of a few methods specified by their contracts of pre/post conditions. Methods are annotated with assertions that allow the automatic verification of the contract satisfaction. This program-proof is made using the state-of-the-art verifier Dafny. We verify not only the standard sortedness property, but also that the algorithm performs a stable sort. Throughout the article, we provide and explain the complete text of the program-proof.",
    "keywords": "theorem proving, Verification, formal methods, dafny, natural mergesort, software engineering, sorting, stability",
    "URL": "http://doi.acm.org/10.1145/2814571",
    "DOI": "10.1145/2814571",
    "page": "6:1-6:22",
    "page-first": "6",
    "volume": "17",
    "issue": "1",
    "_line": "FormalBib.bib:2680"
  },
  "polikarpova_structuring_2019": {
    "id": "polikarpova_structuring_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Polikarpova",
        "given": "Nadia"
      },
      {
        "family": "Sergey",
        "given": "Ilya"
      }
    ],
    "title": "Structuring the Synthesis of Heap-manipulating Programs",
    "container-title": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "This paper describes a deductive approach to synthesizing imperative programs with pointers from declarative specifications expressed in Separation Logic. Our synthesis algorithm takes as input a pair of assertions—a pre- and a postcondition—which describe two states of the symbolic heap, and derives a program that transforms one state into the other, guided by the shape of the heap. Our approach to program synthesis is grounded in proof theory: we introduce the novel framework of Synthetic Separation Logic (SSL), which generalises the classical notion of heap entailment P ⊢ Q to incorporate a possibility of transforming a heap satisfying an assertion P into a heap satisfying an assertion Q. A synthesized program represents a proof term for a transforming entailment statement P ↝ Q, and the synthesis procedure corresponds to a proof search. The derived programs are, thus, correct by construction, in the sense that they satisfy the ascribed pre/postconditions, and are accompanied by complete proof derivations, which can be checked independently.  We have implemented a proof search engine for SSL in a form of the program synthesizer called SuSLik. For efficiency, the engine exploits properties of SSL rules, such as invertibility and commutativity of rule applications on separate heaps, to prune the space of derivations it has to consider. We explain and showcase the use of SSL on characteristic examples, describe the design of SuSLik, and report on our experience of using it to synthesize a series of benchmark programs manipulating heap-based linked data structures.",
    "keywords": "Type Theory, Program Synthesis, Proof Systems, Separation Logic",
    "URL": "http://doi.acm.org/10.1145/3290385",
    "DOI": "10.1145/3290385",
    "page": "72:1-72:30",
    "page-first": "72",
    "volume": "3",
    "_line": "FormalBib.bib:2697"
  },
  "bohrer_veriphy:_2018": {
    "id": "bohrer_veriphy:_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Bohrer",
        "given": "Brandon"
      },
      {
        "family": "Tan",
        "given": "Yong Kiam"
      },
      {
        "family": "Mitsch",
        "given": "Stefan"
      },
      {
        "family": "Myreen",
        "given": "Magnus O."
      },
      {
        "family": "Platzer",
        "given": "André"
      }
    ],
    "title": "VeriPhy: verified controller executables from verified cyber-physical system models",
    "container-title": "Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation  - PLDI 2018",
    "container-title-short": "VeriPhy",
    "title-short": "VeriPhy",
    "event-title": "the 39th ACM SIGPLAN Conference",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "ACM Press",
    "isbn": "978-1-4503-5698-5",
    "URL": "http://dl.acm.org/citation.cfm?doid=3192366.3192406",
    "DOI": "10.1145/3192366.3192406",
    "publisher-place": "Philadelphia, PA, USA",
    "page": "617-630",
    "page-first": "617",
    "language": "en-US",
    "_line": "FormalBib.bib:2714"
  },
  "beckert_verification_2006": {
    "id": "beckert_verification_2006",
    "type": "book",
    "editor": [
      {
        "family": "Beckert",
        "given": "Bernhard"
      },
      {
        "family": "Hähnle",
        "given": "Reiner"
      },
      {
        "family": "Schmitt",
        "given": "Peter H."
      }
    ],
    "title": "Verification of Object-Oriented Software. The KeY Approach",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2006"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-540-68977-5",
    "URL": "http://link.springer.com/10.1007/978-3-540-69061-0",
    "DOI": "10.1007/978-3-540-69061-0",
    "publisher-place": "Berlin, Heidelberg",
    "volume": "4334",
    "language": "en-US",
    "_line": "FormalBib.bib:2731"
  },
  "platzer_complete_2017": {
    "id": "platzer_complete_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Platzer",
        "given": "André"
      }
    ],
    "title": "A Complete Uniform Substitution Calculus for Differential Dynamic Logic",
    "container-title": "Journal of Automated Reasoning",
    "issued": {
      "date-parts": [
        [
          "2017",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "0168-7433, 1573-0670",
    "abstract": "This article introduces a relatively complete proof calculus for differential dynamic logic (dL) that is entirely based on uniform substitution, a proof rule that substitutes a formula for a predicate symbol everywhere. Uniform substitutions make it possible to use axioms instead of axiom schemata, thereby substantially simplifying implementations. Instead of subtle schema variables and soundness-critical side conditions on the occurrence patterns of logical variables to restrict infinitely many axiom schema instances to sound ones, the resulting calculus adopts only a finite number of ordinary dL formulas as axioms, which uniform substitutions instantiate soundly. The static semantics of differential dynamic logic and the soundness-critical restrictions it imposes on proof steps is captured exclusively in uniform substitutions and variable renamings as opposed to being spread in delicate ways across the prover implementation. In addition to sound uniform substitutions, this article introduces differential forms for differential dynamic logic that make it possible to internalize differential invariants, differential substitutions, and derivatives as first-class axioms to reason about differential equations axiomatically. The resulting axiomatization of differential dynamic logic is proved to be sound and relatively complete.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science, F.3.1, F.3.2, F.4.1, 03F03, 03B70, 34A38, I.2.3, Mathematics - Logic",
    "URLtext": "1601.06183",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1601.06183",
    "URL": "http://arxiv.org/abs/1601.06183",
    "DOI": "10.1007/s10817-016-9385-1",
    "page": "219-265",
    "page-first": "219",
    "volume": "59",
    "issue": "2",
    "_line": "FormalBib.bib:2747"
  },
  "platzer_logical_2018": {
    "id": "platzer_logical_2018",
    "type": "book",
    "author": [
      {
        "family": "Platzer",
        "given": "André"
      }
    ],
    "title": "Logical Foundations of Cyber-Physical Systems - Slides",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-63587-3 978-3-319-63588-0",
    "URL": "http://link.springer.com/10.1007/978-3-319-63588-0",
    "DOI": "10.1007/978-3-319-63588-0",
    "publisher-place": "Cham",
    "language": "en-US",
    "_line": "FormalBib.bib:2766"
  },
  "felty_keymaera_2015": {
    "id": "felty_keymaera_2015",
    "type": "chapter",
    "author": [
      {
        "family": "Fulton",
        "given": "Nathan"
      },
      {
        "family": "Mitsch",
        "given": "Stefan"
      },
      {
        "family": "Quesel",
        "given": "Jan-David"
      },
      {
        "family": "Völp",
        "given": "Marcus"
      },
      {
        "family": "Platzer",
        "given": "André"
      }
    ],
    "editor": [
      {
        "family": "Felty",
        "given": "Amy P."
      },
      {
        "family": "Middeldorp",
        "given": "Aart"
      }
    ],
    "title": "KeYmaera X: An Axiomatic Tactical Theorem Prover for Hybrid Systems",
    "container-title": "Automated Deduction - CADE-25",
    "container-title-short": "KeYmaera X",
    "title-short": "KeYmaera X",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-21400-9 978-3-319-21401-6",
    "abstract": "KeYmaera X is a theorem prover for differential dynamic logic (dL), a logic for specifying and verifying properties of hybrid systems. Reasoning about complicated hybrid systems models requires support for sophisticated proof techniques, efﬁcient computation, and a user interface that crystallizes salient properties of the system. KeYmaera X allows users to specify custom proof search techniques as tactics, execute these tactics in parallel, and interface with partial proofs via an extensible user interface.",
    "URL": "http://link.springer.com/10.1007/978-3-319-21401-6_36",
    "DOI": "10.1007/978-3-319-21401-6_36",
    "publisher-place": "Cham",
    "page": "527-538",
    "page-first": "527",
    "volume": "9195",
    "language": "en-US",
    "_line": "FormalBib.bib:2780"
  },
  "platzer_logical_2018-1": {
    "id": "platzer_logical_2018-1",
    "type": "book",
    "author": [
      {
        "family": "Platzer",
        "given": "Andre"
      }
    ],
    "title": "Logical Foundations of Cyber-Physical Systems",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-63587-3",
    "abstract": "Cyber-physical systems (CPSs) combine cyber capabilities, such as computation or communication, with physical capabilities, such as motion or other physical processes. Cars, aircraft, and robots are prime examples, because they move physically in space in a way that is determined by discrete computerized control algorithms. Designing these algorithms is challenging due to their tight coupling with physical behavior, while it is vital that these algorithms be correct because we rely on them for safety-critical tasks. This textbook teaches undergraduate students the core principles behind CPSs. It shows them how to develop models and controls; identify safety specifications and critical properties; reason rigorously about CPS models; leverage multi-dynamical systems compositionality to tame CPS complexity; identify required control constraints; verify CPS models of appropriate scale in logic; and develop an intuition for operational effects. The book is supported with homework exercises, lecture videos, and slides.",
    "URL": "https://www.springer.com/gp/book/9783319635873",
    "language": "en-US",
    "_line": "FormalBib.bib:2800"
  },
  "hutchison_verifying_2007": {
    "id": "hutchison_verifying_2007",
    "type": "chapter",
    "author": [
      {
        "family": "Ahrendt",
        "given": "Wolfgang"
      },
      {
        "family": "Beckert",
        "given": "Bernhard"
      },
      {
        "family": "Hähnle",
        "given": "Reiner"
      },
      {
        "family": "Rümmer",
        "given": "Philipp"
      },
      {
        "family": "Schmitt",
        "given": "Peter H."
      }
    ],
    "editor": [
      {
        "family": "Boer",
        "given": "Frank S.",
        "dropping-particle": "de"
      },
      {
        "family": "Bonsangue",
        "given": "Marcello M."
      },
      {
        "family": "Graf",
        "given": "Susanne"
      },
      {
        "family": "Roever",
        "given": "Willem-Paul",
        "dropping-particle": "de"
      }
    ],
    "title": "Verifying Object-Oriented Programs with KeY: A Tutorial",
    "container-title": "Formal Methods for Components and Objects",
    "container-title-short": "Verifying Object-Oriented Programs with KeY",
    "title-short": "Verifying Object-Oriented Programs with KeY",
    "issued": {
      "date-parts": [
        [
          "2007"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-540-74791-8 978-3-540-74792-5",
    "abstract": "This paper is a tutorial on performing formal speciﬁcation and semi-automatic veriﬁcation of Java programs with the formal software development tool KeY. This tutorial aims to ﬁll the gap between elementary introductions using toy examples and state-of-art case studies by going through a self-contained, yet non-trivial, example. It is hoped that this contributes to explain the problems encountered in veriﬁcation of imperative, object-oriented programs to a readership outside the limited community of active researchers.",
    "URL": "http://link.springer.com/10.1007/978-3-540-74792-5_4",
    "DOI": "10.1007/978-3-540-74792-5_4",
    "publisher-place": "Berlin, Heidelberg",
    "page": "70-101",
    "page-first": "70",
    "volume": "4709",
    "language": "en-US",
    "_line": "FormalBib.bib:2813"
  },
  "platzer_differential_2008": {
    "id": "platzer_differential_2008",
    "type": "article-journal",
    "author": [
      {
        "family": "Platzer",
        "given": "André"
      }
    ],
    "title": "Differential Dynamic Logic for Hybrid Systems",
    "container-title": "Journal of Automated Reasoning",
    "issued": {
      "date-parts": [
        [
          "2008",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "0168-7433, 1573-0670",
    "URL": "http://link.springer.com/10.1007/s10817-008-9103-8",
    "DOI": "10.1007/s10817-008-9103-8",
    "page": "143-189",
    "page-first": "143",
    "volume": "41",
    "issue": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:2835"
  },
  "platzer_differential_2015": {
    "id": "platzer_differential_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Platzer",
        "given": "André"
      }
    ],
    "title": "Differential Game Logic",
    "container-title": "ACM Trans. Comput. Logic",
    "issued": {
      "date-parts": [
        [
          "2015",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "1529-3785",
    "abstract": "Differential game logic (dGL) is a logic for specifying and verifying properties of hybrid games, i.e., games that combine discrete, continuous, and adversarial dynamics. Unlike hybrid systems, hybrid games allow choices in the system dynamics to be resolved adversarially by different players with different objectives. The logic dGL can be used to study the existence of winning strategies for such hybrid games, i.e., ways of resolving the player’s choices in some way so that he wins by achieving his objective for all choices of the opponent. Hybrid games are determined, i.e., from each state, one player has a winning strategy, yet computing their winning regions may take transfinitely many steps. The logic dGL, nevertheless, has a sound and complete axiomatization relative to any expressive logic. Separating axioms are identified that distinguish hybrid games from hybrid systems. Finally, dGL is proved to be strictly more expressive than the corresponding logic of hybrid systems by characterizing the expressiveness of both.",
    "keywords": "axiomatization, expressiveness, Game logic, hybrid games",
    "URL": "http://doi.acm.org/10.1145/2817824",
    "DOI": "10.1145/2817824",
    "page": "1:1-1:51",
    "page-first": "1",
    "volume": "17",
    "issue": "1",
    "_line": "FormalBib.bib:2851"
  },
  "platzer_differential_2018": {
    "id": "platzer_differential_2018",
    "type": "chapter",
    "author": [
      {
        "family": "Platzer",
        "given": "André"
      }
    ],
    "container-author": [
      {
        "family": "Platzer",
        "given": "André"
      }
    ],
    "title": "Differential Equations &amp; Differential Invariants",
    "container-title": "Logical Foundations of Cyber-Physical Systems",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-63587-3 978-3-319-63588-0",
    "URL": "http://link.springer.com/10.1007/978-3-319-63588-0_10",
    "DOI": "10.1007/978-3-319-63588-0_10",
    "publisher-place": "Cham",
    "page": "287-322",
    "page-first": "287",
    "language": "en-US",
    "_line": "FormalBib.bib:2868"
  },
  "crary_modules_2017": {
    "id": "crary_modules_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Crary",
        "given": "Karl"
      }
    ],
    "title": "Modules, Abstraction, and Parametric Polymorphism",
    "container-title": "Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages",
    "collection-title": "POPL 2017",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-4660-3",
    "abstract": "Reynolds's Abstraction theorem forms the mathematical foundation for data abstraction. His setting was the polymorphic lambda calculus. Today, many modern languages, such as the ML family, employ rich module systems designed to give more expressive support for data abstraction than the polymorphic lambda calculus, but analogues of the Abstraction theorem for such module systems have lagged far behind.   We give an account of the Abstraction theorem for a modern module calculus supporting generative and applicative functors, higher-order functors, sealing, and translucent signatures. The main issues to be overcome are: (1) the fact that modules combine both types and terms, so they must be treated as both simultaneously, (2) the effect discipline that models the distinction between transparent and opaque modules, and (3) a very rich language of type constructors supporting singleton kinds. We define logical equivalence for modules and show that it coincides with contextual equivalence. This substantiates the folk theorem that modules are good for data abstraction. All our proofs are formalized in Coq.",
    "keywords": "parametricity, Abstraction, logical relations, modules",
    "URL": "http://doi.acm.org/10.1145/3009837.3009892",
    "DOI": "10.1145/3009837.3009892",
    "publisher-place": "New York, NY, USA",
    "page": "100-113",
    "page-first": "100",
    "note": "crary/crary-mapp.pdf",
    "_line": "FormalBib.bib:2885"
  },
  "bertot_interactive_2004": {
    "id": "bertot_interactive_2004",
    "type": "book",
    "author": [
      {
        "family": "Bertot",
        "given": "Yves"
      },
      {
        "family": "Castéran",
        "given": "P."
      }
    ],
    "title": "Interactive theorem proving and program development: Coq'Art: the calculus of inductive constructions",
    "container-title-short": "Interactive theorem proving and program development",
    "collection-title": "Texts in theoretical computer science",
    "title-short": "Interactive theorem proving and program development",
    "issued": {
      "date-parts": [
        [
          "2004"
        ]
      ]
    },
    "publisher": "Springer",
    "number-of-pages": "469",
    "isbn": "978-3-540-20854-9",
    "keywords": "Automatic theorem proving, Computer programming",
    "URL": "http://www.labri.fr/perso/casteran/CoqArt/index.html",
    "publisher-place": "Berlin ; New York",
    "note": "OCLC: ocm55514299",
    "_line": "FormalBib.bib:2904"
  },
  "chaudhuri_trigger_2016": {
    "id": "chaudhuri_trigger_2016",
    "type": "chapter",
    "author": [
      {
        "family": "Leino",
        "given": "K. R. M."
      },
      {
        "family": "Pit-Claudel",
        "given": "Clément"
      }
    ],
    "editor": [
      {
        "family": "Chaudhuri",
        "given": "Swarat"
      },
      {
        "family": "Farzan",
        "given": "Azadeh"
      }
    ],
    "title": "Trigger Selection Strategies to Stabilize Program Verifiers",
    "container-title": "Computer Aided Verification",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-41527-7 978-3-319-41528-4",
    "URL": "http://link.springer.com/10.1007/978-3-319-41528-4_20",
    "DOI": "10.1007/978-3-319-41528-4_20",
    "publisher-place": "Cham",
    "page": "361-381",
    "page-first": "361",
    "volume": "9779",
    "_line": "FormalBib.bib:2919"
  },
  "urban_roscoq:_2015": {
    "id": "urban_roscoq:_2015",
    "type": "chapter",
    "author": [
      {
        "family": "Anand",
        "given": "Abhishek"
      },
      {
        "family": "Knepper",
        "given": "Ross"
      }
    ],
    "editor": [
      {
        "family": "Urban",
        "given": "Christian"
      },
      {
        "family": "Zhang",
        "given": "Xingyuan"
      }
    ],
    "title": "ROSCoq: Robots Powered by Constructive Reals",
    "container-title": "Interactive Theorem Proving",
    "container-title-short": "ROSCoq",
    "title-short": "ROSCoq",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "8"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-22101-4 978-3-319-22102-1",
    "URL": "http://link.springer.com/10.1007/978-3-319-22102-1_3",
    "DOI": "10.1007/978-3-319-22102-1_3",
    "publisher-place": "Cham",
    "page": "34-50",
    "page-first": "34",
    "volume": "9236",
    "_line": "FormalBib.bib:2936"
  },
  "noauthor_lean_nodate": {
    "id": "noauthor_lean_nodate",
    "type": "webpage",
    "title": "Lean Forward: Usable Computer-Checked Proofs and Computations",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "8"
        ]
      ]
    },
    "URL": "https://lean-forward.github.io/",
    "_line": "FormalBib.bib:2954"
  },
  "ye_verified_2019": {
    "id": "ye_verified_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Ye",
        "given": "Qianchuan"
      },
      {
        "family": "Delaware",
        "given": "Benjamin"
      }
    ],
    "title": "A Verified Protocol Buffer Compiler",
    "container-title": "Proceedings of the 8th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "collection-title": "CPP 2019",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-6222-1",
    "keywords": "Coq, Program verification, Serialization",
    "URL": "http://doi.acm.org/10.1145/3293880.3294105",
    "DOI": "10.1145/3293880.3294105",
    "publisher-place": "New York, NY, USA",
    "page": "222-233",
    "page-first": "222",
    "_line": "FormalBib.bib:2961"
  },
  "gonthier_small_2015": {
    "id": "gonthier_small_2015",
    "type": "report",
    "genre": "report",
    "author": [
      {
        "family": "Gonthier",
        "given": "Georges"
      },
      {
        "family": "Mahboubi",
        "given": "Assia"
      },
      {
        "family": "Tassi",
        "given": "Enrico"
      }
    ],
    "title": "A Small Scale Reflection Extension for the Coq system",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "5"
        ]
      ]
    },
    "publisher": "Inria Saclay Ile de France",
    "abstract": "This is the user manual of Ssreflect, a set of extensions to the proof scripting language of the Coq proof assistant. While these extensions were developed to support a particular proof methodology - small-scale reflection - most of them actually are of a quite general nature, improving the functionality of Coq in basic areas such as script layout and structuring, proof context management, and rewriting. Consequently, and in spite of the title of this document, most of the extensions described here should be of interest for all Coq users, whether they embrace small-scale reflection or not.",
    "URL": "https://hal.inria.fr/inria-00258384/document",
    "language": "en-US",
    "_line": "FormalBib.bib:2978"
  },
  "sozeau_subset_nodate": {
    "id": "sozeau_subset_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Sozeau",
        "given": "Matthieu"
      }
    ],
    "title": "Subset coercions in Coq",
    "container-title": "Springer-Verlag LNCS",
    "abstract": "Abstract. We propose a new language for writing programs with de-pendent types on top of the Coq proof assistant. This language permits to establish a phase distinction between writing and proving algorithms in the Coq environment. Concretely, this means allowing to write al-gorithms as easily as in a practical functional programming language whilst giving them as rich a specification as desired and proving that the code meets the specification using the whole Coq proof apparatus. This is achieved by extending conversion to an equivalence which re-lates types and subsets based on them, a technique originating from the “Predicate subtyping ” feature of PVS and following mathematical con-vention. The typing judgements can be translated to the Calculus of (Co-)Inductive Constructions (Cic) by means of an interpretation which inserts coercions at the appropriate places. These coercions can con-tain existential variables representing the propositional parts of the final term, corresponding to proof obligations (or PVS type-checking condi-tions). A prototype implementation of this process is integrated with the Coq environment. 1",
    "page": "237-252",
    "page-first": "237",
    "_line": "FormalBib.bib:2991"
  },
  "mohamed_first-class_2008": {
    "id": "mohamed_first-class_2008",
    "type": "chapter",
    "author": [
      {
        "family": "Sozeau",
        "given": "Matthieu"
      },
      {
        "family": "Oury",
        "given": "Nicolas"
      }
    ],
    "editor": [
      {
        "family": "Mohamed",
        "given": "Otmane Ait"
      },
      {
        "family": "Muñoz",
        "given": "César"
      },
      {
        "family": "Tahar",
        "given": "Sofiène"
      }
    ],
    "title": "First-Class Type Classes - TPHOLs Talk",
    "container-title": "Theorem Proving in Higher Order Logics",
    "issued": {
      "date-parts": [
        [
          "2008"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-540-71065-3 978-3-540-71067-7",
    "URL": "http://link.springer.com/10.1007/978-3-540-71067-7_23",
    "DOI": "10.1007/978-3-540-71067-7_23",
    "publisher-place": "Berlin, Heidelberg",
    "page": "278-293",
    "page-first": "278",
    "volume": "5170",
    "language": "en-US",
    "_line": "FormalBib.bib:3000"
  },
  "sozeau_first-class_2008": {
    "id": "sozeau_first-class_2008",
    "type": "book",
    "author": [
      {
        "family": "Sozeau",
        "given": "Matthieu"
      },
      {
        "family": "Oury",
        "given": "Nicolas"
      }
    ],
    "title": "First-class type classes",
    "issued": {
      "date-parts": [
        [
          "2008"
        ]
      ]
    },
    "abstract": "Abstract. Type Classes have met a large success in Haskell and Isabelle, as a solution for sharing notations by overloading and for specifying with abstract structures by quantification on contexts. However, both systems are limited by second-class implementations of these constructs, and these limitations are only overcomed by ad-hoc extensions to the respective systems. We propose an embedding of type classes into a dependent type theory that is first-class and supports some of the most popular extensions right away. The implementation is correspondingly cheap, general and integrates well inside the system, as we have experimented in Coq. We show how it can be used to help structured programming and proving by way of examples. 1",
    "_line": "FormalBib.bib:3018"
  },
  "calcagno_moving_nodate": {
    "id": "calcagno_moving_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Calcagno",
        "given": "Cristiano"
      },
      {
        "family": "Distefano",
        "given": "Dino"
      },
      {
        "family": "Dubreil",
        "given": "Jeremy"
      },
      {
        "family": "O'Hearn",
        "given": "Peter"
      }
    ],
    "title": "Moving Fast with Software Verification.Facebook Research",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "For organisations like Facebook, high quality software is important. However, the pace of change and increasing complexity of modern code makes it difficult to produce error free software. Available tools are often lacking in helping programmers develop more reliable and secure applications.",
    "URL": "https://research.fb.com/publications/moving-fast-with-software-verification",
    "language": "en-US",
    "_line": "FormalBib.bib:3026"
  },
  "shrobe_trust-management_2009": {
    "id": "shrobe_trust-management_2009",
    "type": "article-journal",
    "author": [
      {
        "family": "Shrobe",
        "given": "Howard"
      },
      {
        "family": "DeHon",
        "given": "Andre"
      },
      {
        "family": "Knight",
        "given": "Thomas"
      }
    ],
    "title": "Trust-Management, Intrusion-Tolerance, Accountability, and Reconstitution Architecture (TIARA)",
    "issued": {
      "date-parts": [
        [
          "2009",
          "12"
        ]
      ]
    },
    "abstract": "This report describes the Trust-management, Intrusion-tolerance, Accountability, and Reconstitution Architecture (TIARA) system,\na broad design effort including novel computer architecture, operating system and application middleware. TIARA illustrates that a\nhighly secure computer system can be designed without sacrificing performance. TIARA involves three major sub-efforts: A\nhardware security tagged architecture (STA) that tags each word of the computer’s memory with metadata such as the data type and\ncompartment of the data. The STA hardware enforces access rules controlling which principals are allowed to perform which\noperations on which data. This allows the construction of a novel Zero-kernel Operating System (ZKOS) that has no single all\nprivileged kernel and that provides strong guarantees against penetration. Finally TIARA provides a level of application middleware\nthat enforces architectural level constraints and maintains the provenance of application data. All common exploits are preventable\nby the TIARA architecture and this incurs only a minor increase in chip area.",
    "URL": "https://apps.dtic.mil/dtic/tr/fulltext/u2/a511350.pdf",
    "page": "133",
    "page-first": "133",
    "language": "en-US",
    "_line": "FormalBib.bib:3037"
  },
  "minsky_real_nodate": {
    "id": "minsky_real_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Minsky",
        "given": "Yaron"
      },
      {
        "family": "Madhavapeddy",
        "given": "Anil"
      },
      {
        "family": "Hickey",
        "given": "Jason"
      }
    ],
    "title": "Real World OCaml",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "http://dev.realworldocaml.org/",
    "_line": "FormalBib.bib:3057"
  },
  "leroy_ocaml_nodate": {
    "id": "leroy_ocaml_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Leroy",
        "given": "Xavier"
      }
    ],
    "title": "OCaml Home Page",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://ocaml.org/",
    "_line": "FormalBib.bib:3065"
  },
  "jacobs_verifast/verifast:_2019": {
    "id": "jacobs_verifast/verifast:_2019",
    "type": "book",
    "author": [
      {
        "family": "Jacobs",
        "given": "Bart"
      }
    ],
    "title": "verifast/verifast: Research prototype tool for modular formal verification of C and Java programs",
    "container-title-short": "Research prototype tool for modular formal verification of C and Java programs",
    "title-short": "Research prototype tool for modular formal verification of C and Java programs",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1",
          "25"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "verifast",
    "URL": "https://github.com/verifast/verifast",
    "note": "original-date: 2013-11-19T08:57:02Z",
    "_line": "FormalBib.bib:3073"
  },
  "pottier_menhir_nodate": {
    "id": "pottier_menhir_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Pottier",
        "given": "Francois"
      },
      {
        "family": "REgis-Gianas",
        "given": "Yan"
      }
    ],
    "title": "Menhir Reference Manual (version 20181113)",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "http://gallium.inria.fr/~fpottier/menhir/manual.html",
    "_line": "FormalBib.bib:3085"
  },
  "patterson_compositional_nodate": {
    "id": "patterson_compositional_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Patterson",
        "given": "Daniel"
      },
      {
        "family": "Ahmed",
        "given": "Amal"
      }
    ],
    "title": "On Compositional Compiler Correctness and Fully Abstract Compilation - POPL 2018",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://popl18.sigplan.org/event/prisc-2018-on-compositional-compiler-correctness-and-fully-abstract-compilation",
    "_line": "FormalBib.bib:3093"
  },
  "wikibook_latex_nodate": {
    "id": "wikibook_latex_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "",
        "dropping-particle": "wikibook"
      }
    ],
    "title": "LaTeX - Wikibooks, open books for an open world",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://en.wikibooks.org/wiki/LaTeX",
    "_line": "FormalBib.bib:3101"
  },
  "lamport_specifying_nodate": {
    "id": "lamport_specifying_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Lamport",
        "given": "Leslie"
      }
    ],
    "title": "Specifying Systems",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://lamport.azurewebsites.net/tla/book.html",
    "_line": "FormalBib.bib:3109"
  },
  "kubota_foundations_nodate": {
    "id": "kubota_foundations_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Kubota",
        "given": "Ken"
      }
    ],
    "title": "Foundations of Mathematics – Owl of Minerva Press",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "http://owlofminerva.net/foundations-of-mathematics/",
    "language": "en-US",
    "_line": "FormalBib.bib:3117"
  },
  "kaiser_destruct_nodate": {
    "id": "kaiser_destruct_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Kaiser",
        "given": "Jan-Oliver"
      },
      {
        "family": "Ziliani",
        "given": "Beta"
      }
    ],
    "title": "A “destruct” Tactic for Mtac2 - POPL 2018",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://popl18.sigplan.org/event/coqpl-2018-a-destruct-tactic-for-mtac2",
    "_line": "FormalBib.bib:3126"
  },
  "birkedal_iris_nodate": {
    "id": "birkedal_iris_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Birkedal",
        "given": "Lars"
      },
      {
        "family": "Bizjak",
        "given": "Aleš"
      }
    ],
    "title": "Iris Tutorial",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://iris-project.org/tutorial-material.html",
    "_line": "FormalBib.bib:3134"
  },
  "jung_iris_nodate": {
    "id": "jung_iris_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Jung",
        "given": "Ralf"
      }
    ],
    "title": "Iris Project",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://iris-project.org/",
    "_line": "FormalBib.bib:3142"
  },
  "voevodsky_homotopy_nodate": {
    "id": "voevodsky_homotopy_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Voevodsky",
        "given": "Vladimir"
      }
    ],
    "title": "Homotopy Type Theory: Univalent Foundations of Mathematics",
    "page": "490",
    "page-first": "490",
    "language": "en-US",
    "_line": "FormalBib.bib:3150"
  },
  "swamy_project_nodate": {
    "id": "swamy_project_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Swamy",
        "given": "Nikhil"
      }
    ],
    "title": "Project Everest - Verified Secure Implementations of the HTTPS Ecosystem.Microsoft Research",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "This project proposes to deﬁnitively solve the problem of a brittle HTTPS ecosystem by constructing a more secure, high performance, standards-compliant, veriﬁed implementation of the full HTTPS ecosystem. Unlike other veriﬁed software projects, our expedition aims to deploy Everest within existing software as a drop-in replacement in mainstream web browsers, servers, and other popular tools.",
    "URL": "https://www.microsoft.com/en-us/research/project/project-everest-verified-secure-implementations-https-ecosystem/",
    "language": "en-US",
    "_line": "FormalBib.bib:3158"
  },
  "syme_fsharp_2019": {
    "id": "syme_fsharp_2019",
    "type": "book",
    "author": [
      {
        "family": "Syme",
        "given": "Don"
      }
    ],
    "title": "The Fsharp Compiler, Core Library &amp; Tools (F&hash; Software Foundation Repository): fsharp/fsharp",
    "container-title-short": "The F&hash; Compiler, Core Library &amp; Tools (F&hash; Software Foundation Repository)",
    "title-short": "The F&hash; Compiler, Core Library &amp; Tools (F&hash; Software Foundation Repository)",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "F&hash; Software Foundation Repositories",
    "URL": "https://github.com/fsharp/fsharp",
    "note": "original-date: 2010-12-13T00:19:52Z",
    "_line": "FormalBib.bib:3169"
  },
  "syme_fsharp_2019-1": {
    "id": "syme_fsharp_2019-1",
    "type": "book",
    "author": [
      {
        "family": "Syme",
        "given": "Don"
      }
    ],
    "title": "Fsharp design: RFCs and docs related to the F&hash; language design process,",
    "container-title-short": "RFCs and docs related to the F&hash; language design process, see https",
    "title-short": "RFCs and docs related to the F&hash; language design process, see https",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1",
          "29"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "F&hash; Software Foundation Repositories",
    "URL": "https://github.com/fsharp/fslang-design",
    "note": "original-date: 2014-06-25T13:07:35Z",
    "_line": "FormalBib.bib:3181"
  },
  "melquiond_why3_nodate": {
    "id": "melquiond_why3_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Melquiond",
        "given": "Guillaume"
      }
    ],
    "title": "Why3",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "http://why3.lri.fr/",
    "_line": "FormalBib.bib:3192"
  },
  "jeannet_apron_nodate": {
    "id": "jeannet_apron_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Jeannet",
        "given": "Bertrand"
      },
      {
        "family": "Miné",
        "given": "Antoine"
      }
    ],
    "title": "APRON numerical abstract domain library",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "http://apron.cri.ensmp.fr/library/",
    "_line": "FormalBib.bib:3200"
  },
  "cea_frama-c_nodate": {
    "id": "cea_frama-c_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "",
        "dropping-particle": "cea"
      }
    ],
    "title": "Frama-C",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://frama-c.com/",
    "_line": "FormalBib.bib:3208"
  },
  "sozeau_metacoq_2019": {
    "id": "sozeau_metacoq_2019",
    "type": "book",
    "author": [
      {
        "family": "Sozeau",
        "given": "Matthieu"
      }
    ],
    "title": "MetaCoq - Metaprogramming in Coq (Was template-coq)",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "MetaCoq",
    "URL": "https://github.com/MetaCoq/metacoq",
    "note": "original-date: 2017-10-19T11:10:54Z",
    "_line": "FormalBib.bib:3216"
  },
  "anand_towards_nodate": {
    "id": "anand_towards_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Anand",
        "given": "Abhishek"
      },
      {
        "family": "Boulier",
        "given": "Simon"
      },
      {
        "family": "Cohen",
        "given": "Cyril"
      },
      {
        "family": "Sozeau",
        "given": "Matthieu"
      },
      {
        "family": "Tabareau",
        "given": "Nicolas"
      }
    ],
    "title": "Towards Certified Meta-Programming with Typed Template-Coq &bar; SpringerLink",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Template-Coq (https://template-coq.github.io/template-coq) is a plugin for Coq, originally implemented by Malecha \\[18\\], which provides a reifier for Coq terms and global declarations, as represented in the Coq kernel, as well as a denotation command. Initially, it was developed for the purpose of writing functions on Coq’s AST in Gallina. Recently, it was used in the CertiCoq certified compiler project \\[4\\], as its front-end language, to derive parametricity properties \\[3\\], and to extract Coq terms to a CBV   𝜆 -calculus \\[13\\]. However, the syntax lacked semantics, be it typing semantics or operational semantics, which should reflect, as formal specifications in Coq, the semantics of Coq’s type theory itself. The tool was also rather bare bones, providing only rudimentary quoting and unquoting commands. We generalize it to handle the entire Calculus of Inductive Constructions (CIC), as implemented by Coq, including the kernel’s declaration structures for definitions and inductives, and implement a monad for general manipulation of Coq’s logical environment. We demonstrate how this setup allows Coq users to define many kinds of general purpose plugins, whose correctness can be readily proved in the system itself, and that can be run efficiently after extraction. We give a few examples of implemented plugins, including a parametricity translation. We also advocate the use of Template-Coq as a foundation for higher-level tools.",
    "URL": "https://link.springer.com/chapter/10.1007%2F978-3-319-94821-8_2",
    "_line": "FormalBib.bib:3227"
  },
  "appel_deepspecdb_2019": {
    "id": "appel_deepspecdb_2019",
    "type": "book",
    "author": [
      {
        "family": "Appel",
        "given": "Andrew W."
      }
    ],
    "title": "DeepSpecDB - github",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "PrincetonUniversity",
    "URL": "https://github.com/PrincetonUniversity/DeepSpecDB",
    "note": "original-date: 2017-11-30T14:24:30Z",
    "_line": "FormalBib.bib:3236"
  },
  "adewale_implementing_nodate": {
    "id": "adewale_implementing_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Adewale",
        "given": "Oluwatosin"
      }
    ],
    "title": "Implementing a high-performance key-value store using a trie of B+-Trees with cursors &bar; Computer Science Department at Princeton University",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Abstract\nIn this paper, we discuss the implementation of a serial main-memory key-value store based on Masstree\\[6\\]. Similar to Masstree, the key-value store is implemented as a trie-like tree of B+-Trees, where each B+-Tree is responsible for a xed-length slice of a variable-length key. However, one of the major dierences between our key-value store and Masstree is that our B+-tree implementation (a component of the key-value store) takes linear time to insert a set of sorted records. This is compared to a traditional B+-tree implementation that would take linearithmic time. Moreover, partially sorting a sequence of operation leads to substantial performance gains. This is made possible using a data structure for navigating B+-trees called a B+-tree cursor. As our next operation is amortized constant time, our B+-tree does not need to maintain cross links between leaf nodes. We also briefy show that this same data structure can be extended to the trie of B+-Trees to ensure amortized linear time for bulk insertion of key-value pairs in the key-value store. We were inspired with this idea of B+-Tree cursors from the SQLite \\[5\\] B-tree source code.",
    "URL": "https://www.cs.princeton.edu/research/techreps/TR-004-18",
    "_line": "FormalBib.bib:3247"
  },
  "appel_certicoq:_nodate": {
    "id": "appel_certicoq:_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Appel",
        "given": "Andrew W."
      }
    ],
    "title": "CertiCoq: A verified compiler for Coq - POPL 2017",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://popl17.sigplan.org/event/main-certicoq-a-verified-compiler-for-coq",
    "_line": "FormalBib.bib:3257"
  },
  "sozeau_typed_nodate": {
    "id": "sozeau_typed_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Sozeau",
        "given": "Matthieu"
      }
    ],
    "title": "Typed Template Coq - Slides",
    "page": "11",
    "page-first": "11",
    "language": "en-US",
    "_line": "FormalBib.bib:3265"
  },
  "sozeau_typed_nodate-1": {
    "id": "sozeau_typed_nodate-1",
    "type": "webpage",
    "author": [
      {
        "family": "Sozeau",
        "given": "Matthieu"
      }
    ],
    "title": "Typed Template Coq - POPL 2018",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://popl18.sigplan.org/event/coqpl-2018-typed-template-coq",
    "_line": "FormalBib.bib:3273"
  },
  "acm_msc2010_nodate": {
    "id": "acm_msc2010_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "",
        "dropping-particle": "acm"
      }
    ],
    "title": "MSC2010 database",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://mathscinet.ams.org/msc/msc2010.html",
    "_line": "FormalBib.bib:3281"
  },
  "acm_acm_nodate": {
    "id": "acm_acm_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "",
        "dropping-particle": "acm"
      }
    ],
    "title": "ACM Classification Codes",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://cran.r-project.org/web/classifications/ACM.html",
    "_line": "FormalBib.bib:3289"
  },
  "czajka_coqhammer:_nodate": {
    "id": "czajka_coqhammer:_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Czajka",
        "given": "Lukasz"
      },
      {
        "family": "Kaliszyk",
        "given": "Cezary"
      }
    ],
    "title": "CoqHammer: Strong Automation for Program Verification - CoqPL 2018",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "We present CoqHammer: the first full hammer system for\nthe Coq proof assistant. The system translates Coq logic\nto untyped first-order logic and uses external automated\ntheorem provers (ATPs) to prove the translations of user\ngiven conjectures. Based on the output of the ATPs, the\nconjecture is then re-proved in the logic of Coq using an\neauto-type proof search algorithm. Together with machinelearning\nbased selection of relevant premises this constitutes\na full hammer system.\nThe performance of the overall procedure has been evaluated\nin a bootstrapping scenario emulating the development\nof the Coq standard library. Over 40&perc; of the theorems in\nthe Coq standard library can be proved in a push-button\nmode in about 40 seconds of real time on a 8-CPU system.\nThis offers a huge saving of human work in programming\nlanguage formalizations.",
    "URL": "https://popl18.sigplan.org/event/coqpl-2018-coqhammer-strong-automation-for-program-verification",
    "_line": "FormalBib.bib:3297"
  },
  "ahrendt_deductive_nodate": {
    "id": "ahrendt_deductive_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Ahrendt",
        "given": "Wolfgang"
      }
    ],
    "title": "Deductive Software Verification – The KeY BookFrom Theory to Practice – The KeY Project",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "URL": "https://www.key-project.org/thebook2/",
    "note": "cyber-physical/KeY directory has pdfs for the chapters.",
    "language": "en-US",
    "_line": "FormalBib.bib:3321"
  },
  "platzer_keymaera_nodate": {
    "id": "platzer_keymaera_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Platzer",
        "given": "André"
      }
    ],
    "title": "KeYmaera X: Documentation",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "URL": "http://www.ls.cs.cmu.edu/KeYmaeraX/documentation.html",
    "_line": "FormalBib.bib:3331"
  },
  "absint_compcert_nodate": {
    "id": "absint_compcert_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Absint"
      }
    ],
    "title": "CompCert - Publications",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "URL": "http://compcert.inria.fr/publi.html",
    "_line": "FormalBib.bib:3339"
  },
  "hermanns_local_2006": {
    "id": "hermanns_local_2006",
    "type": "chapter",
    "author": [
      {
        "family": "Distefano",
        "given": "Dino"
      },
      {
        "family": "O’Hearn",
        "given": "Peter W."
      },
      {
        "family": "Yang",
        "given": "Hongseok"
      }
    ],
    "editor": [
      {
        "family": "Hermanns",
        "given": "Holger"
      },
      {
        "family": "Palsberg",
        "given": "Jens"
      }
    ],
    "title": "A Local Shape Analysis Based on Separation Logic",
    "container-title": "Tools and Algorithms for the Construction and Analysis of Systems",
    "issued": {
      "date-parts": [
        [
          "2006"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-540-33056-1 978-3-540-33057-8",
    "abstract": "We describe a program analysis for linked list programs where the abstract domain uses formulae from separation logic.",
    "URL": "http://link.springer.com/10.1007/11691372_19",
    "DOI": "10.1007/11691372_19",
    "publisher-place": "Berlin, Heidelberg",
    "page": "287-302",
    "page-first": "287",
    "volume": "3920",
    "language": "en-US",
    "_line": "FormalBib.bib:3347"
  },
  "ohearn_local_2001": {
    "id": "ohearn_local_2001",
    "type": "chapter",
    "author": [
      {
        "family": "O’Hearn",
        "given": "Peter"
      },
      {
        "family": "Reynolds",
        "given": "John"
      },
      {
        "family": "Yang",
        "given": "Hongseok"
      }
    ],
    "editor": [
      {
        "family": "Fribourg",
        "given": "Laurent"
      }
    ],
    "title": "Local Reasoning about Programs that Alter Data Structures",
    "container-title": "Computer Science Logic",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2001"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-540-44802-0",
    "abstract": "We describe an extension of Hoare’s logic for reasoning about programs that alter data structures. We consider a low-level storage model based on a heap with associated lookup, update, allocation and deallocation operations, and unrestricted address arithmetic. The assertion language is based on a possible worlds model of the logic of bunched implications, and includes spatial conjunction and implication connectives alongside those of classical logic. Heap operations are axiomatized using what we call the “small axioms”, each of which mentions only those cells accessed by a particular command. Through these and a number of examples we show that the formalism supports local reasoning: A specification and proof can concentrate on only those cells in memory that a program accesses.This paper builds on earlier work by Burstall, Reynolds, Ishtiaq and O’Hearn on reasoning about data structures.",
    "keywords": "Hoare Logic, Frame Problem, Local Reasoning, Memory Fault, Weak Precondition",
    "page": "1-19",
    "page-first": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:3366"
  },
  "berdine_smallfoot:_2006": {
    "id": "berdine_smallfoot:_2006",
    "type": "paper-conference",
    "author": [
      {
        "family": "Berdine",
        "given": "Josh"
      },
      {
        "family": "Calcagno",
        "given": "Cristiano"
      },
      {
        "family": "O’Hearn",
        "given": "Peter W."
      }
    ],
    "editor": [
      {
        "family": "Boer",
        "given": "Frank S.",
        "dropping-particle": "de"
      },
      {
        "family": "Bonsangue",
        "given": "Marcello M."
      },
      {
        "family": "Graf",
        "given": "Susanne"
      },
      {
        "family": "Roever",
        "given": "Willem-Paul",
        "dropping-particle": "de"
      }
    ],
    "title": "Smallfoot: Modular Automatic Assertion Checking with Separation Logic",
    "container-title": "Formal Methods for Components and Objects",
    "container-title-short": "Smallfoot",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "Smallfoot",
    "issued": {
      "date-parts": [
        [
          "2006"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-540-36750-5",
    "abstract": "Separation logic is a program logic for reasoning about programs that manipulate pointer data structures. We describe Smallfoot, a tool for checking certain lightweight separation logic specifications. The assertions describe the shapes of data structures rather than their detailed contents, and this allows reasoning to be fully automatic. The presentation in the paper is tutorial in style. We illustrate what the tool can do via examples which are oriented toward novel aspects of separation logic, namely: avoidance of frame axioms (which say what a procedure does not change); embracement of “dirty” features such as memory disposal and address arithmetic; information hiding in the presence of pointers; and modular reasoning about concurrent programs.",
    "keywords": "Symbolic Execution, Separation Logic, Free List, Information Hiding, Tree Predicate",
    "page": "115-137",
    "page-first": "115",
    "language": "en-US",
    "_line": "FormalBib.bib:3382"
  },
  "andrew_oracle_2008": {
    "id": "andrew_oracle_2008",
    "type": "book",
    "author": [
      {
        "family": "Andrew",
        "given": "Advisor"
      }
    ],
    "title": "Oracle Semantics Aquinas Hobor",
    "issued": {
      "date-parts": [
        [
          "2008"
        ]
      ]
    },
    "abstract": "We define a Concurrent Separation Logic with first-class locks and threads for the C language, and prove its soundness in Coq with re-spect to a compilable operataional semantics. We define the language Concurrent C minor, an extension of the C minor language of Leroy. C minor was designed as the highest-level intermediate language in the CompCert certified ANSI C compiler, and we add to it lock, unlock, and fork statements to make Concurrent C minor, giving it a standard Pthreads style of concurrency. We define a Concurrent Separation Logic for Concurrent C minor, which extends the original Concurrent Separation Logic of O’Hearn to handle first-class locks and threads. We then prove the soundness of the logic with respect to the opera-tional semantics of the language. First, we define an erased concurrent operational semantics for Concurrent C minor that is a reasonable ab-",
    "_line": "FormalBib.bib:3399"
  },
  "amorim_verified_2013": {
    "id": "amorim_verified_2013",
    "type": "book",
    "author": [
      {
        "family": "Amorim",
        "given": "Arthur Azevedo de"
      },
      {
        "family": "Collins",
        "given": "Nathan"
      },
      {
        "family": "DeHon",
        "given": "André"
      },
      {
        "family": "Demange",
        "given": "Delphine"
      },
      {
        "family": "Hritcu",
        "given": "Cătălin"
      },
      {
        "family": "Pichardie",
        "given": "David"
      },
      {
        "family": "Pierce",
        "given": "Benjamin C."
      },
      {
        "family": "Pollack",
        "given": "Randy"
      },
      {
        "family": "Tolmach",
        "given": "Andrew"
      }
    ],
    "title": "A Verified Information-Flow Architecture (Long version)",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "abstract": "SAFE is a clean-slate effort to build a highly secure computer system, including pervasive mechanisms for tracking and limiting information flows. At the lowest level, the SAFE hardware supports fine-grained programmable tags, with efficient and flexible propagation and combination of tags as instructions are executed. The operating system virtualizes these generic facilities to present an information-flow abstract machine, on which user programs can label sensitive data with rich confidentiality and integrity policies. We present a formal, machine-checked model of the key information-flow mechanisms of the SAFE hardware and software, together with an end-to-end proof of noninterference for this model.",
    "_line": "FormalBib.bib:3407"
  },
  "azevedo_de_amorim_verified_2014": {
    "id": "azevedo_de_amorim_verified_2014",
    "type": "paper-conference",
    "author": [
      {
        "family": "Azevedo de Amorim",
        "given": "Arthur"
      },
      {
        "family": "Collins",
        "given": "Nathan"
      },
      {
        "family": "DeHon",
        "given": "André"
      },
      {
        "family": "Demange",
        "given": "Delphine"
      },
      {
        "family": "Hriţcu",
        "given": "Cătălin"
      },
      {
        "family": "Pichardie",
        "given": "David"
      },
      {
        "family": "Pierce",
        "given": "Benjamin C."
      },
      {
        "family": "Pollack",
        "given": "Randy"
      },
      {
        "family": "Tolmach",
        "given": "Andrew"
      }
    ],
    "title": "A Verified Information-flow Architecture",
    "container-title": "Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "collection-title": "POPL '14",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-2544-8",
    "abstract": "SAFE is a clean-slate design for a highly secure computer system, with pervasive mechanisms for tracking and limiting information flows. At the lowest level, the SAFE hardware supports fine-grained programmable tags, with efficient and flexible propagation and combination of tags as instructions are executed. The operating system virtualizes these generic facilities to present an information-flow abstract machine that allows user programs to label sensitive data with rich confidentiality policies. We present a formal, machine-checked model of the key hardware and software mechanisms used to control information flow in SAFE and an end-to-end proof of noninterference for this model.",
    "keywords": "formal verification, security, clean-slate design, information-flow control, refinement, tagged architecture",
    "URL": "http://doi.acm.org/10.1145/2535838.2535839",
    "DOI": "10.1145/2535838.2535839",
    "publisher-place": "New York, NY, USA",
    "page": "165-178",
    "page-first": "165",
    "_line": "FormalBib.bib:3415"
  },
  "krebbers_type_2011": {
    "id": "krebbers_type_2011",
    "type": "article-journal",
    "author": [
      {
        "family": "Krebbers",
        "given": "Robbert"
      },
      {
        "family": "Spitters",
        "given": "Bas"
      }
    ],
    "title": "Type classes for efficient exact real arithmetic in Coq",
    "container-title": "arXiv:1106.3448 \\[cs, math\\]",
    "issued": {
      "date-parts": [
        [
          "2011",
          "6",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Floating point operations are fast, but require continuous effort on the part of the user in order to ensure that the results are correct. This burden can be shifted away from the user by providing a library of exact analysis in which the computer handles the error estimates. Previously, we \\[Krebbers/Spitters 2011\\] provided a fast implementation of the exact real numbers in the Coq proof assistant. Our implementation improved on an earlier implementation by O'Connor by using type classes to describe an abstract specification of the underlying dense set from which the real numbers are built. In particular, we used dyadic rationals built from Coq's machine integers to obtain a 100 times speed up of the basic operations already. This article is a substantially expanded version of \\[Krebbers/Spitters 2011\\] in which the implementation is extended in the various ways. First, we implement and verify the sine and cosine function. Secondly, we create an additional implementation of the dense set based on Coq's fast rational numbers. Thirdly, we extend the hierarchy to capture order on undecidable structures, while it was limited to decidable structures before. This hierarchy, based on type classes, allows us to share theory on the naturals, integers, rationals, dyadics, and reals in a convenient way. Finally, we obtain another dramatic speed-up by avoiding evaluation of termination proofs at runtime.",
    "keywords": "Computer Science - Logic in Computer Science, D.2.4, F.4.1, G.1, Mathematics - Numerical Analysis",
    "URLtext": "1106.3448",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1106.3448",
    "URL": "http://arxiv.org/abs/1106.3448",
    "DOI": "10.2168/LMCS-9(1:01)2013",
    "_line": "FormalBib.bib:3433"
  },
  "holzl_type_2013": {
    "id": "holzl_type_2013",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hölzl",
        "given": "Johannes"
      },
      {
        "family": "Immler",
        "given": "Fabian"
      },
      {
        "family": "Huffman",
        "given": "Brian"
      }
    ],
    "editor": [
      {
        "family": "Blazy",
        "given": "Sandrine"
      },
      {
        "family": "Paulin-Mohring",
        "given": "Christine"
      },
      {
        "family": "Pichardie",
        "given": "David"
      }
    ],
    "title": "Type Classes and Filters for Mathematical Analysis in Isabelle/HOL",
    "container-title": "Interactive Theorem Proving",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-39634-2",
    "abstract": "The theory of analysis in Isabelle/HOL derives from earlier formalizations that were limited to specific concrete types: ℝ, ℂ and ℝ n . Isabelle’s new analysis theory unifies and generalizes these earlier efforts. The improvements are centered on two primary contributions: a generic theory of limits based on filters, and a new hierarchy of type classes that includes various topological, metric, vector, and algebraic spaces. These let us apply many results in multivariate analysis to types which are not Euclidean spaces, such as the extended real numbers, bounded continuous functions, or finite maps.",
    "keywords": "Topology, Euclidean vector spaces, Filters, Isabelle/HOL, Limits, Mathematical analysis, Type classes",
    "page": "279-294",
    "page-first": "279",
    "language": "en-US",
    "_line": "FormalBib.bib:3448"
  },
  "boldo_formalization_2016": {
    "id": "boldo_formalization_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Boldo",
        "given": "Sylvie"
      },
      {
        "family": "Lelay",
        "given": "Catherine"
      },
      {
        "family": "Melquiond",
        "given": "Guillaume"
      }
    ],
    "title": "Formalization of Real Analysis: A Survey of Proof Assistants and Libraries",
    "container-title": "Mathematical Structures in Computer Science",
    "container-title-short": "Formalization of Real Analysis",
    "title-short": "Formalization of Real Analysis",
    "issued": {
      "date-parts": [
        [
          "2016",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "In the recent years, numerous proof systems have improved enough to be used for formally verifying non-trivial mathematical results. They, however, have different purposes and it is not always easy to choose which one is adapted to undertake a formalization effort. In this survey, we focus on properties related to real analysis: real numbers, arithmetic operators, limits, differentiability, integrability, and so on. We have chosen to look into the formalizations provided in standard by the following systems: Coq, HOL4, HOL Light, Isabelle/HOL, Mizar, ProofPower-HOL, and PVS. We have also accounted for large developments that play a similar role or extend standard libraries: ACL2(r) for ACL2, C-CoRN/MathClasses for Coq, and the NASA PVS library. This survey presents how real numbers have been defined in these various provers and how the notions of real analysis described above have been formalized. We also look at the methods of automation these systems provide for real analysis.",
    "URL": "https://hal.inria.fr/hal-00806920/document",
    "DOI": "10.1017/S0960129514000437",
    "page": "1196-1233",
    "page-first": "1196",
    "volume": "26",
    "issue": "7",
    "language": "en-US",
    "_line": "FormalBib.bib:3464"
  },
  "boldo_coquelicot:_2013": {
    "id": "boldo_coquelicot:_2013",
    "type": "no-type",
    "author": [
      {
        "family": "Boldo",
        "given": "Sylvie"
      },
      {
        "family": "Lelay",
        "given": "Catherine"
      },
      {
        "family": "Melquiond",
        "given": "Guillaume"
      }
    ],
    "title": "Coquelicot: A User-Friendly Library of Real Analysis for Coq",
    "container-title-short": "Coquelicot",
    "title-short": "Coquelicot",
    "issued": {
      "date-parts": [
        [
          "2013",
          "9",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Real analysis is pervasive to many applications, if only because it is a suitable tool for modeling physical or socio-economical systems. As such, its support is warranted in proof assistants, so that the users have a way to formally verify mathematical theorems and correctness of critical systems. The Coq system comes with an axiomatization of standard real numbers and a library of theorems on real analysis. Unfortunately, this standard library is lacking some widely used results. For instance, power series are not developed further than their definition. Moreover, the definitions of integrals and derivatives are based on dependent types, which make them especially cumbersome to use in practice. To palliate these inadequacies, we have designed a user-friendly library: Coquelicot. An easier way of writing formulas and theorem statements is achieved by relying on total functions in place of dependent types for limits, derivatives, integrals, power series, and so on. To help with the proof process, the library comes with a comprehensive set of theorems that cover not only these notions, but also some extensions such as parametric integrals, two-dimensional differentiability, asymptotic behaviors. It also offers some automations for performing differentiability proofs. Moreover, Coquelicot is a conservative extension of Coq's standard library and we provide correspondence theorems between the two libraries. We have exercised the library on several use cases: in an exam at university entry level, for the definitions and properties of Bessel functions, and for the solution of the one-dimensional wave equation.",
    "URL": "https://hal.inria.fr/hal-00860648/document",
    "language": "en-US",
    "_line": "FormalBib.bib:3481"
  },
  "immler_verified_2018": {
    "id": "immler_verified_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Immler",
        "given": "Fabian"
      }
    ],
    "title": "A Verified ODE Solver and the Lorenz Attractor",
    "container-title": "Journal of Automated Reasoning",
    "container-title-short": "J Autom Reasoning",
    "issued": {
      "date-parts": [
        [
          "2018",
          "6",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "1573-0670",
    "abstract": "A rigorous numerical algorithm, formally verified with Isabelle/HOL, is used to certify the computations that Tucker used to prove chaos for the Lorenz attractor. The verification is based on a formalization of a diverse variety of mathematics and algorithms. Formalized mathematics include ordinary differential equations and Poincaré maps. Algorithms include low level approximation schemes based on Runge–Kutta methods and affine arithmetic. On a high level, reachability analysis is guided by static hybridization and adaptive step-size control and splitting. The algorithms are systematically refined towards an implementation that can be executed on Tucker’s original input data.",
    "keywords": "Isabelle/HOL, Lorenz attractor, Ordinary differential equation, Poincaré map, Rigorous numerics",
    "URL": "https://doi.org/10.1007/s10817-017-9448-y",
    "DOI": "10.1007/s10817-017-9448-y",
    "page": "73-111",
    "page-first": "73",
    "volume": "61",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:3493"
  },
  "boldo_round-off_2018": {
    "id": "boldo_round-off_2018",
    "type": "manuscript",
    "author": [
      {
        "family": "Boldo",
        "given": "Sylvie"
      },
      {
        "family": "Faissole",
        "given": "Florian"
      },
      {
        "family": "Chapoutot",
        "given": "Alexandre"
      }
    ],
    "title": "Round-off error and exceptional behavior analysis of explicit Runge-Kutta methods",
    "issued": {
      "date-parts": [
        [
          "2018",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Numerical integration schemes are mandatory to understand complex behaviors of dynamical systems described by ordinary differential equations. Implementation of these numerical methods involve floating-point computations and propagation of round-off errors. This paper presents a new fine-grained analysis of round-off errors in explicit Runge-Kutta integration methods, taking into account exceptional behaviors, such as underflow and overflow. Linear stability properties play a central role in the proposed approach. For a large class of Runge-Kutta methods applied on linear problems, a tight bound of the round-off errors is provided. A simple test is defined and ensures the absence of underflow and a tighter round-off error bound. The absence of overflow is guaranteed as linear stability properties imply that (computed) solutions are non-increasing.",
    "keywords": "Linear stability, Numerical integration, Overflow, Round-off error, Runge-Kutta method, Underflow",
    "URL": "https://hal.archives-ouvertes.fr/hal-01883843",
    "_line": "FormalBib.bib:3512"
  },
  "boldo_round-off_2017": {
    "id": "boldo_round-off_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Boldo",
        "given": "Sylvie"
      },
      {
        "family": "Faissole",
        "given": "Florian"
      },
      {
        "family": "Chapoutot",
        "given": "Alexandre"
      }
    ],
    "title": "Round-off Error Analysis of Explicit One-Step Numerical Integration Methods",
    "container-title": "24th IEEE Symposium on Computer Arithmetic",
    "issued": {
      "date-parts": [
        [
          "2017",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "Ordinary differential equations are ubiquitous in scientific computing. Solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. We are interested in such numerical integration methods, for instance Euler's method or the Runge-Kutta methods. As they are implemented using floating-point arithmetic, round-off errors occur. In order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. Our methodology is to apply a fine-grained analysis to these numerical algorithms. Our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.",
    "keywords": "Floating-Point, Numerical Integration, Ordinary Differential Equation, Round-Off Error, Runge-Kutta Methods, Stability",
    "URL": "https://hal.archives-ouvertes.fr/hal-01581794",
    "DOI": "10.1109/ARITH.2017.22",
    "publisher-place": "London, United Kingdom",
    "_line": "FormalBib.bib:3523"
  },
  "martin-dorel_proving_2016": {
    "id": "martin-dorel_proving_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Martin-Dorel",
        "given": "Érik"
      },
      {
        "family": "Melquiond",
        "given": "Guillaume"
      }
    ],
    "title": "Proving Tight Bounds on Univariate Expressions with Elementary Functions in Coq",
    "container-title": "Journal of Automated Reasoning",
    "container-title-short": "J Autom Reasoning",
    "issued": {
      "date-parts": [
        [
          "2016",
          "10",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "1573-0670",
    "abstract": "The verification of floating-point mathematical libraries requires computing numerical bounds on approximation errors. Due to the tightness of these bounds and the peculiar structure of approximation errors, such a verification is out of the reach of generic tools such as computer algebra systems. In fact, the inherent difficulty of computing such bounds often mandates a formal proof of them. In this paper, we present a tactic for the Coq proof assistant that is designed to automatically and formally prove bounds on univariate expressions. It is based on a formalization of floating-point and interval arithmetic, associated with an on-the-fly computation of Taylor expansions. All the computations are performed inside Coq’s logic, in a reflexive setting. This paper also compares our tactic with various existing tools on a large set of examples.",
    "keywords": "Coq proof assistant, Decision procedure, Floating-point arithmetic, Formal proof, Interval arithmetic, Nonlinear arithmetic",
    "URL": "https://doi.org/10.1007/s10817-015-9350-4",
    "DOI": "10.1007/s10817-015-9350-4",
    "page": "187-217",
    "page-first": "187",
    "volume": "57",
    "issue": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:3537"
  },
  "hutchison_improving_2012": {
    "id": "hutchison_improving_2012",
    "type": "chapter",
    "author": [
      {
        "family": "Boldo",
        "given": "Sylvie"
      },
      {
        "family": "Lelay",
        "given": "Catherine"
      },
      {
        "family": "Melquiond",
        "given": "Guillaume"
      }
    ],
    "editor": [
      {
        "family": "Hawblitzel",
        "given": "Chris"
      },
      {
        "family": "Miller",
        "given": "Dale"
      }
    ],
    "title": "Improving Real Analysis in Coq: A User-Friendly Approach to Integrals and Derivatives",
    "container-title": "Certified Programs and Proofs",
    "container-title-short": "Improving Real Analysis in Coq",
    "title-short": "Improving Real Analysis in Coq",
    "issued": {
      "date-parts": [
        [
          "2012"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-35307-9 978-3-642-35308-6",
    "abstract": "Veriﬁcation of numerical analysis programs requires dealing with derivatives and integrals. High conﬁdence in this process can be achieved using a formal proof checker, such as Coq. Its standard library provides an axiomatization of real numbers and various lemmas about real analysis, which may be used for this purpose. Unfortunately, its deﬁnitions of derivative and integral are unpractical as they are partial functions that demand a proof term. This proof term makes the handling of mathematical formulas cumbersome and does not conform to traditional analysis. Other proof assistants usually do not suﬀer from this issue; for instance, they may rely on Hilbert’s epsilon to get total operators. In this paper, we propose a way to deﬁne total operators for derivative and integral without having to extend Coq’s standard axiomatization of real numbers. We proved the compatibility of our deﬁnitions with the standard library’s in order to leverage existing results. We also greatly improved automation for real analysis proofs that use Coq standard deﬁnitions. We exercised our approach on lemmas involving iterated partial derivatives and diﬀerentiation under the integral sign, that were missing from the formal proof of a numerical program solving the wave equation.",
    "URL": "http://link.springer.com/10.1007/978-3-642-35308-6_22",
    "DOI": "10.1007/978-3-642-35308-6_22",
    "publisher-place": "Berlin, Heidelberg",
    "page": "289-304",
    "page-first": "289",
    "volume": "7679",
    "language": "en-US",
    "_line": "FormalBib.bib:3556"
  },
  "van_renesse_paxos_2015": {
    "id": "van_renesse_paxos_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Van Renesse",
        "given": "Robbert"
      },
      {
        "family": "Altinbuken",
        "given": "Deniz"
      }
    ],
    "title": "Paxos Made Moderately Complex",
    "container-title": "ACM Comput. Surv.",
    "issued": {
      "date-parts": [
        [
          "2015",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "0360-0300",
    "abstract": "This article explains the full reconfigurable multidecree Paxos (or multi-Paxos) protocol. Paxos is by no means a simple protocol, even though it is based on relatively simple invariants. We provide pseudocode and explain it guided by invariants. We initially avoid optimizations that complicate comprehension. Next we discuss liveness, list various optimizations that make the protocol practical, and present variants of the protocol.",
    "keywords": "consensus, Replicated state machines, voting",
    "URL": "http://doi.acm.org/10.1145/2673577",
    "DOI": "10.1145/2673577",
    "page": "42:1-42:36",
    "page-first": "42",
    "volume": "47",
    "issue": "3",
    "_line": "FormalBib.bib:3578"
  },
  "lampson_abcds_2001": {
    "id": "lampson_abcds_2001",
    "type": "paper-conference",
    "author": [
      {
        "family": "Lampson",
        "given": "Butler"
      }
    ],
    "title": "The ABCD's of Paxos",
    "container-title": "Proceedings of the Twentieth Annual ACM Symposium on Principles of Distributed Computing",
    "collection-title": "PODC '01",
    "issued": {
      "date-parts": [
        [
          "2001"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-58113-383-7",
    "abstract": "We explain how consensus is used to implement replicated state machines, the general mechanism for fault-tolerance. We describe an abstract version of Lamport's Paxos algorithm for asynchronous consensus. Then we derive the Byzantine, classic, and disk versions of Paxos from the abstract one, show how they are related to each other, and discuss the safety, liveness, and performance of each one.",
    "URL": "http://doi.acm.org/10.1145/383962.383969",
    "DOI": "10.1145/383962.383969",
    "publisher-place": "New York, NY, USA",
    "page": "13-",
    "page-first": "13",
    "_line": "FormalBib.bib:3595"
  },
  "jacobs_verifast_2017": {
    "id": "jacobs_verifast_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Jacobs",
        "given": "Bart"
      },
      {
        "family": "Smans",
        "given": "Jan"
      },
      {
        "family": "Piessens",
        "given": "Frank"
      }
    ],
    "title": "The VeriFast Program Veriﬁer: A Tutorial",
    "issued": {
      "date-parts": [
        [
          "2017",
          "11",
          "28"
        ]
      ]
    },
    "page": "102",
    "page-first": "102",
    "language": "en-US",
    "_line": "FormalBib.bib:3612"
  },
  "jacobs_verifast_2008": {
    "id": "jacobs_verifast_2008",
    "type": "report",
    "author": [
      {
        "family": "Jacobs",
        "given": "Bart"
      },
      {
        "family": "Piessens",
        "given": "Frank"
      }
    ],
    "title": "The VeriFast program verifier",
    "issued": {
      "date-parts": [
        [
          "2008"
        ]
      ]
    },
    "abstract": "This note describes a separation-logic-based approach for the spec-ification and verification of safety properties of pointer-manipulating imperative programs. We describe the approach for the C language. The safety properties to be verified are specified as annotations in the source code, in the form of function preconditions and post-conditions expressed as separation logic assertions. To enable rich specifications, the user may include additional annotations that de-fine inductive datatypes, primitive recursive pure functions over these datatypes, and abstract predicates (i.e. named, parameterized assertions). A restricted form of existential quantification is sup-ported in assertions in the form of pattern matching. Verification is based on forward symbolic execution, where memory is represented as a separate conjunction of points-to as-sertions and abstract predicate assertions, and data values are rep-",
    "_line": "FormalBib.bib:3621"
  },
  "jacobs_featherweight_2015": {
    "id": "jacobs_featherweight_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Jacobs",
        "given": "Bart"
      },
      {
        "family": "Vogels",
        "given": "Frédéric"
      },
      {
        "family": "Piessens",
        "given": "Frank"
      }
    ],
    "title": "Featherweight VeriFast",
    "container-title": "Logical Methods in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2015",
          "9",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "18605974",
    "abstract": "VeriFast is a leading research prototype tool for the sound modular verification of safety and correctness properties of single-threaded and multithreaded C and Java programs. It has been used as a vehicle for exploration and validation of novel program verification techniques and for industrial case studies; it has served well at a number of program verification competitions; and it has been used for teaching by multiple teachers independent of the authors. However, until now, while VeriFast's operation has been described informally in a number of publications, and specific verification techniques have been formalized, a clear and precise exposition of how VeriFast works has not yet appeared. In this article we present for the first time a formal definition and soundness proof of a core subset of the VeriFast program verification approach. The exposition aims to be both accessible and rigorous: the text is based on lecture notes for a graduate course on program verification, and it is backed by an executable machine-readable definition and machine-checked soundness proof in Coq.",
    "keywords": "Computer Science - Logic in Computer Science",
    "URLtext": "1507.07697",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1507.07697",
    "URL": "http://arxiv.org/abs/1507.07697",
    "DOI": "10.2168/LMCS-11(3:19)2015",
    "volume": "11",
    "issue": "3",
    "_line": "FormalBib.bib:3629"
  },
  "costan_secure_2017": {
    "id": "costan_secure_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Costan",
        "given": "Victor"
      },
      {
        "family": "Lebedev",
        "given": "Ilia"
      },
      {
        "family": "Devadas",
        "given": "Srinivas"
      }
    ],
    "title": "Secure Processors Part II: Intel SGX Security Analysis and MIT Sanctum Architecture",
    "container-title": "Foundations and Trends® in Electronic Design Automation",
    "container-title-short": "Secure Processors Part II",
    "title-short": "Secure Processors Part II",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "1551-3939, 1551-3947",
    "URL": "http://www.nowpublishers.com/article/Details/EDA-052",
    "DOI": "10.1561/1000000052",
    "page": "249-361",
    "page-first": "249",
    "volume": "11",
    "issue": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:3647"
  },
  "costan_secure_2017-1": {
    "id": "costan_secure_2017-1",
    "type": "article-journal",
    "author": [
      {
        "family": "Costan",
        "given": "Victor"
      },
      {
        "family": "Lebedev",
        "given": "Ilia"
      },
      {
        "family": "Devadas",
        "given": "Srinivas"
      }
    ],
    "title": "Secure Processors Part I: Background, Taxonomy for Secure Enclaves and Intel SGX Architecture",
    "container-title": "Foundations and Trends® in Electronic Design Automation",
    "container-title-short": "Secure Processors Part I",
    "title-short": "Secure Processors Part I",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "1551-3939, 1551-3947",
    "URL": "http://www.nowpublishers.com/article/Details/EDA-051",
    "DOI": "10.1561/1000000051",
    "page": "1-248",
    "page-first": "1",
    "volume": "11",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:3664"
  },
  "costan_sanctum:_2016": {
    "id": "costan_sanctum:_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Costan",
        "given": "Victor"
      },
      {
        "family": "Lebedev",
        "given": "Ilia"
      },
      {
        "family": "Devadas",
        "given": "Srinivas"
      }
    ],
    "title": "Sanctum: Minimal Hardware Extensions for Strong Software Isolation",
    "container-title-short": "Sanctum",
    "title-short": "Sanctum",
    "event-title": "25th &lcurly;USENIX&rcurly; Security Symposium (&lcurly;USENIX&rcurly; Security 16)",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/costan",
    "page": "857-874",
    "page-first": "857",
    "language": "en-US",
    "_line": "FormalBib.bib:3681"
  },
  "gasser_building_1988": {
    "id": "gasser_building_1988",
    "type": "book",
    "author": [
      {
        "family": "Gasser",
        "given": "Morrie"
      }
    ],
    "title": "Building a secure computer system",
    "issued": {
      "date-parts": [
        [
          "1988"
        ]
      ]
    },
    "publisher": "Van Nostrand Reinhold Co",
    "number-of-pages": "288",
    "isbn": "978-0-442-23022-7",
    "keywords": "Computer security, System design",
    "publisher-place": "New York",
    "_line": "FormalBib.bib:3694"
  },
  "patterson_compositional_nodate-1": {
    "id": "patterson_compositional_nodate-1",
    "type": "article-journal",
    "author": [
      {
        "family": "Patterson",
        "given": "Daniel"
      },
      {
        "family": "Ahmed",
        "given": "Amal"
      }
    ],
    "title": "On Compositional Compiler Correctness and Fully Abstract Compilation",
    "URL": "https://popl18.sigplan.org/event/prisc-2018-on-compositional-compiler-correctness-and-fully-abstract-compilation",
    "page": "3",
    "page-first": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:3706"
  },
  "costanzo_end--end_2016": {
    "id": "costanzo_end--end_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Costanzo",
        "given": "David"
      },
      {
        "family": "Shao",
        "given": "Zhong"
      },
      {
        "family": "Gu",
        "given": "Ronghui"
      }
    ],
    "title": "End-to-end verification of information-flow security for C and assembly programs",
    "container-title": "ACM SIGPLAN Notices",
    "container-title-short": "SIGPLAN Not.",
    "issued": {
      "date-parts": [
        [
          "2016",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "issn": "0362-1340, 1558-1160",
    "URL": "https://dl.acm.org/doi/10.1145/2980983.2908100",
    "DOI": "10.1145/2980983.2908100",
    "page": "648-664",
    "page-first": "648",
    "volume": "51",
    "issue": "6",
    "language": "en-US",
    "_line": "Security.bib:1193"
  },
  "luo_extended_nodate": {
    "id": "luo_extended_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Luo",
        "given": "Zhaohui"
      }
    ],
    "title": "An Extended Calculus of Constructions",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "http://www.lfcs.inf.ed.ac.uk/reports/90/ECS-LFCS-90-118/",
    "_line": "FormalBib.bib:3733"
  },
  "kang_crellvm:_2018": {
    "id": "kang_crellvm:_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Kang",
        "given": "Jeehoon"
      },
      {
        "family": "Kim",
        "given": "Yoonseung"
      },
      {
        "family": "Song",
        "given": "Youngju"
      },
      {
        "family": "Lee",
        "given": "Juneyoung"
      },
      {
        "family": "Park",
        "given": "Sanghoon"
      },
      {
        "family": "Shin",
        "given": "Mark Dongyeon"
      },
      {
        "family": "Kim",
        "given": "Yonghyun"
      },
      {
        "family": "Cho",
        "given": "Sungkeun"
      },
      {
        "family": "Choi",
        "given": "Joonwon"
      },
      {
        "family": "Hur",
        "given": "Chung-Kil"
      },
      {
        "family": "Yi",
        "given": "Kwangkeun"
      }
    ],
    "title": "Crellvm: Verified Credible Compilation for LLVM",
    "container-title": "Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation",
    "container-title-short": "Crellvm",
    "collection-title": "PLDI 2018",
    "title-short": "Crellvm",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-5698-5",
    "abstract": "Production compilers such as GCC and LLVM are large complex software systems, for which achieving a high level of reliability is hard. Although testing is an effective method for finding bugs, it alone cannot guarantee a high level of reliability. To provide a higher level of reliability, many approaches that examine compilers' internal logics have been proposed. However, none of them have been successfully applied to major optimizations of production compilers. This paper presents Crellvm: a verified credible compilation framework for LLVM, which can be used as a systematic way of providing a high level of reliability for major optimizations in LLVM. Specifically, we augment an LLVM optimizer to generate translation results together with their correctness proofs, which can then be checked by a proof checker formally verified in Coq. As case studies, we applied our approach to two major optimizations of LLVM: register promotion mem2reg and global value numbering gvn, having found four new miscompilation bugs (two in each).",
    "keywords": "LLVM, Coq, compiler verification, credible compilation, relational Hoare logic, translation validation",
    "URL": "http://doi.acm.org/10.1145/3192366.3192377",
    "DOI": "10.1145/3192366.3192377",
    "publisher-place": "New York, NY, USA",
    "page": "631-645",
    "page-first": "631",
    "_line": "FormalBib.bib:3741"
  },
  "letouzey_certified_nodate": {
    "id": "letouzey_certified_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Letouzey",
        "given": "Pierre"
      }
    ],
    "title": "Certified functional programming : Program extraction within Coq proof assistant.ResearchGate",
    "container-title-short": "(8) (PDF) Certified functional programming",
    "title-short": "(8) (PDF) Certified functional programming",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "NOTA: THIS IS THE ENGLISH TRANSLATION OF MY FRENCH PHD MANUSCRIPT. This work concerns the generation of programs which are certified to be correct by construction. These programs are obtained by extracting relevant information from constructive proofs made with the Coq proof assistant. Such a translation, named \"extraction\", of constructive proofs into functional programs is not new, and corresponds to an isomorphism known as Curry-Howard's. An extraction tool has been part of Coq assistant for a long time. But this old extraction tool suffered from several limitations: in particular, some Coq proofs were refused by it, whereas some others led to incorrect programs. In order to overcome these limitations, we built a completely new extraction tool for Coq, including both a new theory and a new implementation. Concerning theory, we developed new correctness proofs for this extraction mechanism. These new proofs are both complex and original. Concerning implementation, we focused on the generation of efficient and realistic code, which can be integrated in large-scale software developments, using modules and interfaces. Finally, we also present several case studies illustrating the capabilities of our new extraction. For example, we describe the certification of a modular library of finite set structures, and the production of programs about real exact arithmetic, starting from a formalization of constructive real analysis. These examples show the progress already achieved, even if the situation is not perfect yet, in particular in the last study.",
    "URL": "https://www.researchgate.net/publication/280790704_Certified_functional_programming_Program_extraction_within_Coq_proof_assistant",
    "language": "en-US",
    "_line": "FormalBib.bib:3760"
  },
  "ebner_metaprogramming_2017": {
    "id": "ebner_metaprogramming_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Ebner",
        "given": "Gabriel"
      },
      {
        "family": "Ullrich",
        "given": "Sebastian"
      },
      {
        "family": "Roesch",
        "given": "Jared"
      },
      {
        "family": "Avigad",
        "given": "Jeremy"
      },
      {
        "family": "Moura",
        "given": "Leonardo",
        "dropping-particle": "de"
      }
    ],
    "title": "A Metaprogramming Framework for Formal Verification",
    "container-title": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2017",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "We describe the metaprogramming framework currently used in Lean, an interactive theorem prover based on dependent type theory. This framework extends Lean's object language with an API to some of Lean's internal structures and procedures, and provides ways of reflecting object-level expressions into the metalanguage. We provide evidence to show that our implementation is performant, and that it provides a convenient and flexible way of writing not only small-scale interactive tactics, but also more substantial kinds of automation.",
    "keywords": "theorem proving, dependent type theory, metaprogramming, tactic language",
    "URL": "http://doi.acm.org/10.1145/3110278",
    "DOI": "10.1145/3110278",
    "page": "34:1-34:29",
    "page-first": "34",
    "volume": "1",
    "_line": "FormalBib.bib:3772"
  },
  "pakin_comprehensive_nodate": {
    "id": "pakin_comprehensive_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Pakin",
        "given": "Scott"
      }
    ],
    "title": "The Comprehensive LaTeX Symbol List",
    "abstract": "This document lists 14283 symbols and the corresponding LATEX commands that produce them. Some of these symbols are guaranteed to be available in every LATEX 2������ system; others require fonts and packages that may not accompany a given distribution and that therefore need to be installed. All of the fonts and packages used to prepare this document—as well as this document itself—are freely available from the Comprehensive TEX Archive Network (http://www.ctan.org/).",
    "page": "358",
    "page-first": "358",
    "language": "en-US",
    "_line": "FormalBib.bib:3789"
  },
  "kubota_foundations_2016": {
    "id": "kubota_foundations_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Kubota",
        "given": "Ken"
      }
    ],
    "title": "Foundations of Mathematics",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "URL": "http://www.owlofminerva.net/doi/10.4444/100/111/",
    "DOI": "10.4444/100.111",
    "language": "en-US",
    "_line": "FormalBib.bib:3798"
  },
  "hathhorn_defining_2015": {
    "id": "hathhorn_defining_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hathhorn",
        "given": "Chris"
      },
      {
        "family": "Ellison",
        "given": "Chucky"
      },
      {
        "family": "Roşu",
        "given": "Grigore"
      }
    ],
    "title": "Defining the Undefinedness of C",
    "container-title": "Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation",
    "collection-title": "PLDI '15",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-3468-6",
    "abstract": "We present a \"negative\" semantics of the C11 language&mdash;a semantics that does not just give meaning to correct programs, but also rejects undefined programs. We investigate undefined behavior in C and discuss the techniques and special considerations needed for formally specifying it. We have used these techniques to modify and extend a semantics of C into one that captures undefined behavior. The amount of semantic infrastructure and effort required to achieve this was unexpectedly high, in the end nearly doubling the size of the original semantics. From our semantics, we have automatically extracted an undefinedness checker, which we evaluate against other popular analysis tools, using our own test suite in addition to a third-party test suite. Our checker is capable of detecting examples of all 77 categories of core language undefinedness appearing in the C11 standard, more than any other tool we considered. Based on this evaluation, we argue that our work is the most comprehensive and complete semantic treatment of undefined behavior in C, and thus of the C language itself.",
    "keywords": "C11, K Framework, Programming language semantics, Undefined behavior",
    "URL": "http://doi.acm.org/10.1145/2737924.2737979",
    "DOI": "10.1145/2737924.2737979",
    "publisher-place": "New York, NY, USA",
    "page": "336-345",
    "page-first": "336",
    "_line": "FormalBib.bib:3809"
  },
  "arias_jscoq:_2017": {
    "id": "arias_jscoq:_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Arias",
        "given": "Emilio Jesús Gallego"
      },
      {
        "family": "Pin",
        "given": "Benoît"
      },
      {
        "family": "Jouvelot",
        "given": "Pierre"
      }
    ],
    "title": "jsCoq: Towards Hybrid Theorem Proving Interfaces",
    "container-title": "Electronic Proceedings in Theoretical Computer Science",
    "container-title-short": "jsCoq",
    "title-short": "jsCoq",
    "issued": {
      "date-parts": [
        [
          "2017",
          "1",
          "24"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "2075-2180",
    "abstract": "We describe jsCcoq, a new platform and user environment for the Coq interactive proof assistant. The jsCoq system targets the HTML5-ECMAScript 2015 specification, and it is typically run inside a standards-compliant browser, without the need of external servers or services. Targeting educational use, jsCoq allows the user to start interaction with proof scripts right away, thanks to its self-contained nature. Indeed, a full Coq environment is packed along the proof scripts, easing distribution and installation. Starting to use jsCoq is as easy as clicking on a link. The current release ships more than 10 popular Coq libraries, and supports popular books such as Software Foundations or Certified Programming with Dependent Types. The new target platform has opened up new interaction and display possibilities. It has also fostered the development of some new Coq-related technology. In particular, we have implemented a new serialization-based protocol for interaction with the proof assistant, as well as a new package format for library distribution.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Human-Computer Interaction",
    "URLtext": "1701.07125",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1701.07125",
    "URL": "http://arxiv.org/abs/1701.07125",
    "DOI": "10.4204/EPTCS.239.2",
    "page": "15-27",
    "page-first": "15",
    "volume": "239",
    "_line": "FormalBib.bib:3827"
  },
  "brockschmidt_t2:_2016": {
    "id": "brockschmidt_t2:_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Brockschmidt",
        "given": "Marc"
      },
      {
        "family": "Cook",
        "given": "Byron"
      },
      {
        "family": "Ishtiaq",
        "given": "Samin"
      },
      {
        "family": "Khlaaf",
        "given": "Heidy"
      },
      {
        "family": "Piterman",
        "given": "Nir"
      }
    ],
    "editor": [
      {
        "family": "Chechik",
        "given": "Marsha"
      },
      {
        "family": "Raskin",
        "given": "Jean-François"
      }
    ],
    "title": "T2: Temporal Property Verification",
    "container-title": "Tools and Algorithms for the Construction and Analysis of Systems",
    "container-title-short": "T2",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "T2",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-662-49674-9",
    "abstract": "We present the open-source tool T2, the first public release from the TERMINATOR project \\[9\\]. T2 has been extended over the past decade to support automatic temporal-logic proving techniques and to handle a general class of user-provided liveness and safety properties. Input can be provided in a native format and in C, via the support of the LLVM compiler framework. We briefly discuss T2’s architecture, its underlying techniques, and conclude with an experimental illustration of its competitiveness and directions for future extensions.",
    "page": "387-393",
    "page-first": "387",
    "language": "en-US",
    "_line": "FormalBib.bib:3846"
  },
  "crick_share_2014": {
    "id": "crick_share_2014",
    "type": "article-journal",
    "author": [
      {
        "family": "Crick",
        "given": "Tom"
      },
      {
        "family": "Hall",
        "given": "Benjamin A."
      },
      {
        "family": "Ishtiaq",
        "given": "Samin"
      },
      {
        "family": "Takeda",
        "given": "Kenji"
      }
    ],
    "title": "\"Share and Enjoy\": Publishing Useful and Usable Scientific Models",
    "container-title": "arXiv:1409.0367 \\[cs\\]",
    "container-title-short": "\"Share and Enjoy\"",
    "title-short": "\"Share and Enjoy\"",
    "issued": {
      "date-parts": [
        [
          "2014",
          "9",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "The reproduction and replication of reported scientific results is a hot topic within the academic community. The retraction of numerous studies from a wide range of disciplines, from climate science to bioscience, has drawn the focus of many commentators, but there exists a wider socio-cultural problem that pervades the scientific community. Sharing code, data and models often requires extra effort; this is currently seen as a significant overhead that may not be worth the time investment. Automated systems, which allow easy reproduction of results, offer the potential to incentivise a culture change and drive the adoption of new techniques to improve the efficiency of scientific exploration. In this paper, we discuss the value of improved access and sharing of the two key types of results arising from work done in the computational sciences: models and algorithms. We propose the development of an integrated cloud-based system underpinning computational science, linking together software and data repositories, toolchains, workflows and outputs, providing a seamless automated infrastructure for the verification and validation of scientific models and in particular, performance benchmarks.",
    "keywords": "Computer Science - Computational Engineering, Finance, and Science",
    "URLtext": "1409.0367",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1409.0367",
    "URL": "http://arxiv.org/abs/1409.0367",
    "_line": "FormalBib.bib:3862"
  },
  "hutchison_seloger:_2013": {
    "id": "hutchison_seloger:_2013",
    "type": "chapter",
    "author": [
      {
        "family": "Haase",
        "given": "Christoph"
      },
      {
        "family": "Ishtiaq",
        "given": "Samin"
      },
      {
        "family": "Ouaknine",
        "given": "Joël"
      },
      {
        "family": "Parkinson",
        "given": "Matthew J."
      }
    ],
    "editor": [
      {
        "family": "Sharygina",
        "given": "Natasha"
      },
      {
        "family": "Veith",
        "given": "Helmut"
      }
    ],
    "title": "SeLoger: A Tool for Graph-Based Reasoning in Separation Logic",
    "container-title": "Computer Aided Verification",
    "container-title-short": "SeLoger",
    "title-short": "SeLoger",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-39798-1 978-3-642-39799-8",
    "abstract": "This paper introduces the tool SeLoger, which is a reasoner for satisﬁability and entailment in a fragment of separation logic with pointers and linked lists. SeLoger builds upon and extends graphbased algorithms that have recently been introduced in order to settle both decision problems in polynomial time. Running SeLoger on standard benchmarks shows that the tool outperforms current state-of-theart tools by orders of magnitude.",
    "URL": "http://link.springer.com/10.1007/978-3-642-39799-8_55",
    "DOI": "10.1007/978-3-642-39799-8_55",
    "publisher-place": "Berlin, Heidelberg",
    "page": "790-795",
    "page-first": "790",
    "volume": "8044",
    "language": "en-US",
    "_line": "FormalBib.bib:3877"
  },
  "ross_exterminators_2005": {
    "id": "ross_exterminators_2005",
    "type": "article-journal",
    "author": [
      {
        "family": "Ross",
        "given": "P. E."
      }
    ],
    "title": "The exterminators \\[software bugs\\]",
    "container-title": "IEEE Spectrum",
    "issued": {
      "date-parts": [
        [
          "2005",
          "9"
        ]
      ]
    },
    "abstract": "This paper describes a sound methodology developed at Praxis High Integrity Systems for detecting and exterminating bugs during all stages of a software project. To develop software, the London-based software house uses mathematically based techniques, known as formal methods, which require that programmers begin their work not by writing code but rather by stringing together special symbols that represent the program's logic. Like a mathematical theorem, these symbol strings can be checked to verify that they form logically correct statements. Once the programmer has checked that the program doesn't have logical flaws, it's a relatively simple matter to convert those symbols into programming code. With an average of less than one error in every 10,000 lines of delivered code, Praxis claims a bug rate that is at least 50 times better than the industry standard.",
    "keywords": "formal methods, software engineering, bug-free software, Computer bugs, formal logic, Logic, mathematical logic, Praxis High Integrity Systems, program debugging, software bugs, software development, software engineering methods, software experts, software project",
    "DOI": "10.1109/MSPEC.2005.1502527",
    "page": "36-41",
    "page-first": "36",
    "volume": "42",
    "issue": "9",
    "_line": "FormalBib.bib:3899"
  },
  "furia_autoproof:_2017": {
    "id": "furia_autoproof:_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Furia",
        "given": "Carlo A."
      },
      {
        "family": "Nordio",
        "given": "Martin"
      },
      {
        "family": "Polikarpova",
        "given": "Nadia"
      },
      {
        "family": "Tschannen",
        "given": "Julian"
      }
    ],
    "title": "AutoProof: auto-active functional verification of object-oriented programs",
    "container-title": "International Journal on Software Tools for Technology Transfer",
    "container-title-short": "AutoProof",
    "title-short": "AutoProof",
    "issued": {
      "date-parts": [
        [
          "2017",
          "11",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "9",
          "23"
        ]
      ]
    },
    "issn": "1433-2787",
    "abstract": "Auto-active verifiers provide a level of automation intermediate between fully automatic and interactive: users supply code with annotations as input while benefiting from a high level of automation in the back-end. This paper presents AutoProof, a state-of-the-art auto-active verifier for object-oriented sequential programs with complex functional specifications. AutoProof fully supports advanced object-oriented features and a powerful methodology for framing and class invariants, which make it applicable in practice to idiomatic object-oriented patterns. The paper focuses on describing AutoProof ’s interface, design, and implementation features, and demonstrates AutoProof ’s performance on a rich collection of benchmark problems. The results attest AutoProof ’s competitiveness among tools in its league on cutting-edge functional verification of object-oriented programs.",
    "keywords": "Auto-active verification, Functional verification, Object-oriented verification, Verification benchmarks",
    "URL": "https://doi.org/10.1007/s10009-016-0419-0",
    "DOI": "10.1007/s10009-016-0419-0",
    "page": "697-716",
    "page-first": "697",
    "volume": "19",
    "issue": "6",
    "language": "en-US",
    "_line": "FormalBib.bib:3912"
  },
  "bjorner_manifest_2017": {
    "id": "bjorner_manifest_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Bjørner",
        "given": "Dines"
      }
    ],
    "title": "Manifest domains: analysis and description",
    "container-title": "Formal Aspects of Computing",
    "container-title-short": "Manifest domains",
    "title-short": "Manifest domains",
    "issued": {
      "date-parts": [
        [
          "2017",
          "3",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "9",
          "23"
        ]
      ]
    },
    "issn": "1433-299X",
    "abstract": "We show that manifest domains, an understanding of which are a prerequisite for software requirements prescriptions, can be precisely described: narrated and formalised. We show that such manifest domains can be understood as a collection of endurant, that is, basically spatial entities: parts, components and materials, and perdurant, that is, basically temporal entities: actions, events and behaviours. We show that parts can be modeled in terms of external qualities whether: atomic or composite parts, having internal qualities: unique identifications, mereologies, which model relations between parts, and attributes. We show that the manifest domain analysis endeavour can be supported by a calculus of manifest domain analysis prompts: is&underscore;entity, is&underscore;endurant, is&underscore;perdurant, is&underscore;part, is&underscore;component, is&underscore;material, is&underscore;atomic, is&underscore;composite, has&underscore;components, has&underscore;materials, has&underscore;concrete&underscore;type, attribute&underscore;names, is&underscore;stationary, etcetera; and show how the manifest domain description endeavour can be supported by a calculus of manifest domain description prompts: observe&underscore;part&underscore;sorts, observe&underscore;part&underscore;type, observe&underscore;components, observe&underscore;materials, observe&underscore;unique&underscore;identifier, observe&underscore;mereology, observe&underscore;attributes. We show how to model attributes, essentially following Michael Jackson (Software requirements &amp; specifications: a lexicon of practice, principles and prejudices. ACM Press, Addison-Wesley, Reading, 1995), but with a twist: The attribute model introduces the attribute analysis prompts is&underscore;static&underscore;attribute, is&underscore;dynamic&underscore;attribute, is&underscore;inert&underscore;attribute, is&underscore;reactive&underscore;attribute, is&underscore;active&underscore;attribute, is&underscore;autonomous&underscore;attribute, is&underscore;biddable&underscore;attribute and is&underscore;programmable&underscore;attribute. The twist suggests ways of modeling “access” to the values of these kinds of attributes: the static attributes by simply “copying” them, once, the reactive and programmable attributes by “carrying” them as function parameters whose values are kept always updated, and the remaining, the external&underscore;attributes, by inquiring, when needed, as to their value, as if they were always offered on CSP-like channels (Hoare, Communicating sequential processes. C.A.R. Hoare series in computer science. Prentice-Hall International, London, 2004). We show how to model essential aspects of perdurants in terms of their signatures based on the concepts of endurants. And we show how one can “compile” descriptions of endurant parts into descriptions of perdurant behaviours. We do not show prompt calculi for perdurants. The above contributions express a method with principles, techniques and tools for constructing domain descriptions. It is important to realise that we do not wish to nor claim that the method can describe all that it is interesting to know about domains.",
    "keywords": "Analysis &amp; description, Domain engineering, Manifest domains, Prompt calculi",
    "URL": "https://doi.org/10.1007/s00165-016-0385-z",
    "DOI": "10.1007/s00165-016-0385-z",
    "page": "175-225",
    "page-first": "175",
    "volume": "29",
    "issue": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:3932"
  },
  "hutchison_40_2014": {
    "id": "hutchison_40_2014",
    "type": "chapter",
    "author": [
      {
        "family": "Bjørner",
        "given": "Dines"
      },
      {
        "family": "Havelund",
        "given": "Klaus"
      }
    ],
    "editor": [
      {
        "family": "Jones",
        "given": "Cliff"
      },
      {
        "family": "Pihlajasaari",
        "given": "Pekka"
      },
      {
        "family": "Sun",
        "given": "Jun"
      }
    ],
    "title": "40 Years of Formal Methods: Some Obstacles and Some Possibilities?",
    "container-title": "FM 2014: Formal Methods",
    "container-title-short": "40 Years of Formal Methods",
    "title-short": "40 Years of Formal Methods",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "9",
          "23"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-06409-3 978-3-319-06410-9",
    "URL": "http://link.springer.com/10.1007/978-3-319-06410-9_4",
    "DOI": "10.1007/978-3-319-06410-9_4",
    "publisher-place": "Cham",
    "page": "42-61",
    "page-first": "42",
    "volume": "8442",
    "language": "en-US",
    "_line": "FormalBib.bib:3952"
  },
  "hoare_verifying_2003": {
    "id": "hoare_verifying_2003",
    "type": "article-journal",
    "author": [
      {
        "family": "Hoare",
        "given": "Tony"
      }
    ],
    "title": "The Verifying Compiler: A Grand Challenge for Computing Research",
    "container-title": "J. ACM",
    "container-title-short": "The Verifying Compiler",
    "title-short": "The Verifying Compiler",
    "issued": {
      "date-parts": [
        [
          "2003",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "9",
          "17"
        ]
      ]
    },
    "issn": "0004-5411",
    "abstract": "This contribution proposes a set of criteria that distinguish a grand challenge in science or engineering from the many other kinds of short-term or long-term research problems that engage the interest of scientists and engineers. As an example drawn from Computer Science, it revives an old challenge: the construction and application of a verifying compiler that guarantees correctness of a program before running it.",
    "URL": "http://doi.acm.org/10.1145/602382.602403",
    "DOI": "10.1145/602382.602403",
    "page": "63-69",
    "page-first": "63",
    "volume": "50",
    "issue": "1",
    "_line": "FormalBib.bib:3973"
  },
  "ringer_qed_2019": {
    "id": "ringer_qed_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Ringer",
        "given": "Talia"
      },
      {
        "family": "Palmskog",
        "given": "Karl"
      },
      {
        "family": "Sergey",
        "given": "Ilya"
      },
      {
        "family": "Gligoric",
        "given": "Milos"
      },
      {
        "family": "Tatlock",
        "given": "Zachary"
      }
    ],
    "title": "QED at Large: A Survey of Engineering of Formally Verified Software",
    "container-title": "Foundations and Trends® in Programming Languages",
    "container-title-short": "QED at Large",
    "title-short": "QED at Large",
    "issued": {
      "date-parts": [
        [
          "2019",
          "9",
          "3"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "9",
          "10"
        ]
      ]
    },
    "issn": "2325-1107, 2325-1131",
    "abstract": "QED at Large: A Survey of Engineering of Formally Verified Software",
    "URL": "https://www.nowpublishers.com/article/Details/PGL-045",
    "DOI": "10.1561/2500000045",
    "page": "102-281",
    "page-first": "102",
    "volume": "5",
    "issue": "2",
    "_line": "FormalBib.bib:3990"
  },
  "noauthor_hazel_2019": {
    "id": "noauthor_hazel_2019",
    "type": "book",
    "title": "Hazel, a live functional programming environment with typed holes: hazelgrove/hazel",
    "container-title-short": "Hazel, a live functional programming environment with typed holes",
    "title-short": "Hazel, a live functional programming environment with typed holes",
    "issued": {
      "date-parts": [
        [
          "2019",
          "8",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "8",
          "30"
        ]
      ]
    },
    "publisher": "hazelgrove",
    "URL": "https://github.com/hazelgrove/hazel",
    "note": "original-date: 2017-01-27T02:14:05Z",
    "_line": "FormalBib.bib:4008"
  },
  "noauthor_hazel_nodate": {
    "id": "noauthor_hazel_nodate",
    "type": "webpage",
    "title": "Hazel, a live functional programming environment featuring typed holes.",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "8",
          "30"
        ]
      ]
    },
    "URL": "http://hazel.org/",
    "_line": "FormalBib.bib:4019"
  },
  "padon_ivy:_2016": {
    "id": "padon_ivy:_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Padon",
        "given": "Oded"
      },
      {
        "family": "McMillan",
        "given": "Kenneth L."
      },
      {
        "family": "Panda",
        "given": "Aurojit"
      },
      {
        "family": "Sagiv",
        "given": "Mooly"
      },
      {
        "family": "Shoham",
        "given": "Sharon"
      }
    ],
    "title": "Ivy: Safety Verification by Interactive Generalization",
    "container-title": "Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation",
    "container-title-short": "Ivy",
    "collection-title": "PLDI '16",
    "title-short": "Ivy",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "8",
          "30"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-4261-2",
    "abstract": "Despite several decades of research, the problem of formal verification of infinite-state systems has resisted effective automation. We describe a system &mdash; Ivy &mdash; for interactively verifying safety of infinite-state systems. Ivy's key principle is that whenever verification fails, Ivy graphically displays a concrete counterexample to induction. The user then interactively guides generalization from this counterexample. This process continues until an inductive invariant is found. Ivy searches for universally quantified invariants, and uses a restricted modeling language. This ensures that all verification conditions can be checked algorithmically. All user interactions are performed using graphical models, easing the user's task. We describe our initial experience with verifying several distributed protocols.",
    "keywords": "counterexamples to induction, distributed systems, invariant inference, safety verification",
    "URL": "http://doi.acm.org/10.1145/2908080.2908118",
    "DOI": "10.1145/2908080.2908118",
    "publisher-place": "New York, NY, USA",
    "page": "614-630",
    "page-first": "614",
    "note": "event-place: Santa Barbara, CA, USA",
    "_line": "FormalBib.bib:4026"
  },
  "mullen_oeuf:_2018": {
    "id": "mullen_oeuf:_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Mullen",
        "given": "Eric"
      },
      {
        "family": "Pernsteiner",
        "given": "Stuart"
      },
      {
        "family": "Wilcox",
        "given": "James R."
      },
      {
        "family": "Tatlock",
        "given": "Zachary"
      },
      {
        "family": "Grossman",
        "given": "Dan"
      }
    ],
    "title": "ŒUf: Minimizing the Coq Extraction TCB",
    "container-title": "Proceedings of the 7th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "container-title-short": "ŒUf",
    "collection-title": "CPP 2018",
    "title-short": "ŒUf",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "8",
          "30"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-5586-5",
    "abstract": "Verifying systems by implementing them in the programming language of a proof assistant (e.g., Gallina for Coq) lets us directly leverage the full power of the proof assistant for verifying the system. But, to execute such an implementation requires extraction, a large complicated process that is in the trusted computing base (TCB).  This paper presents Œuf, a verified compiler from a subset of Gallina to assembly. Œuf’s correctness theorem ensures that compilation preserves the semantics of the source Gallina program. We describe how Œuf’s specification can be used as a foreign function interface to reason about the interaction between compiled Gallina programs and surrounding shim code. Additionally, Œufmaintains a small TCB for its front-end by reflecting Gallina programs to Œufsource and automatically ensuring equivalence using computational denotation. This design enabled us to implement some early compiler passes (e.g., lambda lifting) in the untrusted reflection and ensure their correctness via translation validation. To evaluate Œuf, we compile Appel’s SHA256 specification from Gallina to x86 and write a shim for the generated code, yielding a verified sha256sum implementation with a small TCB.",
    "keywords": "Coq, Compilers, Formal Verification, Verified Systems",
    "URL": "http://doi.acm.org/10.1145/3167089",
    "DOI": "10.1145/3167089",
    "publisher-place": "New York, NY, USA",
    "page": "172-185",
    "page-first": "172",
    "note": "event-place: Los Angeles, CA, USA",
    "_line": "FormalBib.bib:4046"
  },
  "manna_temporal_1995": {
    "id": "manna_temporal_1995",
    "type": "book",
    "author": [
      {
        "family": "Manna",
        "given": "Zohar"
      },
      {
        "family": "Pnueli",
        "given": "Amir"
      }
    ],
    "title": "Temporal Verification of Reactive Systems: Safety",
    "container-title-short": "Temporal Verification of Reactive Systems",
    "collection-title": "Manna,Z.;Pnueli,A.:Temporal Logic of Reactive Systems",
    "title-short": "Temporal Verification of Reactive Systems",
    "issued": {
      "date-parts": [
        [
          "1995"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "8",
          "29"
        ]
      ]
    },
    "publisher": "Springer-Verlag",
    "isbn": "978-0-387-94459-3",
    "abstract": "This book is about the verification of reactive systems. A reactive system is a system that maintains an ongoing interaction with its environment, as opposed to computing some final value on termination. The family of reactive systems includes many classes of programs whose correct and reliable construction is con­ sidered to be particularly challenging, including concurrent programs, embedded and process control programs, and operating systems. Typical examples of such systems are an air traffic control system, programs controlling mechanical devices such as a train, or perpetually ongoing processes such as a nuclear reactor. With the expanding use of computers in safety-critical areas, where failure is potentially disastrous, correctness is crucial. This has led to the introduction of formal verification techniques, which give both users and designers of software and hardware systems greater confidence that the systems they build meet the desired specifications. Framework The approach promoted in this book is based on the use of temporal logic for specifying properties of reactive systems, and develops an extensive verification methodology for proving that a system meets its temporal specification. Reactive programs must be specified in terms of their ongoing behavior, and temporal logic provides an expressive and natural language for specifying this behavior. Our framework for specifying and verifying temporal properties of reactive systems is based on the following four components: 1. A computational model to describe the behavior of reactive systems. The model adopted in this book is that of a Fair Transition System (FTS).",
    "URL": "https://www.springer.com/gp/book/9780387944593",
    "publisher-place": "New York",
    "language": "en-US",
    "_line": "FormalBib.bib:4066"
  },
  "feldman_inferring_2019": {
    "id": "feldman_inferring_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Feldman",
        "given": "Yotam M. Y."
      },
      {
        "family": "Wilcox",
        "given": "James R."
      },
      {
        "family": "Shoham",
        "given": "Sharon"
      },
      {
        "family": "Sagiv",
        "given": "Mooly"
      }
    ],
    "title": "Inferring Inductive Invariants from Phase Structures",
    "container-title": "arXiv:1905.07739 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2019",
          "5",
          "19"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "8",
          "26"
        ]
      ]
    },
    "abstract": "Infinite-state systems such as distributed protocols are challenging to verify using interactive theorem provers or automatic verification tools. Of these techniques, deductive verification is highly expressive but requires the user to annotate the system with inductive invariants. To relieve the user from this labor-intensive and challenging task, invariant inference aims to find inductive invariants automatically. Unfortunately, when applied to infinite-state systems such as distributed protocols, existing inference techniques often diverge, which limits their applicability. This paper proposes user-guided invariant inference based on phase invariants, which capture the different logical phases of the protocol. Users conveys their intuition by specifying a phase structure, an automaton with edges labeled by program transitions; the tool automatically infers assertions that hold in the automaton's states, resulting in a full safety proof.The additional structure from phases guides the inference procedure towards finding an invariant. Our results show that user guidance by phase structures facilitates successful inference beyond the state of the art. We find that phase structures are pleasantly well matched to the intuitive reasoning routinely used by domain experts to understand why distributed protocols are correct, so that providing a phase structure reuses this existing intuition.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "1905.07739",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1905.07739",
    "URL": "http://arxiv.org/abs/1905.07739",
    "_line": "FormalBib.bib:4082"
  },
  "bjorner_programming_nodate": {
    "id": "bjorner_programming_nodate",
    "type": "no-type",
    "author": [
      {
        "family": "Bjørner",
        "given": "Nikolaj"
      },
      {
        "family": "Moura",
        "given": "Leonardo",
        "dropping-particle": "de"
      },
      {
        "family": "Nachmanson",
        "given": "lev"
      },
      {
        "family": "Wintersteiger",
        "given": "Christoph"
      }
    ],
    "title": "Programming Z3",
    "abstract": "This tutorial provides a programmer's introduction to the Satisfiability Modulo Theories Solver Z3. It describes how to use Z3 through scripts, provided in the Python scripting language, and it describes several of the algorithms underlying the decision procedures within Z3. It aims to broadly cover almost all available features of Z3 and the essence of the underlying algorithms.",
    "URL": "http://theory.stanford.edu/~nikolaj/programmingz3.html",
    "_line": "FormalBib.bib:4096"
  },
  "yurichev_sat/smt_nodate": {
    "id": "yurichev_sat/smt_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Yurichev",
        "given": "Dennis"
      }
    ],
    "title": "SAT/SMT by Example",
    "page": "555",
    "page-first": "555",
    "language": "en-US",
    "_line": "FormalBib.bib:4103"
  },
  "bardin_bringing_2019": {
    "id": "bardin_bringing_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Bardin",
        "given": "Sébastien"
      },
      {
        "family": "Bjørner",
        "given": "Nikolaj"
      },
      {
        "family": "Cadar",
        "given": "Cristian"
      }
    ],
    "editor": [
      {
        "family": "Bardin",
        "given": "Sébastien"
      },
      {
        "family": "Bjørner",
        "given": "Nikolaj S."
      },
      {
        "family": "Cadar",
        "given": "Cristian"
      }
    ],
    "title": "Bringing CP, SAT and SMT together: Next Challenges in Constraint Solving (Dagstuhl Seminar 19062)",
    "container-title": "Dagstuhl Reports",
    "container-title-short": "Bringing CP, SAT and SMT together",
    "title-short": "Bringing CP, SAT and SMT together",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "8",
          "23"
        ]
      ]
    },
    "issn": "2192-5283",
    "keywords": "Automated Decision Procedures, Constraint Programming, SAT, SMT",
    "URL": "http://drops.dagstuhl.de/opus/volltexte/2019/10857",
    "DOI": "10.4230/DagRep.9.2.27",
    "page": "27-47",
    "page-first": "27",
    "volume": "9",
    "issue": "2",
    "_line": "FormalBib.bib:4111"
  },
  "selsam_guiding_2019": {
    "id": "selsam_guiding_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Selsam",
        "given": "Daniel"
      },
      {
        "family": "Bjørner",
        "given": "Nikolaj"
      }
    ],
    "editor": [
      {
        "family": "Janota",
        "given": "Mikoláš"
      },
      {
        "family": "Lynce",
        "given": "Inês"
      }
    ],
    "title": "Guiding High-Performance SAT Solvers with Unsat-Core Predictions",
    "container-title": "Theory and Applications of Satisfiability Testing – SAT 2019",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-24258-9",
    "abstract": "The NeuroSAT neural network architecture was introduced in \\[37\\] for predicting properties of propositional formulae. When trained to predict the satisfiability of toy problems, it was shown to find solutions and unsatisfiable cores on its own. However, the authors saw “no obvious path” to using the architecture to improve the state-of-the-art. In this work, we train a simplified NeuroSAT architecture to directly predict the unsatisfiable cores of real problems. We modify several state-of-the-art SAT solvers to periodically replace their variable activity scores with NeuroSAT’s prediction of how likely the variables are to appear in an unsatisfiable core. The modified MiniSat solves 10&perc; more problems on SATCOMP-2018 within the standard 5,000 second timeout than the original does. The modified Glucose solves 11&perc; more problems than the original, while the modified Z3 solves 6&perc; more. The gains are even greater when the training is specialized for a specific distribution of problems; on a benchmark of hard problems from a scheduling domain, the modified Glucose solves 20&perc; more problems than the original does within a one-hour timeout. Our results demonstrate that NeuroSAT can provide effective guidance to high-performance SAT solvers on real problems.",
    "page": "336-353",
    "page-first": "336",
    "language": "en-US",
    "_line": "FormalBib.bib:4129"
  },
  "hoder_z_2011": {
    "id": "hoder_z_2011",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hoder",
        "given": "Kryštof"
      },
      {
        "family": "Bjørner",
        "given": "Nikolaj"
      },
      {
        "family": "Moura",
        "given": "Leonardo",
        "dropping-particle": "de"
      }
    ],
    "editor": [
      {
        "family": "Gopalakrishnan",
        "given": "Ganesh"
      },
      {
        "family": "Qadeer",
        "given": "Shaz"
      }
    ],
    "title": "μZ– An Efficient Engine for Fixed Points with Constraints",
    "container-title": "Computer Aided Verification",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2011"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-22110-1",
    "abstract": "The μZ tool is a scalable, efficient engine for fixed points with constraints. It supports high-level declarative fixed point constraints over a combination of built-in and plugin domains. The built-in domains include formulas presented to the SMT solver Z3 and domains known from abstract interpretation. We present the interface to μZ, a number of the domains, and a set of examples illustrating the use of μZ.",
    "keywords": "Abstract Interpretation, Abstract Machine, Default Representation, Hash Table, Relational Algebra",
    "page": "457-462",
    "page-first": "457",
    "language": "en-US",
    "_line": "FormalBib.bib:4143"
  },
  "noauthor_mezzo_nodate": {
    "id": "noauthor_mezzo_nodate",
    "type": "webpage",
    "title": "The Mezzo programming language",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "8",
          "5"
        ]
      ]
    },
    "URL": "http://protz.github.io/mezzo/",
    "_line": "FormalBib.bib:4159"
  },
  "gueneau_procrastination_nodate": {
    "id": "gueneau_procrastination_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Guéneau",
        "given": "Armaël"
      }
    ],
    "title": "Procrastination",
    "abstract": "We present a small Coq library for collecting side conditions and deferring their proof.",
    "page": "7",
    "page-first": "7",
    "language": "en-US",
    "_line": "FormalBib.bib:4166"
  },
  "mullen_oeuf:_2018-1": {
    "id": "mullen_oeuf:_2018-1",
    "type": "paper-conference",
    "author": [
      {
        "family": "Mullen",
        "given": "Eric"
      },
      {
        "family": "Pernsteiner",
        "given": "Stuart"
      },
      {
        "family": "Wilcox",
        "given": "James R."
      },
      {
        "family": "Tatlock",
        "given": "Zachary"
      },
      {
        "family": "Grossman",
        "given": "Dan"
      }
    ],
    "title": "ŒUf: Minimizing the Coq Extraction TCB",
    "container-title": "Proceedings of the 7th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "container-title-short": "ŒUf",
    "collection-title": "CPP 2018",
    "title-short": "ŒUf",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "7",
          "30"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-5586-5",
    "abstract": "Verifying systems by implementing them in the programming language of a proof assistant (e.g., Gallina for Coq) lets us directly leverage the full power of the proof assistant for verifying the system. But, to execute such an implementation requires extraction, a large complicated process that is in the trusted computing base (TCB).  This paper presents Œuf, a verified compiler from a subset of Gallina to assembly. Œuf’s correctness theorem ensures that compilation preserves the semantics of the source Gallina program. We describe how Œuf’s specification can be used as a foreign function interface to reason about the interaction between compiled Gallina programs and surrounding shim code. Additionally, Œufmaintains a small TCB for its front-end by reflecting Gallina programs to Œufsource and automatically ensuring equivalence using computational denotation. This design enabled us to implement some early compiler passes (e.g., lambda lifting) in the untrusted reflection and ensure their correctness via translation validation. To evaluate Œuf, we compile Appel’s SHA256 specification from Gallina to x86 and write a shim for the generated code, yielding a verified sha256sum implementation with a small TCB.",
    "keywords": "Coq, Compilers, Formal Verification, Verified Systems",
    "URL": "http://doi.acm.org/10.1145/3167089",
    "DOI": "10.1145/3167089",
    "publisher-place": "New York, NY, USA",
    "page": "172-185",
    "page-first": "172",
    "note": "event-place: Los Angeles, CA, USA",
    "_line": "FormalBib.bib:4175"
  },
  "fogarty_concoqtion:_2007": {
    "id": "fogarty_concoqtion:_2007",
    "type": "paper-conference",
    "author": [
      {
        "family": "Fogarty",
        "given": "Seth"
      },
      {
        "family": "Pasalic",
        "given": "Emir"
      },
      {
        "family": "Siek",
        "given": "Jeremy"
      },
      {
        "family": "Taha",
        "given": "Walid"
      }
    ],
    "title": "Concoqtion: Indexed Types Now!",
    "container-title": "Proceedings of the 2007 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-based Program Manipulation",
    "container-title-short": "Concoqtion",
    "collection-title": "PEPM '07",
    "title-short": "Concoqtion",
    "issued": {
      "date-parts": [
        [
          "2007"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "7",
          "7"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-59593-620-2",
    "abstract": "Almost twenty years after the pioneering efforts of Cardelli, the programming languages community is vigorously pursuing ways to incorporate Fω-style indexed types into programming languages. This paper advocates Concoqtion, a practical approach to adding such highly expressive types to full-fledged programming languages. The approach is applied to MetaOCaml using the Coq proof checker to conservatively extend Hindley-Milner type inference. The implementation of MetaOCaml Concoqtion requires minimal modifications to the syntax, the type checker, and the compiler; and yields a language comparable in notation to the leading proposals. The resulting language provides unlimited expressiveness in the type system while maintaining decidability. Furthermore, programmers can take advantage of a wide range of libraries not only for the programming language but also for the indexed types. Programming in MetaOCaml Concoqtion is illustrated with small examples and a case study implementing a statically-typed domain-specific language.",
    "URL": "http://doi.acm.org/10.1145/1244381.1244400",
    "DOI": "10.1145/1244381.1244400",
    "publisher-place": "New York, NY, USA",
    "page": "112-121",
    "page-first": "112",
    "note": "event-place: Nice, France",
    "_line": "FormalBib.bib:4195"
  },
  "brady_idris_2013": {
    "id": "brady_idris_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Brady",
        "given": "Edwin"
      }
    ],
    "title": "Idris, a general-purpose dependently typed programming language: Design and implementation",
    "container-title": "Journal of Functional Programming",
    "container-title-short": "Idris, a general-purpose dependently typed programming language",
    "title-short": "Idris, a general-purpose dependently typed programming language",
    "issued": {
      "date-parts": [
        [
          "2013",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "6",
          "30"
        ]
      ]
    },
    "issn": "0956-7968, 1469-7653",
    "abstract": "Many components of a dependently-typed programming language are by now well understood, for example the underlying type theory, type checking, uniﬁcation and evaluation. How to combine these components into a realistic and usable high-level language is, however, folklore, discovered anew by successive language implementators. In this paper, I describe the implementation of IDRIS, a new dependently-typed functional programming language. IDRIS is intended to be a general purpose programming language and as such provides high-level concepts such as implicit syntax, type classes and do notation. I describe the high-level language and the underlying type theory, and present a tactic-based method for elaborating concrete high-level syntax with implicit arguments and type classes into a fully explicit type theory. Furthermore, I show how this method facilitates the implementation of new high-level language constructs.",
    "URL": "https://www.cambridge.org/core/product/identifier/S095679681300018X/type/journal_article",
    "DOI": "10.1017/S095679681300018X",
    "page": "552-593",
    "page-first": "552",
    "volume": "23",
    "issue": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:4214"
  },
  "harrison_can_1997": {
    "id": "harrison_can_1997",
    "type": "paper-conference",
    "author": [
      {
        "family": "Harrison",
        "given": "Luddy"
      }
    ],
    "editor": [
      {
        "family": "Van Hentenryck",
        "given": "Pascal"
      }
    ],
    "title": "Can abstract interpretation become a mainstream compiler technology?",
    "container-title": "Static Analysis",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "1997"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-540-69576-9",
    "abstract": "Abstract interpretation has enormous promise and yet remains at the margins of compiler practice. In this talk I will argue that abstract interpretation cannot become a mainstream compiler technlogy until its computational, algorithmic aspects are as well-developed as its mathematical, foundational aspects have been to date. I will put the problem into perspective by comparing abstract interpretation to dataflow analysis, for which a well-developed body of compuational methods exists. This comparison reveals that abstract interpretation is most appropriately seen as a method for specifying problems (that is, equations to be solved), and not as a method for specifying solutions (that is, algorithms for solving equations). In particular, efficient solution methods for the equations that arise from abstract interpretations are seldom obvious from the surface of the equations themselves. In other words, the “algorithms” that are strongly suggested by an abstract interpretation, in which the semantic domains are viewed as “data structures, the semantic functions as procedures”, and a simple fixed point engine used to integrate these parts into a workable whole, is naive and is at best suitable for use in prototyping program analyzers. This point of view calls into question the casual dismissal of abstract interpretation as inefficient, by questioning what can be inferred about the complexity of an abstract interpretation problem by superficial examination of the domains and semantic functions involved.",
    "page": "395-395",
    "page-first": "395",
    "language": "en-US",
    "_line": "FormalBib.bib:4232"
  },
  "ammarguellat_control-flow_1992": {
    "id": "ammarguellat_control-flow_1992",
    "type": "article-journal",
    "author": [
      {
        "family": "Ammarguellat",
        "given": "Z."
      }
    ],
    "title": "A control-flow normalization algorithm and its complexity",
    "container-title": "IEEE Transactions on Software Engineering",
    "issued": {
      "date-parts": [
        [
          "1992",
          "3"
        ]
      ]
    },
    "issn": "0098-5589",
    "abstract": "A single method for normalizing the control-flow of programs to facilitate program transformations, program analysis, and automatic parallelization is presented. While previous methods result in programs whose control flowgraphs are reducible, programs normalized by this technique satisfy a stronger condition than reducibility and are therefore simpler in their syntax and structure than with previous methods. In particular, all control-flow cycles are normalized into single-entry, single-exit while loops and all GOTOs are eliminated. Furthermore, the method avoids problems of code replication that are characteristic of node-splitting techniques. This restructuring obviates the control dependence graph, since afterwards control dependence relations are manifest in the syntax tree of the program. Transformations that effect this normalization are presented, and the complexity of the method is studied.&lt;&gt;",
    "keywords": "graph theory, Algorithm design and analysis, Automatic control, automatic parallelization, complexity, computational complexity, control dependence relations, control flowgraphs, control-flow cycles, control-flow normalization algorithm, Data analysis, GOTOs, node-splitting techniques, parallel algorithms, Pathology, Performance analysis, Program processors, structured programming, syntax tree, Tree graphs",
    "DOI": "10.1109/32.126773",
    "page": "237-251",
    "page-first": "237",
    "volume": "18",
    "issue": "3",
    "_line": "FormalBib.bib:4246"
  },
  "erosa_taming_1994": {
    "id": "erosa_taming_1994",
    "type": "paper-conference",
    "author": [
      {
        "family": "Erosa",
        "given": "A. M."
      },
      {
        "family": "Hendren",
        "given": "L. J."
      }
    ],
    "title": "Taming control flow: a structured approach to eliminating goto statements",
    "container-title": "Proceedings of 1994 IEEE International Conference on Computer Languages (ICCL'94)",
    "container-title-short": "Taming control flow",
    "title-short": "Taming control flow",
    "event-title": "Proceedings of 1994 IEEE International Conference on Computer Languages (ICCL'94)",
    "issued": {
      "date-parts": [
        [
          "1994",
          "5"
        ]
      ]
    },
    "abstract": "In designing optimizing and parallelizing compilers, it is often simpler and more efficient to deal with programs that have structured control flow. Although most programmers naturally program in a structured fashion, there remain many important programs and benchmarks that include some number of goto statements, thus rendering the entire program unstructured. Such unstructured programs cannot be handled with compilers built with analyses and transformations for structured programs. In this paper we present a straight-forward algorithm to structure C programs by eliminating all goto statements. The method works directly on a high-level abstract syntax tree (AST) representation of the program and could easily be integrated into any compiler that uses an AST-based intermediate representation. The actual algorithm proceeds by eliminating each goto by first applying a sequence of goto-movement transformations followed by the appropriate goto-elimination transformation. We have implemented the method in the McCAT (McGill Compiler Architecture Testbed) optimizing/parallelizing C compiler and we present experimental results that demonstrate that the method is both efficient and effective.&lt;&gt;",
    "keywords": "Computer science, Optimizing compilers, Program processors, AST representation, C language, C programs, control flow, Design optimization, Flow graphs, goto statements, goto-elimination, goto-movement transformations, high-level abstract syntax tree, Information analysis, intermediate representation, McCAT, McGill Compiler Architecture Testbed, optimizing compilers, parallel programming, parallelizing compilers, program compilers, Programming profession, Software engineering, Software testing, Switches",
    "DOI": "10.1109/ICCL.1994.288377",
    "page": "229-240",
    "page-first": "229",
    "_line": "FormalBib.bib:4261"
  },
  "hoang_spark_2015": {
    "id": "hoang_spark_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Hoang",
        "given": "Duc"
      },
      {
        "family": "Moy",
        "given": "Yannick"
      },
      {
        "family": "Wallenburg",
        "given": "Angela"
      },
      {
        "family": "Chapman",
        "given": "Roderick"
      }
    ],
    "title": "SPARK 2014 and GNATprove",
    "container-title": "International Journal on Software Tools for Technology Transfer",
    "container-title-short": "Int J Softw Tools Technol Transfer",
    "issued": {
      "date-parts": [
        [
          "2015",
          "11",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "6",
          "11"
        ]
      ]
    },
    "issn": "1433-2787",
    "abstract": "Extensive and expensive testing is the method most widely used for gaining confidence in safety-critical software. With a few exceptions, such as SPARK, formal verification is rarely used in industry due to its high cost and level of skill required. The grand challenge of building a verifying compiler for static formal verification of programs aims at bringing formal verification to non-expert users of powerful programming languages. This challenge has nurtured competition and collaboration among verification tool builders; an example is the VerifyThis competition Huisman et al. (http://digbib.ubka.uni-karlsruhe.de/volltexte/1000034373, 2013). In this paper, we describe our approach to popularising formal verification in the design of the SPARK 2014 language and the associated formal verification tool GNATprove. In particular, we present our solution to combining tests and proofs, which provides a cost-competitive way to develop software to standards such as do-178. At the heart of our technique are executable contracts, and the ability to both test and prove those. We use running examples from the VerifyThis 2012 competition and discuss the results of using our tools on those problems.",
    "keywords": "Static analysis, Program verification, Deductive verification, SPARK, Ada, Verifying compiler",
    "URL": "https://doi.org/10.1007/s10009-014-0322-5",
    "DOI": "10.1007/s10009-014-0322-5",
    "page": "695-707",
    "page-first": "695",
    "volume": "17",
    "issue": "6",
    "language": "en-US",
    "_line": "FormalBib.bib:4275"
  },
  "chapman_fumble_nodate": {
    "id": "chapman_fumble_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Chapman",
        "given": "Roderick"
      }
    ],
    "title": "The Fumble Programmer",
    "abstract": "This paper reflects on the need for formality, discipline and humility in\nprogramming. Starting with the work of Turing, Dijkstra and Humphrey, this paper\ngoes on to cover our experience with the Personal Software Process, formal pro-\ngramming with SPARK, and the impact of putting the two together.",
    "URL": "https://proteancode.com/wp-content/uploads/2018/02/the_fumble_programmer.pdf",
    "_line": "FormalBib.bib:4294"
  },
  "salvia_mixed_2019": {
    "id": "salvia_mixed_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Salvia",
        "given": "Rocco"
      },
      {
        "family": "Titolo",
        "given": "Laura"
      },
      {
        "family": "Feliú",
        "given": "Marco A."
      },
      {
        "family": "Moscato",
        "given": "Mariano M."
      },
      {
        "family": "Muñoz",
        "given": "César A."
      },
      {
        "family": "Rakamarić",
        "given": "Zvonimir"
      }
    ],
    "editor": [
      {
        "family": "Badger",
        "given": "Julia M."
      },
      {
        "family": "Rozier",
        "given": "Kristin Yvonne"
      }
    ],
    "title": "A Mixed Real and Floating-Point Solver",
    "container-title": "NASA Formal Methods",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-20652-9",
    "abstract": "Reasoning about mixed real and floating-point constraints is essential for developing accurate analysis tools for floating-point programs. This paper presents FPRoCK, a prototype tool for solving mixed real and floating-point formulas. FPRoCK transforms a mixed formula into an equisatisfiable one over the reals. This formula is then solved using an off-the-shelf SMT solver. FPRoCK is also integrated with the PRECiSA static analyzer, which computes a sound estimation of the round-off error of a floating-point program. It is used to detect infeasible computational paths, thereby improving the accuracy of PRECiSA.",
    "page": "363-370",
    "page-first": "363",
    "language": "en-US",
    "_line": "FormalBib.bib:4304"
  },
  "badger_extracting_2019": {
    "id": "badger_extracting_2019",
    "type": "chapter",
    "author": [
      {
        "family": "Ioannidis",
        "given": "Eleftherios"
      },
      {
        "family": "Kaashoek",
        "given": "Frans"
      },
      {
        "family": "Zeldovich",
        "given": "Nickolai"
      }
    ],
    "editor": [
      {
        "family": "Badger",
        "given": "Julia M."
      },
      {
        "family": "Rozier",
        "given": "Kristin Yvonne"
      }
    ],
    "title": "Extracting and Optimizing Formally Verified Code for Systems Programming",
    "container-title": "NASA Formal Methods",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "6",
          "7"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-20651-2 978-3-030-20652-9",
    "abstract": "MCQC is a compiler for extracting veriﬁed systems programs to low-level assembly, with no runtime or garbage collection requirements and an emphasis on performance. MCQC targets the Gallina functional language used in the Coq proof assistant. MCQC translates pure and recursive functions into C++17, while compiling monadic eﬀectful functions to imperative C++ system calls. With a few memory and performance optimizations, MCQC combines veriﬁability with memory and runtime performance. By handling eﬀectful and pure functions separately MCQC can generate executable veriﬁed code directly from Gallina, reducing the eﬀort of implementing and executing veriﬁed systems.",
    "URL": "http://link.springer.com/10.1007/978-3-030-20652-9_15",
    "DOI": "10.1007/978-3-030-20652-9_15",
    "publisher-place": "Cham",
    "page": "228-236",
    "page-first": "228",
    "volume": "11460",
    "language": "en-US",
    "_line": "FormalBib.bib:4319"
  },
  "fleury_optimizing_2019": {
    "id": "fleury_optimizing_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Fleury",
        "given": "Mathias"
      }
    ],
    "editor": [
      {
        "family": "Badger",
        "given": "Julia M."
      },
      {
        "family": "Rozier",
        "given": "Kristin Yvonne"
      }
    ],
    "title": "Optimizing a Verified SAT Solver",
    "container-title": "NASA Formal Methods",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-20652-9",
    "abstract": "In previous work, I verified a SAT solver with dedicated imperative data structures, including the two-watched-literal scheme. In this paper, I extend this formalization with four additional optimizations. The approach is still based on refining an abstract calculus to a deterministic program. In turn, an imperative version is synthesized from the latter, which is then exported to Standard ML. The first optimization is the extension with blocking literals. Then, the memory management is improved in order to implement the heuristics necessary to implement search restart and forget, which were subsequently implemented. This required changes to the abstract calculus. Finally, the solver uses machine words until they overflow before switching to unbounded integers. Performance has improved and is now closer to MiniSAT without preprocessing.",
    "page": "148-165",
    "page-first": "148",
    "language": "en-US",
    "_line": "FormalBib.bib:4338"
  },
  "badger_towards_2019": {
    "id": "badger_towards_2019",
    "type": "chapter",
    "author": [
      {
        "family": "Blanchard",
        "given": "Allan"
      },
      {
        "family": "Loulergue",
        "given": "Frédéric"
      },
      {
        "family": "Kosmatov",
        "given": "Nikolai"
      }
    ],
    "editor": [
      {
        "family": "Badger",
        "given": "Julia M."
      },
      {
        "family": "Rozier",
        "given": "Kristin Yvonne"
      }
    ],
    "title": "Towards Full Proof Automation in Frama-C Using Auto-active Verification",
    "container-title": "NASA Formal Methods",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "6",
          "7"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-20651-2 978-3-030-20652-9",
    "abstract": "While deductive veriﬁcation is increasingly used on real-life code, making it fully automatic remains difﬁcult. The development of powerful SMT solvers has improved the situation, but some proofs still require interactive theorem provers in order to achieve full formal veriﬁcation. Auto-active veriﬁcation relies on additional guiding annotations (assertions, ghost code, lemma functions, etc.) and provides an important step towards a greater automation of the proof. However, the support of this methodology often remains partial and depends on the veriﬁcation tool. This paper presents an experience report on a complete functional veriﬁcation of several C programs from the literature and real-life code using auto-active veriﬁcation with the C software analysis platform FRAMA-C and its deductive veriﬁcation plugin WP. The goal is to use automatic solvers to verify properties that are classically veriﬁed with interactive provers. Based on our experience, we discuss the beneﬁts of this methodology and the current limitations of the tool, as well as proposals of new features to overcome them.",
    "URL": "http://link.springer.com/10.1007/978-3-030-20652-9_6",
    "DOI": "10.1007/978-3-030-20652-9_6",
    "publisher-place": "Cham",
    "page": "88-105",
    "page-first": "88",
    "volume": "11460",
    "language": "en-US",
    "_line": "FormalBib.bib:4352"
  },
  "knaggs_practical_1993": {
    "id": "knaggs_practical_1993",
    "type": "paper-conference",
    "author": [
      {
        "family": "Knaggs",
        "given": "Peter J."
      }
    ],
    "title": "Practical and theoretical aspects of FORTH software development",
    "issued": {
      "date-parts": [
        [
          "1993"
        ]
      ]
    },
    "abstract": "This is an investigation into the use of the Forth programming environment. The main areas of enquiry were: interfacing Forth to other languages; interfacing Forth and local area networks; and the use of RISC processors with stack based architecture such as the NC4000 and Harris RTX series. We describe how t o i n terface Forth a n d C. W e also provide a system with a multi-tasking interrupt driven interface to the Ibm NetBios networking software and a simple, generic, method of task activation through message passing. Many aspects of the investigation proved to be dependent on a more thorough theoretical underpinning for the Forth language. The use of a typeless parameter stack means that a programmer must concern himself with the intellectual burden of managing the parameter stack. The mismatching of stack elements can be the cause of subtle logic errors. We therefore investigated the possibility o f d e v eloping a type algebra\" that would allow u s t o d e v elop a typed version of Forth. This thesis includes a theory for a type signature algebra\" for the stack based argument passing method used by Forth. To support the use of multi-tasking we provide a simple, but formal, theory of concurrent tasks based on state machines that synchronise on events. This has a graphical notation for people who are not familiar with formal notations. We also looked at how formalisms might be used to deene a semantic model for the Forth language and how formalisms can help to deene the relationship between Forth's stack based virtual machine and register based target processors.",
    "keywords": "Central processing unit, Computer multitasking, Forth, Graphical user interface, Harris affine region detector, Integrated development environment, Message passing, NetBIOS, Programmer, Software development, Type signature, Virtual machine",
    "_line": "FormalBib.bib:4371"
  },
  "stoddart_forth_2012": {
    "id": "stoddart_forth_2012",
    "type": "paper-conference",
    "author": [
      {
        "family": "Stoddart",
        "given": "Bill"
      },
      {
        "family": "Ritchie",
        "given": "C."
      },
      {
        "family": "Dunne",
        "given": "Steve"
      }
    ],
    "title": "Forth Semantics for Compiler Verification",
    "issued": {
      "date-parts": [
        [
          "2012"
        ]
      ]
    },
    "abstract": "Here we are interested in the semantics of Forth from the point of view of using Forth as a target language for a formally verified compiler for Ruth-R, a reversible sequential programming language we are currently developing. We limit out attention to those Forth operations and constructs which will be targetted by the Ruth-R compiler. To facilitate the comparison of meanings of source and target languages, we represent the semantics of Forth code by translation into a form which can be described using the ”prospective value” semantics we use for Ruth-R.",
    "keywords": "Compiler, Concurrent computing, Formal verification, Foundations, HL7PublishingSubSection &lt;operations&gt;, Programming language, Prospective search, Verification of Theories",
    "_line": "FormalBib.bib:4380"
  },
  "hutchison_picard_2013": {
    "id": "hutchison_picard_2013",
    "type": "chapter",
    "author": [
      {
        "family": "Makarov",
        "given": "Evgeny"
      },
      {
        "family": "Spitters",
        "given": "Bas"
      }
    ],
    "editor": [
      {
        "family": "Blazy",
        "given": "Sandrine"
      },
      {
        "family": "Paulin-Mohring",
        "given": "Christine"
      },
      {
        "family": "Pichardie",
        "given": "David"
      }
    ],
    "title": "The Picard Algorithm for Ordinary Differential Equations in Coq",
    "container-title": "Interactive Theorem Proving",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-39633-5 978-3-642-39634-2",
    "abstract": "Ordinary Diﬀerential Equations (ODEs) are ubiquitous in physical applications of mathematics. The Picard-Lindelo¨f theorem is the ﬁrst fundamental theorem in the theory of ODEs. It allows one to solve diﬀerential equations numerically. We provide a constructive development of the Picard-Lindelo¨f theorem which includes a program together with suﬃcient conditions for its correctness. The proof/program is written in the Coq proof assistant and uses the implementation of eﬃcient real numbers from the CoRN library and the MathClasses library. Our proof makes heavy use of operators and functionals, functions on spaces of functions. This is faithful to the usual mathematical description, but a novel level of abstraction for certiﬁed exact real computation.",
    "URL": "http://link.springer.com/10.1007/978-3-642-39634-2_34",
    "DOI": "10.1007/978-3-642-39634-2_34",
    "publisher-place": "Berlin, Heidelberg",
    "page": "463-468",
    "page-first": "463",
    "volume": "7998",
    "language": "en-US",
    "_line": "FormalBib.bib:4389"
  },
  "davenport_computer_2011": {
    "id": "davenport_computer_2011",
    "type": "chapter",
    "author": [
      {
        "family": "Krebbers",
        "given": "Robbert"
      },
      {
        "family": "Spitters",
        "given": "Bas"
      }
    ],
    "editor": [
      {
        "family": "Davenport",
        "given": "James H."
      },
      {
        "family": "Farmer",
        "given": "William M."
      },
      {
        "family": "Urban",
        "given": "Josef"
      },
      {
        "family": "Rabe",
        "given": "Florian"
      }
    ],
    "title": "Computer Certified Efficient Exact Reals in Coq",
    "container-title": "Intelligent Computer Mathematics",
    "issued": {
      "date-parts": [
        [
          "2011"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-22672-4 978-3-642-22673-1",
    "abstract": "Floating point operations are fast, but require continuous effort on the part of the user in order to ensure that the results are correct. This burden can be shifted away from the user by providing a library of exact analysis in which the computer handles the error estimates. We provide an implementation of the exact real numbers in the Coq proof assistant. This improves on the earlier Coq-implementation by O’Connor in two ways: we use dyadic rationals built from the machine integers and we optimize computation of power series by using approximate division. Moreover, we use type classes for clean mathematical interfaces. This appears to be the ﬁrst time that type classes are used in heavy computation. We obtain over a 100 times speed up of the basic operations and indications for improving the Coq system.",
    "URL": "http://link.springer.com/10.1007/978-3-642-22673-1_7",
    "DOI": "10.1007/978-3-642-22673-1_7",
    "publisher-place": "Berlin, Heidelberg",
    "page": "90-106",
    "page-first": "90",
    "volume": "6824",
    "language": "en-US",
    "_line": "FormalBib.bib:4410"
  },
  "rand_formally_nodate": {
    "id": "rand_formally_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Rand",
        "given": "Robert"
      }
    ],
    "title": "Formally Verified Quantum Programming",
    "page": "222",
    "page-first": "222",
    "language": "en-US",
    "_line": "FormalBib.bib:4429"
  },
  "mansky_verifying_nodate": {
    "id": "mansky_verifying_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Mansky",
        "given": "William"
      }
    ],
    "title": "Verifying Concurrent Programs with VST",
    "page": "15",
    "page-first": "15",
    "language": "en-US",
    "_line": "FormalBib.bib:4437"
  },
  "koh_c_2019": {
    "id": "koh_c_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Koh",
        "given": "Nicolas"
      },
      {
        "family": "Li",
        "given": "Yao"
      },
      {
        "family": "Li",
        "given": "Yishuai"
      },
      {
        "family": "Xia",
        "given": "Li-yao"
      },
      {
        "family": "Beringer",
        "given": "Lennart"
      },
      {
        "family": "Honoré",
        "given": "Wolf"
      },
      {
        "family": "Mansky",
        "given": "William"
      },
      {
        "family": "Pierce",
        "given": "Benjamin C."
      },
      {
        "family": "Zdancewic",
        "given": "Steve"
      }
    ],
    "title": "From C to Interaction Trees: Specifying, Verifying, and Testing a Networked Server",
    "container-title": "Proceedings of the 8th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "container-title-short": "From C to Interaction Trees",
    "collection-title": "CPP 2019",
    "title-short": "From C to Interaction Trees",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-6222-1",
    "abstract": "We present the first formal verification of a networked server implemented in C. Interaction trees, a general structure for representing reactive computations, are used to tie together disparate verification and testing tools (Coq, VST, and QuickChick) and to axiomatize the behavior of the operating system on which the server runs (CertiKOS). The main theorem connects a specification of acceptable server behaviors, written in a straightforward “one client at a time” style, with the CompCert semantics of the C program. The variability introduced by low-level buffering of messages and interleaving of multiple TCP connections is captured using network refinement, a variant of observational refinement.",
    "keywords": "formal verification, interaction trees, network refinement, QuickChick, TCP, testing, VST",
    "URL": "http://doi.acm.org/10.1145/3293880.3294106",
    "DOI": "10.1145/3293880.3294106",
    "publisher-place": "New York, NY, USA",
    "page": "234-248",
    "page-first": "234",
    "note": "event-place: Cascais, Portugal",
    "_line": "FormalBib.bib:4445"
  },
  "da_rocha_pinto_tada:_2014": {
    "id": "da_rocha_pinto_tada:_2014",
    "type": "paper-conference",
    "author": [
      {
        "family": "Rocha Pinto",
        "given": "Pedro",
        "dropping-particle": "da"
      },
      {
        "family": "Dinsdale-Young",
        "given": "Thomas"
      },
      {
        "family": "Gardner",
        "given": "Philippa"
      }
    ],
    "editor": [
      {
        "family": "Jones",
        "given": "Richard"
      }
    ],
    "title": "TaDA: A Logic for Time and Data Abstraction",
    "container-title": "ECOOP 2014 – Object-Oriented Programming",
    "container-title-short": "TaDA",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "TaDA",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-662-44202-9",
    "abstract": "To avoid data races, concurrent operations should either be at distinct times or on distinct data. Atomicity is the abstraction that an operation takes effect at a single, discrete instant in time, with linearisability being a well-known correctness condition which asserts that concurrent operations appear to behave atomically. Disjointness is the abstraction that operations act on distinct data resource, with concurrent separation logics enabling reasoning about threads that appear to operate independently on disjoint resources.We present TaDA, a program logic that combines the benefits of abstract atomicity and abstract disjointness. Our key contribution is the introduction of atomic triples, which offer an expressive approach to specifying program modules. By building up examples, we show that TaDA supports elegant modular reasoning in a way that was not previously possible.",
    "keywords": "Abstract State, Data Abstraction, Label Transition System, Proof Rule, Shared Region",
    "page": "207-231",
    "page-first": "207",
    "language": "en-US",
    "_line": "FormalBib.bib:4465"
  },
  "hughes_why_1989": {
    "id": "hughes_why_1989",
    "type": "article-journal",
    "author": [
      {
        "family": "Hughes",
        "given": "J."
      }
    ],
    "title": "Why Functional Programming Matters",
    "container-title": "The Computer Journal",
    "issued": {
      "date-parts": [
        [
          "1989",
          "2",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "issn": "0010-4620, 1460-2067",
    "abstract": "As software becomes more and more complex, it is more and more important to structure it well. Well-structured software is easy to write and to debug, and provides a collection of modules that can be reused to reduce future programming costs. In this paper we show that two features of functional languages in particular, higher-order functions and lazy evaluation, can contribute signiﬁcantly to modularity. As examples, we manipulate lists and trees, program several numerical algorithms, and implement the alpha-beta heuristic (an algorithm from Artiﬁcial Intelligence used in game-playing programs). We conclude that since modularity is the key to successful programming, functional programming oﬀers important advantages for software development.",
    "URL": "https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/32.2.98",
    "DOI": "10.1093/comjnl/32.2.98",
    "page": "98-107",
    "page-first": "98",
    "volume": "32",
    "issue": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:4482"
  },
  "noauthor_ieee_nodate": {
    "id": "noauthor_ieee_nodate",
    "type": "article-journal",
    "title": "IEEE Std 754™-2008 (Revision of IEEE Std 754-1985), IEEE Standard for Floating-Point Arithmetic",
    "abstract": "Abstract: This standard specifies interchange and arithmetic formats and methods for binary and decimal floating-point arithmetic in computer programming environments. This standard specifies exception conditions and their default handling. An implementation of a floating-point system conforming to this standard may be realized entirely in software, entirely in hardware, or in any combination of software and hardware. For operations specified in the normative part of this standard, numerical results and exceptions are uniquely determined by the values of the input data, sequence of operations, and destination formats, all under user control.",
    "page": "70",
    "page-first": "70",
    "language": "en-US",
    "_line": "FormalBib.bib:4499"
  },
  "noauthor_ieee_nodate-1": {
    "id": "noauthor_ieee_nodate-1",
    "type": "report",
    "title": "IEEE Standard for Universal Verification Methodology Language Reference Manual",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "IEEE",
    "abstract": "The Universal Verification Methodology (UVM) that can improve interoperability, reduce the cost of using intellectual property (IP) for new projects or electronic design automation (EDA) tools, and make it easier to reuse verification components is provided. Overall, using this standard will lower verification costs and improve design quality throughout the industry. The primary audiences for this standard are the implementors of the UVM base class library, the implementors of tools supporting the UVM base class library, and the users of the UVM base class library.",
    "URL": "http://ieeexplore.ieee.org/document/7932212/",
    "DOI": "10.1109/IEEESTD.2017.7932212",
    "language": "en-US",
    "_line": "FormalBib.bib:4507"
  },
  "noauthor_ieee_nodate-2": {
    "id": "noauthor_ieee_nodate-2",
    "type": "article-journal",
    "title": "IEEE Std 1800™-2012 (Revision of IEEE Std 1800-2009) IEEE Standard for SystemVerilog—Unified Hardware Design, Specification, and Verification Language",
    "abstract": "Abstract: The definition of the language syntax and semantics for SystemVerilog, which is a unified hardware design, specification, and verification language, is provided. This standard includes support for modeling hardware at the behavioral, register transfer level (RTL), and gate-level abstraction levels, and for writing testbenches using coverage, assertions, object-oriented programming, and constrained random verification. The standard also provides application programming interfaces (APIs) to foreign programming languages.",
    "page": "1315",
    "page-first": "1315",
    "language": "en-US",
    "_line": "FormalBib.bib:4518"
  },
  "noauthor_handbook_nodate": {
    "id": "noauthor_handbook_nodate",
    "type": "webpage",
    "title": "Handbook Of Floating Point Arithmetic Download eBook for Free",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "abstract": "Download handbook of floating point arithmetic ebook free in PDF and EPUB Format. handbook of floating point arithmetic also available in docx and mobi. Read handbook of floating point arithmetic online, read in mobile or Kindle.",
    "URL": "http://ebook4scaricare.com/gratis/handbook-of-floating-point-arithmetic/",
    "language": "en-US",
    "_line": "FormalBib.bib:4526"
  },
  "chailloux_developing_2000": {
    "id": "chailloux_developing_2000",
    "type": "book",
    "author": [
      {
        "family": "Chailloux",
        "given": "Emmanuel"
      },
      {
        "family": "Manoury",
        "given": "Pascal"
      },
      {
        "family": "Pagano",
        "given": "Bruno"
      }
    ],
    "title": "Developing Applications with Objective Caml",
    "issued": {
      "date-parts": [
        [
          "2000"
        ]
      ]
    },
    "publisher": "O'Reilly",
    "isbn": "978-2-84177-121-9",
    "publisher-place": "Paris",
    "note": "OCLC: 803160552",
    "language": "en-US",
    "_line": "FormalBib.bib:4535"
  },
  "bidmeshki_vericoq:_2015": {
    "id": "bidmeshki_vericoq:_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Bidmeshki",
        "given": "Mohammad-Mahdi"
      },
      {
        "family": "Makris",
        "given": "Yiorgos"
      }
    ],
    "title": "VeriCoq: A Verilog-to-Coq converter for proof-carrying hardware automation",
    "container-title": "2015 IEEE International Symposium on Circuits and Systems (ISCAS)",
    "container-title-short": "VeriCoq",
    "title-short": "VeriCoq",
    "event-title": "2015 IEEE International Symposium on Circuits and Systems (ISCAS)",
    "issued": {
      "date-parts": [
        [
          "2015",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-4799-8391-9",
    "abstract": "Proof carrying hardware intellectual property (PCHIP) introduces a new framework in which a hardware (semiconductor) Intellectual Property (IP) is accompanied by formal proofs of certain security-related properties, ensuring that the acquired IP is trustworthy and free from hardware Trojans. In the PCHIP framework, conversion of the design from a hardware description language (HDL) to a formal representation is an essential step. Towards automating this process, herein we introduce VeriCoq, a converter of designs described in Register Transfer Level (RTL) Verilog to their corresponding representation in the Coq theorem proving language, based on the rules deﬁned in the PCHIP framework. VeriCoq supports most of the synthesizable Verilog constructs and is the ﬁrst step towards automating the entire framework, in order to simplify adoption of PCHIP by hardware IP developers and consumers and, thereby, increase IP trustworthiness.",
    "URL": "http://ieeexplore.ieee.org/document/7168562/",
    "DOI": "10.1109/ISCAS.2015.7168562",
    "publisher-place": "Lisbon, Portugal",
    "page": "29-32",
    "page-first": "29",
    "language": "en-US",
    "_line": "FormalBib.bib:4547"
  },
  "braibant_formal_2013": {
    "id": "braibant_formal_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Braibant",
        "given": "Thomas"
      },
      {
        "family": "Chlipala",
        "given": "Adam"
      }
    ],
    "title": "Formal Verification of Hardware Synthesis",
    "container-title": "arXiv:1301.4779 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "abstract": "We report on the implementation of a certified compiler for a high-level hardware description language (HDL) called Fe-Si (FEatherweight SynthesIs). Fe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded atomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq. The target language of the compiler corresponds to a synthesisable subset of Verilog or VHDL. A key aspect of our approach is that input programs to the compiler can be defined and proved correct inside Coq. Then, we use extraction and a Verilog back-end (written in OCaml) to get a certified version of a hardware design.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "1301.4779",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1301.4779",
    "URL": "http://arxiv.org/abs/1301.4779",
    "DOI": "10.1007/978-3-642-39799-8_14",
    "page": "213-228",
    "page-first": "213",
    "volume": "8044",
    "_line": "FormalBib.bib:4566"
  },
  "power_formal_nodate": {
    "id": "power_formal_nodate",
    "type": "book",
    "author": [
      {
        "family": "Power",
        "given": "James F."
      },
      {
        "family": "Sinclair",
        "given": "David"
      }
    ],
    "title": "A Formal Model of Forth Control Words in the Pi-Calculus",
    "abstract": "Abstract: In this paper we develop a formal specification of aspects of the Forth programming language. We describe the operation of the Forth compiler as it translates Forth control words, dealing in particular with the interpretation of immediate words during compilation. Our goal here is to provide a basis for the study of safety properties of embedded systems, many of which are constructed using Forth or Forth-like languages. To this end we construct a model of the Forth compiler in the π-calculus, and have simulated its execution by animating this model using the Pict programming language.",
    "_line": "FormalBib.bib:4583"
  },
  "poial_forth_nodate": {
    "id": "poial_forth_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Poial",
        "given": "Jaanus"
      }
    ],
    "title": "Forth and Formal Language Theory",
    "abstract": "Forth is an excellent programming paradigm, but it is also an interesting object of investigation for formal language theory. With its clear interface based on stacks Forth programs are easily generated by some formal system (e.g. syntax directed translation scheme). Usually it is possible to make such a formal system to generate only \"useful\" programs, but this subset of programs is often very small and does not cover interesting features of the Forth language itself.",
    "page": "6",
    "page-first": "6",
    "language": "en-US",
    "_line": "FormalBib.bib:4590"
  },
  "noauthor_algebraic_nodate": {
    "id": "noauthor_algebraic_nodate",
    "type": "webpage",
    "title": "Algebraic Specification of Stack Effects for Forth Programs.ResearchGate",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "abstract": "ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.",
    "URL": "https://www.researchgate.net/publication/269399251_Algebraic_Specification_of_Stack_Effects_for_Forth_Programs",
    "language": "en-US",
    "_line": "FormalBib.bib:4599"
  },
  "noauthor_ghc_nodate": {
    "id": "noauthor_ghc_nodate",
    "type": "webpage",
    "title": "GHC User’s Guide — Glasgow Haskell Compiler 8.6.5 User's Guide",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "URL": "https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/index.html",
    "_line": "FormalBib.bib:4608"
  },
  "jung_iris_2018-1": {
    "id": "jung_iris_2018-1",
    "type": "article-journal",
    "author": [
      {
        "family": "Jung",
        "given": "Ralf"
      },
      {
        "family": "Krebbers",
        "given": "Robbert"
      },
      {
        "family": "Jourdan",
        "given": "Jacques-Henri"
      },
      {
        "family": "Bizjak",
        "given": "Aleš"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      }
    ],
    "title": "Iris from the ground up: A modular foundation for higher-order concurrent separation logic",
    "container-title": "Journal of Functional Programming",
    "container-title-short": "Iris from the ground up",
    "title-short": "Iris from the ground up",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "issn": "0956-7968, 1469-7653",
    "abstract": "Iris is a framework for higher-order concurrent separation logic, which has been implemented in the Coq proof assistant and deployed very effectively in a wide variety of veriﬁcation projects. Iris was designed with the express goal of simplifying and consolidating the foundations of modern separation logics, but it has evolved over time, and the design and semantic foundations of Iris itself have yet to be fully written down and explained together properly in one place. Here, we attempt to ﬁll this gap, presenting a reasonably complete picture of the latest version of Iris (version 3.1), from ﬁrst principles and in one coherent narrative.",
    "URL": "https://www.cambridge.org/core/product/identifier/S0956796818000151/type/journal_article",
    "DOI": "10.1017/S0956796818000151",
    "volume": "28",
    "language": "en-US",
    "_line": "FormalBib.bib:4615"
  },
  "menon_shakti-t:_2017": {
    "id": "menon_shakti-t:_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Menon",
        "given": "Arjun"
      },
      {
        "family": "Murugan",
        "given": "Subadra"
      },
      {
        "family": "Rebeiro",
        "given": "Chester"
      },
      {
        "family": "Gala",
        "given": "Neel"
      },
      {
        "family": "Veezhinathan",
        "given": "Kamakoti"
      }
    ],
    "title": "Shakti-T: A RISC-V Processor with Light Weight Security Extensions",
    "container-title": "Proceedings of the Hardware and Architectural Support for Security and Privacy",
    "container-title-short": "Shakti-T",
    "collection-title": "HASP '17",
    "title-short": "Shakti-T",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-5266-6",
    "abstract": "With increased usage of compute cores for sensitive applications, including e-commerce, there is a need to provide additional hardware support for securing information from memory based attacks. This work presents a unified hardware framework for handling spatial and temporal memory attacks. The paper integrates the proposed hardware framework with a RISC-V based micro-architecture with an enhanced application binary interface that enables software layers to use these features to protect sensitive data. We demonstrate the effectiveness of the proposed scheme through practical case studies in addition to taking the design through a VLSI CAD design flow. The proposed processor reduces the metadata storage overhead up to 4 x in comparison with the existing solutions, while incurring an area overhead of just 1914 LUTs and 2197 flip flops on an FPGA, without affecting the critical path delay of the processor.",
    "keywords": "Buffer Overflow, Memory Security, Secure Processor Architecture, Spatial Attacks, Tagged Architecture, Temporal Attacks",
    "URL": "http://doi.acm.org/10.1145/3092627.3092629",
    "DOI": "10.1145/3092627.3092629",
    "publisher-place": "New York, NY, USA",
    "page": "2:1-2:8",
    "page-first": "2",
    "note": "event-place: Toronto, ON, Canada",
    "_line": "FormalBib.bib:4631"
  },
  "montagu_theory_2013": {
    "id": "montagu_theory_2013",
    "type": "paper-conference",
    "author": [
      {
        "family": "Montagu",
        "given": "B."
      },
      {
        "family": "Pierce",
        "given": "B. C."
      },
      {
        "family": "Pollack",
        "given": "R."
      }
    ],
    "title": "A Theory of Information-Flow Labels",
    "container-title": "2013 IEEE 26th Computer Security Foundations Symposium",
    "event-title": "2013 IEEE 26th Computer Security Foundations Symposium",
    "issued": {
      "date-parts": [
        [
          "2013",
          "6"
        ]
      ]
    },
    "abstract": "The security literature offers a multitude of calculi, languages, and systems for information-flow control, each with some set of labels encoding security policies that can be attached to data and computations. The exact form of these labels varies widely, with different systems offering many different combinations of features addressing issues such as confidentiality, integrity, and policy ownership. This variation makes it difficult to compare the expressive power of different information-flow frameworks. To enable such comparisons, we introduce label algebras, an abstract interface for information-flow labels equipped with a notion of authority, and study several notions of embedding between them. The simplest is a straightforward notion of injection between label algebras, but this lacks a clear computational motivation and rejects some reasonable encodings between label models. We obtain a more refined account by defining a space of encodings parameterized by an interpretation of labels and authorities, thus giving a semantic flavor to the definition of encoding. We study the theory of semantic encodings and consider two specific instances, one based on the possible observations of boolean values and one based on the behavior of programs in a small lambda-calculus parameterized over an arbitrary label algebra. We use this framework to define and compare a number of concrete label algebras, including realizations of the familiar taint, endorsement, readers, and distrust models, as well as label algebras based on several existing programming languages and operating systems.",
    "keywords": "Security, Semantics, Languages, Syntactics, programming languages, information-flow control, Algebra, Asbestos, Boolean algebra, Boolean values, calculus, computational motivation, decentralized label model (DLM), Design, DIFC, disjunction category model, encoding, Encoding, Flume, HiStar, Information flow control (IFC), information-flow frameworks, information-flow labels, JIF, label algebras, label models, labels encoding security policies, lambda-calculus, Lattices, LIO, Observers, operating systems, semantic encodings theory, telecommunication security, Theory",
    "DOI": "10.1109/CSF.2013.8",
    "page": "3-17",
    "page-first": "3",
    "_line": "FormalBib.bib:4651"
  },
  "lescuyer_provencore:_2015": {
    "id": "lescuyer_provencore:_2015",
    "type": "manuscript",
    "author": [
      {
        "family": "Lescuyer",
        "given": "Stéphane"
      }
    ],
    "title": "ProvenCore: Towards a Verified Isolation Micro-Kernel",
    "container-title-short": "ProvenCore",
    "title-short": "ProvenCore",
    "issued": {
      "date-parts": [
        [
          "2015",
          "1",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "abstract": "We report on an ongoing project aiming at a fully secure micro-kernel named ProvenCore. This operating system is both developed and specified in a single specification language called Smart. The Smart models are used to generate efficient C code and express low- and high-level properties of the implementation, and first among them guarantees of integrity and confidentiality for the various processes running on the kernel. ProvenCore is designed to be used as a secure world operating system in mobile devices, beneath a professional application platform or a Trusted Execution Environment.",
    "keywords": "Certification Toolchain, Formal Proof, Isolation, Separation Kernel,",
    "URL": "https://zenodo.org/record/47990#.XOrmF-tKi24",
    "_line": "FormalBib.bib:4664"
  },
  "lescuyer_provencore:_nodate": {
    "id": "lescuyer_provencore:_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Lescuyer",
        "given": "Stéphane"
      }
    ],
    "title": "ProvenCore: Towards a Verified Isolation Micro-Kernel",
    "page": "69",
    "page-first": "69",
    "language": "en-US",
    "_line": "FormalBib.bib:4676"
  },
  "goguen_unwinding_1984": {
    "id": "goguen_unwinding_1984",
    "type": "paper-conference",
    "author": [
      {
        "family": "Goguen",
        "given": "Joseph A."
      },
      {
        "family": "Meseguer",
        "given": "Jose"
      }
    ],
    "title": "Unwinding and Inference Control",
    "container-title": "1984 IEEE Symposium on Security and Privacy",
    "event-title": "1984 IEEE Symposium on Security and Privacy",
    "issued": {
      "date-parts": [
        [
          "1984",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-0-8186-0532-1",
    "URL": "http://ieeexplore.ieee.org/document/6234812/",
    "DOI": "10.1109/SP.1984.10019",
    "publisher-place": "Oakland, CA, USA",
    "page": "75-75",
    "page-first": "75",
    "language": "en-US",
    "_line": "FormalBib.bib:4684"
  },
  "jajodia_termination-insensitive_2008": {
    "id": "jajodia_termination-insensitive_2008",
    "type": "chapter",
    "author": [
      {
        "family": "Askarov",
        "given": "Aslan"
      },
      {
        "family": "Hunt",
        "given": "Sebastian"
      },
      {
        "family": "Sabelfeld",
        "given": "Andrei"
      },
      {
        "family": "Sands",
        "given": "David"
      }
    ],
    "editor": [
      {
        "family": "Jajodia",
        "given": "Sushil"
      },
      {
        "family": "Lopez",
        "given": "Javier"
      }
    ],
    "title": "Termination-Insensitive Noninterference Leaks More Than Just a Bit",
    "container-title": "Computer Security - ESORICS 2008",
    "issued": {
      "date-parts": [
        [
          "2008"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-540-88312-8 978-3-540-88313-5",
    "abstract": "Current tools for analysing information ﬂow in programs build upon ideas going back to Denning’s work from the 70’s. These systems enforce an imperfect notion of information ﬂow which has become known as terminationinsensitive noninterference. Under this version of noninterference, information leaks are permitted if they are transmitted purely by the program’s termination behaviour (i.e., whether it terminates or not). This imperfection is the price to pay for having a security condition which is relatively liberal (e.g. allowing whileloops whose termination may depend on the value of a secret) and easy to check. But what is the price exactly? We argue that, in the presence of output, the price is higher than the “one bit” often claimed informally in the literature, and effectively such programs can leak all of their secrets. In this paper we develop a deﬁnition of termination-insensitive noninterference suitable for reasoning about programs with outputs. We show that the deﬁnition generalises “batch-job” style deﬁnitions from the literature and that it is indeed satisﬁed by a Denning-style program analysis with output. Although more than a bit of information can be leaked by programs satisfying this condition, we show that the best an attacker can do is a brute-force attack, which means that the attacker cannot reliably (in a technical sense) learn the secret in polynomial time in the size of the secret. If we further assume that secrets are uniformly distributed, we show that the advantage the attacker gains when guessing the secret after observing a polynomial amount of output is negligible in the size of the secret.",
    "URL": "http://link.springer.com/10.1007/978-3-540-88313-5_22",
    "DOI": "10.1007/978-3-540-88313-5_22",
    "publisher-place": "Berlin, Heidelberg",
    "page": "333-348",
    "page-first": "333",
    "volume": "5283",
    "language": "en-US",
    "_line": "FormalBib.bib:4701"
  },
  "sjosten_information_2018": {
    "id": "sjosten_information_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Sjösten",
        "given": "Alexander"
      },
      {
        "family": "Hedin",
        "given": "Daniel"
      },
      {
        "family": "Sabelfeld",
        "given": "Andrei"
      }
    ],
    "editor": [
      {
        "family": "Baier",
        "given": "Christel"
      },
      {
        "family": "Caires",
        "given": "Luís"
      }
    ],
    "title": "Information Flow Tracking for Side-Effectful Libraries",
    "container-title": "Formal Techniques for Distributed Objects, Components, and Systems",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-92612-4",
    "abstract": "Dynamic information flow control is a promising technique for ensuring confidentiality and integrity of applications that manipulate sensitive information. While much progress has been made on increasingly powerful programming languages ranging from low-level machine languages to high-level languages for distributed systems, surprisingly little attention has been devoted to libraries and APIs. The state of the art is largely an all-or-nothing choice: either a shallow or deep library modeling approach. Seeking to break out of this restrictive choice, we formalize a general mechanism that tracks information flow for a language that includes higher-order functions, structured data types and references. A key feature of our approach is the model heap, a part of the memory, where security information is kept to enable the interaction between the labeled program and the unlabeled library. We provide a proof-of-concept implementation and report on experiments with a file system library. The system has been proved correct using Coq.",
    "page": "141-160",
    "page-first": "141",
    "language": "en-US",
    "_line": "FormalBib.bib:4720"
  },
  "chiricescu_safe:_2013": {
    "id": "chiricescu_safe:_2013",
    "type": "paper-conference",
    "author": [
      {
        "family": "Chiricescu",
        "given": "Silviu"
      },
      {
        "family": "DeHon",
        "given": "Andre"
      },
      {
        "family": "Demange",
        "given": "Delphine"
      },
      {
        "family": "Iyer",
        "given": "Suraj"
      },
      {
        "family": "Kliger",
        "given": "Aleksey"
      },
      {
        "family": "Morrisett",
        "given": "Greg"
      },
      {
        "family": "Pierce",
        "given": "Benjamin C."
      },
      {
        "family": "Reubenstein",
        "given": "Howard"
      },
      {
        "family": "Smith",
        "given": "Jonathan M."
      },
      {
        "family": "Sullivan",
        "given": "Gregory T."
      },
      {
        "family": "Thomas",
        "given": "Arun"
      },
      {
        "family": "Tov",
        "given": "Jesse"
      },
      {
        "family": "White",
        "given": "Christopher M."
      },
      {
        "family": "Wittenberg",
        "given": "David"
      }
    ],
    "title": "SAFE: A clean-slate architecture for secure systems",
    "container-title": "2013 IEEE International Conference on Technologies for Homeland Security (HST)",
    "container-title-short": "SAFE",
    "title-short": "SAFE",
    "event-title": "2013 IEEE International Conference on Technologies for Homeland Security (HST)",
    "issued": {
      "date-parts": [
        [
          "2013",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-4799-1535-4 978-1-4799-3963-3",
    "abstract": "SAFE is a large-scale, clean-slate co-design project encompassing hardware architecture, programming languages, and operating systems. Funded by DARPA, the goal of SAFE is to create a secure computing system from the ground up. SAFE hardware provides memory safety, dynamic type checking, and native support for dynamic information ﬂow control. The Breeze programming language leverages the security features of the underlying machine, and the “zero kernel” operating system avoids relying on any single privileged component for overall system security. The SAFE project is working towards formally verifying security properties of the runtime software. The SAFE system sets a new high-water mark for system security, allowing secure applications to be built on a solid foundation rather than on the inherently vulnerable conventional platforms available today.",
    "URL": "http://ieeexplore.ieee.org/document/6699066/",
    "DOI": "10.1109/THS.2013.6699066",
    "publisher-place": "Waltham, MA, USA",
    "page": "570-576",
    "page-first": "570",
    "language": "en-US",
    "_line": "FormalBib.bib:4735"
  },
  "hedin_perspective_nodate": {
    "id": "hedin_perspective_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Hedin",
        "given": "Daniel"
      },
      {
        "family": "Sabelfeld",
        "given": "Andrei"
      }
    ],
    "title": "A Perspective on Information-Flow Control",
    "abstract": "Information-ﬂow control tracks how information propagates through the program during execution to make sure that the program handles the information securely. Secure information ﬂow is comprised of two related aspects: information conﬁdentiality and information integrity — intuitively pertaining to the reading and writing of the information. The prevailing basic semantic notion of secure information ﬂow is noninterference, demanding independence of public (or, in the case of integrity, trusted) output from secret (or, in the case of integrity, untrusted) input. This document gives an account of the state-of-the-art in conﬁdentiality and integrity policies and their enforcement with a systematic formalization of four dominant formulations of noninterference: termination-insensitive, termination-sensitive, progress-insensitive, and progress-sensitive, cast in the setting of two minimal while languages.",
    "page": "29",
    "page-first": "29",
    "language": "en-US",
    "_line": "FormalBib.bib:4754"
  },
  "dijkstra_political_1978": {
    "id": "dijkstra_political_1978",
    "type": "article-journal",
    "author": [
      {
        "family": "Dijkstra",
        "given": "Edsger W."
      },
      {
        "family": "DeMillo",
        "given": "R. A."
      },
      {
        "family": "Lipton",
        "given": "R. J."
      },
      {
        "family": "Perlis",
        "given": "A J."
      }
    ],
    "title": "On a Political Pamphlet from the Middle Ages",
    "container-title": "SIGSOFT Softw. Eng. Notes",
    "issued": {
      "date-parts": [
        [
          "1978",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "issn": "0163-5948",
    "URL": "http://doi.acm.org/10.1145/1005888.1005890",
    "DOI": "10.1145/1005888.1005890",
    "page": "14-16",
    "page-first": "14",
    "volume": "3",
    "issue": "2",
    "_line": "FormalBib.bib:4763"
  },
  "de_millo_social_1979": {
    "id": "de_millo_social_1979",
    "type": "article-journal",
    "author": [
      {
        "family": "De Millo",
        "given": "Richard A."
      },
      {
        "family": "Lipton",
        "given": "Richard J."
      },
      {
        "family": "Perlis",
        "given": "Alan J."
      }
    ],
    "title": "Social Processes and Proofs of Theorems and Programs",
    "container-title": "Commun. ACM",
    "issued": {
      "date-parts": [
        [
          "1979",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "issn": "0001-0782",
    "abstract": "It is argued that formal verifications of programs, no matter how obtained, will not play the same key role in the development of computer science and software engineering as proofs do in mathematics. Furthermore the absence of continuity, the inevitability of change, and the complexity of specification of significantly many real programs make the formal verification process difficult to justify and manage. It is felt that ease of formal verification should not dominate program language design.",
    "keywords": "formal mathematics, program verification, mathematical proofs, program specification",
    "URL": "http://doi.acm.org/10.1145/359104.359106",
    "DOI": "10.1145/359104.359106",
    "page": "271-280",
    "page-first": "271",
    "volume": "22",
    "issue": "5",
    "_line": "FormalBib.bib:4778"
  },
  "weirich_specification_2017": {
    "id": "weirich_specification_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Weirich",
        "given": "Stephanie"
      },
      {
        "family": "Voizard",
        "given": "Antoine"
      },
      {
        "family": "Amorim",
        "given": "Pedro Henrique Azevedo",
        "dropping-particle": "de"
      },
      {
        "family": "Eisenberg",
        "given": "Richard A."
      }
    ],
    "title": "A Specification for Dependent Types in Haskell",
    "container-title": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2017",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "We propose a core semantics for Dependent Haskell, an extension of Haskell with full-spectrum dependent types. Our semantics consists of two related languages. The first is a Curry-style dependently-typed language with nontermination, irrelevant arguments, and equality abstraction. The second, inspired by the Glasgow Haskell Compiler's core language FC, is its explicitly-typed analogue, suitable for implementation in GHC. All of our results&mdash;chiefly, type safety, along with theorems that relate these two languages&mdash;have been formalized using the Coq proof assistant. Because our work is backwards compatible with Haskell, our type safety proof holds in the presence of nonterminating computation. However, unlike other full-spectrum dependently-typed languages, such as Coq, Agda or Idris, because of this nontermination, Haskell's term language does not correspond to a consistent logic.",
    "keywords": "Haskell, Dependent Types",
    "URL": "http://doi.acm.org/10.1145/3110275",
    "DOI": "10.1145/3110275",
    "page": "31:1-31:29",
    "page-first": "31",
    "volume": "1",
    "_line": "FormalBib.bib:4795"
  },
  "eisenberg_dependent_2016": {
    "id": "eisenberg_dependent_2016",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Eisenberg",
        "given": "Richard A"
      }
    ],
    "title": "DEPENDENT TYPES IN HASKELL: THEORY AND PRACTICE",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "publisher": "Pennsylvania",
    "number-of-pages": "351",
    "publisher-place": "Philadelphia, PA, USA",
    "language": "en-US",
    "_line": "FormalBib.bib:4812"
  },
  "casinghino_combining_2014": {
    "id": "casinghino_combining_2014",
    "type": "paper-conference",
    "author": [
      {
        "family": "Casinghino",
        "given": "Chris"
      },
      {
        "family": "Sjöberg",
        "given": "Vilhelm"
      },
      {
        "family": "Weirich",
        "given": "Stephanie"
      }
    ],
    "title": "Combining Proofs and Programs in a Dependently Typed Language",
    "container-title": "Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "collection-title": "POPL '14",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-2544-8",
    "abstract": "Most dependently-typed programming languages either require that all expressions terminate (e.g. Coq, Agda, and Epigram), or allow infinite loops but are inconsistent when viewed as logics (e.g. Haskell, ATS, Ωmega. Here, we combine these two approaches into a single dependently-typed core language. The language is composed of two fragments that share a common syntax and overlapping semantics: a logic that guarantees total correctness, and a call-by-value programming language that guarantees type safety but not termination. The two fragments may interact: logical expressions may be used as programs; the logic may soundly reason about potentially nonterminating programs; programs can require logical proofs as arguments; and \"mobile\" program values, including proofs computed at runtime, may be used as evidence by the logic. This language allows programmers to work with total and partial functions uniformly, providing a smooth path from functional programming to dependently-typed programming.",
    "keywords": "dependent types, termination, general recursion",
    "URL": "http://doi.acm.org/10.1145/2535838.2535883",
    "DOI": "10.1145/2535838.2535883",
    "publisher-place": "New York, NY, USA",
    "page": "33-45",
    "page-first": "33",
    "note": "event-place: San Diego, California, USA",
    "_line": "FormalBib.bib:4824"
  },
  "casinghino_combining_2014-1": {
    "id": "casinghino_combining_2014-1",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Casinghino",
        "given": "Chris"
      }
    ],
    "title": "Combining Proofs and Programs",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "University of Pennsylvania",
    "URL": "https://www.seas.upenn.edu/~sweirich/papers/casinghino-thesis.pdf",
    "publisher-place": "Philadelphia, PA, USA",
    "language": "en-US",
    "_line": "FormalBib.bib:4843"
  },
  "breitner_ready_2018": {
    "id": "breitner_ready_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Breitner",
        "given": "Joachim"
      },
      {
        "family": "Spector-Zabusky",
        "given": "Antal"
      },
      {
        "family": "Li",
        "given": "Yao"
      },
      {
        "family": "Rizkallah",
        "given": "Christine"
      },
      {
        "family": "Wiegley",
        "given": "John"
      },
      {
        "family": "Weirich",
        "given": "Stephanie"
      }
    ],
    "title": "Ready, Set, Verify! Applying Hs-to-coq to Real-world Haskell Code (Experience Report)",
    "container-title": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2018",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "Good tools can bring mechanical verification to programs written in mainstream functional languages. We use &lt;pre&gt;hs-to-coq&lt;/pre&gt; to translate significant portions of Haskell’s &lt;pre&gt;containers&lt;/pre&gt; library into Coq, and verify it against specifications that we derive from a variety of sources including type class laws, the library’s test suite, and interfaces from Coq’s standard library. Our work shows that it is feasible to verify mature, widely-used, highly optimized, and unmodified Haskell code. We also learn more about the theory of weight-balanced trees, extend &lt;pre&gt;hs-to-coq&lt;/pre&gt; to handle partiality, and – since we found no bugs – attest to the superb quality of well-tested functional code.",
    "keywords": "Coq, verification, Haskell",
    "URL": "http://doi.acm.org/10.1145/3236784",
    "DOI": "10.1145/3236784",
    "page": "89:1-89:16",
    "page-first": "89",
    "volume": "2",
    "_line": "FormalBib.bib:4856"
  },
  "gueneau_formal_nodate": {
    "id": "gueneau_formal_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Guéneau",
        "given": "Armaël"
      },
      {
        "family": "Jourdan",
        "given": "Jacques-Henri"
      },
      {
        "family": "Charguéraud",
        "given": "Arthur"
      },
      {
        "family": "Pottier",
        "given": "François"
      }
    ],
    "title": "Formal Proof and Analysis of an Incremental Cycle Detection Algorithm",
    "page": "23",
    "page-first": "23",
    "language": "en-US",
    "_line": "FormalBib.bib:4873"
  },
  "timany_cumulative_2018": {
    "id": "timany_cumulative_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Timany",
        "given": "Amin"
      },
      {
        "family": "Sozeau",
        "given": "Matthieu"
      }
    ],
    "title": "Cumulative Inductive Types In Coq",
    "container-title": "FSCD",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "abstract": "In order to avoid well-known paradoxes associated with self-referential definitions, higher-order dependent type theories stratify the theory using a countably infinite hierarchy of universes (also known as sorts), Type0 : Type1 : · · · . Such type systems are called cumulative if for any type A we have that A : Typei implies A : Typei+1. The Predicative Calculus of Inductive Constructions (pCIC) which forms the basis of the Coq proof assistant, is one such system. In this paper we present the Predicative Calculus of Cumulative Inductive Constructions (pCuIC) which extends the cumulativity relation to inductive types. We discuss cumulative inductive types as present in Coq 8.7 and their application to formalization and definitional translations. 2012 ACM Subject Classification Theory of computation → Type theory, Theory of computation → Lambda calculus",
    "keywords": "Calculi, Calculus of constructions, Coq (software), Dependent type, Inductive type, Proof assistant, Type system",
    "DOI": "10.4230/LIPIcs.FSCD.2018.29",
    "_line": "FormalBib.bib:4881"
  },
  "gilbert_definitional_2019": {
    "id": "gilbert_definitional_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Gilbert",
        "given": "Gaëtan"
      },
      {
        "family": "Cockx",
        "given": "Jesper"
      },
      {
        "family": "Sozeau",
        "given": "Matthieu"
      },
      {
        "family": "Tabareau",
        "given": "Nicolas"
      }
    ],
    "title": "Definitional Proof-irrelevance Without K",
    "container-title": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "Definitional equality—or conversion—for a type theory with a decidable type checking is the simplest tool to prove that two objects are the same, letting the system decide just using computation. Therefore, the more things are equal by conversion, the simpler it is to use a language based on type theory. Proof-irrelevance, stating that any two proofs of the same proposition are equal, is a possible way to extend conversion to make a type theory more powerful. However, this new power comes at a price if we integrate it naively, either by making type checking undecidable or by realizing new axioms—such as uniqueness of identity proofs (UIP)—that are incompatible with other extensions, such as univalence. In this paper, taking inspiration from homotopy type theory, we propose a general way to extend a type theory with definitional proof irrelevance, in a way that keeps type checking decidable and is compatible with univalence. We provide a new criterion to decide whether a proposition can be eliminated over a type (correcting and improving the so-called singleton elimination of Coq) by using techniques coming from recent development on dependent pattern matching without UIP. We show the generality of our approach by providing implementations for both Coq and Agda, both of which are planned to be integrated in future versions of those proof assistants.",
    "keywords": "proof assistants, proof irrelevance, type theory",
    "URL": "http://doi.acm.org/10.1145/3290316",
    "DOI": "10.1145/3290316",
    "page": "3:1-3:28",
    "page-first": "3",
    "volume": "3",
    "_line": "FormalBib.bib:4892"
  },
  "sozeau_metacoq_nodate": {
    "id": "sozeau_metacoq_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Sozeau",
        "given": "Matthieu"
      }
    ],
    "title": "The MetaCoq Project",
    "abstract": "The MetaCoq project1 aims to provide a certiﬁed meta-programming environment in Coq. It builds on Template-Coq, a plugin for Coq originally implemented by Malecha (2014), which provided a reiﬁer for Coq terms and global declarations, as represented in the Coq kernel, as well as a denotation command. Recently, it was used in the CertiCoq certiﬁed compiler project (Anand et al., 2017), as its front-end language, to derive parametricity properties (Anand and Morrisett, 2018). However, the syntax lacked semantics, be it typing semantics or operational semantics, which should reﬂect, as formal speciﬁcations in Coq, the semantics of Coq’s type theory itself. The tool was also rather bare bones, providing only rudimentary quoting and unquoting commands. We generalize it to handle the entire Polymorphic Calculus of Cumulative Inductive Constructions (pCUIC), as implemented by Coq, including the kernel’s declaration structures for deﬁnitions and inductives, and implement a monad for general manipulation of Coq’s logical environment. We demonstrate how this setup allows Coq users to deﬁne many kinds of general purpose plugins, whose correctness can be readily proved in the system itself, and that can be run eﬃciently after extraction. We give a few examples of implemented plugins, including a parametricity translation and a certifying extraction to call-by-value λ-calculus. We also advocate the use of MetaCoq as a foundation for higher-level tools.",
    "page": "39",
    "page-first": "39",
    "language": "en-US",
    "_line": "FormalBib.bib:4909"
  },
  "noauthor_learning_2019": {
    "id": "noauthor_learning_2019",
    "type": "book",
    "title": "A Learning Environment for Theorem Proving with the Coq proof assistant: princeton-vl/CoqGym",
    "container-title-short": "A Learning Environment for Theorem Proving with the Coq proof assistant",
    "title-short": "A Learning Environment for Theorem Proving with the Coq proof assistant",
    "issued": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "Princeton Vision &amp; Learning Lab",
    "URL": "https://github.com/princeton-vl/CoqGym",
    "note": "original-date: 2019-05-24T22:31:20Z",
    "_line": "FormalBib.bib:4918"
  },
  "yang_learning_2019": {
    "id": "yang_learning_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Yang",
        "given": "Kaiyu"
      },
      {
        "family": "Deng",
        "given": "Jia"
      }
    ],
    "title": "Learning to Prove Theorems via Interacting with Proof Assistants",
    "container-title": "arXiv:1905.09381 \\[cs, stat\\]",
    "issued": {
      "date-parts": [
        [
          "2019",
          "5",
          "21"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "abstract": "Humans prove theorems by relying on substantial high-level reasoning and problem-specific insights. Proof assistants offer a formalism that resembles human mathematical reasoning, representing theorems in higher-order logic and proofs as high-level tactics. However, human experts have to construct proofs manually by entering tactics into the proof assistant. In this paper, we study the problem of using machine learning to automate the interaction with proof assistants. We construct CoqGym, a large-scale dataset and learning environment containing 71K human-written proofs from 123 projects developed with the Coq proof assistant. We develop ASTactic, a deep learning-based model that generates tactics as programs in the form of abstract syntax trees (ASTs). Experiments show that ASTactic trained on CoqGym can generate effective tactics and can be used to prove new theorems not previously provable by automated methods. Code is available at https://github.com/princeton-vl/CoqGym.",
    "keywords": "Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning",
    "URLtext": "1905.09381",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1905.09381",
    "URL": "http://arxiv.org/abs/1905.09381",
    "_line": "FormalBib.bib:4929"
  },
  "bansal_holist:_2019": {
    "id": "bansal_holist:_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Bansal",
        "given": "Kshitij"
      },
      {
        "family": "Loos",
        "given": "Sarah M."
      },
      {
        "family": "Rabe",
        "given": "Markus N."
      },
      {
        "family": "Szegedy",
        "given": "Christian"
      },
      {
        "family": "Wilcox",
        "given": "Stewart"
      }
    ],
    "title": "HOList: An Environment for Machine Learning of Higher-Order Theorem Proving (extended version)",
    "container-title": "arXiv:1904.03241 \\[cs\\]",
    "container-title-short": "HOList",
    "title-short": "HOList",
    "issued": {
      "date-parts": [
        [
          "2019",
          "4",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "5",
          "26"
        ]
      ]
    },
    "abstract": "We present an environment, benchmark, and deep learning driven automated theorem prover for higher-order logic. Higher-order interactive theorem provers enable the formalization of arbitrary mathematical theories and thereby present an interesting, open-ended challenge for deep learning. We provide an open-source framework based on the HOL Light theorem prover that can be used as a reinforcement learning environment. HOL Light comes with a broad coverage of basic mathematical theorems on calculus and the formal proof of the Kepler conjecture, from which we derive a challenging benchmark for automated reasoning. We also present a deep reinforcement learning driven automated theorem prover, DeepHOL, with strong initial results on this benchmark.",
    "keywords": "Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Artificial Intelligence",
    "URLtext": "1904.03241",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1904.03241",
    "URL": "http://arxiv.org/abs/1904.03241",
    "_line": "FormalBib.bib:4943"
  },
  "strange_loop_proof_nodate": {
    "id": "strange_loop_proof_nodate",
    "type": "motion-picture",
    "author": [
      {
        "family": "Strange Loop"
      }
    ],
    "title": "\"Proof Theory Impressionism: Blurring the Curry-Howard Line\" by Dan Pittman",
    "container-title-short": "\"Proof Theory Impressionism",
    "title-short": "\"Proof Theory Impressionism",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "3",
          "22"
        ]
      ]
    },
    "URL": "https://www.youtube.com/watch?v=jrVPB-Ad5Gc&t=31s",
    "_line": "FormalBib.bib:4958"
  },
  "altenkirch_why_nodate": {
    "id": "altenkirch_why_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Altenkirch",
        "given": "Thorsten"
      },
      {
        "family": "McBride",
        "given": "Conor"
      },
      {
        "family": "McKinna",
        "given": "James"
      }
    ],
    "title": "Why Dependent Types Matter",
    "abstract": "We exhibit the rationale behind the design of Epigram, a dependently typed programming language and interactive program development system, using reﬁnements of a well known program—merge sort—as a running example. We discuss its relationship with other proposals to introduce aspects of dependent types into functional programming languages and sketch some topics for further work in this area.",
    "page": "21",
    "page-first": "21",
    "language": "en-US",
    "_line": "FormalBib.bib:4966"
  },
  "mckinna_why_2006": {
    "id": "mckinna_why_2006",
    "type": "paper-conference",
    "author": [
      {
        "family": "McKinna",
        "given": "James"
      }
    ],
    "title": "Why Dependent Types Matter",
    "container-title": "Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "collection-title": "POPL '06",
    "issued": {
      "date-parts": [
        [
          "2006"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "3",
          "22"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-59593-027-9",
    "abstract": "Language designers have in recent years proposed a wealth of richer type systems for programming which seek to extend the range of statically enforced guarantees on data and code. Most such proposals have been evolutionary extensions of ML or Haskell, offering programmers a balanced compromise between expressive strength and existing well-understood technology. Typically they revolve around type- or kind-indexed types such as GADTs, supported by limited equality reasoning at the type-checking level, thus separating the dynamic behaviour of programs from the (simpler) static behaviour of indexing information occurring in their types.I want to argue in this talk for a more radical departure from such practice by examining full spectrum type dependency, lifting such restrictions on the data upon which types may depend. Conor McBride and I designed the language EPIGRAM for experiments in programming with inductive families of data (of which GADTs are a special case). Using it for illustration, I will explore some of the possibilities and challenges afforded by full spectrum type dependency at the static and dynamic level: types directly support modelling complex invariants in terms of other data (rather than their types), with a Curry-Howard flavour of data-as-evidence; such complexity is on a 'pay-as-you-go' basis, while keeping type annotations and other syntactic overheads to a minimum; data decomposition steps, e.g. case analysis, furnish more informative interactions between types and values during typechecking; such steps may moreover be abstractly specified by their types, and thus user definable; this supports a style of programming embracing 'learning by testing', views, and Burstall's 'hand simulation plus a little induction'; the absence of a rigid phase distinction need not lead to type-passing or excessive run-time overhead; effectful computation, in particular partiality, can be incorporated via variations on existing ideas such as monads.This talk is based on joint work with Conor McBride, Edwin Brady and Thorsten Altenkirch.",
    "URL": "http://doi.acm.org/10.1145/1111037.1111038",
    "DOI": "10.1145/1111037.1111038",
    "publisher-place": "New York, NY, USA",
    "page": "1-1",
    "page-first": "1",
    "note": "event-place: Charleston, South Carolina, USA",
    "_line": "FormalBib.bib:4975"
  },
  "stewart_verified_nodate": {
    "id": "stewart_verified_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Stewart",
        "given": "Gordon"
      }
    ],
    "title": "Verified Separate Compilation for C &bar; Computer Science Department at Princeton University",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "3",
          "3"
        ]
      ]
    },
    "URL": "https://www.cs.princeton.edu/research/techreps/TR-980-15",
    "_line": "FormalBib.bib:4993"
  },
  "noauthor_formalization_2019": {
    "id": "noauthor_formalization_2019",
    "type": "book",
    "title": "Formalization of the Interaction Tree Datatype in Coq: DeepSpec/InteractionTrees",
    "container-title-short": "Formalization of the Interaction Tree Datatype in Coq",
    "title-short": "Formalization of the Interaction Tree Datatype in Coq",
    "issued": {
      "date-parts": [
        [
          "2019",
          "2",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "28"
        ]
      ]
    },
    "publisher": "DeepSpec",
    "URL": "https://github.com/DeepSpec/InteractionTrees",
    "note": "original-date: 2018-06-21T18:53:18Z",
    "_line": "FormalBib.bib:5001"
  },
  "sewell_ott_2019": {
    "id": "sewell_ott_2019",
    "type": "book",
    "author": [
      {
        "family": "Sewell",
        "given": "Peter"
      }
    ],
    "title": "The Ott tool for writing definitions of programming languages and calculi: ott-lang/ott",
    "container-title-short": "The Ott tool for writing definitions of programming languages and calculi",
    "title-short": "The Ott tool for writing definitions of programming languages and calculi",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "28"
        ]
      ]
    },
    "publisher": "ott-lang",
    "URL": "https://github.com/ott-lang/ott",
    "note": "original-date: 2016-10-27T15:44:41Z",
    "_line": "FormalBib.bib:5012"
  },
  "noauthor_lem_2019": {
    "id": "noauthor_lem_2019",
    "type": "book",
    "title": "Lem semantic definition language.",
    "issued": {
      "date-parts": [
        [
          "2019",
          "2",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "28"
        ]
      ]
    },
    "publisher": "REMS",
    "URL": "https://github.com/rems-project/lem",
    "note": "original-date: 2018-01-31T15:35:24Z",
    "_line": "FormalBib.bib:5024"
  },
  "mulligan_lem:_2014": {
    "id": "mulligan_lem:_2014",
    "type": "paper-conference",
    "author": [
      {
        "family": "Mulligan",
        "given": "Dominic P."
      },
      {
        "family": "Owens",
        "given": "Scott"
      },
      {
        "family": "Gray",
        "given": "Kathryn E."
      },
      {
        "family": "Ridge",
        "given": "Tom"
      },
      {
        "family": "Sewell",
        "given": "Peter"
      }
    ],
    "title": "Lem: Reusable Engineering of Real-world Semantics",
    "container-title": "Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming",
    "container-title-short": "Lem",
    "collection-title": "ICFP '14",
    "title-short": "Lem",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "28"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-2873-9",
    "abstract": "Recent years have seen remarkable successes in rigorous engineering: using mathematically rigorous semantic models (not just idealised calculi) of real-world processors, programming languages, protocols, and security mechanisms, for testing, proof, analysis, and design. Building these models is challenging, requiring experimentation, dialogue with vendors or standards bodies, and validation; their scale adds engineering issues akin to those of programming to the task of writing clear and usable mathematics. But language and tool support for specification is lacking. Proof assistants can be used but bring their own difficulties, and a model produced in one, perhaps requiring many person-years effort and maintained over an extended period, cannot be used by those familiar with another. We introduce Lem, a language for engineering reusable large-scale semantic models. The Lem design takes inspiration both from functional programming languages and from proof assistants, and Lem definitions are translatable into OCaml for testing, Coq, HOL4, and Isabelle/HOL for proof, and LaTeX and HTML for presentation. This requires a delicate balance of expressiveness, careful library design, and implementation of transformations - akin to compilation, but subject to the constraint of producing usable and human-readable code for each target. Lem's effectiveness is demonstrated by its use in practice.",
    "keywords": "proof assistants, lem, real-world semantics, specification languages",
    "URL": "http://doi.acm.org/10.1145/2628136.2628143",
    "DOI": "10.1145/2628136.2628143",
    "publisher-place": "New York, NY, USA",
    "page": "175-188",
    "page-first": "175",
    "note": "event-place: Gothenburg, Sweden",
    "_line": "FormalBib.bib:5033"
  },
  "kell_missing_2016": {
    "id": "kell_missing_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Kell",
        "given": "Stephen"
      },
      {
        "family": "Mulligan",
        "given": "Dominic P."
      },
      {
        "family": "Sewell",
        "given": "Peter"
      }
    ],
    "title": "The Missing Link: Explaining ELF Static Linking, Semantically",
    "container-title": "Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications",
    "container-title-short": "The Missing Link",
    "collection-title": "OOPSLA 2016",
    "title-short": "The Missing Link",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "28"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-4444-9",
    "abstract": "Beneath the surface, software usually depends on complex linker behaviour to work as intended. Even linking &lt;pre&gt;hello&underscore;world.c&lt;/pre&gt; is surprisingly involved, and systems software such as &lt;pre&gt;libc&lt;/pre&gt; and operating system kernels rely on a host of linker features. But linking is poorly understood by working programmers and has largely been neglected by language researchers.  In this paper we survey the many use-cases that linkers support and the poorly specified linker speak by which they are controlled: metadata in object files, command-line options, and linker-script language. We provide the first validated formalisation of a realistic executable and linkable format (ELF), and capture aspects of the Application Binary Interfaces for four mainstream platforms (AArch64, AMD64, Power64, and IA32). Using these, we develop an executable specification of static linking, covering (among other things) enough to link small C programs (we use the example of bzip2) into a correctly running executable. We provide our specification in Lem and Isabelle/HOL forms. This is the first formal specification of mainstream linking. We have used the Isabelle/HOL version to prove a sample correctness property for one case of AMD64 ABI relocation, demonstrating that the specification supports formal proof, and as a first step towards the much more ambitious goal of verified linking. Our work should enable several novel strands of research, including linker-aware verified compilation and program analysis, and better languages for controlling linking.",
    "keywords": "Executable and Linkable Format (ELF), formal specification, Linking, theorem-proving",
    "URL": "http://doi.acm.org/10.1145/2983990.2983996",
    "DOI": "10.1145/2983990.2983996",
    "publisher-place": "New York, NY, USA",
    "page": "607-623",
    "page-first": "607",
    "note": "event-place: Amsterdam, Netherlands",
    "_line": "FormalBib.bib:5053"
  },
  "nienhuis_operational_2016": {
    "id": "nienhuis_operational_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Nienhuis",
        "given": "Kyndylan"
      },
      {
        "family": "Memarian",
        "given": "Kayvan"
      },
      {
        "family": "Sewell",
        "given": "Peter"
      }
    ],
    "title": "An Operational Semantics for C/C++11 Concurrency",
    "container-title": "Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications",
    "collection-title": "OOPSLA 2016",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "28"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-4444-9",
    "abstract": "The C/C++11 concurrency model balances two goals: it is relaxed enough to be efficiently implementable and (leaving aside the \"thin-air\" problem) it is strong enough to give useful guarantees to programmers. It is mathematically precise and has been used in verification research and compiler testing. However, the model is expressed in an axiomatic style, as predicates on complete candidate executions. This suffices for computing the set of allowed executions of a small litmus test, but it does not directly support the incremental construction of executions of larger programs. It is also at odds with conventional operational semantics, as used implicitly in the rest of the C/C++ standards.   Our main contribution is the development of an operational model for C/C++11 concurrency. This covers all the features of the previous formalised axiomatic model, and we have a mechanised proof that the two are equivalent, in Isabelle/HOL. We also integrate this semantics with an operational semantics for sequential C (described elsewhere); the combined semantics can incrementally execute programs in a small fragment of C.   Doing this uncovered several new aspects of the C/C++11 model: we show that one cannot build an equivalent operational model that simply follows program order, sequential consistent order, or the synchronises-with order. The first negative result is forced by hardware-observable behaviour, but the latter two are not, and so might be ameliorated by changing C/C++11. More generally, we hope that this work, with its focus on incremental construction of executions, will inform the future design of new concurrency models.",
    "keywords": "C/C++, Concurrency",
    "URL": "http://doi.acm.org/10.1145/2983990.2983997",
    "DOI": "10.1145/2983990.2983997",
    "publisher-place": "New York, NY, USA",
    "page": "111-128",
    "page-first": "111",
    "note": "event-place: Amsterdam, Netherlands",
    "_line": "FormalBib.bib:5073"
  },
  "pulte_simplifying_2017": {
    "id": "pulte_simplifying_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Pulte",
        "given": "Christopher"
      },
      {
        "family": "Flur",
        "given": "Shaked"
      },
      {
        "family": "Deacon",
        "given": "Will"
      },
      {
        "family": "French",
        "given": "Jon"
      },
      {
        "family": "Sarkar",
        "given": "Susmit"
      },
      {
        "family": "Sewell",
        "given": "Peter"
      }
    ],
    "title": "Simplifying ARM Concurrency: Multicopy-atomic Axiomatic and Operational Models for ARMv8",
    "container-title": "Proc. ACM Program. Lang.",
    "container-title-short": "Simplifying ARM Concurrency",
    "title-short": "Simplifying ARM Concurrency",
    "issued": {
      "date-parts": [
        [
          "2017",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "28"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "ARM has a relaxed memory model, previously specified in informal prose for ARMv7 and ARMv8. Over time, and partly due to work building formal semantics for ARM concurrency, it has become clear that some of the complexity of the model is not justified by the potential benefits. In particular, the model was originally non-multicopy-atomic: writes could become visible to some other threads before becoming visible to all — but this has not been exploited in production implementations, the corresponding potential hardware optimisations are thought to have insufficient benefits in the ARM context, and it gives rise to subtle complications when combined with other ARMv8 features. The ARMv8 architecture has therefore been revised: it now has a multicopy-atomic model. It has also been simplified in other respects, including more straightforward notions of dependency, and the architecture now includes a formal concurrency model.  In this paper we detail these changes and discuss their motivation. We define two formal concurrency models: an operational one, simplifying the Flowing model of Flur et al., and the axiomatic model of the revised ARMv8 specification. The models were developed by an academic group and by ARM staff, respectively, and this extended collaboration partly motivated the above changes. We prove the equivalence of the two models. The operational model is integrated into an executable exploration tool with new web interface, demonstrated by exhaustively checking the possible behaviours of a loop-unrolled version of a Linux kernel lock implementation, a previously known bug due to unprevented speculation, and a fixed version.",
    "keywords": "Semantics, Axiomatic, Operational, Relaxed Memory Models",
    "URL": "http://doi.acm.org/10.1145/3158107",
    "DOI": "10.1145/3158107",
    "page": "19:1-19:29",
    "page-first": "19",
    "volume": "2",
    "_line": "FormalBib.bib:5092"
  },
  "armstrong_isa_2019": {
    "id": "armstrong_isa_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Armstrong",
        "given": "Alasdair"
      },
      {
        "family": "Bauereiss",
        "given": "Thomas"
      },
      {
        "family": "Campbell",
        "given": "Brian"
      },
      {
        "family": "Reid",
        "given": "Alastair"
      },
      {
        "family": "Gray",
        "given": "Kathryn E."
      },
      {
        "family": "Norton",
        "given": "Robert M."
      },
      {
        "family": "Mundkur",
        "given": "Prashanth"
      },
      {
        "family": "Wassell",
        "given": "Mark"
      },
      {
        "family": "French",
        "given": "Jon"
      },
      {
        "family": "Pulte",
        "given": "Christopher"
      },
      {
        "family": "Flur",
        "given": "Shaked"
      },
      {
        "family": "Stark",
        "given": "Ian"
      },
      {
        "family": "Krishnaswami",
        "given": "Neel"
      },
      {
        "family": "Sewell",
        "given": "Peter"
      }
    ],
    "title": "ISA Semantics for ARMv8-a, RISC-v, and CHERI-MIPS",
    "container-title": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "28"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "Architecture specifications notionally define the fundamental interface between hardware and software: the envelope of allowed behaviour for processor implementations, and the basic assumptions for software development and verification. But in practice, they are typically prose and pseudocode documents, not rigorous or executable artifacts, leaving software and verification on shaky ground.   In this paper, we present rigorous semantic models for the sequential behaviour of large parts of the mainstream ARMv8-A, RISC-V, and MIPS architectures, and the research CHERI-MIPS architecture, that are complete enough to boot operating systems, variously Linux, FreeBSD, or seL4. Our ARMv8-A models are automatically translated from authoritative ARM-internal definitions, and (in one variant) tested against the ARM Architecture Validation Suite.   We do this using a custom language for ISA semantics, Sail, with a lightweight dependent type system, that supports automatic generation of emulator code in C and OCaml, and automatic generation of proof-assistant definitions for Isabelle, HOL4, and (currently only for MIPS) Coq. We use the former for validation, and to assess specification coverage. To demonstrate the usability of the latter, we prove (in Isabelle) correctness of a purely functional characterisation of ARMv8-A address translation. We moreover integrate the RISC-V model into the RMEM tool for (user-mode) relaxed-memory concurrency exploration. We prove (on paper) the soundness of the core Sail type system.   We thereby take a big step towards making the architectural abstraction actually well-defined, establishing foundations for verification and reasoning.",
    "keywords": "Semantics, Instruction Set Architectures, Theorem Proving",
    "URL": "http://doi.acm.org/10.1145/3290384",
    "DOI": "10.1145/3290384",
    "page": "71:1-71:31",
    "page-first": "71",
    "volume": "3",
    "_line": "FormalBib.bib:5110"
  },
  "memarian_exploring_2019": {
    "id": "memarian_exploring_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Memarian",
        "given": "Kayvan"
      },
      {
        "family": "Gomes",
        "given": "Victor B. F."
      },
      {
        "family": "Davis",
        "given": "Brooks"
      },
      {
        "family": "Kell",
        "given": "Stephen"
      },
      {
        "family": "Richardson",
        "given": "Alexander"
      },
      {
        "family": "Watson",
        "given": "Robert N. M."
      },
      {
        "family": "Sewell",
        "given": "Peter"
      }
    ],
    "title": "Exploring C Semantics and Pointer Provenance",
    "container-title": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "28"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "The semantics of pointers and memory objects in C has been a vexed question for many years. C values cannot be treated as either purely abstract or purely concrete entities: the language exposes their representations, but compiler optimisations rely on analyses that reason about provenance and initialisation status, not just runtime representations. The ISO WG14 standard leaves much of this unclear, and in some respects differs with de facto standard usage &mdash; which itself is difficult to investigate.   In this paper we explore the possible source-language semantics for memory objects and pointers, in ISO C and in C as it is used and implemented in practice, focussing especially on pointer provenance. We aim to, as far as possible, reconcile the ISO C standard, mainstream compiler behaviour, and the semantics relied on by the corpus of existing C code. We present two coherent proposals, tracking provenance via integers and not; both address many design questions. We highlight some pros and cons and open questions, and illustrate the discussion with a library of test cases. We make our semantics executable as a test oracle, integrating it with the Cerberus semantics for much of the rest of C, which we have made substantially more complete and robust, and equipped with a web-interface GUI. This allows us to experimentally assess our proposals on those test cases. To assess their viability with respect to larger bodies of C code, we analyse the changes required and the resulting behaviour for a port of FreeBSD to CHERI, a research architecture supporting hardware capabilities, which (roughly speaking) traps on the memory safety violations which our proposals deem undefined behaviour. We also develop a new runtime instrumentation tool to detect possible provenance violations in normal C code, and apply it to some of the SPEC benchmarks. We compare our proposal with a source-language variant of the twin-allocation LLVM semantics proposal of Lee et al. Finally, we describe ongoing interactions with WG14, exploring how our proposals could be incorporated into the ISO standard.",
    "keywords": "C",
    "URL": "http://doi.acm.org/10.1145/3290380",
    "DOI": "10.1145/3290380",
    "page": "67:1-67:32",
    "page-first": "67",
    "volume": "3",
    "_line": "FormalBib.bib:5127"
  },
  "bishop_engineering_2018": {
    "id": "bishop_engineering_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Bishop",
        "given": "Steve"
      },
      {
        "family": "Fairbairn",
        "given": "Matthew"
      },
      {
        "family": "Mehnert",
        "given": "Hannes"
      },
      {
        "family": "Norrish",
        "given": "Michael"
      },
      {
        "family": "Ridge",
        "given": "Tom"
      },
      {
        "family": "Sewell",
        "given": "Peter"
      },
      {
        "family": "Smith",
        "given": "Michael"
      },
      {
        "family": "Wansbrough",
        "given": "Keith"
      }
    ],
    "title": "Engineering with Logic: Rigorous Test-Oracle Specification and Validation for TCP/IP and the Sockets API",
    "container-title": "J. ACM",
    "container-title-short": "Engineering with Logic",
    "title-short": "Engineering with Logic",
    "issued": {
      "date-parts": [
        [
          "2018",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "28"
        ]
      ]
    },
    "issn": "0004-5411",
    "abstract": "Conventional computer engineering relies on test-and-debug development processes, with the behavior of common interfaces described (at best) with prose specification documents. But prose specifications cannot be used in test-and-debug development in any automated way, and prose is a poor medium for expressing complex (and loose) specifications. The TCP/IP protocols and Sockets API are a good example of this: they play a vital role in modern communication and computation, and interoperability between implementations is essential. But what exactly they are is surprisingly obscure: their original development focused on “rough consensus and running code,” augmented by prose RFC specifications that do not precisely define what it means for an implementation to be correct. Ultimately, the actual standard is the de facto one of the common implementations, including, for example, the 15 000 to 20 000 lines of the BSD implementation—optimized and multithreaded C code, time dependent, with asynchronous event handlers, intertwined with the operating system, and security critical. This article reports on work done in the Netsem project to develop lightweight mathematically rigorous techniques that can be applied to such systems: to specify their behavior precisely (but loosely enough to permit the required implementation variation) and to test whether these specifications and the implementations correspond with specifications that are executable as test oracles. We developed post hoc specifications of TCP, UDP, and the Sockets API, both of the service that they provide to applications (in terms of TCP bidirectional stream connections) and of the internal operation of the protocol (in terms of TCP segments and UDP datagrams), together with a testable abstraction function relating the two. These specifications are rigorous, detailed, readable, with broad coverage, and rather accurate. Working within a general-purpose proof assistant (HOL4), we developed language idioms (within higher-order logic) in which to write the specifications: operational semantics with nondeterminism, time, system calls, monadic relational programming, and so forth. We followed an experimental semantics approach, validating the specifications against several thousand traces captured from three implementations (FreeBSD, Linux, and WinXP). Many differences between these were identified, as were a number of bugs. Validation was done using a special-purpose symbolic model checker programmed above HOL4. Having demonstrated that our logic-based engineering techniques suffice for handling real-world protocols, we argue that similar techniques could be applied to future critical software infrastructure at design time, leading to cleaner designs and (via specification-based testing) more robust and predictable implementations. In cases where specification looseness can be controlled, this should be possible with lightweight techniques, without the need for a general-purpose proof assistant, at relatively little cost.",
    "keywords": "network protocols, Rigorous engineering, specification",
    "URL": "http://doi.acm.org/10.1145/3243650",
    "DOI": "10.1145/3243650",
    "page": "1:1-1:77",
    "page-first": "1",
    "volume": "66",
    "issue": "1",
    "_line": "FormalBib.bib:5144"
  },
  "sewell_rems_nodate": {
    "id": "sewell_rems_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Sewell",
        "given": "Peter"
      }
    ],
    "title": "REMS - Rigorous Engineering of Mainstream Systems",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "28"
        ]
      ]
    },
    "URL": "https://www.cl.cam.ac.uk/~pes20/rems/index.html",
    "_line": "FormalBib.bib:5162"
  },
  "lamport_if_2018": {
    "id": "lamport_if_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Lamport",
        "given": "Leslie"
      },
      {
        "family": "Distributed Computing &bslash;&amp; Education Column by Juraj Hromkovic",
        "given": "Stefan Schmid"
      }
    ],
    "title": "If You’re Not Writing a Program, Don’t Use a Programming Language",
    "container-title": "Bulletin of EATCS",
    "issued": {
      "date-parts": [
        [
          "2018",
          "6",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "8"
        ]
      ]
    },
    "abstract": "The need to handle large programs and to produce ecient compiled codeadds complexity to programming languages and limits their expressiveness.Algorithms are not programs, and they can be expressed in a simpler and more expressive language. That language is the one used by almost every branch of science and engineering to precisely describe and reason about the objects they study: the language of mathematics. Math is useful for describing a more general class of algorithms than are studied in algorithmcourses.",
    "URL": "http://eatcs.org/beatcs/index.php/beatcs/article/view",
    "volume": "2",
    "issue": "125",
    "language": "en-US",
    "_line": "FormalBib.bib:5170"
  },
  "herklotz_vericert_2021": {
    "id": "herklotz_vericert_2021",
    "type": "book",
    "author": [
      {
        "family": "Herklotz",
        "given": "Yann"
      }
    ],
    "title": "Vericert",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "abstract": "A formally verified high-level synthesis tool based on CompCert and written in Coq.",
    "keywords": "verification, coq, c, compcert, high-level-synthesis, semantics, verilog",
    "URL": "https://github.com/ymherklotz/vericert",
    "note": "original-date: 2019-10-02T18:48:56Z",
    "_line": "FormalBib.bib:5185"
  },
  "herklotz_formal_2017": {
    "id": "herklotz_formal_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Herklotz",
        "given": "Yann"
      },
      {
        "family": "Pollard",
        "given": "James"
      },
      {
        "family": "Ramanathan",
        "given": "Nadesh"
      },
      {
        "family": "Wickerson",
        "given": "John"
      }
    ],
    "title": "Formal Verification of High-Level Synthesis",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "abstract": "High-level synthesis (HLS), which refers to the automatic compilation of software into hardware, is rapidly gaining popularity. In a world increasingly reliant on applicationspecific hardware accelerators, HLS promises hardware designs of comparable performance and energy efficiency to those coded by hand in a hardware description language like Verilog, while maintaining the convenience and the rich ecosystem of software development. However, current HLS tools cannot always guarantee that the hardware designs they produce are equivalent to the software they were given, thus undermining any reasoning conducted at the software level. Worse, there is mounting evidence that existing HLS tools are quite unreliable, sometimes generating wrong hardware or crashing when given valid inputs. To address this problem, we present the first HLS tool that is mechanically verified to preserve the behaviour of its input software. Our tool, called Vericert, extends the CompCert verified C compiler with a new hardware-oriented intermediate language and a Verilog back end, and has been proven correct in Coq. Vericert supports all C constructs except for case statements, function pointers, recursive function calls, integers larger than 32 bits, floats, and global variables. An evaluation on the PolyBench/C benchmark suite indicates that Vericert generates hardware that is around an order of magnitude slower and larger than hardware generated by an existing, optimising (but unverified) HLS tool.",
    "page": "14",
    "page-first": "14",
    "language": "en-US",
    "_line": "FormalBib.bib:5197"
  },
  "swamy_dependent_2016": {
    "id": "swamy_dependent_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Swamy",
        "given": "Nikhil"
      },
      {
        "family": "Hriţcu",
        "given": "Cătălin"
      },
      {
        "family": "Keller",
        "given": "Chantal"
      },
      {
        "family": "Rastogi",
        "given": "Aseem"
      },
      {
        "family": "Delignat-Lavaud",
        "given": "Antoine"
      },
      {
        "family": "Forest",
        "given": "Simon"
      },
      {
        "family": "Bhargavan",
        "given": "Karthikeyan"
      },
      {
        "family": "Fournet",
        "given": "Cédric"
      },
      {
        "family": "Strub",
        "given": "Pierre-Yves"
      },
      {
        "family": "Kohlweiss",
        "given": "Markulf"
      },
      {
        "family": "Zinzindohoue",
        "given": "Jean-Karim"
      },
      {
        "family": "Zanella-Béguelin",
        "given": "Santiago"
      }
    ],
    "title": "Dependent Types and Multi-monadic Effects in Fstar",
    "container-title": "Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "collection-title": "POPL '16",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-3549-2",
    "abstract": "We present a new, completely redesigned, version of F\\*, a language that works both as a proof assistant as well as a general-purpose, verification-oriented, effectful programming language. In support of these complementary roles, F\\* is a dependently typed, higher-order, call-by-value language with &underscore;primitive&underscore; effects including state, exceptions, divergence and IO. Although primitive, programmers choose the granularity at which to specify effects by equipping each effect with a monadic, predicate transformer semantics. F\\* uses this to efficiently compute weakest preconditions and discharges the resulting proof obligations using a combination of SMT solving and manual proofs. Isolated from the effects, the core of F\\* is a language of pure functions used to write specifications and proof terms&mdash;its consistency is maintained by a semantic termination check based on a well-founded order. We evaluate our design on more than 55,000 lines of F\\* we have authored in the last year, focusing on three main case studies. Showcasing its use as a general-purpose programming language, F\\* is programmed (but not verified) in F\\*, and bootstraps in both OCaml and F&hash;. Our experience confirms F\\*'s pay-as-you-go cost model: writing idiomatic ML-like code with no finer specifications imposes no user burden. As a verification-oriented language, our most significant evaluation of F\\* is in verifying several key modules in an implementation of the TLS-1.2 protocol standard. For the modules we considered, we are able to prove more properties, with fewer annotations using F\\* than in a prior verified implementation of TLS-1.2. Finally, as a proof assistant, we discuss our use of F\\* in mechanizing the metatheory of a range of lambda calculi, starting from the simply typed lambda calculus to System F-omega and even micro-F\\*, a sizeable fragment of F\\* itself&mdash;these proofs make essential use of F\\*'s flexible combination of SMT automation and constructive proofs, enabling a tactic-free style of programming and proving at a relatively large scale.",
    "keywords": "proof assistants, verification, effectful programming",
    "URL": "http://doi.acm.org/10.1145/2837614.2837655",
    "DOI": "10.1145/2837614.2837655",
    "publisher-place": "New York, NY, USA",
    "page": "256-270",
    "page-first": "256",
    "_line": "FormalBib.bib:5207"
  },
  "protzenko_verified_2017": {
    "id": "protzenko_verified_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Protzenko",
        "given": "Jonathan"
      },
      {
        "family": "Zinzindohoué",
        "given": "Jean-Karim"
      },
      {
        "family": "Rastogi",
        "given": "Aseem"
      },
      {
        "family": "Ramananandro",
        "given": "Tahina"
      },
      {
        "family": "Wang",
        "given": "Peng"
      },
      {
        "family": "Zanella-Béguelin",
        "given": "Santiago"
      },
      {
        "family": "Delignat-Lavaud",
        "given": "Antoine"
      },
      {
        "family": "Hriţcu",
        "given": "Cătălin"
      },
      {
        "family": "Bhargavan",
        "given": "Karthikeyan"
      },
      {
        "family": "Fournet",
        "given": "Cédric"
      },
      {
        "family": "Swamy",
        "given": "Nikhil"
      }
    ],
    "title": "Verified Low-level Programming Embedded in Fstar",
    "container-title": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2017",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "We present Low\\*, a language for low-level programming and verification, and its application to high-assurance optimized cryptographic libraries. Low\\* is a shallow embedding of a small, sequential, well-behaved subset of C in F\\*, a dependently- typed variant of ML aimed at program verification. Departing from ML, Low\\* does not involve any garbage collection or implicit heap allocation; instead, it has a structured memory model à la CompCert, and it provides the control required for writing efficient low-level security-critical code.   By virtue of typing, any Low\\* program is memory safe. In addition, the programmer can make full use of the verification power of F\\* to write high-level specifications and verify the functional correctness of Low\\* code using a combination of SMT automation and sophisticated manual proofs. At extraction time, specifications and proofs are erased, and the remaining code enjoys a predictable translation to C. We prove that this translation preserves semantics and side-channel resistance.   We provide a new compiler back-end from Low\\* to C and, to evaluate our approach, we implement and verify various cryptographic algorithms, constructions, and tools for a total of about 28,000 lines of code. We show that our Low\\* code delivers performance competitive with existing (unverified) C cryptographic libraries, suggesting our approach may be applicable to larger-scale low-level software.",
    "keywords": "Compilers, Semantics, Functional languages, Software verifcation, Source code generation, Type theory, źSoftware and its engineering ź Correctness, źTheory of computation ź Hoare logic",
    "URL": "http://doi.acm.org/10.1145/3110261",
    "DOI": "10.1145/3110261",
    "page": "17:1-17:29",
    "page-first": "17",
    "volume": "1",
    "_line": "FormalBib.bib:5225"
  },
  "martinez_meta-fstar:_2018": {
    "id": "martinez_meta-fstar:_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Martínez",
        "given": "Guido"
      },
      {
        "family": "Ahman",
        "given": "Danel"
      },
      {
        "family": "Dumitrescu",
        "given": "Victor"
      },
      {
        "family": "Giannarakis",
        "given": "Nick"
      },
      {
        "family": "Hawblitzel",
        "given": "Chris"
      },
      {
        "family": "Hritcu",
        "given": "Catalin"
      },
      {
        "family": "Narasimhamurthy",
        "given": "Monal"
      },
      {
        "family": "Paraskevopoulou",
        "given": "Zoe"
      },
      {
        "family": "Pit-Claudel",
        "given": "Clément"
      },
      {
        "family": "Protzenko",
        "given": "Jonathan"
      },
      {
        "family": "Ramananandro",
        "given": "Tahina"
      },
      {
        "family": "Rastogi",
        "given": "Aseem"
      },
      {
        "family": "Swamy",
        "given": "Nikhil"
      }
    ],
    "title": "Meta-Fstar: Proof Automation with SMT, Tactics, and Metaprograms",
    "container-title": "arXiv:1803.06547 \\[cs\\]",
    "container-title-short": "Meta-F\\*",
    "title-short": "Meta-F\\*",
    "issued": {
      "date-parts": [
        [
          "2018",
          "3",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "We introduce Meta-F\\*, a tactics and metaprogramming framework for the F\\* program verifier. The main novelty of Meta-F\\* is allowing to use tactics and metaprogramming to discharge assertions not solvable by SMT, or to just simplify them into well-behaved SMT fragments. Plus, Meta-F\\* can be used to generate verified code automatically. Meta-F\\* is implemented as an F\\* effect, which, given the powerful effect system of F\\*, heavily increases code reuse and even enables the lightweight verification of metaprograms. Metaprograms can be either interpreted, or compiled to efficient native code that can be dynamically loaded into the F\\* type-checker and can interoperate with interpreted code. Evaluation on realistic case studies shows that Meta-F\\* provides substantial gains in proof development, efficiency, and robustness.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "1803.06547",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1803.06547",
    "URL": "http://arxiv.org/abs/1803.06547",
    "_line": "FormalBib.bib:5242"
  },
  "fstar-team_fstar_2021": {
    "id": "fstar-team_fstar_2021",
    "type": "webpage",
    "author": [
      {
        "family": "",
        "dropping-particle": "fstar-team"
      }
    ],
    "title": "Fstar: A Higher-Order Effectful Language Designed for Program Verification",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "URL": "https://www.fstar-lang.org/",
    "_line": "FormalBib.bib:5257"
  },
  "cousot_a2i:_2019": {
    "id": "cousot_a2i:_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Cousot",
        "given": "Patrick"
      },
      {
        "family": "Giacobazzi",
        "given": "Roberto"
      },
      {
        "family": "Ranzato",
        "given": "Francesco"
      }
    ],
    "title": "A2I: Abstract2 Interpretation",
    "container-title": "Proc. ACM Program. Lang.",
    "container-title-short": "A&dollar;&caret;2&dollar;I",
    "title-short": "A&dollar;&caret;2&dollar;I",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "1",
          "31"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "The fundamental idea of Abstract2 Interpretation (A2I), also called meta-abstract interpretation, is to apply abstract interpretation to abstract interpretation-based static program analyses. A2I is generally meant to use abstract interpretation to analyse properties of program analysers. A2I can be either offline or online. Offline A2I is performed either before the program analysis, such as variable packing used by the Astrée program analyser, or after the program analysis, such as in alarm diagnosis. Online A2I is performed during the program analysis, such as Venet’s cofibred domains or Halbwachs et al.’s and Singh et al.’s variable partitioning techniques for fast polyhedra/numerical abstract domains. We formalize offline and online meta-abstract interpretation and illustrate this notion with the design of widenings and the decomposition of relational abstract domains to speed-up program analyses. This shows how novel static analyses can be extracted as meta-abstract interpretations to design efficient and precise program analysis algorithms.",
    "keywords": "program analysis, Abstract interpretation, meta-abstract interpretation",
    "URL": "http://doi.acm.org/10.1145/3290355",
    "DOI": "10.1145/3290355",
    "page": "42:1-42:31",
    "page-first": "42",
    "volume": "3",
    "_line": "FormalBib.bib:5266"
  },
  "hellerstein_keeping_2020": {
    "id": "hellerstein_keeping_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Hellerstein",
        "given": "Joseph M."
      },
      {
        "family": "Alvaro",
        "given": "Peter"
      }
    ],
    "title": "Keeping CALM: when distributed consistency is easy",
    "container-title": "Communications of the ACM",
    "container-title-short": "Keeping CALM",
    "title-short": "Keeping CALM",
    "issued": {
      "date-parts": [
        [
          "2020",
          "8",
          "21"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "18"
        ]
      ]
    },
    "issn": "0001-0782, 1557-7317",
    "URL": "https://dl.acm.org/doi/10.1145/3369736",
    "DOI": "10.1145/3369736",
    "page": "72-81",
    "page-first": "72",
    "volume": "63",
    "issue": "9",
    "language": "en-US",
    "_line": "FormalBib.bib:5284"
  },
  "delignat-lavaud_security_nodate": {
    "id": "delignat-lavaud_security_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Delignat-Lavaud",
        "given": "Antoine"
      },
      {
        "family": "Fournet",
        "given": "Cédric"
      },
      {
        "family": "Parno",
        "given": "Bryan"
      },
      {
        "family": "Protzenko",
        "given": "Jonathan"
      },
      {
        "family": "Ramananandro",
        "given": "Tahina"
      },
      {
        "family": "Bosamiya",
        "given": "Jay"
      },
      {
        "family": "Lallemand",
        "given": "Joseph"
      },
      {
        "family": "Rakotonirina",
        "given": "Itsaka"
      },
      {
        "family": "Zhou",
        "given": "Yi"
      }
    ],
    "title": "A Security Model and Fully Veriﬁed Implementation for the IETF QUIC Record Layer",
    "abstract": "Drawing on earlier protocol-veriﬁcation work, we investigate the security of the QUIC record layer, as standardized by the IETF in draft version 30. This version features major differences compared to Google’s original protocol and early IETF drafts. It serves as a useful test case for our veriﬁcation methodology and toolchain, while also, hopefully, drawing attention to a little studied yet crucially important emerging standard. We model QUIC packet and header encryption, which uses a custom construction for privacy. To capture its goals, we propose a security deﬁnition for authenticated encryption with semi-implicit nonces. We show that QUIC uses an instance of a generic construction parameterized by a standard AEAD-secure scheme and a PRF-secure cipher. We formalize and verify the security of this construction in F . The proof uncovers interesting limitations of nonce conﬁdentiality, due to the malleability of short headers and the ability to choose the number of least signiﬁcant bits included in the packet counter. We propose improvements that simplify the proof and increase robustness against strong attacker models. In addition to the veriﬁed security model, we also give a concrete functional speciﬁcation for the record layer, and prove that it satisﬁes important functionality properties (such as the correct successful decryption of encrypted packets) after ﬁxing more errors in the draft. We then provide a highperformance implementation of the record layer that we prove to be memory safe, correct with respect to our concrete speciﬁcation (inheriting its functional correctness properties), and secure with respect to our veriﬁed model. To evaluate this component, we develop a provably-safe implementation of the rest of the QUIC protocol. Our record layer achieves nearly 2 GB/s throughput, and our QUIC implementation’s performance is within 21&perc; of an unveriﬁed baseline.",
    "page": "17",
    "page-first": "17",
    "language": "en-US",
    "_line": "FormalBib.bib:5302"
  },
  "arya_blockchain_2021": {
    "id": "arya_blockchain_2021",
    "type": "book",
    "author": [
      {
        "family": "Arya",
        "given": "Jaswant"
      },
      {
        "family": "Kumar",
        "given": "Arun"
      },
      {
        "family": "Singh",
        "given": "Akhilendra"
      },
      {
        "family": "Mishra",
        "given": "Tapas"
      },
      {
        "family": "Chong",
        "given": "Peter"
      }
    ],
    "title": "BLOCKCHAIN: BASICS, APPLICATIONS, CHALLENGES AND OPPORTUNITIES",
    "container-title-short": "BLOCKCHAIN",
    "title-short": "BLOCKCHAIN",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "7"
        ]
      ]
    },
    "DOI": "10.13140/RG.2.2.33899.16160",
    "_line": "FormalBib.bib:5311"
  },
  "begay_developing_2021": {
    "id": "begay_developing_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bégay",
        "given": "Pierre-Léo"
      },
      {
        "family": "Crégut",
        "given": "Pierre"
      },
      {
        "family": "Monin",
        "given": "Jean-François"
      }
    ],
    "title": "Developing and Certifying Datalog Optimizations in Coq/MathComp",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "We introduce a static analysis and two program transformations for Datalog to circumvent performance issues that arise with the implementation of primitive predicates, notably in the framework of a large scale telecommunication application. To this effect, we introduce a new trace semantics for Datalog with a verified mechanization. This work can be seen as both a first step and a proof of concept for the creation of a full-blown library of verified Datalog optimizations, on top of an existing Coq/MathComp formalization of Datalog\\[5, 14\\] towards the development of a realistic environment for certified data-centric applications.",
    "page": "15",
    "page-first": "15",
    "language": "en-US",
    "_line": "FormalBib.bib:5319"
  },
  "mahboubi_machine-checked_nodate": {
    "id": "mahboubi_machine-checked_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Mahboubi",
        "given": "Assia"
      }
    ],
    "title": "Machine-checked computer-aided mathematics",
    "page": "111",
    "page-first": "111",
    "language": "en-US",
    "_line": "Mathematics.bib:2"
  },
  "le_compositional_2021": {
    "id": "le_compositional_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Le",
        "given": "Quang Loc"
      }
    ],
    "editor": [
      {
        "family": "Henglein",
        "given": "Fritz"
      },
      {
        "family": "Shoham",
        "given": "Sharon"
      },
      {
        "family": "Vizel",
        "given": "Yakir"
      }
    ],
    "title": "Compositional Satisfiability Solving in Separation Logic",
    "container-title": "Verification, Model Checking, and Abstract Interpretation",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-67067-2",
    "abstract": "We introduce a novel decision procedure to the satisfiability problem in array separation logic combined with general inductively defined predicates and arithmetic. Our proposal differentiates itself from existing works by solving satisfiability through compositional reasoning. First, following Fermat’s method of infinite descent, it infers for every inductive definition a “base” that precisely characterises the satisfiability. It then utilises the base to derive such a base for any formula where these inductive predicates reside in. Especially, we identify an expressive decidable fragment for the compositionality. We have implemented the proposal in a tool and evaluated it over challenging problems. The experimental results show that the compositional satisfiability solving is efficient and our tool is effective and efficient when compared with existing solvers.",
    "keywords": "Separation logic, Regular proofs, Satisfiability",
    "DOI": "10.1007/978-3-030-67067-2_26",
    "publisher-place": "Cham",
    "page": "578-602",
    "page-first": "578",
    "language": "en-US",
    "_line": "FormalBib.bib:5337"
  },
  "namjoshi_self-certifying_2021": {
    "id": "namjoshi_self-certifying_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Namjoshi",
        "given": "Kedar S."
      },
      {
        "family": "Xue",
        "given": "Anton"
      }
    ],
    "editor": [
      {
        "family": "Henglein",
        "given": "Fritz"
      },
      {
        "family": "Shoham",
        "given": "Sharon"
      },
      {
        "family": "Vizel",
        "given": "Yakir"
      }
    ],
    "title": "A Self-certifying Compilation Framework for WebAssembly",
    "container-title": "Verification, Model Checking, and Abstract Interpretation",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-67067-2",
    "abstract": "A self-certifying compiler is designed to generate a correctness proof for each optimization performed during compilation. The generated proofs are checked automatically by an independent proof validator. The outcome is formally verified compilation, achieved without formally verifying the compiler. This paper describes the design and implementation of a self-certifying compilation framework for WebAssembly, a new intermediate language supported by all major browsers.",
    "DOI": "10.1007/978-3-030-67067-2_7",
    "publisher-place": "Cham",
    "page": "127-148",
    "page-first": "127",
    "language": "en-US",
    "_line": "FormalBib.bib:5354"
  },
  "goudsmid_compositional_2021": {
    "id": "goudsmid_compositional_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Goudsmid",
        "given": "Ohad"
      },
      {
        "family": "Grumberg",
        "given": "Orna"
      },
      {
        "family": "Sheinvald",
        "given": "Sarai"
      }
    ],
    "editor": [
      {
        "family": "Henglein",
        "given": "Fritz"
      },
      {
        "family": "Shoham",
        "given": "Sharon"
      },
      {
        "family": "Vizel",
        "given": "Yakir"
      }
    ],
    "title": "Compositional Model Checking for Multi-properties",
    "container-title": "Verification, Model Checking, and Abstract Interpretation",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-67067-2",
    "abstract": "Hyperproperties lift conventional trace properties in a way that describes how a system behaves in its entirety, and not just based on its individual traces. We generalize this notion to multi-properties, which describe the behavior of not just a single system, but of a set of systems, which we call a multi-model. We demonstrate the usefulness of our setting with practical examples. We show that model-checking multi-properties is equivalent to model-checking hyperproperties. However, our framework has the immediate advantage of being compositional. We introduce sound and complete compositional proof rules for model-checking multi-properties, based on over- and under-approximations of the systems in the multi-model. We then describe methods of computing such approximations. The first is abstraction-refinement based, in which a coarse initial abstraction is continuously refined using counterexamples, until a suitable approximation is found. The second, tailored for models with finite traces, finds suitable approximations via the &bslash;(L&caret;\\*&bslash;) learning algorithm. Our methods can produce much smaller models than the original ones, and can therefore be used for accelerating model-checking for both multi-properties and hyperproperties.",
    "DOI": "10.1007/978-3-030-67067-2_4",
    "publisher-place": "Cham",
    "page": "55-80",
    "page-first": "55",
    "language": "en-US",
    "_line": "FormalBib.bib:5370"
  },
  "dietsch_verification_2021": {
    "id": "dietsch_verification_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Dietsch",
        "given": "Daniel"
      },
      {
        "family": "Heizmann",
        "given": "Matthias"
      },
      {
        "family": "Klumpp",
        "given": "Dominik"
      },
      {
        "family": "Naouar",
        "given": "Mehdi"
      },
      {
        "family": "Podelski",
        "given": "Andreas"
      },
      {
        "family": "Schätzle",
        "given": "Claus"
      }
    ],
    "editor": [
      {
        "family": "Henglein",
        "given": "Fritz"
      },
      {
        "family": "Shoham",
        "given": "Sharon"
      },
      {
        "family": "Vizel",
        "given": "Yakir"
      }
    ],
    "title": "Verification of Concurrent Programs Using Petri Net Unfoldings",
    "container-title": "Verification, Model Checking, and Abstract Interpretation",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-67067-2",
    "abstract": "Given a verification problem for a concurrent program (with a fixed number of threads) over infinite data domains, we can construct a model checking problem for an abstraction of the concurrent program through a Petri net (a problem which can be solved using McMillan’s unfoldings technique). We present a method of abstraction refinement which translates Floyd/Hoare-style proofs for sample traces into additional synchronization constraints for the Petri net.",
    "keywords": "Verification, Concurrency, Petri nets, Unfoldings",
    "DOI": "10.1007/978-3-030-67067-2_9",
    "publisher-place": "Cham",
    "page": "174-195",
    "page-first": "174",
    "language": "en-US",
    "_line": "FormalBib.bib:5386"
  },
  "vedrine_runtime_2021": {
    "id": "vedrine_runtime_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Védrine",
        "given": "Franck"
      },
      {
        "family": "Jacquemin",
        "given": "Maxime"
      },
      {
        "family": "Kosmatov",
        "given": "Nikolai"
      },
      {
        "family": "Signoles",
        "given": "Julien"
      }
    ],
    "editor": [
      {
        "family": "Henglein",
        "given": "Fritz"
      },
      {
        "family": "Shoham",
        "given": "Sharon"
      },
      {
        "family": "Vizel",
        "given": "Yakir"
      }
    ],
    "title": "Runtime Abstract Interpretation for Numerical Accuracy and Robustness",
    "container-title": "Verification, Model Checking, and Abstract Interpretation",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-67067-2",
    "abstract": "Verification of numerical accuracy properties in modern software remains an important and challenging task. One of its difficulties is related to unstable tests, where the execution can take different branches for real and floating-point numbers. This paper presents a new verification technique for numerical properties, named Runtime Abstract Interpretation (RAI), that, given an annotated source code, embeds into it an abstract analyzer in order to analyze the program behavior at runtime. RAI is a hybrid technique combining abstract interpretation and runtime verification that aims at being sound as the former while taking benefit from the concrete run to gain greater precision from the latter when necessary. It solves the problem of unstable tests by surrounding an unstable test by two carefully defined program points, forming a so-called split-merge section, for which it separately analyzes different executions and merges the computed domains at the end of the section. Our implementation of this technique in a toolchain called FLDBox relies on two basic tools, FLDCompiler, that performs a source-to-source transformation of the given program and defines the split-merge sections, and an instrumentation library FLDLib that provides necessary primitives to explore relevant (partial) executions of each section and propagate accuracy properties. Initial experiments show that the proposed technique can efficiently and soundly analyze numerical accuracy for industrial programs on thin numerical scenarios.",
    "DOI": "10.1007/978-3-030-67067-2_12",
    "publisher-place": "Cham",
    "page": "243-266",
    "page-first": "243",
    "language": "en-US",
    "_line": "FormalBib.bib:5403"
  },
  "zhang_netter_2021": {
    "id": "zhang_netter_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Zhang",
        "given": "Han"
      },
      {
        "family": "Zhang",
        "given": "Chi"
      },
      {
        "family": "Azevedo de Amorim",
        "given": "Arthur"
      },
      {
        "family": "Agarwal",
        "given": "Yuvraj"
      },
      {
        "family": "Fredrikson",
        "given": "Matt"
      },
      {
        "family": "Jia",
        "given": "Limin"
      }
    ],
    "editor": [
      {
        "family": "Henglein",
        "given": "Fritz"
      },
      {
        "family": "Shoham",
        "given": "Sharon"
      },
      {
        "family": "Vizel",
        "given": "Yakir"
      }
    ],
    "title": "Netter: Probabilistic, Stateful Network Models",
    "container-title": "Verification, Model Checking, and Abstract Interpretation",
    "container-title-short": "Netter",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "Netter",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-67067-2",
    "abstract": "We study the problem of using probabilistic network models to formally analyze their quantitative properties, such as the effect of different load-balancing strategies on the long-term traffic on a server farm. Compared to prior work, we explore a different design space in terms of tradeoffs between model expressiveness and analysis scalability, which we realize in a language we call Netter. Netter code is compiled to probabilistic automata, undergoing optimization passes to reduce the state space of the generated models, thus helping verification scale. We evaluate Netter on several case studies, including a probabilistic load balancer, a routing scheme reminiscent of MPLS, and a network defense mechanism against link-flooding attacks. Our results show that Netter can analyze quantitative properties of interesting routing schemes that prior work hadn’t addressed, for networks of small size (4–9 nodes and a few different types of flows). Moreover, when specialized to simpler, stateless networks, Netter can parallel the performance of previous state-of-the-art tools, scaling up to millions of nodes.",
    "keywords": "Discrete-time Markov chains, Probabilistic model checking, Stateful networks",
    "DOI": "10.1007/978-3-030-67067-2_22",
    "publisher-place": "Cham",
    "page": "486-508",
    "page-first": "486",
    "language": "en-US",
    "_line": "FormalBib.bib:5419"
  },
  "maillardspan_dijkstra_2019": {
    "id": "maillardspan_dijkstra_2019",
    "type": "chapter",
    "author": [
      {
        "family": "Maillard &bslash;textless&rcurly;/span &bslash;textgreater&rcurly;",
        "given": "&lcurly;&bslash;textless&rcurly;span itemprop=\"author\" itemtype=\"http://schema org/Person\"&lcurly;&bslash;textgreater&rcurly;Kenji"
      },
      {
        "family": "Ahman &bslash;textless&rcurly;/span &bslash;textgreater&rcurly;",
        "given": "&lcurly;&bslash;textless&rcurly;span itemprop=\"author\" itemtype=\"http://schema org/Person\"&lcurly;&bslash;textgreater&rcurly;Danel"
      },
      {
        "family": "Atkey &bslash;textless&rcurly;/span &bslash;textgreater&rcurly;",
        "given": "&lcurly;&bslash;textless&rcurly;span itemprop=\"author\" itemtype=\"http://schema org/Person\"&lcurly;&bslash;textgreater&rcurly;Robert"
      },
      {
        "family": "Martínez &bslash;textless&rcurly;/span &bslash;textgreater&rcurly;",
        "given": "&lcurly;&bslash;textless&rcurly;span itemprop=\"author\" itemtype=\"http://schema org/Person\"&lcurly;&bslash;textgreater&rcurly;Guido"
      },
      {
        "family": "Hritcu &bslash;textless&rcurly;/span &bslash;textgreater&rcurly;",
        "given": "&lcurly;&bslash;textless&rcurly;span itemprop=\"author\" itemtype=\"http://schema org/Person\"&lcurly;&bslash;textgreater&rcurly;Catalin"
      },
      {
        "family": "Rivas &bslash;textless&rcurly;/span &bslash;textgreater&rcurly;",
        "given": "&lcurly;&bslash;textless&rcurly;span itemprop=\"author\" itemtype=\"http://schema org/Person\"&lcurly;&bslash;textgreater&rcurly;Exequiel"
      },
      {
        "family": "Tanter &bslash;textless&rcurly;/span &bslash;textgreater&rcurly;",
        "given": "&lcurly;&bslash;textless&rcurly;span itemprop=\"author\" itemtype=\"http://schema org/Person\"&lcurly;&bslash;textgreater&rcurly;Éric"
      }
    ],
    "title": "Dijkstra Monads for All",
    "container-title": "24th ACM SIGPLAN International Conference on Functional Programming (ICFP)",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "URL": "https://arxiv.org/abs/1903.01237",
    "_line": "FormalBib.bib:5437"
  },
  "swamy_steelcore_2020": {
    "id": "swamy_steelcore_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Swamy",
        "given": "Nikhil"
      },
      {
        "family": "Rastogi",
        "given": "Aseem"
      },
      {
        "family": "Fromherz",
        "given": "Aymeric"
      },
      {
        "family": "Merigoux",
        "given": "Denis"
      },
      {
        "family": "Ahman",
        "given": "Danel"
      },
      {
        "family": "Martínez",
        "given": "Guido"
      }
    ],
    "title": "SteelCore: an extensible concurrent separation logic for effectful dependently typed programs",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "SteelCore",
    "title-short": "SteelCore",
    "issued": {
      "date-parts": [
        [
          "2020",
          "8",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "15"
        ]
      ]
    },
    "abstract": "Much recent research has been devoted to modeling effects within type theory. Building on this work, we observe that effectful type theories can provide a foundation on which to build semantics for more complex programming constructs and program logics, extending the reasoning principles that apply within the host effectful type theory itself. Concretely, our main contribution is a semantics for concurrent separation logic (CSL) within the F⋆ proof assistant in a manner that enables dependently typed, effectful F⋆ programs to make use of concurrency and to be specified and verified using a full-featured, extensible CSL. In contrast to prior approaches, we directly derive the partial-correctness Hoare rules for CSL from the denotation of computations in the effectful semantics of non-deterministically interleaved atomic actions. Demonstrating the flexibility of our semantics, we build generic, verified libraries that support various concurrency constructs, ranging from dynamically allocated, storable spin locks, to protocol-indexed channels. We conclude that our effectful semantics provides a simple yet expressive basis on which to layer domain-specific languages and logics for verified, concurrent programming.",
    "keywords": "Separation Logic, Concurrency, Program Proofs",
    "URL": "https://doi.org/10.1145/3409003",
    "DOI": "10.1145/3409003",
    "page": "121:1-121:30",
    "page-first": "121",
    "volume": "4",
    "_line": "FormalBib.bib:5446"
  },
  "almeida_last_2020": {
    "id": "almeida_last_2020",
    "type": "paper-conference",
    "author": [
      {
        "family": "Almeida",
        "given": "Jose Bacelar"
      },
      {
        "family": "Barbosa",
        "given": "Manuel"
      },
      {
        "family": "Barthe",
        "given": "Gilles"
      },
      {
        "family": "Gregoire",
        "given": "Benjamin"
      },
      {
        "family": "Koutsos",
        "given": "Adrien"
      },
      {
        "family": "Laporte",
        "given": "Vincent"
      },
      {
        "family": "Oliveira",
        "given": "Tiago"
      },
      {
        "family": "Strub",
        "given": "Pierre-Yves"
      }
    ],
    "title": "The Last Mile: High-Assurance and High-Speed Cryptographic Implementations",
    "container-title": "2020 IEEE Symposium on Security and Privacy (SP)",
    "container-title-short": "The Last Mile",
    "title-short": "The Last Mile",
    "event-title": "2020 IEEE Symposium on Security and Privacy (SP)",
    "issued": {
      "date-parts": [
        [
          "2020",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "14"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-72813-497-0",
    "abstract": "We develop a new approach for building cryptographic implementations. Our approach goes the last mile and delivers assembly code that is provably functionally correct, protected against side-channels, and as efﬁcient as handwritten assembly. We illustrate our approach using ChaCha20Poly1305, one of the two ciphersuites recommended in TLS 1.3, and deliver formally veriﬁed vectorized implementations which outperform the fastest non-veriﬁed code.",
    "URL": "https://ieeexplore.ieee.org/document/9152665/",
    "DOI": "10.1109/SP40000.2020.00028",
    "publisher-place": "San Francisco, CA, USA",
    "page": "965-982",
    "page-first": "965",
    "language": "en-US",
    "_line": "FormalBib.bib:5464"
  },
  "ferles_verifying_2021": {
    "id": "ferles_verifying_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Ferles",
        "given": "Kostas"
      },
      {
        "family": "Stephens",
        "given": "Jon"
      },
      {
        "family": "Dillig",
        "given": "Isil"
      }
    ],
    "title": "Verifying correct usage of context-free API protocols",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "14"
        ]
      ]
    },
    "abstract": "Several real-world libraries (e.g., reentrant locks, GUI frameworks, serialization libraries) require their clients to use the provided API in a manner that conforms to a context-free specification. Motivated by this observation, this paper describes a new technique for verifying the correct usage of context-free API protocols. The key idea underlying our technique is to over-approximate the program’s feasible API call sequences using a context-free grammar (CFG) and then check language inclusion between this grammar and the specification. However, since this inclusion check may fail due to imprecision in the program’s CFG abstraction, we propose a novel refinement technique to progressively improve the CFG. In particular, our method obtains counterexamples from CFG inclusion queries and uses them to introduce new non-terminals and productions to the grammar while still over-approximating the program’s relevant behavior. We have implemented the proposed algorithm in a tool called CFPChecker and evaluate it on 10 popular Java applications that use at least one API with a context-free specification. Our evaluation shows that CFPChecker is able to verify correct usage of the API in clients that use it correctly and produces counterexamples for those that do not. We also compare our method against three relevant baselines and demonstrate that CFPChecker enables verification of safety properties that are beyond the reach of existing tools.",
    "keywords": "Program Verification, Abstraction Refinement, Context-Free API Protocols",
    "URL": "https://doi.org/10.1145/3434298",
    "DOI": "10.1145/3434298",
    "page": "17:1-17:30",
    "page-first": "17",
    "volume": "5",
    "_line": "FormalBib.bib:5483"
  },
  "watertor_assessing_nodate": {
    "id": "watertor_assessing_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Watertor",
        "given": "Rick"
      }
    ],
    "title": "Assessing the standard-compliance for multi-threading primitives in C compilers",
    "page": "58",
    "page-first": "58",
    "language": "en-US",
    "_line": "FormalBib.bib:5500"
  },
  "backes_one-click_2019": {
    "id": "backes_one-click_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Backes",
        "given": "John"
      },
      {
        "family": "Varming",
        "given": "Carsten"
      },
      {
        "family": "Whalen",
        "given": "Michael"
      },
      {
        "family": "Bolignano",
        "given": "Pauline"
      },
      {
        "family": "Cook",
        "given": "Byron"
      },
      {
        "family": "Gacek",
        "given": "Andrew"
      },
      {
        "family": "Luckow",
        "given": "Kasper Soe"
      },
      {
        "family": "Rungta",
        "given": "Neha"
      },
      {
        "family": "Schaef",
        "given": "Martin"
      },
      {
        "family": "Schlesinger",
        "given": "Cole"
      },
      {
        "family": "Tanash",
        "given": "Rima"
      }
    ],
    "title": "One-Click Formal Methods",
    "container-title": "IEEE Software",
    "container-title-short": "IEEE Softw.",
    "issued": {
      "date-parts": [
        [
          "2019",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "14"
        ]
      ]
    },
    "issn": "0740-7459, 1937-4194",
    "URL": "https://ieeexplore.ieee.org/document/8880058/",
    "DOI": "10.1109/MS.2019.2930609",
    "page": "61-65",
    "page-first": "61",
    "volume": "36",
    "issue": "6",
    "language": "en-US",
    "_line": "Networking.bib:764"
  },
  "liu_p4v_2018": {
    "id": "liu_p4v_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Liu",
        "given": "Jed"
      },
      {
        "family": "Hallahan",
        "given": "William"
      },
      {
        "family": "Schlesinger",
        "given": "Cole"
      },
      {
        "family": "Sharif",
        "given": "Milad"
      },
      {
        "family": "Lee",
        "given": "Jeongkeun"
      },
      {
        "family": "Soulé",
        "given": "Robert"
      },
      {
        "family": "Wang",
        "given": "Han"
      },
      {
        "family": "Caşcaval",
        "given": "Călin"
      },
      {
        "family": "McKeown",
        "given": "Nick"
      },
      {
        "family": "Foster",
        "given": "Nate"
      }
    ],
    "title": "p4v: practical verification for programmable data planes",
    "container-title": "Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication",
    "container-title-short": "p4v",
    "title-short": "p4v",
    "event-title": "SIGCOMM '18: ACM SIGCOMM 2018 Conference",
    "issued": {
      "date-parts": [
        [
          "2018",
          "8",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "14"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-5567-4",
    "abstract": "We present the design and implementation of p4v, a practical tool for verifying data planes described using the P4 programming language. The design of p4v is based on classic verification techniques but adds several key innovations including a novel mechanism for incorporating assumptions about the control plane and domain-specific optimizations which are needed to scale to large programs. We present case studies showing that p4v verifies important properties and finds bugs in real-world programs. We conduct experiments to quantify the scalability of p4v on a wide range of additional examples. We show that with just a few hundred lines of control-plane annotations, p4v is able to verify critical safety properties for switch.p4, a program that implements the functionality of on a modern data center switch, in under three minutes.",
    "URL": "https://dl.acm.org/doi/10.1145/3230543.3230582",
    "DOI": "10.1145/3230543.3230582",
    "publisher-place": "Budapest Hungary",
    "page": "490-503",
    "page-first": "490",
    "language": "en-US",
    "_line": "Networking.bib:710"
  },
  "foster_using_2020": {
    "id": "foster_using_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "McKeown",
        "given": "Nick"
      },
      {
        "family": "Rexford",
        "given": "Jennifer"
      },
      {
        "family": "Parulkar",
        "given": "Guru"
      },
      {
        "family": "Peterson",
        "given": "Larry"
      },
      {
        "family": "Sunay",
        "given": "Oguz"
      }
    ],
    "title": "Using deep programmability to put network owners in control",
    "container-title": "ACM SIGCOMM Computer Communication Review",
    "container-title-short": "SIGCOMM Comput. Commun. Rev.",
    "issued": {
      "date-parts": [
        [
          "2020",
          "10",
          "26"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "10"
        ]
      ]
    },
    "issn": "0146-4833",
    "abstract": "Controlling an opaque system by reading some \"dials\" and setting some \"knobs,\" without really knowing what they do, is a hazardous and fruitless endeavor, particularly at scale. What we need are transparent networks, that start at the top with a high-level intent and map all the way down, through the control plane to the data plane. If we can specify the behavior we want in software, then we can check that the system behaves as we expect. This is impossible if the implementation is opaque. We therefore need to use open-source software or write it ourselves (or both), and have mechanisms for checking actual behavior against the specified intent. With fine-grain checking (e.g., every packet, every state variable), we can build networks that are more reliable, secure, and performant. In the limit, we can build networks that run autonomously under verifiable, closed-loop control. We believe this vision, while ambitious, is finally within our reach, due to deep programmability across the stack, both vertically (control and data plane) and horizontally (end to end). It will emerge naturally in some networks, as network owners take control of their software and engage in open-source efforts; whereas in enterprise networks it may take longer. In 5G access networks, there is a pressing need for our community to engage, so these networks, too, can operate autonomously under verifiable, closed-loop control.",
    "keywords": "network verification, programmable networks, software defined networks (SDN), telemetry",
    "URL": "https://doi.org/10.1145/3431832.3431842",
    "DOI": "10.1145/3431832.3431842",
    "page": "82-88",
    "page-first": "82",
    "volume": "50",
    "issue": "4",
    "_line": "Networking.bib:981"
  },
  "benzaken_coq_2021": {
    "id": "benzaken_coq_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Benzaken",
        "given": "Véronique"
      },
      {
        "family": "Cohen-Boulakia",
        "given": "Sarah"
      },
      {
        "family": "Contejean",
        "given": "Évelyne"
      },
      {
        "family": "Keller",
        "given": "Chantal"
      },
      {
        "family": "Zucchini",
        "given": "Rébecca"
      }
    ],
    "title": "A Coq Formalization of Data Provenance",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "CCS Concepts: • Software and its engineering → Formal software verification; Correctness; Software verification; • Theory of computation → Logic and verification; Type theory; Program verification; Program semantics; Data provenance.",
    "page": "18",
    "page-first": "18",
    "language": "en-US",
    "_line": "FormalBib.bib:5561"
  },
  "meseguer_symbolic_nodate": {
    "id": "meseguer_symbolic_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Meseguer",
        "given": "Jose"
      }
    ],
    "title": "Symbolic Computation in Maude: Some Tapas",
    "abstract": "Programming in Maude is executable mathematical modeling. Your mathematical model is the code you execute. Both deterministic systems, specified equationally as so-called functional modules and concurrent ones, specified in rewriting logic as system modules, are mathematically modeled and programmed this way. But rewriting logic is also a logical framework in which many different logics can be naturally represented. And one would like not only to execute these models, but to reason about them at a high level. For this, symbolic methods that can automate much of the reasoning are crucial. Many of them are actually supported by Maude itself or by some of its tools. These methods are very general: they apply not just to Maude, but to many other logics, languages and tools. This paper presents some tapas about these Maude-based symbolic methods in an informal way to make it easy for many other people to learn about, and benefit from, them.",
    "page": "26",
    "page-first": "26",
    "language": "en-US",
    "_line": "FormalBib.bib:5571"
  },
  "chiplunkar_automated_nodate": {
    "id": "chiplunkar_automated_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Chiplunkar",
        "given": "Shardul"
      },
      {
        "family": "Pit-Claudel",
        "given": "Clément"
      },
      {
        "family": "Chlipala",
        "given": "Adam"
      }
    ],
    "title": "Automated Synthesis of Verified Firewalls",
    "abstract": "We demonstrate correct-by-construction firewalls—stateful packet filters for TCP/IP packets—using the Fiat synthesis library \\[3\\]. We present a general DSL for specifying their behavior independent of algorithmic implementation. We outline the design of a verified compiler in Coq, detail a few verified efficiency optimizations, and show how the compiler can easily be extended to support custom optimizations for user-defined policies.",
    "page": "3",
    "page-first": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:5580"
  },
  "bosshart_programming_2014": {
    "id": "bosshart_programming_2014",
    "type": "article-journal",
    "author": [
      {
        "family": "Bosshart",
        "given": "Pat"
      },
      {
        "family": "Daly",
        "given": "Dan"
      },
      {
        "family": "Izzard",
        "given": "Martin"
      },
      {
        "family": "McKeown",
        "given": "Nick"
      },
      {
        "family": "Rexford",
        "given": "Jennifer"
      },
      {
        "family": "Schlesinger",
        "given": "Cole"
      },
      {
        "family": "Talayco",
        "given": "Dan"
      },
      {
        "family": "Vahdat",
        "given": "Amin"
      },
      {
        "family": "Varghese",
        "given": "George"
      },
      {
        "family": "Walker",
        "given": "David"
      }
    ],
    "title": "Programming Protocol-Independent Packet Processors",
    "container-title": "arXiv:1312.1719 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2014",
          "5",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "13"
        ]
      ]
    },
    "abstract": "P4 is a high-level language for programming protocol-independent packet processors. P4 works in conjunction with SDN control protocols like OpenFlow. In its current form, OpenFlow explicitly specifies protocol headers on which it operates. This set has grown from 12 to 41 fields in a few years, increasing the complexity of the specification while still not providing the flexibility to add new headers. In this paper we propose P4 as a strawman proposal for how OpenFlow should evolve in the future. We have three goals: (1) Reconfigurability in the field: Programmers should be able to change the way switches process packets once they are deployed. (2) Protocol independence: Switches should not be tied to any specific network protocols. (3) Target independence: Programmers should be able to describe packet-processing functionality independently of the specifics of the underlying hardware. As an example, we describe how to use P4 to configure a switch to add a new hierarchical label.",
    "keywords": "Computer Science - Networking and Internet Architecture",
    "URLtext": "1312.1719",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1312.1719",
    "URL": "http://arxiv.org/abs/1312.1719",
    "_line": "Networking.bib:916"
  },
  "leijen_mimalloc_2019": {
    "id": "leijen_mimalloc_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Leijen",
        "given": "Daan"
      },
      {
        "family": "Zorn",
        "given": "Benjamin"
      },
      {
        "family": "Moura",
        "given": "Leonardo",
        "dropping-particle": "de"
      }
    ],
    "editor": [
      {
        "family": "Lin",
        "given": "Anthony Widjaja"
      }
    ],
    "title": "Mimalloc: Free List Sharding in Action",
    "container-title": "Programming Languages and Systems",
    "container-title-short": "Mimalloc",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "Mimalloc",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-34175-6",
    "abstract": "Modern memory allocators have to balance many simultaneous demands, including performance, security, the presence of concurrency, and application-specific demands depending on the context of their use. One increasing use-case for allocators is as back-end implementations of languages, such as Swift and Python, that use reference counting to automatically deallocate objects. We present mimalloc, a memory allocator that effectively balances these demands, shows significant performance advantages over existing allocators, and is tailored to support languages that rely on the memory allocator as a backend for reference counting. Mimalloc combines several innovations to achieve this result. First, it uses three page-local sharded free lists to increase locality, avoid contention, and support a highly-tuned allocate and free fast path. These free lists also support temporal cadence, which allows the allocator to predictably leave the fast path for regular maintenance tasks such as supporting deferred freeing, handling frees from non-local threads, etc. While influenced by the allocation workload of the reference-counted Lean and Koka programming language, we show that mimalloc has superior performance to modern commercial memory allocators, including tcmalloc and jemalloc, with speed improvements of 7&perc; and 14&perc;, respectively, on redis, and consistently out performs over a wide range of sequential and concurrent benchmarks. Allocators tailored to provide an efficient runtime for reference-counting languages reduce the implementation burden on developers and encourage the creation of innovative new language designs.",
    "DOI": "10.1007/978-3-030-34175-6_13",
    "publisher-place": "Cham",
    "page": "244-265",
    "page-first": "244",
    "language": "en-US",
    "_line": "FormalBib.bib:5603"
  },
  "selsam_tabled_2020": {
    "id": "selsam_tabled_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Selsam",
        "given": "Daniel"
      },
      {
        "family": "Ullrich",
        "given": "Sebastian"
      },
      {
        "family": "Moura",
        "given": "Leonardo",
        "dropping-particle": "de"
      }
    ],
    "title": "Tabled Typeclass Resolution",
    "container-title": "arXiv:2001.04301 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2020",
          "1",
          "21"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "5"
        ]
      ]
    },
    "abstract": "Typeclasses provide an elegant and effective way of managing ad-hoc polymorphism in both programming languages and interactive proof assistants. However, the increasingly sophisticated uses of typeclasses within proof assistants, especially within Lean's burgeoning mathematics library, mathlib, have elevated once-theoretical limitations of existing typeclass resolution procedures into major impediments to ongoing progress. The two most devastating limitations of existing procedures are exponential running times in the presence of diamonds and divergence in the presence of cycles. We present a new procedure, tabled typeclass resolution, that solves both problems by tabling, which is a generalization of memoizing originally introduced to address similar limitations of early logic programming systems. We have implemented our procedure for the upcoming version (v4) of Lean, and have confirmed empirically that our implementation is exponentially faster than existing systems in the presence of diamonds. Although tabling is notoriously difficult to implement, our procedure is notably lightweight and could easily be implemented in other systems. We hope our new procedure facilitates even more sophisticated uses of typeclasses in both software development and interactive theorem proving.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "2001.04301",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2001.04301",
    "URL": "http://arxiv.org/abs/2001.04301",
    "_line": "FormalBib.bib:5620"
  },
  "carneiro_metamath_2020": {
    "id": "carneiro_metamath_2020",
    "type": "paper-conference",
    "author": [
      {
        "family": "Carneiro",
        "given": "Mario"
      }
    ],
    "editor": [
      {
        "family": "Benzmüller",
        "given": "Christoph"
      },
      {
        "family": "Miller",
        "given": "Bruce"
      }
    ],
    "title": "Metamath Zero: Designing a Theorem Prover Prover",
    "container-title": "Intelligent Computer Mathematics",
    "container-title-short": "Metamath Zero",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "Metamath Zero",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-53518-6",
    "abstract": "As the usage of theorem prover technology expands, so too does the reliance on correctness of the tools. Metamath Zero is a verification system that aims for simplicity of logic and implementation, without compromising on efficiency of verification. It is formally specified in its own language, and supports a number of translations to and from other proof languages. This paper describes the abstract logic of Metamath Zero, essentially a multi-sorted first order logic, as well as the binary proof format and the way in which it can ensure essentially linear time verification while still being concise and efficient at scale. Metamath Zero currently holds the record for fastest verification of the set.mm Metamath library of proofs in ZFC (including 71 of Wiedijk’s 100 formalization targets), at less than 200 ms. Ultimately, we intend to use it to verify the correctness of the implementation of the verifier down to binary executable, so it can be used as a root of trust for more complex proof systems.",
    "keywords": "Verification, Formal proof, Mathematics, Metamath zero, Metamathematics",
    "DOI": "10.1007/978-3-030-53518-6_5",
    "publisher-place": "Cham",
    "page": "71-88",
    "page-first": "71",
    "language": "en-US",
    "_line": "FormalBib.bib:5634"
  },
  "wang_formalization_2018": {
    "id": "wang_formalization_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Wang",
        "given": "Huiwen"
      },
      {
        "family": "Zhu",
        "given": "Huibiao"
      },
      {
        "family": "Xiao",
        "given": "Lili"
      },
      {
        "family": "Fei",
        "given": "Yuan"
      }
    ],
    "title": "Formalization and Verification of the OpenFlow Bundle Mechanism Using CSP",
    "container-title": "International Journal of Software Engineering and Knowledge Engineering",
    "container-title-short": "Int. J. Soft. Eng. Knowl. Eng.",
    "issued": {
      "date-parts": [
        [
          "2018",
          "11",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "5"
        ]
      ]
    },
    "issn": "0218-1940",
    "abstract": "Software-Defined Networking (SDN) is an emerging architecture of computer networking. OpenFlow is considered as the first and currently most popular standard southbound interface of SDN. It is a communication protocol which enables the SDN controller to directly interact with the forwarding plane, which makes the network more flexible and programmable. The promising and widespread use makes the reliability of OpenFlow important. The OpenFlow bundle mechanism is a new mechanism proposed by OpenFlow protocol to guarantee the completeness and consistency of the messages transmitted between SDN devices like switches and controllers. In this paper, we use Communication Sequential Processes (CSP) to formally model the OpenFlow bundle mechanism. By adopting the models into the model checker Process Analysis Toolkit (PAT), we verify the relevant properties of the mechanism, including deadlock freeness, parallelism, atomicity, order property and schedulability. Our formalization and verification show that the mechanism can satisfy these properties, from which we can conclude that the mechanism offers a better way to guarantee the completeness and consistency.",
    "URL": "https://www.worldscientific.com/doi/abs/10.1142/S0218194018400223",
    "DOI": "10.1142/S0218194018400223",
    "page": "1657-1677",
    "page-first": "1657",
    "volume": "28",
    "note": "Publisher: World Scientific Publishing Co.",
    "issue": "11",
    "_line": "FormalBib.bib:5653"
  },
  "kang_formal_2013": {
    "id": "kang_formal_2013",
    "type": "book",
    "author": [
      {
        "family": "Kang",
        "given": "Miyoung"
      },
      {
        "family": "Kang",
        "given": "Eun-Young"
      },
      {
        "family": "Hwang",
        "given": "Dae-Yon"
      },
      {
        "family": "Kim",
        "given": "Beom-Jin"
      },
      {
        "family": "Nam",
        "given": "Ki-Hyuk"
      },
      {
        "family": "Shin",
        "given": "Myung-Ki"
      },
      {
        "family": "Choi",
        "given": "Jin-Young"
      }
    ],
    "title": "Formal Modeling and Verification of SDN-OpenFlow",
    "issued": {
      "date-parts": [
        [
          "2013",
          "3",
          "1"
        ]
      ]
    },
    "number-of-pages": "481",
    "isbn": "978-1-4673-5961-0",
    "abstract": "Software-Defined Networking (SDN) is a network architecture where a controller manages flow control to enable intelligent networking. Currently, a popular specification for creating an SDN is an open standard called OpenFlow. The behavior of the SDN OpenFlow (SDN-OF) is critical to the safety of the network system and its correctness must be proven so as to avoid system failures. In this paper, we report our experience in applying formal techniques for modeling and analysis of SDN-OF. The formal model of SDN-OF is described in detail and its correctness is formalized in logical formulas based on the informal specification. The desired properties are verified over the model using VERSA and UPPAAL. Our work-in-progressinvolves the development of a model translation tool that facilitates automatic conversion of the verified model to Python for modular code synthesis on the application platform",
    "DOI": "10.1109/ICST.2013.69",
    "note": "Pages: 482",
    "_line": "Networking.bib:575"
  },
  "bordg_certified_2020": {
    "id": "bordg_certified_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Bordg",
        "given": "Anthony"
      },
      {
        "family": "Lachnitt",
        "given": "Hanna"
      },
      {
        "family": "He",
        "given": "Yijun"
      }
    ],
    "title": "Certified Quantum Computation in Isabelle/HOL",
    "container-title": "Journal of Automated Reasoning",
    "container-title-short": "Journal of Automated Reasoning",
    "issued": {
      "date-parts": [
        [
          "2020",
          "12",
          "24"
        ]
      ]
    },
    "abstract": "In this article we present an ongoing effort to formalise quantum algorithms and results in quantum information theory using the proof assistant Isabelle/HOL. Formal methods being critical for the safety and security of algorithms and protocols, we foresee their widespread use for quantum computing in the future. We have developed a large library for quantum computing in Isabelle based on a matrix representation for quantum circuits, successfully formalising the no-cloning theorem, quantum teleportation, Deutsch’s algorithm, the Deutsch–Jozsa algorithm and the quantum Prisoner’s Dilemma. We discuss the design choices made and report on an outcome of our work in the field of quantum game theory.",
    "DOI": "10.1007/s10817-020-09584-7",
    "page": "1-19",
    "page-first": "1",
    "_line": "FormalBib.bib:5683"
  },
  "silver_dijkstra_nodate": {
    "id": "silver_dijkstra_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Silver",
        "given": "Lucas"
      },
      {
        "family": "Zdancewic",
        "given": "Steve"
      }
    ],
    "title": "Dijkstra Monads Forever: Termination-Sensitive Specifications for Interaction Trees",
    "abstract": "STEVE ZDANCEWIC, University of Pennsylvania, USA This paper extends the Dijkstra monad framework, designed for writing specifications over effectful programs using monadic effects, to handle termination sensitive specifications over interactive programs. We achieve this by introducing base specification monads for non-terminating programs with uninterpreted events. We model such programs using interaction trees, a coinductive datatype for representing programs with algebraic effects in Coq, which we further develop by adding trace semantics. We show that this approach subsumes typical, simple proof principles. The framework is implemented as an extension of the Interaction Trees Coq library. CCS Concepts: • Theory of computation → Logic and verification; Programming logic; Hoare logic; Program specifications; Pre- and post-conditions; Program verification; Invariants.",
    "page": "28",
    "page-first": "28",
    "volume": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:5695"
  },
  "myreen_minimalistic_2021": {
    "id": "myreen_minimalistic_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Myreen",
        "given": "Magnus O."
      }
    ],
    "title": "A minimalistic verified bootstrapped compiler (proof pearl)",
    "container-title": "Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "event-title": "CPP '21: 10th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "17"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-8299-1",
    "abstract": "This paper shows how a small verified bootstrapped compiler can be developed inside an interactive theorem prover (ITP). Throughout, emphasis is put on clarity and minimalism.",
    "URL": "https://dl.acm.org/doi/10.1145/3437992.3439915",
    "DOI": "10.1145/3437992.3439915",
    "publisher-place": "Virtual Denmark",
    "page": "32-45",
    "page-first": "32",
    "language": "en-US",
    "_line": "Security.bib:717"
  },
  "astrauskas_leveraging_2019": {
    "id": "astrauskas_leveraging_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Astrauskas",
        "given": "Vytautas"
      },
      {
        "family": "Müller",
        "given": "Peter"
      },
      {
        "family": "Poli",
        "given": "Federico"
      },
      {
        "family": "Summers",
        "given": "Alexander J."
      }
    ],
    "title": "Leveraging rust types for modular specification and verification",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2019",
          "10",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "3",
          "16"
        ]
      ]
    },
    "abstract": "Rust's type system ensures memory safety: well-typed Rust programs are guaranteed to not exhibit problems such as dangling pointers, data races, and unexpected side effects through aliased references. Ensuring correctness properties beyond memory safety, for instance, the guaranteed absence of assertion failures or more-general functional correctness, requires static program verification. For traditional system programming languages, formal verification is notoriously difficult and requires complex specifications and logics to reason about pointers, aliasing, and side effects on mutable state. This complexity is a major obstacle to the more-widespread verification of system software. In this paper, we present a novel verification technique that leverages Rust's type system to greatly simplify the specification and verification of system software written in Rust. We analyse information from the Rust compiler and synthesise a corresponding core proof for the program in a flavour of separation logic tailored to automation. To verify correctness properties beyond memory safety, users can annotate Rust programs with specifications at the abstraction level of Rust expressions; our technique weaves them into the core proof to verify modularly whether these specifications hold. Crucially, our proofs are constructed and checked automatically without exposing the underlying formal logic, allowing users to work exclusively at the level of abstraction of the programming language. As such, our work enables a new kind of verification tool, with the potential to impact a wide audience and allow the Rust community to benefit from state-of-the-art verification techniques. We have implemented our techniques for a subset of Rust; our evaluation on several thousand functions from widely-used Rust crates demonstrates its effectiveness.",
    "keywords": "concurrency, heap-manipulating programs, Rust, type systems",
    "URL": "https://doi.org/10.1145/3360573",
    "DOI": "10.1145/3360573",
    "page": "147:1-147:30",
    "page-first": "147",
    "volume": "3",
    "_line": "FormalBib.bib:5715"
  },
  "cousot_automatic_2013": {
    "id": "cousot_automatic_2013",
    "type": "paper-conference",
    "author": [
      {
        "family": "Cousot",
        "given": "Patrick"
      },
      {
        "family": "Cousot",
        "given": "Radhia"
      },
      {
        "family": "Fähndrich",
        "given": "Manuel"
      },
      {
        "family": "Logozzo",
        "given": "Francesco"
      }
    ],
    "editor": [
      {
        "family": "Giacobazzi",
        "given": "Roberto"
      },
      {
        "family": "Berdine",
        "given": "Josh"
      },
      {
        "family": "Mastroeni",
        "given": "Isabella"
      }
    ],
    "title": "Automatic Inference of Necessary Preconditions",
    "container-title": "Verification, Model Checking, and Abstract Interpretation",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "publisher": "Springer",
    "isbn": "978-3-642-35873-9",
    "abstract": "We consider the problem of automatic precondition inference. We argue that the common notion of sufficient precondition inference (i.e., under which precondition is the program correct?) imposes too large a burden on callers, and hence it is unfit for automatic program analysis. Therefore, we define the problem of necessary precondition inference (i.e., under which precondition, if violated, will the program always be incorrect?). We designed and implemented several new abstract interpretation-based analyses to infer atomic, disjunctive, universally and existentially quantified necessary preconditions.We experimentally validated the analyses on large scale industrial code. For unannotated code, the inference algorithms find necessary preconditions for almost 64&perc; of methods which contained warnings. In 27&perc; of these cases the inferred preconditions were also sufficient, meaning all warnings within the method body disappeared. For annotated code, the inference algorithms find necessary preconditions for over 68&perc; of methods with warnings. In almost 50&perc; of these cases the preconditions were also sufficient. Overall, the precision improvement obtained by precondition inference (counted as the additional number of methods with no warnings) ranged between 9&perc; and 21&perc;.",
    "keywords": "Abstract Interpretation, Abstract Domain, Inference Algorithm, Predicate Abstraction, Proof Obligation",
    "DOI": "10.1007/978-3-642-35873-9_10",
    "publisher-place": "Berlin, Heidelberg",
    "page": "128-148",
    "page-first": "128",
    "language": "en-US",
    "_line": "FormalBib.bib:5732"
  },
  "andronick_formal_2018": {
    "id": "andronick_formal_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Andronick",
        "given": "June"
      },
      {
        "family": "Klein",
        "given": "Gerwin"
      },
      {
        "family": "Lewis",
        "given": "Corey"
      }
    ],
    "title": "Formal Model of a Multi-Core Kernel-based System",
    "issued": {
      "date-parts": [
        [
          "2018",
          "10",
          "10"
        ]
      ]
    },
    "page": "33",
    "page-first": "33",
    "language": "en-US",
    "_line": "FormalBib.bib:5750"
  },
  "hur_power_2013": {
    "id": "hur_power_2013",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hur",
        "given": "Chung-Kil"
      },
      {
        "family": "Neis",
        "given": "Georg"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      },
      {
        "family": "Vafeiadis",
        "given": "Viktor"
      }
    ],
    "title": "The power of parameterization in coinductive proof",
    "container-title": "Proceedings of the 40th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages",
    "collection-title": "POPL '13",
    "issued": {
      "date-parts": [
        [
          "2013",
          "1",
          "23"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "3",
          "2"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-1832-7",
    "abstract": "Coinduction is one of the most basic concepts in computer science. It is therefore surprising that the commonly-known lattice-theoretic accounts of the principles underlying coinductive proofs are lacking in two key respects: they do not support compositional reasoning (i.e. breaking proofs into separate pieces that can be developed in isolation), and they do not support incremental reasoning (i.e. developing proofs interactively by starting from the goal and generalizing the coinduction hypothesis repeatedly as necessary). In this paper, we show how to support coinductive proofs that are both compositional and incremental, using a dead simple construction we call the parameterized greatest fixed point. The basic idea is to parameterize the greatest fixed point of interest over the accumulated knowledge of \"the proof so far\". While this idea has been proposed before, by Winskel in 1989 and by Moss in 2001, neither of the previous accounts suggests its general applicability to improving the state of the art in interactive coinductive proof. In addition to presenting the lattice-theoretic foundations of parameterized coinduction, demonstrating its utility on representative examples, and studying its composition with \"up-to\" techniques, we also explore its mechanization in proof assistants like Coq and Isabelle. Unlike traditional approaches to mechanizing coinduction (e.g. Coq's cofix), which employ syntactic \"guardedness checking\", parameterized coinduction offers a semantic account of guardedness. This leads to faster and more robust proof development, as we demonstrate using our new Coq library, Paco.",
    "keywords": "interactive theorem proving, coinduction, compositionality, lattice theory, parameterized greatest fixed point, simulation",
    "URL": "https://doi.org/10.1145/2429069.2429093",
    "DOI": "10.1145/2429069.2429093",
    "publisher-place": "Rome, Italy",
    "page": "193-206",
    "page-first": "193",
    "_line": "FormalBib.bib:5759"
  },
  "lin_symbolic_2020": {
    "id": "lin_symbolic_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Lin",
        "given": "Yu-Yang"
      },
      {
        "family": "Tzevelekos",
        "given": "Nikos"
      }
    ],
    "title": "Symbolic Execution Game Semantics",
    "container-title": "arXiv:2002.09115 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2020",
          "2",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "2",
          "28"
        ]
      ]
    },
    "abstract": "We present a framework for symbolically executing and model checking higher-order programs with external (open) methods. We focus on the client-library paradigm and in particular we aim to check libraries with respect to any definable client. We combine traditional symbolic execution techniques with operational game semantics to build a symbolic execution semantics that captures arbitrary external behaviour. We prove the symbolic semantics to be sound and complete. This yields a bounded technique by imposing bounds on the depth of recursion and callbacks. We provide an implementation of our technique in the K framework and showcase its performance on a custom benchmark based on higher-order coding errors such as reentrancy bugs.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2002.09115",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2002.09115",
    "URL": "http://arxiv.org/abs/2002.09115",
    "_line": "FormalBib.bib:5777"
  },
  "matsushita_rusthorn_2020": {
    "id": "matsushita_rusthorn_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Matsushita",
        "given": "Yusuke"
      },
      {
        "family": "Tsukada",
        "given": "Takeshi"
      },
      {
        "family": "Kobayashi",
        "given": "Naoki"
      }
    ],
    "title": "RustHorn: CHC-based Verification for Rust Programs (full version)",
    "container-title": "arXiv:2002.09002 \\[cs\\]",
    "container-title-short": "RustHorn",
    "title-short": "RustHorn",
    "issued": {
      "date-parts": [
        [
          "2020",
          "2",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "2",
          "28"
        ]
      ]
    },
    "abstract": "Reduction to the satisfiablility problem for constrained Horn clauses (CHCs) is a widely studied approach to automated program verification. The current CHC-based methods for pointer-manipulating programs, however, are not very scalable. This paper proposes a novel translation of pointer-manipulating Rust programs into CHCs, which clears away pointers and heaps by leveraging ownership. We formalize the translation for a simplified core of Rust and prove its correctness. We have implemented a prototype verifier for a subset of Rust and confirmed the effectiveness of our method.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2002.09002",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2002.09002",
    "URL": "http://arxiv.org/abs/2002.09002",
    "_line": "FormalBib.bib:5791"
  },
  "bao_unifying_2018": {
    "id": "bao_unifying_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Bao",
        "given": "Yuyan"
      },
      {
        "family": "Leavens",
        "given": "Gary T."
      },
      {
        "family": "Ernst",
        "given": "Gidon"
      }
    ],
    "title": "Unifying separation logic and region logic to allow interoperability",
    "container-title": "Formal Aspects of Computing",
    "container-title-short": "Form Asp Comp",
    "issued": {
      "date-parts": [
        [
          "2018",
          "8",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "2",
          "7"
        ]
      ]
    },
    "issn": "1433-299X",
    "abstract": "Framing is important for specification and verification, especially in programs that mutate data structures with shared data, such as DAGs. Both separation logic and region logic are successful approaches to framing, with separation logic providing a concise way to reason about data structures that are disjoint, and region logic providing the ability to reason about framing for shared mutable data. In order to obtain the benefits of both logics for programs with shared mutable data, this paper unifies them into a single logic, which can encode both of them and allows them to interoperate. The new logic thus provides a way to reason about program modules specified in a mix of styles.",
    "URL": "https://doi.org/10.1007/s00165-018-0455-5",
    "DOI": "10.1007/s00165-018-0455-5",
    "page": "381-441",
    "page-first": "381",
    "volume": "30",
    "issue": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:5806"
  },
  "polikarpova_fully_2018": {
    "id": "polikarpova_fully_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Polikarpova",
        "given": "Nadia"
      },
      {
        "family": "Tschannen",
        "given": "Julian"
      },
      {
        "family": "Furia",
        "given": "Carlo A."
      }
    ],
    "title": "A fully verified container library",
    "container-title": "Formal Aspects of Computing",
    "container-title-short": "Form Asp Comp",
    "issued": {
      "date-parts": [
        [
          "2018",
          "9",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "2",
          "7"
        ]
      ]
    },
    "issn": "1433-299X",
    "abstract": "The comprehensive functionality and nontrivial design of realistic general-purpose container libraries pose challenges to formal verification that go beyond those of individual benchmark problems mainly targeted by the state of the art. We present our experience verifying the full functional correctness of EiffelBase2: a container library offering all the features customary in modern language frameworks, such as external iterators, and hash tables with generic mutable keys and load balancing. Verification uses the automated deductive verifier AutoProof, which we extended as part of the present work. Our results indicate that verification of a realistic container library (135 public methods, 8400 LOC) is possible with moderate annotation overhead (1.4 lines of specification per LOC) and good performance (0.2 s per method on average).",
    "URL": "https://doi.org/10.1007/s00165-017-0435-1",
    "DOI": "10.1007/s00165-017-0435-1",
    "page": "495-523",
    "page-first": "495",
    "volume": "30",
    "issue": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:5823"
  },
  "sitaraman_building_2011": {
    "id": "sitaraman_building_2011",
    "type": "article-journal",
    "author": [
      {
        "family": "Sitaraman",
        "given": "Murali"
      },
      {
        "family": "Adcock",
        "given": "Bruce"
      },
      {
        "family": "Avigad",
        "given": "Jeremy"
      },
      {
        "family": "Bronish",
        "given": "Derek"
      },
      {
        "family": "Bucci",
        "given": "Paolo"
      },
      {
        "family": "Frazier",
        "given": "David"
      },
      {
        "family": "Friedman",
        "given": "Harvey M."
      },
      {
        "family": "Harton",
        "given": "Heather"
      },
      {
        "family": "Heym",
        "given": "Wayne"
      },
      {
        "family": "Kirschenbaum",
        "given": "Jason"
      },
      {
        "family": "Krone",
        "given": "Joan"
      },
      {
        "family": "Smith",
        "given": "Hampton"
      },
      {
        "family": "Weide",
        "given": "Bruce W."
      }
    ],
    "title": "Building a push-button RESOLVE verifier: Progress and challenges",
    "container-title": "Formal Aspects of Computing",
    "container-title-short": "Building a push-button RESOLVE verifier",
    "title-short": "Building a push-button RESOLVE verifier",
    "issued": {
      "date-parts": [
        [
          "2011",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "2",
          "7"
        ]
      ]
    },
    "issn": "0934-5043, 1433-299X",
    "URL": "http://link.springer.com/10.1007/s00165-010-0154-3",
    "DOI": "10.1007/s00165-010-0154-3",
    "page": "607-626",
    "page-first": "607",
    "volume": "23",
    "issue": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:5841"
  },
  "kirschenbaum_verifying_2009": {
    "id": "kirschenbaum_verifying_2009",
    "type": "paper-conference",
    "author": [
      {
        "family": "Kirschenbaum",
        "given": "Jason"
      },
      {
        "family": "Adcock",
        "given": "Bruce"
      },
      {
        "family": "Bronish",
        "given": "Derek"
      },
      {
        "family": "Smith",
        "given": "Hampton"
      },
      {
        "family": "Harton",
        "given": "Heather"
      },
      {
        "family": "Sitaraman",
        "given": "Murali"
      },
      {
        "family": "Weide",
        "given": "Bruce W."
      }
    ],
    "editor": [
      {
        "family": "Edwards",
        "given": "Stephen H."
      },
      {
        "family": "Kulczycki",
        "given": "Gregory"
      }
    ],
    "title": "Verifying Component-Based Software: Deep Mathematics or Simple Bookkeeping?",
    "container-title": "Formal Foundations of Reuse and Domain Engineering",
    "container-title-short": "Verifying Component-Based Software",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "Verifying Component-Based Software",
    "issued": {
      "date-parts": [
        [
          "2009"
        ]
      ]
    },
    "publisher": "Springer",
    "isbn": "978-3-642-04211-9",
    "abstract": "Anecdotal experience constructing proofs of correctness of code built from reusable software components reveals that they tend to be relatively trivial bookkeeping exercises: they rarely require a substantive mathematical deduction. A careful empirical analysis of hundreds of verification conditions (VCs) for a library of component-client code shows the level of sophistication each proof requires, and suggests how to use the results to characterize a notion of mathematical “obviousness.”",
    "keywords": "Proof Rule, Client Code, Grand Challenge, Loop Invariant, Proof System",
    "DOI": "10.1007/978-3-642-04211-9_4",
    "publisher-place": "Berlin, Heidelberg",
    "page": "31-40",
    "page-first": "31",
    "language": "en-US",
    "_line": "FormalBib.bib:5858"
  },
  "welch_scaling_2017": {
    "id": "welch_scaling_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Welch",
        "given": "Daniel"
      }
    ],
    "title": "Scaling Up Automated Verification: A Case Study and Formal-IDE for the Construction of High Integrity Software",
    "container-title": "Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education",
    "container-title-short": "Scaling Up Automated Verification",
    "collection-title": "SIGCSE '17",
    "title-short": "Scaling Up Automated Verification",
    "issued": {
      "date-parts": [
        [
          "2017",
          "3",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "2",
          "7"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-4698-6",
    "abstract": "This work aims to show through a detailed case study that scaling up automated verification to larger non-trivial data structures is not only possible, but when combined with appropriate tool support, can be made more comprehensible and practicable to users in a variety of settings, including the undergraduate curriculum. The study involves an interplay of multiple components annotated with formal interface contracts and the components are all designed to be modular, reusable, and amenable to automated verification and analysis. The components are built using a formal integrated development environment (F-IDE). The plan is to evaluate the F-IDE in an upper-level undergraduate software engineering course in the Spring semester at Clemson University.",
    "keywords": "verification, formal methods, contracts, IDE, software components",
    "URL": "https://doi.org/10.1145/3017680.3022456",
    "DOI": "10.1145/3017680.3022456",
    "publisher-place": "Seattle, Washington, USA",
    "page": "785-786",
    "page-first": "785",
    "_line": "FormalBib.bib:5877"
  },
  "filliatre_toolchain_nodate": {
    "id": "filliatre_toolchain_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Filliâtre",
        "given": "Jean-Christophe"
      },
      {
        "family": "Gondelman",
        "given": "Léon"
      },
      {
        "family": "Lourenço",
        "given": "Cláudio"
      },
      {
        "family": "Paskevich",
        "given": "Andrei"
      },
      {
        "family": "Pereira",
        "given": "Mário"
      }
    ],
    "title": "A Toolchain to Produce Verified OCaml Libraries",
    "abstract": "In this paper, we present a methodology to produce veri ed OCaml libraries, using the GOSPEL speci cation language and the Why3 program veri cation tool. First, a formal behavioral speci cation of the library is written in OCaml/GOSPEL, in the form of an OCaml module signature extended with type invariants and function contracts. Second, an implementation is written in WhyML, the programming language of Why3, and then veri ed with respect to the GOSPEL speci cation. Finally, WhyML code is automatically translated into OCaml source code by Why3. Our methodology is illustrated with two examples:  rst, a small binary search function; then, a union- nd data structure that is part of a larger OCaml veri ed library.",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "FormalBib.bib:5896"
  },
  "jacobs_introduction_2017": {
    "id": "jacobs_introduction_2017",
    "type": "book",
    "author": [
      {
        "family": "Jacobs",
        "given": "Bart"
      }
    ],
    "title": "Introduction to Coalgebra: Towards Mathematics of States and Observation",
    "container-title-short": "Introduction to Coalgebra",
    "title-short": "Introduction to Coalgebra",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "2",
          "2"
        ]
      ]
    },
    "publisher": "Cambridge University Press",
    "isbn": "978-1-316-82318-7",
    "URL": "http://ebooks.cambridge.org/ref/id/CBO9781316823187",
    "DOI": "10.1017/CBO9781316823187",
    "publisher-place": "Cambridge",
    "_line": "FormalBib.bib:5905"
  },
  "celik_mutation_2019": {
    "id": "celik_mutation_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Celik",
        "given": "Ahmet"
      },
      {
        "family": "Palmskog",
        "given": "Karl"
      },
      {
        "family": "Parovic",
        "given": "Marinela"
      },
      {
        "family": "Jesus Gallego Arias",
        "given": "Emilio"
      },
      {
        "family": "Gligoric",
        "given": "Milos"
      }
    ],
    "title": "Mutation Analysis for Coq",
    "container-title": "2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
    "event-title": "2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
    "issued": {
      "date-parts": [
        [
          "2019",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "20"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-72812-508-4",
    "abstract": "Mutation analysis, which introduces artiﬁcial defects into software systems, is the basis of mutation testing, a technique widely applied to evaluate and enhance the quality of test suites. However, despite the deep analogy between tests and formal proofs, mutation analysis has seldom been considered in the context of deductive veriﬁcation. We propose mutation proving, a technique for analyzing veriﬁcation projects that use proof assistants. We implemented our technique for the Coq proof assistant in a tool dubbed MCOQ. MCOQ applies a set of mutation operators to Coq deﬁnitions of functions and datatypes, inspired by operators previously proposed for functional programming languages. MCOQ then checks proofs of lemmas affected by operator application. To make our technique feasible in practice, we implemented several optimizations in MCOQ such as parallel proof checking. We applied MCOQ to several medium and large scale Coq projects, and recorded whether proofs passed or failed when applying different mutation operators. We then qualitatively analyzed the mutants, ﬁnding many instances of incomplete speciﬁcations. For our evaluation, we made several improvements to serialization of Coq ﬁles and even discovered a notable bug in Coq itself, all acknowledged by developers. We believe MCOQ can be useful both to proof engineers for improving the quality of their veriﬁcation projects and to researchers for evaluating proof engineering techniques.",
    "URL": "https://ieeexplore.ieee.org/document/8952421/",
    "DOI": "10.1109/ASE.2019.00057",
    "publisher-place": "San Diego, CA, USA",
    "page": "539-551",
    "page-first": "539",
    "language": "en-US",
    "_line": "FormalBib.bib:5919"
  },
  "epstein_computability_1989": {
    "id": "epstein_computability_1989",
    "type": "book",
    "author": [
      {
        "family": "Epstein",
        "given": "Richard L."
      },
      {
        "family": "Carnielli",
        "given": "Walter Alexandr"
      }
    ],
    "title": "Computability: Computable Functions Logic and the Foundations of Math",
    "container-title-short": "Computability",
    "title-short": "Computability",
    "issued": {
      "date-parts": [
        [
          "1989",
          "11",
          "9"
        ]
      ]
    },
    "publisher": "Chapman and Hall/CRC",
    "number-of-pages": "320",
    "edition": "1 edition",
    "isbn": "978-0-534-10356-9",
    "abstract": "This book should be of interest to intermediate mathematics undergraduates; postgraduates in theoretical computer science/philosophy of mathematics.",
    "publisher-place": "Pacific Grove, Calif",
    "_line": "FormalBib.bib:5937"
  },
  "hutchison_modular_2013": {
    "id": "hutchison_modular_2013",
    "type": "chapter",
    "author": [
      {
        "family": "Svendsen",
        "given": "Kasper"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      },
      {
        "family": "Parkinson",
        "given": "Matthew"
      }
    ],
    "editor": [
      {
        "family": "Felleisen",
        "given": "Matthias"
      },
      {
        "family": "Gardner",
        "given": "Philippa"
      }
    ],
    "title": "Modular Reasoning about Separation of Concurrent Data Structures",
    "container-title": "Programming Languages and Systems",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "15"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-37035-9 978-3-642-37036-6",
    "abstract": "In a concurrent setting, the usage protocol of standard separation logic speciﬁcations are not reﬁnable by clients, because standard speciﬁcations abstract all information about potential interleavings. This breaks modularity, as libraries cannot be veriﬁed in isolation, since the appropriate speciﬁcation depends on how clients intend to use the library. In this paper we propose a new logic and a new style of speciﬁcation for thread-safe concurrent data structures. Our speciﬁcations allow clients to reﬁne usage protocols and associate ownership of additional resources with instances of these data structures.",
    "URL": "http://link.springer.com/10.1007/978-3-642-37036-6_11",
    "DOI": "10.1007/978-3-642-37036-6_11",
    "publisher-place": "Berlin, Heidelberg",
    "page": "169-188",
    "page-first": "169",
    "volume": "7792",
    "language": "en-US",
    "_line": "FormalBib.bib:5950"
  },
  "hutchison_impredicative_2014": {
    "id": "hutchison_impredicative_2014",
    "type": "chapter",
    "author": [
      {
        "family": "Svendsen",
        "given": "Kasper"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      }
    ],
    "editor": [
      {
        "family": "Shao",
        "given": "Zhong"
      }
    ],
    "title": "Impredicative Concurrent Abstract Predicates",
    "container-title": "Programming Languages and Systems",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "15"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-54832-1 978-3-642-54833-8",
    "abstract": "We present impredicative concurrent abstract predicates –iCAP – a program logic for modular reasoning about concurrent, higherorder, reentrant, imperative code. Building on earlier work, iCAP uses protocols to reason about shared mutable state. A key novel feature of iCAP is the ability to deﬁne impredicative protocols; protocols that are parameterized on arbitrary predicates, including predicates that themselves refer to protocols. We demonstrate the utility of impredicative protocols through a series of examples, including the speciﬁcation and veriﬁcation, in the logic, of a spin-lock, a reentrant event loop, and a concurrent bag implemented using cooperation, against modular speciﬁcations.",
    "URL": "http://link.springer.com/10.1007/978-3-642-54833-8_9",
    "DOI": "10.1007/978-3-642-54833-8_9",
    "publisher-place": "Berlin, Heidelberg",
    "page": "149-168",
    "page-first": "149",
    "volume": "8410",
    "language": "en-US",
    "_line": "FormalBib.bib:5971"
  },
  "friedman_elementary_2016": {
    "id": "friedman_elementary_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Friedman",
        "given": "Greg"
      }
    ],
    "title": "An elementary illustrated introduction to simplicial sets",
    "container-title": "arXiv:0809.4221 \\[math\\]",
    "issued": {
      "date-parts": [
        [
          "2016",
          "10",
          "3"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "15"
        ]
      ]
    },
    "abstract": "This is an expository introduction to simplicial sets and simplicial homotopy theory with particular focus on relating the combinatorial aspects of the theory to their geometric/topological origins. It is intended to be accessible to students familiar with just the fundamentals of algebraic topology.",
    "keywords": "18G30, 55U10, Mathematics - Algebraic Topology, Mathematics - Category Theory, Mathematics - Geometric Topology",
    "URLtext": "0809.4221",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "0809.4221",
    "URL": "http://arxiv.org/abs/0809.4221",
    "_line": "FormalBib.bib:5992"
  },
  "leinster_higher_2003": {
    "id": "leinster_higher_2003",
    "type": "article-journal",
    "author": [
      {
        "family": "Leinster",
        "given": "Tom"
      }
    ],
    "title": "Higher Operads, Higher Categories",
    "container-title": "arXiv:math/0305049",
    "issued": {
      "date-parts": [
        [
          "2003",
          "5",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "15"
        ]
      ]
    },
    "abstract": "Higher-dimensional category theory is the study of n-categories, operads, braided monoidal categories, and other such exotic structures. It draws its inspiration from areas as diverse as topology, quantum algebra, mathematical physics, logic, and theoretical computer science. This is the first book on the subject and lays its foundations. Many examples are given throughout. There is also an introductory chapter motivating the subject for topologists.",
    "keywords": "Mathematics - Algebraic Topology, Mathematics - Category Theory, Mathematics - Algebraic Geometry, Mathematics - Quantum Algebra",
    "URLtext": "math/0305049",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "math/0305049",
    "URL": "http://arxiv.org/abs/math/0305049",
    "_line": "FormalBib.bib:6006"
  },
  "appel_program_2014": {
    "id": "appel_program_2014",
    "type": "book",
    "author": [
      {
        "family": "Appel",
        "given": "Andrew W."
      },
      {
        "family": "Dockins",
        "given": "Robert"
      },
      {
        "family": "Hobor",
        "given": "Aquinas"
      },
      {
        "family": "Beringer",
        "given": "Lennart"
      },
      {
        "family": "Dodds",
        "given": "Josiah"
      },
      {
        "family": "Stewart",
        "given": "Gordon"
      },
      {
        "family": "Blazy",
        "given": "Sandrine"
      },
      {
        "family": "Leroy",
        "given": "Xavier"
      }
    ],
    "title": "Program Logics for Certified Compilers",
    "issued": {
      "date-parts": [
        [
          "2014",
          "4",
          "21"
        ]
      ]
    },
    "publisher": "Cambridge University Press",
    "number-of-pages": "472",
    "edition": "1 edition",
    "abstract": "Separation logic is the twenty-first-century variant of Hoare logic that permits verification of pointer-manipulating programs. This book covers practical and theoretical aspects of separation logic at a level accessible to beginning graduate students interested in software verification. On the practical side it offers an introduction to verification in Hoare and separation logics, simple case studies for toy languages, and the Verifiable C program logic for the C programming language. On the theoretical side it presents separation algebras as models of separation logics; step-indexed models of higher-order logical features for higher-order programs; indirection theory for constructing step-indexed separation algebras; tree-shares as models for shared ownership; and the semantic construction (and soundness proof) of Verifiable C. In addition, the book covers several aspects of the CompCert verified C compiler, and its connection to foundationally verified software analysis tools. All constructions and proofs are made rigorous and accessible in the Coq developments of the open-source Verified Software Toolchain.",
    "_line": "FormalBib.bib:6020"
  },
  "riehl_category_2017": {
    "id": "riehl_category_2017",
    "type": "book",
    "author": [
      {
        "family": "Riehl",
        "given": "Emily"
      }
    ],
    "title": "Category Theory in Context",
    "issued": {
      "date-parts": [
        [
          "2017",
          "3",
          "9"
        ]
      ]
    },
    "publisher": "Dover Publications",
    "number-of-pages": "272",
    "abstract": "Category theory has provided the foundations for many of the twentieth century's greatest advances in pure mathematics. This concise, original text for a one-semester course on the subject is derived from courses that author Emily Riehl taught at Harvard and Johns Hopkins Universities. The treatment introduces the essential concepts of category theory: categories, functors, natural transformations, the Yoneda lemma, limits and colimits, adjunctions, monads, and other topics. Suitable for advanced undergraduates and graduate students in mathematics, the text provides tools for understanding and attacking difficult problems in algebra, number theory, algebraic geometry, and algebraic topology. Drawing upon a broad range of mathematical examples from the categorical perspective, the author illustrates how the concepts and constructions of category theory arise from and illuminate more basic mathematical ideas. Prerequisites are limited to familiarity with some basic set theory and logic.",
    "_line": "FormalBib.bib:6030"
  },
  "adamek_abstract_2004": {
    "id": "adamek_abstract_2004",
    "type": "article-journal",
    "author": [
      {
        "family": "Adamek",
        "given": "Jiri"
      },
      {
        "family": "Herrlich",
        "given": "Horst"
      },
      {
        "family": "Strecker",
        "given": "George E"
      },
      {
        "family": "Schubert",
        "given": "Christoph"
      }
    ],
    "title": "Abstract and Concrete Categories - The Joy of Cats",
    "issued": {
      "date-parts": [
        [
          "2004",
          "1",
          "12"
        ]
      ]
    },
    "abstract": "Abstract and Concrete Categories was published by John Wiley and Sons, Inc, in 1990, and after several reprints, the book has been sold out and unavailable for several years. We now present an improved and corrected version as an open access file. This was made possible due to the return of copyright to the authors, and due to many hours of hard work and the exceptional skill of Christoph Schubert, to whom we wish to express our profound gratitude. The illustrations of Edward Gorey are unfortunately missing in the current version (for copyright reasons), but fortunately additional original illustrations by Marcel Erné, to whom additional special thanks of the authors belong, counterbalance the loss.\nOpen access includes the right of any reader to copy, store or distribute the book or parts of it freely. (See the GNU Free Documentation License at the end of the text.) Besides the acknowledgements appearing at the end of the original preface (below), we wish to thank all those who have helped to eliminate mistakes that survived the first printing of the text, particularly H. Bargenda, J. Jürjens W. Meyer, L. Schröder A. M. Torkabud, and O. Wyler.\nJanuary 12, 2004",
    "URL": "http://katmat.math.uni-bremen.de/acc/acc.pdf",
    "page": "524",
    "page-first": "524",
    "language": "en-US",
    "_line": "FormalBib.bib:6039"
  },
  "noauthor_alloy_nodate": {
    "id": "noauthor_alloy_nodate",
    "type": "webpage",
    "title": "Alloy - software modeling",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "15"
        ]
      ]
    },
    "abstract": "Alloy is an open source language and analyzer for software modeling. It has been used in a wide range of applications, from finding holes in security mechanisms to designing telephone switching networks. This site provides language documentation, tool downloads, and a repository of links to case studies and applications. As the open source community grows, this site will also provide access to extensions of the Alloy Analyzer, and tools built on top of it and on top of Kodkod, its model finding engine.",
    "URL": "http://alloytools.org/",
    "_line": "FormalBib.bib:6052"
  },
  "noauthor_iron_nodate": {
    "id": "noauthor_iron_nodate",
    "type": "webpage",
    "title": "Iron: Managing Obligations in Higher-Order Concurrent Separation Logic (POPL 2019)",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "14"
        ]
      ]
    },
    "URL": "https://iris-project.org/iron/",
    "_line": "FormalBib.bib:6060"
  },
  "bizjak_iron_2019": {
    "id": "bizjak_iron_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Bizjak",
        "given": "Aleš"
      },
      {
        "family": "Gratzer",
        "given": "Daniel"
      },
      {
        "family": "Krebbers",
        "given": "Robbert"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      }
    ],
    "title": "Iron: managing obligations in higher-order concurrent separation logic",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Iron",
    "title-short": "Iron",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "14"
        ]
      ]
    },
    "issn": "24751421",
    "URL": "http://dl.acm.org/citation.cfm?doid=3302515.3290378",
    "DOI": "10.1145/3290378",
    "page": "1-30",
    "page-first": "1",
    "volume": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:6067"
  },
  "birkedal_lecture_nodate": {
    "id": "birkedal_lecture_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Birkedal",
        "given": "Lars"
      },
      {
        "family": "Bizjak",
        "given": "Aleš"
      }
    ],
    "title": "Lecture Notes on Iris: Higher-Order Concurrent Separation Logic",
    "page": "138",
    "page-first": "138",
    "language": "en-US",
    "_line": "FormalBib.bib:6084"
  },
  "noauthor_formal_nodate": {
    "id": "noauthor_formal_nodate",
    "type": "webpage",
    "title": "Formal Versus Agile: Survival of the Fittest.ResearchGate",
    "container-title-short": "(17) (PDF) Formal Versus Agile",
    "title-short": "(17) (PDF) Formal Versus Agile",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "14"
        ]
      ]
    },
    "abstract": "ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.",
    "URL": "https://www.researchgate.net/publication/224587383_Formal_Versus_Agile_Survival_of_the_Fittest",
    "language": "en-US",
    "_line": "FormalBib.bib:6092"
  },
  "girard_linear_1995": {
    "id": "girard_linear_1995",
    "type": "chapter",
    "author": [
      {
        "family": "Girard",
        "given": "J.-Y."
      }
    ],
    "editor": [
      {
        "family": "Girard",
        "given": "Jean-Yves"
      },
      {
        "family": "Lafont",
        "given": "Yves"
      },
      {
        "family": "Regnier",
        "given": "Laurent"
      }
    ],
    "title": "Linear Logic: its syntax and semantics",
    "container-title": "Advances in Linear Logic",
    "container-title-short": "Linear Logic",
    "title-short": "Linear Logic",
    "issued": {
      "date-parts": [
        [
          "1995"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "13"
        ]
      ]
    },
    "publisher": "Cambridge University Press",
    "isbn": "978-0-511-62915-0",
    "URL": "https://www.cambridge.org/core/product/identifier/CBO9780511629150A008/type/book_part",
    "DOI": "10.1017/CBO9780511629150.002",
    "publisher-place": "Cambridge",
    "page": "1-42",
    "page-first": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:6103"
  },
  "di_cosmo_linear_2019": {
    "id": "di_cosmo_linear_2019",
    "type": "chapter",
    "author": [
      {
        "family": "Di Cosmo",
        "given": "Roberto"
      },
      {
        "family": "Miller",
        "given": "Dale"
      }
    ],
    "editor": [
      {
        "family": "Zalta",
        "given": "Edward N."
      }
    ],
    "title": "Linear Logic",
    "container-title": "The Stanford Encyclopedia of Philosophy",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "13"
        ]
      ]
    },
    "publisher": "Metaphysics Research Lab, Stanford University",
    "edition": "Summer 2019",
    "abstract": "Linear logic is a refinement of classical and intuitionistic logic.Instead of emphasizing truth, as in classical logic, orproof, as in intuitionistic logic, linear logic emphasizes therole of formulas as resources. To achieve this focus, linearlogic does not allow the usual structural rules of contraction andweakening to apply to all formulas but only those formulas marked withcertain modals. Linear logic contains a fully involutive negation whilemaintaining a strong constructive interpretation. Linear logic alsoprovides new insights into the nature of proofs in both classical andintuitionistic logic. Given its focus on resources, linear logic hasfound many applications in Computer Science.",
    "keywords": "logic: and games, logic: classical, logic: dialogical, logic: intuitionistic, logic: substructural, proof theory",
    "URL": "https://plato.stanford.edu/archives/sum2019/entries/logic-linear/",
    "_line": "FormalBib.bib:6121"
  },
  "noauthor_introduction_nodate": {
    "id": "noauthor_introduction_nodate",
    "type": "webpage",
    "title": "Introduction to Domain Theory",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "13"
        ]
      ]
    },
    "URL": "http://www.cs.nott.ac.uk/~pszgmh/domains.html",
    "_line": "FormalBib.bib:6136"
  },
  "hutchison_embedding_2007": {
    "id": "hutchison_embedding_2007",
    "type": "chapter",
    "author": [
      {
        "family": "Cousineau",
        "given": "Denis"
      },
      {
        "family": "Dowek",
        "given": "Gilles"
      }
    ],
    "editor": [
      {
        "family": "Della Rocca",
        "given": "Simona Ronchi"
      }
    ],
    "title": "Embedding Pure Type Systems in the Lambda-Pi-Calculus Modulo",
    "container-title": "Typed Lambda Calculi and Applications",
    "issued": {
      "date-parts": [
        [
          "2007"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "13"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-540-73227-3 978-3-540-73228-0",
    "abstract": "The lambda-Pi-calculus allows to express proofs of minimal predicate logic. It can be extended, in a very simple way, by adding computation rules. This leads to the lambda-Pi-calculus modulo. We show in this paper that this simple extension is surprisingly expressive and, in particular, that all functional Pure Type Systems, such as the system F, or the Calculus of Constructions, can be embedded in it. And, moreover, that this embedding is conservative under termination hypothesis.",
    "URL": "http://link.springer.com/10.1007/978-3-540-73228-0_9",
    "DOI": "10.1007/978-3-540-73228-0_9",
    "publisher-place": "Berlin, Heidelberg",
    "page": "102-117",
    "page-first": "102",
    "volume": "4583",
    "language": "en-US",
    "_line": "FormalBib.bib:6143"
  },
  "krebbers_mosel_2018": {
    "id": "krebbers_mosel_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Krebbers",
        "given": "Robbert"
      },
      {
        "family": "Jourdan",
        "given": "Jacques-Henri"
      },
      {
        "family": "Jung",
        "given": "Ralf"
      },
      {
        "family": "Tassarotti",
        "given": "Joseph"
      },
      {
        "family": "Kaiser",
        "given": "Jan-Oliver"
      },
      {
        "family": "Timany",
        "given": "Amin"
      },
      {
        "family": "Charguéraud",
        "given": "Arthur"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      }
    ],
    "title": "MoSeL: a general, extensible modal framework for interactive proofs in separation logic",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "MoSeL",
    "title-short": "MoSeL",
    "issued": {
      "date-parts": [
        [
          "2018",
          "7",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "13"
        ]
      ]
    },
    "issn": "24751421",
    "URL": "http://dl.acm.org/citation.cfm?doid=3243631.3236772",
    "DOI": "10.1145/3236772",
    "page": "1-30",
    "page-first": "1",
    "volume": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:6164"
  },
  "saillard_typechecking_2015": {
    "id": "saillard_typechecking_2015",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Saillard",
        "given": "Ronan"
      }
    ],
    "title": "Typechecking in the lambda-Pi-Calculus Modulo : Theory and Practice",
    "container-title-short": "Typechecking in the lambda-Pi-Calculus Modulo",
    "title-short": "Typechecking in the lambda-Pi-Calculus Modulo",
    "issued": {
      "date-parts": [
        [
          "2015",
          "9",
          "25"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "13"
        ]
      ]
    },
    "publisher": "Ecole Nationale Supérieure des Mines de Paris",
    "abstract": "Automatic proof checking is about using a computer to check the validity of proofs of mathematical statements. Since this verification is purely computational, it offers a high degree of confidence. Therefore, it is particularly useful for checking that a critical software, i.e., a software that when malfunctioning may result in death or serious injury to people, loss or severe damage to equipment or environmental harm, corresponds to its specification. DEDUKTI is such a proof checker. It implements a type system, the lambda-Pi-Calculus Modulo, that is an extension of the dependently-typed lambda-calculus with first-order rewrite rules. Through the Curry-Howard correspondence, DEDUKTI implements both a powerful programming language and an expressive logical system. Furthermore, this language is particularly well suited for encoding other proof systems. For instance, we can import in DEDUKTI theorems proved using other tools such as COQ, HOL or ZENON, a first step towards creating interoperability between these systems.The lambda-Pi-Calculus Modulo is a very expressive language. On the other hand, some fundamental properties such as subject reduction (i.e., the stability of typing by reduction) and uniqueness of types are not guaranteed in general and depend on the rewrite rules considered. Yet, these properties are necessary for guaranteeing the coherence of the proof system, but also for provingthe soundness and completeness of the type-checking algorithms implemented in DEDUKTI. Unfortunately, these properties are undecidable. In this thesis, we design new criteria for subject reduction and uniqueness of types that are decidable in order to be implemented in DEDUKTI.For this purpose, we give a new definition of the lambda-Pi-Calculus Modulo that takes into account the iterative aspect of the addition of rewrite rules in the typing context. A detailed study of this new system shows that the problems of subject reduction and uniqueness of types can be reduced to two simpler properties that we call product compatibility and well-typedness of rewrite rules.Hence, we study these two properties separately and give effective sufficient conditions for them to hold.These ideas have been implemented in DEDUKTI, increasing its generality and reliability.",
    "URL": "https://pastel.archives-ouvertes.fr/tel-01299180",
    "language": "en-US",
    "_line": "FormalBib.bib:6181"
  },
  "noauthor_cerco_nodate": {
    "id": "noauthor_cerco_nodate",
    "type": "webpage",
    "title": "CerCo - Certified Complexity",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "13"
        ]
      ]
    },
    "abstract": "CerCo (Certified Complexity) is a European research project in the ​7th Research Framework Programme (FP7) of the ​European Commission (project number 243381). The project is situated in the FP7 theme ​Information &amp; Communication Technologies (ICT) in the topic Future and Emerging Technologies (FET Open). The project has started February 1st, 2010, and will have a duration of 3 years.\n\nThe project aims to the construction of a formally verified complexity preserving compiler from a large subset of C to some typical microcontroller assembly, of the kind traditionally used in embedded systems. The work comprise the definition of cost models for the input and target languages, and the machine-checked proof of preservation of complexity (concrete, not asymptotic) along compilation. The compiler will also return tight and certified cost annotations for the source program, providing a reliable infrastructure to draw temporal assertions on the executable code while reasoning on the source. The compiler will be open source, and all proofs will be public domain.",
    "URL": "http://cerco.cs.unibo.it/",
    "_line": "FormalBib.bib:6195"
  },
  "noauthor_matita_nodate": {
    "id": "noauthor_matita_nodate",
    "type": "webpage",
    "title": "Matita - Interactive Theorem Prover",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "13"
        ]
      ]
    },
    "abstract": "Matita (that means pencil in italian) is an experimental, interactive theorem prover under development at the Computer Science Department of the University of Bologna.",
    "URL": "http://matita.cs.unibo.it/",
    "_line": "FormalBib.bib:6205"
  },
  "noauthor_lambdapi_2020": {
    "id": "noauthor_lambdapi_2020",
    "type": "book",
    "title": "Lambdapi, a proof assistant based on the λΠ-calculus modulo rewriting",
    "issued": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "13"
        ]
      ]
    },
    "publisher": "Deducteam",
    "abstract": "Proof assistant based on the λΠ-calculus modulo rewriting",
    "keywords": "dependent-types, logical-framework, proof-assistant, proof-checker, rewriting",
    "URL": "https://github.com/Deducteam/lambdapi",
    "note": "original-date: 2017-09-10T20:32:16Z",
    "_line": "FormalBib.bib:6213"
  },
  "birkedal_taste_nodate": {
    "id": "birkedal_taste_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Birkedal",
        "given": "Lars"
      }
    ],
    "title": "A Taste of Categorical Logic — Tutorial Notes",
    "page": "41",
    "page-first": "41",
    "language": "en-US",
    "_line": "FormalBib.bib:6224"
  },
  "gross_experience_2014": {
    "id": "gross_experience_2014",
    "type": "article-journal",
    "author": [
      {
        "family": "Gross",
        "given": "Jason"
      },
      {
        "family": "Chlipala",
        "given": "Adam"
      },
      {
        "family": "Spivak",
        "given": "David I."
      }
    ],
    "title": "Experience Implementing a Performant Category-Theory Library in Coq",
    "container-title": "arXiv:1401.7694 \\[cs, math\\]",
    "issued": {
      "date-parts": [
        [
          "2014",
          "4",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "13"
        ]
      ]
    },
    "abstract": "We describe our experience implementing a broad categorytheory library in Coq. Category theory and computational performance are not usually mentioned in the same breath, but we have needed substantial engineering effort to teach Coq to cope with large categorical constructions without slowing proof script processing unacceptably. In this paper, we share the lessons we have learned about how to represent very abstract mathematical objects and arguments in Coq and how future proof assistants might be designed to better support such reasoning. One particular encoding trick to which we draw attention allows category-theoretic arguments involving duality to be internalized in Coq’s logic with definitional equality. Ours may be the largest Coq development to date that uses the relatively new Coq version developed by homotopy type theorists, and we reflect on which new features were especially helpful.",
    "keywords": "Computer Science - Logic in Computer Science, Mathematics - Category Theory",
    "URLtext": "1401.7694",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1401.7694",
    "URL": "http://arxiv.org/abs/1401.7694",
    "language": "en-US",
    "_line": "FormalBib.bib:6232"
  },
  "aydemir_engineering_nodate": {
    "id": "aydemir_engineering_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Aydemir",
        "given": "Brian"
      },
      {
        "family": "Chargueraud",
        "given": "Arthur"
      },
      {
        "family": "Pierce",
        "given": "Benjamin C"
      },
      {
        "family": "Pollack",
        "given": "Randy"
      },
      {
        "family": "Weirich",
        "given": "Stephanie"
      }
    ],
    "title": "Engineering Formal Metatheory",
    "abstract": "Machine-checked proofs of properties of programming languages have become a critical need, both for increased conﬁdence in large and complex designs and as a foundation for technologies such as proof-carrying code. However, constructing these proofs remains a black art, involving many choices in the formulation of deﬁnitions and theorems that make a huge cumulative difference in the difﬁculty of carrying out large formal developments. The representation and manipulation of terms with variable binding is a key issue.",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "FormalBib.bib:6247"
  },
  "scott_continuous_nodate": {
    "id": "scott_continuous_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Scott",
        "given": "Dana"
      }
    ],
    "title": "Continuous lattices.ResearchGate",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "12"
        ]
      ]
    },
    "abstract": "Starting from the topological point of view a certain wide class of To-spaces is introduced having a very strong extension property for continuous functions with values in these spaces. It is then shown that all such spaces are complete lattices whose lattice structure determines the topology — these are the continuous lattices — and every such lattice has the extension property. With this foundation the lattices are studied in detail with respect to projections, subspaces, embeddings, and constructions such as products, sums, function spaces, and inverse limits. The main result of the paper is a proof that every topological space can be embedded in a continuous lattice which is homeomorphic (and isomorphic) to its own function space. The function algebra of such spaces provides mathematical models for the Church-Curry λ-calculus.",
    "URL": "https://www.researchgate.net/publication/251394986_Continuous_lattices",
    "language": "en-US",
    "_line": "FormalBib.bib:6256"
  },
  "assaf_dedukti_nodate": {
    "id": "assaf_dedukti_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Assaf",
        "given": "Ali"
      },
      {
        "family": "Burel",
        "given": "Guillaume"
      },
      {
        "family": "Cauderlier",
        "given": "Raphaël"
      },
      {
        "family": "Dowek",
        "given": "Gilles"
      },
      {
        "family": "Dubois",
        "given": "Catherine"
      },
      {
        "family": "Gilbert",
        "given": "Frédéric"
      },
      {
        "family": "Halmagrand",
        "given": "Pierre"
      },
      {
        "family": "Hermant",
        "given": "Olivier"
      },
      {
        "family": "Saillard",
        "given": "Ronan"
      }
    ],
    "title": "Dedukti: a Logical Framework based on the λΠ-Calculus Modulo Theory",
    "abstract": "Dedukti is a Logical Framework based on the λΠ-Calculus Modulo Theory. We show that many theories can be expressed in Dedukti: constructive and classical predicate logic, Simple type theory, programming languages, Pure type systems, the Calculus of inductive constructions with universes, etc. and that permits to used it to check large libraries of proofs developed in other proof systems: Zenon, iProver, FoCaLiZe, HOL Light, and Matita.",
    "page": "36",
    "page-first": "36",
    "language": "en-US",
    "_line": "FormalBib.bib:6267"
  },
  "noauthor_dedukti_nodate": {
    "id": "noauthor_dedukti_nodate",
    "type": "webpage",
    "title": "Dedukti - a Logical Framework",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "URL": "https://deducteam.github.io/",
    "_line": "FormalBib.bib:6276"
  },
  "noauthor_deducteamdedukti_2019": {
    "id": "noauthor_deducteamdedukti_2019",
    "type": "book",
    "title": "Deducteam/Dedukti",
    "issued": {
      "date-parts": [
        [
          "2019",
          "12",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "publisher": "Deducteam",
    "abstract": "Implementation of the λΠ-calculus modulo rewriting",
    "URL": "https://github.com/Deducteam/Dedukti",
    "note": "original-date: 2017-11-16T15:34:07Z",
    "_line": "FormalBib.bib:6283"
  },
  "noauthor_deducteamholide_2019": {
    "id": "noauthor_deducteamholide_2019",
    "type": "book",
    "title": "Deducteam/Holide",
    "issued": {
      "date-parts": [
        [
          "2019",
          "11",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "publisher": "Deducteam",
    "abstract": "A translator from OpenTheory to Dedukti. Contribute to Deducteam/Holide development by creating an account on GitHub.",
    "URL": "https://github.com/Deducteam/Holide",
    "note": "original-date: 2018-02-08T13:18:34Z",
    "_line": "FormalBib.bib:6293"
  },
  "xi_introduction_nodate": {
    "id": "xi_introduction_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Xi",
        "given": "Hongwei"
      }
    ],
    "title": "Introduction to Programming in ATS",
    "page": "252",
    "page-first": "252",
    "language": "en-US",
    "_line": "FormalBib.bib:6303"
  },
  "xi_applied_2017": {
    "id": "xi_applied_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Xi",
        "given": "Hongwei"
      }
    ],
    "title": "Applied Type System: An Approach to Practical Programming with Theorem-Proving",
    "container-title": "arXiv:1703.08683 \\[cs\\]",
    "container-title-short": "Applied Type System",
    "title-short": "Applied Type System",
    "issued": {
      "date-parts": [
        [
          "2017",
          "3",
          "25"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "abstract": "The framework Pure Type System (PTS) offers a simple and general approach to designing and formalizing type systems. However, in the presence of dependent types, there often exist certain acute problems that make it difficult for PTS to directly accommodate many common realistic programming features such as general recursion, recursive types, effects (e.g., exceptions, references, input/output), etc. In this paper, Applied Type System (ATS) is presented as a framework for designing and formalizing type systems in support of practical programming with advanced types (including dependent types). In particular, it is demonstrated that ATS can readily accommodate a paradigm referred to as programming with theorem-proving (PwTP) in which programs and proofs are constructed in a syntactically intertwined manner, yielding a practical approach to internalizing constraint-solving needed during type-checking. The key salient feature of ATS lies in a complete separation between statics, where types are formed and reasoned about, and dynamics, where programs are constructed and evaluated. With this separation, it is no longer possible for a program to occur in a type as is otherwise allowed in PTS. The paper contains not only a formal development of ATS but also some examples taken from ats-lang.org, a programming language with a type system rooted in ATS, in support of employing ATS as a framework to formulate advanced type systems for practical programming.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "1703.08683",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1703.08683",
    "URL": "http://arxiv.org/abs/1703.08683",
    "_line": "FormalBib.bib:6311"
  },
  "noauthor_ats_nodate": {
    "id": "noauthor_ats_nodate",
    "type": "webpage",
    "title": "The ATS Programming Language",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "URL": "http://www.ats-lang.org/",
    "_line": "FormalBib.bib:6326"
  },
  "joe_leslie-hurd_slowest_2015": {
    "id": "joe_leslie-hurd_slowest_2015",
    "type": "webpage",
    "author": [
      {
        "family": "Joe Leslie-Hurd"
      }
    ],
    "title": "The Slowest Software Development Methodology in the World.The Robot Mathematician",
    "issued": {
      "date-parts": [
        [
          "2015",
          "7",
          "19"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "abstract": "For some time now I’ve been practicing what can only be described as the slowest software development methodology in the world: a three step waltz of Prototyping, Verification and Export. Pro…",
    "URL": "https://gilith.wordpress.com/2015/07/19/the-slowest-software-development-methodology-in-the-world/",
    "language": "en-US",
    "_line": "FormalBib.bib:6333"
  },
  "noauthor_proofpower_nodate": {
    "id": "noauthor_proofpower_nodate",
    "type": "webpage",
    "title": "The ProofPower Web Pages",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "URL": "http://www.lemma-one.com/ProofPower/index/",
    "_line": "FormalBib.bib:6345"
  },
  "bobaru_opentheory_2011": {
    "id": "bobaru_opentheory_2011",
    "type": "chapter",
    "author": [
      {
        "family": "Hurd",
        "given": "Joe"
      }
    ],
    "editor": [
      {
        "family": "Bobaru",
        "given": "Mihaela"
      },
      {
        "family": "Havelund",
        "given": "Klaus"
      },
      {
        "family": "Holzmann",
        "given": "Gerard J."
      },
      {
        "family": "Joshi",
        "given": "Rajeev"
      }
    ],
    "title": "The OpenTheory Standard Theory Library",
    "container-title": "NASA Formal Methods",
    "issued": {
      "date-parts": [
        [
          "2011"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-20397-8 978-3-642-20398-5",
    "abstract": "Interactive theorem proving is tackling ever larger formalization and veriﬁcation projects, and there is a critical need for theory engineering techniques to support these eﬀorts. One such technique is cross-prover package management, which has the potential to simplify the development of logical theories and eﬀectively share theories between diﬀerent theorem prover implementations. The OpenTheory project has developed standards for packaging theories of the higher order logic implemented by the HOL family of theorem provers. What is currently missing is a standard theory library that can serve as a published contract of interoperability and contain proofs of basic properties that would otherwise appear in many theory packages. The core contribution of this paper is the presentation of a standard theory library for higher order logic represented as an OpenTheory package. We identify the core theory set of the HOL family of theorem provers, and describe the process of instrumenting the HOL Light theorem prover to extract a standardized version of its core theory development. We proﬁle the axioms and theorems of our standard theory library and investigate the performance cost of separating the standard theory library into coherent hierarchical theory packages.",
    "URL": "http://link.springer.com/10.1007/978-3-642-20398-5_14",
    "DOI": "10.1007/978-3-642-20398-5_14",
    "publisher-place": "Berlin, Heidelberg",
    "page": "177-191",
    "page-first": "177",
    "volume": "6617",
    "language": "en-US",
    "_line": "FormalBib.bib:6352"
  },
  "noauthor_metis_nodate": {
    "id": "noauthor_metis_nodate",
    "type": "webpage",
    "title": "Metis Theorem Prover - Gilith",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "URL": "http://www.gilith.com/metis/",
    "_line": "FormalBib.bib:6371"
  },
  "noauthor_opentheory_nodate": {
    "id": "noauthor_opentheory_nodate",
    "type": "webpage",
    "title": "OpenTheory Project - Gilith",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "URL": "http://www.gilith.com/opentheory/",
    "_line": "FormalBib.bib:6378"
  },
  "lombardi_commutative_2015": {
    "id": "lombardi_commutative_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Lombardi",
        "given": "Henri"
      },
      {
        "family": "Quitté",
        "given": "Claude"
      }
    ],
    "title": "Commutative algebra: Constructive methods. Finite projective modules",
    "container-title": "arXiv:1605.04832 \\[math\\]",
    "container-title-short": "Commutative algebra",
    "title-short": "Commutative algebra",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "abstract": "This book is an introductory course to basic commutative algebra with a particular emphasis on finitely generated projective modules. We adopt the constructive point of view, with which all existence theorems have an explicit algorithmic content content. In particular, when a theorem affirms the existence of an object &ndash; the solution of a problem &ndash; a construction algorithm of the object can always be extracted from the given proof. We revisit with a new and often simplifying eye several abstract classical theories. In particular, we review theories which did not have any algorithmic content in their general natural framework, such as Galois theory, the Dedekind domains, the finitely generated projective modules or the Krull dimension.",
    "keywords": "13-02 (13C10), Mathematics - Commutative Algebra",
    "URLtext": "1605.04832",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1605.04832",
    "URL": "http://arxiv.org/abs/1605.04832",
    "DOI": "10.1007/978-94-017-9944-7",
    "volume": "20",
    "_line": "FormalBib.bib:6385"
  },
  "devai_embedding_2009": {
    "id": "devai_embedding_2009",
    "type": "paper-conference",
    "author": [
      {
        "family": "Dévai",
        "given": "Gergely"
      }
    ],
    "title": "Embedding a Proof System in Haskell",
    "container-title": "Central European Functional Programming School",
    "event-title": "Central European Functional Programming School",
    "issued": {
      "date-parts": [
        [
          "2009",
          "5",
          "21"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "publisher": "Springer, Berlin, Heidelberg",
    "abstract": "This article reports about a work-in-progress project that aims at embedding a proof system \\[4\\] in the Haskellprogramming language. The goal of the system is to create formally verified software...",
    "URL": "http://link.springer.com/chapter/10.1007/978-3-642-17685-2_10",
    "DOI": "10.1007/978-3-642-17685-2_10",
    "page": "354-371",
    "page-first": "354",
    "language": "en-US",
    "_line": "FormalBib.bib:6402"
  },
  "noauthor_galois_nodate": {
    "id": "noauthor_galois_nodate",
    "type": "motion-picture",
    "title": "Galois, Inc. Tech Talk: JaVerT: a JavaScript Verification Toolchain (Dr. Philippa Gardner)",
    "container-title-short": "Galois, Inc. Tech Talk",
    "title-short": "Galois, Inc. Tech Talk",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "abstract": "Abstract:\n\nThe dynamic nature of JavaScript and its complex semantics make it a difficult target for logic-based verification. In this talk, I will describe JaVerT, a semi-automatic JavaScript Verification Toolchain\nbased on separation logic. JaVerT is aimed at the specialist developer wanting rich, mechanically verified specifications of critical JavaScript code. The specification challenge is to design specifications that are readable by developers. The verification challenge is to handle the complex, dynamic nature of JavaScript\nwithout simplification. The validation challenge is to understand what it means for the verification to be trusted.\n\nBio:\n\nPhilippa Gardner is a professor in the Department of Computing at Imperial College London and leader of the research group working on Verified Trustworthy Software Specification. Her current research focusses on reasoning about web programs (JavaScript and DOM); and reasoning about concurrent programs. \nShe completed her PhD thesis, supervised by Professor Gordon Plotkin FRS at Edinburgh in 1992. She moved to Cambridge in 1998 on an EPSRC Advanced Fellowship, hosted by Professor Robin Milner FRS. She obtained a lectureship at Imperial in 2001, and became professor in 2009. She held a Microsoft Research Cambridge/Royal Academy of Engineering Senior Fellowship from 2005 to 2010 at Imperial.\n\nPhilippa directs the Research Institute on Verified Trustworthy Software Systems (VeTSS), funded by EPSRC, from 2017 to 2022. She also chairs the BCS awards committee, which decides the Lovelace medal (senior) and Roger Needham award (mid-career) for computer science and engineering.",
    "URL": "https://www.youtube.com/watch?v=uNVAmCYL1Jo",
    "_line": "FormalBib.bib:6418"
  },
  "carneiro_specifying_2019": {
    "id": "carneiro_specifying_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Carneiro",
        "given": "Mario"
      }
    ],
    "title": "Specifying verified x86 software from scratch",
    "container-title": "arXiv:1907.01283 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2019",
          "7",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "abstract": "We present a simple framework for specifying and proving facts about the input/output behavior of ELF binary files on the x86-64 architecture. A strong emphasis has been placed on simplicity at all levels: the specification says only what it needs to about the target executable, the specification is performed inside a simple logic (equivalent to first-order Peano Arithmetic), and the verification language and proof checker are custom-designed to have only what is necessary to perform efficient general purpose verification. This forms a part of the Metamath Zero project, to build a minimal verifier that is capable of verifying its own binary. In this paper, we will present the specification of the dynamic semantics of x86 machine code, together with enough information about Linux system calls to perform simple IO.",
    "keywords": "Computer Science - Logic in Computer Science, 68Q60 (Primary) 68N30 (Secondary)",
    "URLtext": "1907.01283",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1907.01283",
    "URL": "http://arxiv.org/abs/1907.01283",
    "_line": "FormalBib.bib:6437"
  },
  "carneiro_type_2019": {
    "id": "carneiro_type_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Carneiro",
        "given": "Mario"
      }
    ],
    "title": "The Type Theory of Lean",
    "issued": {
      "date-parts": [
        [
          "2019",
          "4",
          "16"
        ]
      ]
    },
    "URL": "https://github.com/digama0/lean-type-theory/releases/download/v1.0/main.pdf",
    "_line": "FormalBib.bib:6451"
  },
  "carneiro_metamath_2020-1": {
    "id": "carneiro_metamath_2020-1",
    "type": "book",
    "author": [
      {
        "family": "Carneiro",
        "given": "Mario"
      }
    ],
    "title": "Metamath Zero",
    "issued": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "abstract": "Metamath Zero specification language. Contribute to digama0/mm0 development by creating an account on GitHub.",
    "URL": "https://github.com/digama0/mm0",
    "note": "original-date: 2019-02-25T07:34:19Z",
    "_line": "FormalBib.bib:6459"
  },
  "ullrich_counting_2019": {
    "id": "ullrich_counting_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Ullrich",
        "given": "Sebastian"
      },
      {
        "family": "Moura",
        "given": "Leonardo",
        "dropping-particle": "de"
      }
    ],
    "title": "Counting Immutable Beans: Reference Counting Optimized for Purely Functional Programming",
    "container-title": "arXiv:1908.05647 \\[cs\\]",
    "container-title-short": "Counting Immutable Beans",
    "title-short": "Counting Immutable Beans",
    "issued": {
      "date-parts": [
        [
          "2019",
          "9",
          "3"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "abstract": "Most functional languages rely on some garbage collection for automatic memory management. They usually eschew reference counting in favor of a tracing garbage collector, which has less bookkeeping overhead at runtime. On the other hand, having an exact reference count of each value can enable optimizations, such as destructive updates. We explore these optimization opportunities in the context of an eager, purely functional programming language. We propose a new mechanism for efficiently reclaiming memory used by nonshared values, reducing stress on the global memory allocator. We describe an approach for minimizing the number of reference counts updates using borrowed references and a heuristic for automatically inferring borrow annotations. We implemented all these techniques in a new compiler for an eager and purely functional programming language with support for multi-threading. Our preliminary experimental results demonstrate our approach is competitive and often outperforms state-of-the-art compilers.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "1908.05647",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1908.05647",
    "URL": "http://arxiv.org/abs/1908.05647",
    "_line": "FormalBib.bib:6470"
  },
  "shafiq_integrating_2014": {
    "id": "shafiq_integrating_2014",
    "type": "article-journal",
    "author": [
      {
        "family": "Shafiq",
        "given": "Shagufta"
      },
      {
        "family": "Minhas",
        "given": "Nasir Mehmood"
      }
    ],
    "title": "Integrating Formal Methods in XP—A Conceptual Solution",
    "container-title": "Journal of Software Engineering and Applications",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "issn": "1945-3116, 1945-3124",
    "URL": "http://www.scirp.org/journal/doi.aspx?DOI=10.4236/jsea.2014.74029",
    "DOI": "10.4236/jsea.2014.74029",
    "page": "299-310",
    "page-first": "299",
    "volume": "07",
    "issue": "4",
    "_line": "FormalBib.bib:6485"
  },
  "noauthor_theorem_nodate": {
    "id": "noauthor_theorem_nodate",
    "type": "webpage",
    "title": "Theorem Proving in Lean — Theorem Proving in Lean 3.4.0 documentation",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "URL": "https://leanprover.github.io/theorem_proving_in_lean/index.html",
    "_line": "FormalBib.bib:6500"
  },
  "noauthor_event-b_nodate": {
    "id": "noauthor_event-b_nodate",
    "type": "webpage",
    "title": "Event-B and the Rodin Platform",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "URL": "http://www.event-b.org/",
    "_line": "FormalBib.bib:6507"
  },
  "noauthor_coqeal_2020": {
    "id": "noauthor_coqeal_2020",
    "type": "book",
    "title": "CoqEAL",
    "issued": {
      "date-parts": [
        [
          "2020",
          "1",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "publisher": "CoqEAL",
    "abstract": "CoqEAL &ndash; The Coq Effective Algebra Library. Contribute to CoqEAL/CoqEAL development by creating an account on GitHub.",
    "URL": "https://github.com/CoqEAL/CoqEAL",
    "note": "original-date: 2014-02-10T12:35:29Z",
    "_line": "FormalBib.bib:6514"
  },
  "noauthor_iris_nodate": {
    "id": "noauthor_iris_nodate",
    "type": "webpage",
    "title": "Iris / stdpp.GitLab",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "abstract": "An extended \"Standard Library\" for Coq. \\[\\[coqdoc\\]\\](https://plv.mpi-sws.org/coqdoc/stdpp/)",
    "URL": "https://gitlab.mpi-sws.org/iris/stdpp",
    "language": "en-US",
    "_line": "FormalBib.bib:6524"
  },
  "noauthor_acsl_nodate": {
    "id": "noauthor_acsl_nodate",
    "type": "webpage",
    "title": "ACSL by Example.GitHub",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "10"
        ]
      ]
    },
    "abstract": "Public snapshots of \"ACSL by Example\". Contribute to fraunhoferfokus/acsl-by-example development by creating an account on GitHub.",
    "URL": "https://github.com/fraunhoferfokus/acsl-by-example",
    "language": "en-US",
    "_line": "FormalBib.bib:6534"
  },
  "kennedy_types_2010": {
    "id": "kennedy_types_2010",
    "type": "chapter",
    "author": [
      {
        "family": "Kennedy",
        "given": "Andrew"
      }
    ],
    "editor": [
      {
        "family": "Horváth",
        "given": "Zoltán"
      },
      {
        "family": "Plasmeijer",
        "given": "Rinus"
      },
      {
        "family": "Zsók",
        "given": "Viktória"
      }
    ],
    "title": "Types for Units-of-Measure: Theory and Practice",
    "container-title": "Central European Functional Programming School: Third Summer School, CEFP 2009, Budapest, Hungary, May 21-23, 2009 and Komárno, Slovakia, May 25-30, 2009, Revised Selected Lectures",
    "container-title-short": "Types for Units-of-Measure",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "Types for Units-of-Measure",
    "issued": {
      "date-parts": [
        [
          "2010"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "1"
        ]
      ]
    },
    "publisher": "Springer",
    "isbn": "978-3-642-17685-2",
    "abstract": "Units-of-measure are to science what types are to programming. In science and engineering, dimensional and unit consistency provides a first check on the correctness of an equation or formula, just as in programming the validation of a program by the type-checker eliminates one possible reason for failure.",
    "keywords": "Inference Algorithm, Equational Theory, Type Inference, Type Scheme, Type System",
    "URL": "https://doi.org/10.1007/978-3-642-17685-2_8",
    "DOI": "10.1007/978-3-642-17685-2_8",
    "publisher-place": "Berlin, Heidelberg",
    "page": "268-305",
    "page-first": "268",
    "language": "en-US",
    "_line": "FormalBib.bib:6544"
  },
  "shin_wormspace:_2019": {
    "id": "shin_wormspace:_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Shin",
        "given": "Ji-Yong"
      },
      {
        "family": "Kim",
        "given": "Jieung"
      },
      {
        "family": "Honoré",
        "given": "Wolf"
      },
      {
        "family": "Vanzetto",
        "given": "Hernán"
      },
      {
        "family": "Radhakrishnan",
        "given": "Srihari"
      },
      {
        "family": "Balakrishnan",
        "given": "Mahesh"
      },
      {
        "family": "Shao",
        "given": "Zhong"
      }
    ],
    "title": "WormSpace: A Modular Foundation for Simple, Verifiable Distributed Systems",
    "container-title": "Proceedings of the ACM Symposium on Cloud Computing  - SoCC '19",
    "container-title-short": "WormSpace",
    "title-short": "WormSpace",
    "event-title": "the ACM Symposium",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "12",
          "31"
        ]
      ]
    },
    "publisher": "ACM Press",
    "isbn": "978-1-4503-6973-2",
    "abstract": "We propose the Write-Once Register (WOR) as an abstraction for building and verifying distributed systems. A WOR exposes a simple, data-centric API: clients can capture, write, and read it. Applications can use a sequence or a set of WORs to obtain properties such as durability, concurrency control, and failure atomicity. By hiding the logic for distributed coordination underneath a data-centric API, the WOR abstraction enables easy, incremental, and extensible implementation and verification of applications built above it. We present the design, implementation, and verification of a system called WormSpace that provides developers with an address space of WORs, implementing each WOR via a Paxos instance. We describe three applications built over WormSpace: a flexible, efficient Multi-Paxos implementation; a shared log implementation with lower append latency than the state-of-the-art; and a faulttolerant transaction coordinator that uses an optimal number of round-trips. We show that these applications are simple, easy to verify, and match the performance of unverified monolithic implementations. We use a modular layered verification approach to link the proofs for WormSpace, its applications, and a verified operating system to produce the first verified distributed system stack from the application to the operating system.",
    "URL": "http://dl.acm.org/citation.cfm?doid=3357223.3362739",
    "DOI": "10.1145/3357223.3362739",
    "publisher-place": "Santa Cruz, CA, USA",
    "page": "299-311",
    "page-first": "299",
    "language": "en-US",
    "_line": "FormalBib.bib:6565"
  },
  "liu_virtual_2019": {
    "id": "liu_virtual_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Liu",
        "given": "Mengqi"
      },
      {
        "family": "Rieg",
        "given": "Lionel"
      },
      {
        "family": "Shao",
        "given": "Zhong"
      },
      {
        "family": "Gu",
        "given": "Ronghui"
      },
      {
        "family": "Costanzo",
        "given": "David"
      },
      {
        "family": "Kim",
        "given": "Jung-Eun"
      },
      {
        "family": "Yoon",
        "given": "Man-Ki"
      }
    ],
    "title": "Virtual timeline: a formal abstraction for verifying preemptive schedulers with temporal isolation",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Virtual timeline",
    "title-short": "Virtual timeline",
    "issued": {
      "date-parts": [
        [
          "2019",
          "12",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "12",
          "31"
        ]
      ]
    },
    "issn": "24751421",
    "URL": "http://dl.acm.org/citation.cfm?doid=3377388.3371088",
    "DOI": "10.1145/3371088",
    "page": "1-31",
    "page-first": "1",
    "volume": "4",
    "language": "en-US",
    "_line": "FormalBib.bib:6584"
  },
  "lamport_pretending_2005": {
    "id": "lamport_pretending_2005",
    "type": "webpage",
    "author": [
      {
        "family": "Lamport",
        "given": "Leslie"
      },
      {
        "family": "Schneider",
        "given": "Fred B"
      }
    ],
    "title": "Pretending Atomicity, Digital Systems Research Center: Report 44",
    "container-title-short": "Pretending Atomicity",
    "title-short": "Pretending Atomicity",
    "issued": {
      "date-parts": [
        [
          "2005",
          "12",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "12",
          "30"
        ]
      ]
    },
    "abstract": "We present a theorem for deriving properties of a concurrent program by reasoning about a simpler, coarser-grained version. The theorem generalizes a result that Lipton proved for partial correctness and deadlock-freedom. Our theorem applies to all safety properties.",
    "URL": "https://web.archive.org/web/20051227134748/http://gatekeeper.research.compaq.com/pub/DEC/SRC/research-reports/abstracts/src-rr-044.html",
    "_line": "FormalBib.bib:6601"
  },
  "ford_specification-based_1997": {
    "id": "ford_specification-based_1997",
    "type": "paper-conference",
    "author": [
      {
        "family": "Ford",
        "given": "R.L."
      },
      {
        "family": "Simon",
        "given": "R.T."
      },
      {
        "family": "Bevier",
        "given": "W.R."
      },
      {
        "family": "Smith",
        "given": "L.M."
      }
    ],
    "title": "The specification-based testing of a trusted kernel: MK++",
    "container-title": "First IEEE International Conference on Formal Engineering Methods",
    "container-title-short": "The specification-based testing of a trusted kernel",
    "title-short": "The specification-based testing of a trusted kernel",
    "event-title": "First IEEE International Conference on Formal Engineering Methods",
    "issued": {
      "date-parts": [
        [
          "1997",
          "11"
        ]
      ]
    },
    "abstract": "The MK++ kernel, a descendant of Mach, was designed and implemented at the Open Group Research Institute. Independently, Computational Logic Inc. had developed a formal specification for the Mach kernel interface. We report on the adaptation of this specification to MK++, and its use in the derivation of a testing strategy for the MK++ implementation. The results and utility of the tests are discussed.",
    "keywords": "Kernel, program testing, Computer bugs, Logic, formal specification, Atomic layer deposition, Formal specifications, Law, Legal factors, Mach kernel interface, MK++ implementation, MK++ kernel, operating system kernels, Performance evaluation, software reliability, specification based testing, System testing, testing strategy, trusted kernel, Yarn",
    "DOI": "10.1109/ICFEM.1997.630422",
    "page": "151-160",
    "page-first": "151",
    "note": "ISSN: null",
    "_line": "FormalBib.bib:6612"
  },
  "noauthor_rems:_nodate": {
    "id": "noauthor_rems:_nodate",
    "type": "webpage",
    "title": "REMS: Rigorous Engineering of Mainsteam Systems, Papers",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "12",
          "27"
        ]
      ]
    },
    "URL": "https://www.cl.cam.ac.uk/~pes20/rems/rems-all.html",
    "_line": "FormalBib.bib:6627"
  },
  "bourque_guide_2014": {
    "id": "bourque_guide_2014",
    "type": "book",
    "author": [
      {
        "family": "IEEE Computer Society"
      }
    ],
    "editor": [
      {
        "family": "Bourque",
        "given": "Pierre"
      },
      {
        "family": "Fairley",
        "given": "R. E"
      }
    ],
    "title": "Guide to the software engineering body of knowledge",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "isbn": "978-0-7695-5166-1",
    "note": "OCLC: 973217192",
    "language": "en-US",
    "_line": "FormalBib.bib:6634"
  },
  "giuffrida_safe_2013": {
    "id": "giuffrida_safe_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Giuffrida",
        "given": "Cristiano"
      },
      {
        "family": "Kuijsten",
        "given": "Anton"
      },
      {
        "family": "Tanenbaum",
        "given": "Andrew S."
      },
      {
        "family": "Giuffrida",
        "given": "Cristiano"
      },
      {
        "family": "Kuijsten",
        "given": "Anton"
      },
      {
        "family": "Tanenbaum",
        "given": "Andrew S."
      },
      {
        "family": "Giuffrida",
        "given": "Cristiano"
      },
      {
        "family": "Kuijsten",
        "given": "Anton"
      },
      {
        "family": "Tanenbaum",
        "given": "Andrew S."
      }
    ],
    "title": "Safe and automatic live update for operating systems",
    "container-title": "ACM SIGARCH Computer Architecture News",
    "issued": {
      "date-parts": [
        [
          "2013",
          "3",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "12",
          "4"
        ]
      ]
    },
    "issn": "0163-5964, 0362-1340",
    "URL": "http://dl.acm.org/citation.cfm?id=2451116.2451147",
    "DOI": "10.1145/2451116.2451147",
    "page": "279-292",
    "page-first": "279",
    "volume": "41",
    "issue": "1",
    "_line": "FormalBib.bib:6645"
  },
  "giuffrida_safe_2014": {
    "id": "giuffrida_safe_2014",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Giuffrida",
        "given": "C"
      }
    ],
    "title": "Safe and automatic live update",
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "note": "OCLC: 876276706",
    "language": "en-US",
    "_line": "FormalBib.bib:6660"
  },
  "dang_rustbelt_nodate": {
    "id": "dang_rustbelt_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Dang",
        "given": "Hoang-Hai"
      },
      {
        "family": "Jourdan",
        "given": "Jacques-Henri"
      },
      {
        "family": "Kaiser",
        "given": "Jan-Oliver"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      }
    ],
    "title": "RustBelt Meets Relaxed Memory",
    "abstract": "The Rust programming language supports safe systems programming by means of a strong ownership-tracking type system. In their prior work on RustBelt, Jung et al. began the task of setting Rust’s safety claims on a more rigorous formal foundation. Specifically, they used Iris, a Coq-based separation logic framework, to build a machine-checked proof of semantic soundness for a λ-calculus model of Rust, as well as for a number of widely-used Rust libraries that internally employ unsafe language features. However, they also made the significant simplifying assumption that the language is sequentially consistent. In this paper, we adapt RustBelt to account for the relaxed-memory operations that concurrent Rust libraries actually use, in the process uncovering a data race in the Arc library. We focus on the most interesting technical problem: how to reason about resource reclamation under relaxed memory, using a logical construction we call synchronized ghost state. CCS Concepts: • Theory of computation → Separation logic; Operational semantics; Programming logic.",
    "page": "29",
    "page-first": "29",
    "volume": "4",
    "language": "en-US",
    "_line": "FormalBib.bib:6670"
  },
  "jones_function_2013": {
    "id": "jones_function_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Jones",
        "given": "Capers"
      }
    ],
    "title": "Function Points As a Universal Software Metric",
    "container-title": "SIGSOFT Softw. Eng. Notes",
    "issued": {
      "date-parts": [
        [
          "2013",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "11",
          "27"
        ]
      ]
    },
    "issn": "0163-5948",
    "abstract": "Function point metrics are the most accurate and effective metrics yet developed for software sizing and also for studying software productivity, quality, costs, risks, and economic value. Unlike the older \"lines of code\" metric function points can be used to study requirements, design, and in fact all software activities from development through maintenance. In the future function point metrics can easily become a universal metric used for all software applications and for all software contracts in all countries. The government of Brazil already requires function points for all software contracts, and South Korea and Italy may soon follow. However, there are some logistical problems with function point metrics that need to be understood and overcome in order for function point metrics to become the primary metric for software economic analysis. Manual function point counting is too slow and costly to be used on large software projects above 10,000 function points in size. Also, application size is not constant but grows at about 2&perc; per calendar month during development and 8&perc; or more per calendar year for as long as software is in active use. This paper discusses a method of high-speed function point counting that can size any application in less than two minutes, and which can predict application growth during development and for five years after release. This new method is based on pattern matching and is covered by U.S. utility patent application and hence is patent pending.",
    "URL": "http://doi.acm.org/10.1145/2492248.2492268",
    "DOI": "10.1145/2492248.2492268",
    "page": "1-27",
    "page-first": "1",
    "volume": "38",
    "issue": "4",
    "_line": "FormalBib.bib:6680"
  },
  "clark_instructors_2019": {
    "id": "clark_instructors_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Clark",
        "given": "Pete L."
      }
    ],
    "title": "The Instructor’s Guide to Real Induction",
    "container-title": "Mathematics Magazine",
    "container-title-short": "Mathematics Magazine",
    "issued": {
      "date-parts": [
        [
          "2019",
          "3",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "11",
          "27"
        ]
      ]
    },
    "issn": "0025-570X, 1930-0980",
    "URL": "https://www.tandfonline.com/doi/full/10.1080/0025570X.2019.1549902",
    "DOI": "10.1080/0025570X.2019.1549902",
    "page": "136-150",
    "page-first": "136",
    "volume": "92",
    "issue": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:6696"
  },
  "malecha_towards_2016": {
    "id": "malecha_towards_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Malecha",
        "given": "Gregory"
      },
      {
        "family": "Ricketts",
        "given": "Daniel"
      },
      {
        "family": "Alvarez",
        "given": "Mario M."
      },
      {
        "family": "Lerner",
        "given": "Sorin"
      }
    ],
    "title": "Towards foundational verification of cyber-physical systems",
    "container-title": "2016 Science of Security for Cyber-Physical Systems Workshop (SOSCYPS)",
    "event-title": "2016 Science of Security for Cyber-Physical Systems Workshop (SOSCYPS)",
    "issued": {
      "date-parts": [
        [
          "2016",
          "4"
        ]
      ]
    },
    "abstract": "The safety-critical aspects of cyber-physical systems motivate the need for rigorous analysis of these systems. In the literature this work is often done using idealized models of systems where the analysis can be carried out using high-level reasoning techniques such as Lyapunov functions and model checking. In this paper we present VERIDRONE, a foundational framework for reasoning about cyber-physical systems at all levels from high-level models to C code that implements the system. VERIDRONE is a library within the Coq proof assistant enabling us to build on its foundational implementation, its interactive development environments, and its wealth of libraries capturing interesting theories ranging from real numbers and differential equations to verified compilers and floating point numbers. These features make proof assistants in general, and Coq in particular, a powerful platform for unifying foundational results about safety-critical systems and ensuring interesting properties at all levels of the stack.",
    "keywords": "formal verification, model checking, verified compilers, Monitoring, Coq proof assistant, program compilers, Biomedical monitoring, Cognition, cyber-physical systems, Cyber-physical systems, differential equations, floating point numbers, foundational framework, high-level models, idealized models, interactive development environments, Lyapunov functions, Robustness, safety-critical aspects, safety-critical software, Software, Stability analysis, towards foundational verification",
    "DOI": "10.1109/SOSCYPS.2016.7580000",
    "page": "1-5",
    "page-first": "1",
    "note": "ISSN: null",
    "_line": "FormalBib.bib:6713"
  },
  "protzenko_formally_2019": {
    "id": "protzenko_formally_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Protzenko",
        "given": "Jonathan"
      },
      {
        "family": "Beurdouche",
        "given": "Benjamin"
      },
      {
        "family": "Merigoux",
        "given": "Denis"
      },
      {
        "family": "Bhargavan",
        "given": "Karthikeyan"
      }
    ],
    "title": "Formally Verified Cryptographic Web Applications in WebAssembly",
    "container-title": "2019 IEEE Symposium on Security and Privacy (SP)",
    "event-title": "2019 IEEE Symposium on Security and Privacy (SP)",
    "issued": {
      "date-parts": [
        [
          "2019",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "11",
          "26"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-5386-6660-9",
    "abstract": "After suffering decades of high-proﬁle attacks, the need for formal veriﬁcation of security-critical software has never been clearer. Veriﬁcation-oriented programming languages like F∗ are now being used to build high-assurance cryptographic libraries and implementations of standard protocols like TLS. In this paper, we seek to apply these veriﬁcation techniques to modern Web applications, like WhatsApp, that embed sophisticated custom cryptographic components. The problem is that these components are often implemented in JavaScript, a language that is both hostile to cryptographic code and hard to reason about. So we instead target WebAssembly, a new instruction set that is supported by all major JavaScript runtimes.",
    "URL": "https://ieeexplore.ieee.org/document/8835291/",
    "DOI": "10.1109/SP.2019.00064",
    "publisher-place": "San Francisco, CA, USA",
    "page": "1256-1274",
    "page-first": "1256",
    "language": "en-US",
    "_line": "FormalBib.bib:6726"
  },
  "cooper_incomputable_2017": {
    "id": "cooper_incomputable_2017",
    "type": "book",
    "editor": [
      {
        "family": "Cooper",
        "given": "S. Barry"
      },
      {
        "family": "Soskova",
        "given": "Mariya I."
      }
    ],
    "title": "The Incomputable",
    "collection-title": "Theory and Applications of Computability",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "11",
          "15"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-43667-8 978-3-319-43669-2",
    "URL": "http://link.springer.com/10.1007/978-3-319-43669-2",
    "DOI": "10.1007/978-3-319-43669-2",
    "publisher-place": "Cham",
    "_line": "FormalBib.bib:6744"
  },
  "soare_turing_2016": {
    "id": "soare_turing_2016",
    "type": "book",
    "author": [
      {
        "family": "Soare",
        "given": "Robert I."
      }
    ],
    "title": "Turing Computability",
    "collection-title": "Theory and Applications of Computability",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "11",
          "15"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-31932-7 978-3-642-31933-4",
    "URL": "http://link.springer.com/10.1007/978-3-642-31933-4",
    "DOI": "10.1007/978-3-642-31933-4",
    "publisher-place": "Berlin, Heidelberg",
    "_line": "FormalBib.bib:6758"
  },
  "longley_higher-order_2015": {
    "id": "longley_higher-order_2015",
    "type": "book",
    "author": [
      {
        "family": "Longley",
        "given": "John"
      },
      {
        "family": "Normann",
        "given": "Dag"
      }
    ],
    "title": "Higher-Order Computability",
    "collection-title": "Theory and Applications of Computability",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "11",
          "15"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-662-47991-9 978-3-662-47992-6",
    "URL": "http://link.springer.com/10.1007/978-3-662-47992-6",
    "DOI": "10.1007/978-3-662-47992-6",
    "publisher-place": "Berlin, Heidelberg",
    "_line": "FormalBib.bib:6772"
  },
  "downey_algorithmic_2010": {
    "id": "downey_algorithmic_2010",
    "type": "book",
    "author": [
      {
        "family": "Downey",
        "given": "Rodney G."
      },
      {
        "family": "Hirschfeldt",
        "given": "Denis R."
      }
    ],
    "title": "Algorithmic Randomness and Complexity",
    "collection-title": "Theory and Applications of Computability",
    "issued": {
      "date-parts": [
        [
          "2010"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "11",
          "15"
        ]
      ]
    },
    "publisher": "Springer New York",
    "isbn": "978-0-387-95567-4 978-0-387-68441-3",
    "URL": "http://link.springer.com/10.1007/978-0-387-68441-3",
    "DOI": "10.1007/978-0-387-68441-3",
    "publisher-place": "New York, NY",
    "_line": "FormalBib.bib:6786"
  },
  "bridges_apartness_2011": {
    "id": "bridges_apartness_2011",
    "type": "book",
    "author": [
      {
        "family": "Bridges",
        "given": "Douglas S."
      },
      {
        "family": "Vîţă",
        "given": "Luminiţa Simona"
      }
    ],
    "title": "Apartness and Uniformity",
    "collection-title": "Theory and Applications of Computability",
    "issued": {
      "date-parts": [
        [
          "2011"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "11",
          "15"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-22414-0 978-3-642-22415-7",
    "URL": "http://link.springer.com/10.1007/978-3-642-22415-7",
    "DOI": "10.1007/978-3-642-22415-7",
    "publisher-place": "Berlin, Heidelberg",
    "_line": "FormalBib.bib:6800"
  },
  "oconnor_computer-verified_2010": {
    "id": "oconnor_computer-verified_2010",
    "type": "article-journal",
    "author": [
      {
        "family": "O’Connor",
        "given": "Russell"
      },
      {
        "family": "Spitters",
        "given": "Bas"
      }
    ],
    "title": "A computer-verified monadic functional implementation of the integral",
    "container-title": "Theoretical Computer Science",
    "container-title-short": "Theoretical Computer Science",
    "issued": {
      "date-parts": [
        [
          "2010",
          "8",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "11",
          "6"
        ]
      ]
    },
    "issn": "0304-3975",
    "abstract": "We provide a computer-verified exact monadic functional implementation of the Riemann integral in type theory. Together with previous work by O’Connor, this may be seen as the beginning of the realization of Bishop’s vision to use constructive mathematics as a programming language for exact analysis.",
    "keywords": "Type theory, Exact real analysis, Functional programming, Monads",
    "URL": "http://www.sciencedirect.com/science/article/pii/S0304397510003233",
    "DOI": "10.1016/j.tcs.2010.05.031",
    "page": "3386-3402",
    "page-first": "3386",
    "volume": "411",
    "issue": "37",
    "language": "en-US",
    "_line": "FormalBib.bib:6814"
  },
  "noauthor_coquelicot.coquelicot_nodate": {
    "id": "noauthor_coquelicot.coquelicot_nodate",
    "type": "webpage",
    "title": "Coquelicot.Coquelicot",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "11",
          "3"
        ]
      ]
    },
    "URL": "http://coquelicot.saclay.inria.fr/html/Coquelicot.Coquelicot.html",
    "_line": "FormalBib.bib:6833"
  },
  "hutchison_pragmatic_2013": {
    "id": "hutchison_pragmatic_2013",
    "type": "chapter",
    "author": [
      {
        "family": "Cohen",
        "given": "Cyril"
      }
    ],
    "editor": [
      {
        "family": "Blazy",
        "given": "Sandrine"
      },
      {
        "family": "Paulin-Mohring",
        "given": "Christine"
      },
      {
        "family": "Pichardie",
        "given": "David"
      }
    ],
    "title": "Pragmatic Quotient Types in Coq",
    "container-title": "Interactive Theorem Proving",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "10",
          "18"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-39633-5 978-3-642-39634-2",
    "abstract": "In intensional type theory, it is not always possible to form the quotient of a type by an equivalence relation. However, quotients are extremely useful when formalizing mathematics, especially in algebra. We provide a Coq library with a pragmatic approach in two complementary components. First, we provide a framework to work with quotient types in an axiomatic manner. Second, we program construction mechanisms for some speciﬁc cases where it is possible to build a quotient type. This library was helpful in implementing the types of rational fractions, multivariate polynomials, ﬁeld extensions and real algebraic numbers.",
    "URL": "http://link.springer.com/10.1007/978-3-642-39634-2_17",
    "DOI": "10.1007/978-3-642-39634-2_17",
    "publisher-place": "Berlin, Heidelberg",
    "page": "213-228",
    "page-first": "213",
    "volume": "7998",
    "language": "en-US",
    "_line": "FormalBib.bib:6840"
  },
  "garillot_packaging_2009": {
    "id": "garillot_packaging_2009",
    "type": "paper-conference",
    "author": [
      {
        "family": "Garillot",
        "given": "François"
      },
      {
        "family": "Gonthier",
        "given": "Georges"
      },
      {
        "family": "Mahboubi",
        "given": "Assia"
      },
      {
        "family": "Rideau",
        "given": "Laurence"
      }
    ],
    "editor": [
      {
        "family": "Berghofer",
        "given": "Stefan"
      },
      {
        "family": "Nipkow",
        "given": "Tobias"
      },
      {
        "family": "Urban",
        "given": "Christian"
      },
      {
        "family": "Wenzel",
        "given": "Makarius"
      }
    ],
    "title": "Packaging Mathematical Structures",
    "container-title": "Theorem Proving in Higher Order Logics",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2009"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-03359-9",
    "abstract": "This paper proposes generic design patterns to define and combine algebraic structures, using dependent records, coercions and type inference, inside the Coq system. This alternative to telescopes in particular supports multiple inheritance, maximal sharing of notations and theories, and automated structure inference. Our methodology is robust enough to handle a hierarchy comprising a broad variety of algebraic structures, from types with a choice operator to algebraically closed fields. Interfaces for the structures enjoy the convenience of a classical setting, without requiring any axiom. Finally, we present two applications of our proof techniques: a key lemma for characterising the discrete logarithm, and a matrix decomposition problem.",
    "keywords": "Coq, Coercive subtyping, Formalization of Algebra, SSReflect, Type inference",
    "page": "327-342",
    "page-first": "327",
    "language": "en-US",
    "_line": "FormalBib.bib:6861"
  },
  "noauthor_fm_nodate": {
    "id": "noauthor_fm_nodate",
    "type": "webpage",
    "title": "FM folks - richardlford@gmail.com - Gmail",
    "accessed": {
      "date-parts": [
        [
          "2019",
          "10",
          "14"
        ]
      ]
    },
    "URL": "https://mail.google.com/mail/u/0/#inbox/FMfcgxwDrlVnZmDccTxHFBnzPRMfbmpn?projector=1&messagePartId=0.1",
    "_line": "FormalBib.bib:6877"
  },
  "moscato_provably_2019": {
    "id": "moscato_provably_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Moscato",
        "given": "Mariano M."
      },
      {
        "family": "Titolo",
        "given": "Laura"
      },
      {
        "family": "Feliú",
        "given": "Marco A."
      },
      {
        "family": "Muñoz",
        "given": "César A."
      }
    ],
    "editor": [
      {
        "family": "Beek",
        "given": "Maurice H.",
        "dropping-particle": "ter"
      },
      {
        "family": "McIver",
        "given": "Annabelle"
      },
      {
        "family": "Oliveira",
        "given": "José N."
      }
    ],
    "title": "Provably Correct Floating-Point Implementation of a Point-in-Polygon Algorithm",
    "container-title": "Formal Methods – The Next 30 Years",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-30942-8",
    "abstract": "The problem of determining whether or not a point lies inside a given polygon occurs in many applications. In air traffic management concepts, a correct solution to the point-in-polygon problem is critical to geofencing systems for Unmanned Aerial Vehicles and in weather avoidance applications. Many mathematical methods can be used to solve the point-in-polygon problem. Unfortunately, a straightforward floating-point implementation of these methods can lead to incorrect results due to round-off errors. In particular, these errors may cause the control flow of the program to diverge with respect to the ideal real-number algorithm. This divergence potentially results in an incorrect point-in-polygon determination even when the point is far from the edges of the polygon. This paper presents a provably correct implementation of a point-in-polygon method that is based on the computation of the winding number. This implementation is mechanically generated from a source-to-source transformation of the ideal real-number specification of the algorithm. The correctness of this implementation is formally verified within the Frama-C analyzer, where the proof obligations are discharged using the Prototype Verification System (PVS).",
    "page": "21-37",
    "page-first": "21",
    "language": "en-US",
    "_line": "FormalBib.bib:6884"
  },
  "klein_formally_2018": {
    "id": "klein_formally_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Klein",
        "given": "Gerwin"
      },
      {
        "family": "Andronick",
        "given": "June"
      },
      {
        "family": "Fernandez",
        "given": "Matthew"
      },
      {
        "family": "Kuz",
        "given": "Ihor"
      },
      {
        "family": "Murray",
        "given": "Toby"
      },
      {
        "family": "Heiser",
        "given": "Gernot"
      }
    ],
    "title": "Formally verified software in the real world",
    "container-title": "Communications of the ACM",
    "container-title-short": "Commun. ACM",
    "issued": {
      "date-parts": [
        [
          "2018",
          "9",
          "26"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "10",
          "14"
        ]
      ]
    },
    "issn": "00010782",
    "URL": "http://dl.acm.org/citation.cfm?doid=3281635.3230627",
    "DOI": "10.1145/3230627",
    "page": "68-77",
    "page-first": "68",
    "volume": "61",
    "issue": "10",
    "language": "en-US",
    "_line": "FormalBib.bib:6898"
  },
  "ter_beek_gospelproviding_2019": {
    "id": "ter_beek_gospelproviding_2019",
    "type": "chapter",
    "author": [
      {
        "family": "Charguéraud",
        "given": "Arthur"
      },
      {
        "family": "Filliâtre",
        "given": "Jean-Christophe"
      },
      {
        "family": "Lourenço",
        "given": "Cláudio"
      },
      {
        "family": "Pereira",
        "given": "Mário"
      }
    ],
    "editor": [
      {
        "family": "Beek",
        "given": "Maurice H.",
        "dropping-particle": "ter"
      },
      {
        "family": "McIver",
        "given": "Annabelle"
      },
      {
        "family": "Oliveira",
        "given": "José N."
      }
    ],
    "title": "GOSPEL—Providing OCaml with a Formal Specification Language",
    "container-title": "Formal Methods – The Next 30 Years",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "10",
          "14"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-30941-1 978-3-030-30942-8",
    "abstract": "This paper introduces GOSPEL, a behavioral speciﬁcation language for OCaml. It is designed to enable modular veriﬁcation of data structures and algorithms. GOSPEL is a contract-based, strongly typed language, with a formal semantics deﬁned by means of translation into Separation Logic. Compared with writing speciﬁcations directly in Separation Logic, GOSPEL provides a high-level syntax that greatly improves conciseness and makes it accessible to programmers with no familiarity with Separation Logic. Although GOSPEL has been developed for specifying OCaml code, we believe that many aspects of its design could apply to other programming languages. This paper presents the design and semantics of GOSPEL, and reports on its application for the development of a formally veriﬁed library of general-purpose OCaml data structures.",
    "URL": "http://link.springer.com/10.1007/978-3-030-30942-8_29",
    "DOI": "10.1007/978-3-030-30942-8_29",
    "publisher-place": "Cham",
    "page": "484-501",
    "page-first": "484",
    "volume": "11800",
    "language": "en-US",
    "_line": "FormalBib.bib:6915"
  },
  "easterbrook_formal_nodate": {
    "id": "easterbrook_formal_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Easterbrook",
        "given": "Steve"
      },
      {
        "family": "Callahan",
        "given": "John"
      }
    ],
    "title": "Formal Methods for V&amp;V of partial specifications: An experience report",
    "abstract": "This paper describes our work exploring the suitability of formal specification methods f o r independ e n t verification and validation (IVi3V) of software specifications for large, safety critical systems. An IV&amp;V contractor often has to perform rapid analysis on incomplete specifications, with no control over how those speciJcations are represented. Lightweight formal methods show significant promise in this context, as they offer a way of uncovering major errors, without the burden of full proofs of correctness. We describe an experiment in the application of the method SCR to testing for consistency properties of a partial model of th,e requirements for Fault Detection Isolation and Recovery o n th,e space station. We conclude that the insights gained from formalizing a specification is valuable, and it is the process of formalization, rather than the end product that is important. It was only necessary to build enough of the formal model to test the properties in which we were interested. Maintenance of fidelity between multiple representations of the same requirements (as they evolve) is still a problem,, and deserves further study.",
    "page": "9",
    "page-first": "9",
    "language": "en-US",
    "_line": "FormalBib.bib:6934"
  },
  "easterbrook_formal_1997": {
    "id": "easterbrook_formal_1997",
    "type": "paper-conference",
    "author": [
      {
        "family": "Easterbrook",
        "given": "S."
      },
      {
        "family": "Callahan",
        "given": "J."
      }
    ],
    "title": "Formal methods for V &amp; V of partial specifications: an experience report",
    "container-title": "Proceedings of ISRE '97: 3rd IEEE International Symposium on Requirements Engineering",
    "container-title-short": "Formal methods for V amp;V of partial specifications",
    "title-short": "Formal methods for V amp;V of partial specifications",
    "event-title": "Proceedings of ISRE '97: 3rd IEEE International Symposium on Requirements Engineering",
    "issued": {
      "date-parts": [
        [
          "1997",
          "1"
        ]
      ]
    },
    "abstract": "This paper describes our work exploring the suitability of formal specification methods for independent verification and validation (IV&amp;V) of software specifications for large, safety critical systems. An IV&amp;V contractor often has to perform rapid analysis on incomplete specifications, with no control over how those specifications are represented. Lightweight formal methods show significant promise in this context, as they offer a way of uncovering major errors, without the burden of full proofs of correctness. We describe an experiment in the application of the method SCR to testing for consistency properties of a partial model of the requirements for fault detection isolation and recovery on the space station. We conclude that the insights gained from formalizing a specification is valuable, and it is the process of formalization, rather than the end product that is important. It was only necessary to build enough of the formal model to test the properties in which we were interested. Maintenance of fidelity between multiple representations of the same requirements (as they evolve) is still a problem, and deserves further study.",
    "keywords": "program testing, program verification, formal methods, Performance analysis, testing, formal specification, Formal specifications, safety-critical software, aerospace computing, Aerospace safety, artificial satellites, consistency properties, Error correction, errors, fault detection isolation, fault diagnosis, fault recovery, formal specification methods, incomplete specifications, independent verification, International Space Station, large safety critical systems, NASA, partial specification verification, SCR, Software safety, space station, Space stations, Testing, Thyristors",
    "DOI": "10.1109/ISRE.1997.566865",
    "page": "160-168",
    "page-first": "160",
    "_line": "FormalBib.bib:6943"
  },
  "chihani_certication_nodate": {
    "id": "chihani_certication_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Chihani",
        "given": "Zakaria"
      }
    ],
    "title": "Certiﬁcation of First-order proofs in classical and intuitionistic logics",
    "page": "167",
    "page-first": "167",
    "language": "en-US",
    "_line": "FormalBib.bib:6956"
  },
  "gonthier_how_2013": {
    "id": "gonthier_how_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Gonthier",
        "given": "Georges"
      },
      {
        "family": "Ziliani",
        "given": "Beta"
      },
      {
        "family": "Nanevski",
        "given": "Aleksandar"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      }
    ],
    "title": "How to make ad hoc proof automation less ad hoc",
    "container-title": "Journal of Functional Programming",
    "container-title-short": "J. Funct. Prog.",
    "issued": {
      "date-parts": [
        [
          "2013",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "10",
          "13"
        ]
      ]
    },
    "issn": "0956-7968, 1469-7653",
    "abstract": "Most interactive theorem provers provide support for some form of user-customizable proof automation. In a number of popular systems, such as Coq and Isabelle, this automation is achieved primarily through tactics, which are programmed in a separate language from that of the prover’s base logic. While tactics are clearly useful in practice, they can be difﬁcult to maintain and compose because, unlike lemmas, their behavior cannot be speciﬁed within the expressive type system of the prover itself.",
    "URL": "https://www.cambridge.org/core/product/identifier/S0956796813000051/type/journal_article",
    "DOI": "10.1017/S0956796813000051",
    "page": "357-401",
    "page-first": "357",
    "volume": "23",
    "issue": "4",
    "language": "en-US",
    "_line": "FormalBib.bib:6964"
  },
  "farrell_robotics_2018": {
    "id": "farrell_robotics_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Farrell",
        "given": "Marie"
      },
      {
        "family": "Luckcuck",
        "given": "Matt"
      },
      {
        "family": "Fisher",
        "given": "Michael"
      }
    ],
    "title": "Robotics and Integrated Formal Methods: Necessity meets Opportunity",
    "container-title": "arXiv:1805.11996 \\[cs\\]",
    "container-title-short": "Robotics and Integrated Formal Methods",
    "title-short": "Robotics and Integrated Formal Methods",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "10",
          "5"
        ]
      ]
    },
    "abstract": "Robotic systems are multi-dimensional entities, combining both hardware and software, that are heavily dependent on, and influenced by, interactions with the real world. They can be variously categorised as embedded, cyberphysical, real-time, hybrid, adaptive and even autonomous systems, with a typical robotic system being likely to contain all of these aspects. The techniques for developing and verifying each of these system varieties are often quite distinct. This, together with the sheer complexity of robotic systems, leads us to argue that diverse formal techniques must be integrated in order to develop, verify, and provide certification evidence for, robotic systems. Furthermore, we propose the fast evolving field of robotics as an ideal catalyst for the advancement of integrated formal methods research, helping to drive the field in new and exciting directions and shedding light on the development of large-scale, dynamic, complex systems.",
    "keywords": "Computer Science - Software Engineering, Computer Science - Robotics",
    "URLtext": "1805.11996",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1805.11996",
    "URL": "http://arxiv.org/abs/1805.11996",
    "DOI": "10.1007/978-3-319-98938-9_10",
    "page": "161-171",
    "page-first": "161",
    "volume": "11023",
    "_line": "FormalBib.bib:6982"
  },
  "cofer_formal_nodate": {
    "id": "cofer_formal_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Cofer",
        "given": "Darren"
      },
      {
        "family": "Miller",
        "given": "Steven P"
      },
      {
        "family": "Collins",
        "given": "Rockwell"
      }
    ],
    "title": "Formal Methods Case Studies for DO-333",
    "page": "203",
    "page-first": "203",
    "language": "en-US",
    "_line": "FormalBib.bib:7000"
  },
  "hutchison_formal_2009": {
    "id": "hutchison_formal_2009",
    "type": "chapter",
    "author": [
      {
        "family": "Tschantz",
        "given": "Michael Carl"
      },
      {
        "family": "Wing",
        "given": "Jeannette M."
      }
    ],
    "editor": [
      {
        "family": "Cavalcanti",
        "given": "Ana"
      },
      {
        "family": "Dams",
        "given": "Dennis R."
      }
    ],
    "title": "Formal Methods for Privacy",
    "container-title": "FM 2009: Formal Methods",
    "issued": {
      "date-parts": [
        [
          "2009"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "10",
          "4"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-05088-6 978-3-642-05089-3",
    "URL": "http://link.springer.com/10.1007/978-3-642-05089-3_1",
    "DOI": "10.1007/978-3-642-05089-3_1",
    "publisher-place": "Berlin, Heidelberg",
    "page": "1-15",
    "page-first": "1",
    "volume": "5850",
    "language": "en-US",
    "_line": "FormalBib.bib:7008"
  },
  "collins_secure_nodate": {
    "id": "collins_secure_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Collins",
        "given": "Rockwell"
      }
    ],
    "title": "SECURE MATHEMATICALLY- ASSURED COMPOSITION OF CONTROL MODELS",
    "page": "134",
    "page-first": "134",
    "language": "en-US",
    "_line": "FormalBib.bib:7028"
  },
  "ringer_qed_2019-1": {
    "id": "ringer_qed_2019-1",
    "type": "book",
    "author": [
      {
        "family": "Ringer",
        "given": "T."
      },
      {
        "family": "Palmskog",
        "given": "K."
      },
      {
        "family": "Sergey",
        "given": "I."
      },
      {
        "family": "Gligoric",
        "given": "M."
      },
      {
        "family": "Tatlock",
        "given": "Z."
      }
    ],
    "title": "QED at Large: A Survey of Engineering of Formally Verified Software",
    "container-title-short": "QED at Large",
    "title-short": "QED at Large",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "9",
          "26"
        ]
      ]
    },
    "publisher": "now",
    "abstract": "Development of formal proofs of correctness of programs can increase actual and perceived reliability and facilitate better understanding of program specifications and their underlying assumptions. Tools supporting such development have been available for over 40 years but have only recently seen wide practical use. Projects based on construction of machine-checked formal proofs are now reaching an unprecedented scale, comparable to large software projects, which leads to new challenges in proof development and maintenance. Despite its increasing importance, the field of proof engineering is seldom considered in its own right; related theories, techniques, and tools span many fields and venues. QED at Large covers the timeline and research literature concerning proof development for program verification, including theories, languages, and tools. It emphasizes challenges and breakthroughs at each stage in history and highlights challenges that are currently present due to the increasing scale of proof developments. This monograph is intended for use by researchers and students who are new to the field. It provides the reader with an insightful overview of the work that has led to modern-day techniques for formally verifying software. In times of increasing automation, this underpins many software systems so future trends are also highlighted.",
    "URL": "http://ieeexplore.ieee.org/document/8824174",
    "_line": "FormalBib.bib:7036"
  },
  "tuch_types_2007": {
    "id": "tuch_types_2007",
    "type": "paper-conference",
    "author": [
      {
        "family": "Tuch",
        "given": "Harvey"
      },
      {
        "family": "Klein",
        "given": "Gerwin"
      },
      {
        "family": "Norrish",
        "given": "Michael"
      }
    ],
    "title": "Types, Bytes, and Separation Logic",
    "container-title": "Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "collection-title": "POPL '07",
    "issued": {
      "date-parts": [
        [
          "2007"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "9",
          "26"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-59593-575-5",
    "abstract": "We present a formal model of memory that both captures the low-level features of C's pointers and memory, and that forms the basis for an expressive implementation of separation logic. At the low level, we do not commit common oversimplifications, but correctly deal with C's model of programming language values and the heap. At the level of separation logic, we are still able to reason abstractly and efficiently. We implement this framework in the theorem prover Isabelle/HOL and demonstrate it on two case studies. We show that the divide between detailed and abstract does not impose undue verification overhead, and that simple programs remain easy to verify. We also show that the framework is applicable to real, security- and safety-critical code by formally verifying the memory allocator of the L4 microkernel.",
    "keywords": "C, separation logic, interactive theorem proving",
    "URL": "http://doi.acm.org/10.1145/1190216.1190234",
    "DOI": "10.1145/1190216.1190234",
    "publisher-place": "New York, NY, USA",
    "page": "97-108",
    "page-first": "97",
    "note": "event-place: Nice, France",
    "_line": "FormalBib.bib:7047"
  },
  "ross_exterminators_2005-1": {
    "id": "ross_exterminators_2005-1",
    "type": "article-journal",
    "author": [
      {
        "family": "Ross",
        "given": "P.E."
      }
    ],
    "title": "The exterminators \\[software bugs",
    "container-title": "IEEE Spectrum",
    "container-title-short": "IEEE Spectr.",
    "issued": {
      "date-parts": [
        [
          "2005",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          "9",
          "24"
        ]
      ]
    },
    "issn": "0018-9235",
    "URL": "http://ieeexplore.ieee.org/document/1502527/",
    "DOI": "10.1109/MSPEC.2005.1502527",
    "page": "36-41",
    "page-first": "36",
    "volume": "42",
    "issue": "9",
    "language": "en-US",
    "_line": "FormalBib.bib:7066"
  },
  "zhang_decision_nodate": {
    "id": "zhang_decision_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Zhang",
        "given": "Liao"
      },
      {
        "family": "Blaauwbroek",
        "given": "Lasse"
      },
      {
        "family": "Piotrowski",
        "given": "Bartosz"
      },
      {
        "family": "Kaliszyk",
        "given": "Cezary"
      },
      {
        "family": "Urban",
        "given": "Josef"
      }
    ],
    "title": "Decision Trees for Tactic Prediction in Coq",
    "page": "3",
    "page-first": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:7083"
  },
  "yuan_verified_nodate": {
    "id": "yuan_verified_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Yuan",
        "given": "Shenghao"
      },
      {
        "family": "Talpin",
        "given": "Jean-Pierre"
      }
    ],
    "title": "Verified functional programming of an IoT operating system's bootloader",
    "abstract": "The fault of one device on a grid may incur severe economical or physical damages. Among the many critical components in such IoT devices, the operating system’s bootloader comes ﬁrst to initiate the trusted function of the device on the network. However, a bootloader uses hardware-dependent features that make its functional correctness proof difﬁcult. This paper uses veriﬁed programming to automate the veriﬁcation of both the C libraries and assembly boot-sequence of such a, real-world, bootloader in an operating system for ARM-based IoT devices: RIOT. We ﬁrst deﬁne the ARM ISA speciﬁcation, semantics and properties in F to model its critical assembly code boot sequence. We then use Low , a DSL rendering a Clike memory model in F , to implement the complete bootloader library and verify its functional correctness and memory safety. Other than ﬁxing potential faults and vulnerabilities in the source C and ASM bootloader, our evaluation provides an optimized and formally documented code structure, a reasonable speciﬁcation/implementation ratio, a high degree of proof automation and an equally efﬁcient generated code.",
    "page": "17",
    "page-first": "17",
    "language": "en-US",
    "_line": "FormalBib.bib:7091"
  },
  "danvy_abstracting_1990": {
    "id": "danvy_abstracting_1990",
    "type": "paper-conference",
    "author": [
      {
        "family": "Danvy",
        "given": "Olivier"
      },
      {
        "family": "Filinski",
        "given": "Andrzej"
      }
    ],
    "title": "Abstracting control",
    "container-title": "Proceedings of the 1990 ACM conference on LISP and functional programming",
    "collection-title": "LFP '90",
    "issued": {
      "date-parts": [
        [
          "1990",
          "5",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "14"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-0-89791-368-3",
    "abstract": "The last few years have seen a renewed interest in continuations for expressing advanced control structures in programming languages, and new models such as Abstract Continuations have been proposed to capture these dimensions. This article investigates an alternative formulation, exploiting the latent expressive power of the standard continuation-passing style (CPS) instead of introducing yet other new concepts. We build on a single foundation: abstracting control as a hierarchy of continuations, each one modeling a specific language feature as acting on nested evaluation contexts. We show how iterating the continuation-passing conversion allows us to specify a wide range of control behavior. For example, two conversions yield an abstraction of Prolog-style backtracking. A number of other constructs can likewise be expressed in this framework; each is defined independently of the others, but all are arranged in a hierarchy making any interactions between them explicit. This approach preserves all the traditional results about CPS, e.g., its evaluation order independence. Accordingly, our semantics is directly implementable in a call-by-value language such as Scheme or ML. Furthermore, because the control operators denote simple, typable lambda-terms in CPS, they themselves can be statically typed. Contrary to intuition, the iterated CPS transformation does not yield huge results: except where explicitly needed, all continuations beyond the first one disappear due to the extensionality principle (η-reduction). Besides presenting a new motivation for control operators, this paper also describes an improved conversion into applicative-order CPS. The conversion operates in one pass by performing all administrative reductions at translation time; interestingly, it can be expressed very concisely using the new control operators. The paper also presents some examples of nondeterministic programming in direct style.",
    "URL": "https://doi.org/10.1145/91556.91622",
    "DOI": "10.1145/91556.91622",
    "publisher-place": "New York, NY, USA",
    "page": "151-160",
    "page-first": "151",
    "_line": "FormalBib.bib:7100"
  },
  "sivaramakrishnan_retrofitting_2021": {
    "id": "sivaramakrishnan_retrofitting_2021",
    "type": "chapter",
    "author": [
      {
        "family": "Sivaramakrishnan",
        "given": "KC"
      },
      {
        "family": "Dolan",
        "given": "Stephen"
      },
      {
        "family": "White",
        "given": "Leo"
      },
      {
        "family": "Kelly",
        "given": "Tom"
      },
      {
        "family": "Jaffer",
        "given": "Sadiq"
      },
      {
        "family": "Madhavapeddy",
        "given": "Anil"
      }
    ],
    "title": "Retrofitting effect handlers onto OCaml",
    "container-title": "Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
    "issued": {
      "date-parts": [
        [
          "2021",
          "6",
          "19"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "14"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8391-2",
    "abstract": "Effect handlers have been gathering momentum as a mechanism for modular programming with user-defined effects. Effect handlers allow for non-local control flow mechanisms such as generators, async/await, lightweight threads and coroutines to be composably expressed. We present a design and evaluate a full-fledged efficient implementation of effect handlers for OCaml, an industrial-strength multi-paradigm programming language. Our implementation strives to maintain the backwards compatibility and performance profile of existing OCaml code. Retrofitting effect handlers onto OCaml is challenging since OCaml does not currently have any non-local control flow mechanisms other than exceptions. Our implementation of effect handlers for OCaml: (i) imposes a mean 1&perc; overhead on a comprehensive macro benchmark suite that does not use effect handlers; (ii) remains compatible with program analysis tools that inspect the stack; and (iii) is efficient for new code that makes use of effect handlers.",
    "keywords": "Backtraces, Backwards compatibility, Continuations, Effect handlers, Fibers",
    "DOI": "10.1145/3453483.3454039",
    "publisher-place": "New York, NY, USA",
    "page": "206-221",
    "page-first": "206",
    "_line": "FormalBib.bib:7117"
  },
  "kulik_survey_2021": {
    "id": "kulik_survey_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Kulik",
        "given": "Tomas"
      },
      {
        "family": "Dongol",
        "given": "Brijesh"
      },
      {
        "family": "Larsen",
        "given": "Peter Gorm"
      },
      {
        "family": "Macedo",
        "given": "Hugo Daniel"
      },
      {
        "family": "Schneider",
        "given": "Steve"
      },
      {
        "family": "Tran-Jørgensen",
        "given": "Peter Würtz Vinther"
      },
      {
        "family": "Woodcock",
        "given": "Jim"
      }
    ],
    "title": "A Survey of Practical Formal Methods for Security",
    "container-title": "arXiv:2109.01362 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "3"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "13"
        ]
      ]
    },
    "abstract": "In today's world, critical infrastructure is often controlled by computing systems. This introduces new risks for cyber attacks, which can compromise the security and disrupt the functionality of these systems. It is therefore necessary to build such systems with strong guarantees of resiliency against cyber attacks. One way to achieve this level of assurance is using formal verification, which provides proofs of system compliance with desired cyber security properties. The use of Formal Methods (FM) in aspects of cyber security and safety-critical systems are reviewed in this article. We split FM into the three main classes: theorem proving, model checking and lightweight FM. To allow the different uses of FM to be compared, we define a common set of terms. We further develop categories based on the type of computing system FM are applied in. Solutions in each class and category are presented, discussed, compared and summarised. We describe historical highlights and developments and present a state-of-the-art review in the area of FM in cyber security. This review is presented from the point of view of FM practitioners and researchers, commenting on the trends in each of the classes and categories. This is achieved by considering all types of FM, several types of security and safety critical systems and by structuring the taxonomy accordingly. The article hence provides a comprehensive overview of FM and techniques available to system designers of security-critical systems, simplifying the process of choosing the right tool for the task. The article concludes by summarising the discussion of the review, focusing on best practices, challenges, general future trends and directions of research within this field.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Formal Languages and Automata Theory",
    "URLtext": "2109.01362",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2109.01362",
    "URL": "http://arxiv.org/abs/2109.01362",
    "_line": "FormalBib.bib:7133"
  },
  "de_boer_formal_nodate": {
    "id": "de_boer_formal_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Boer",
        "dropping-particle": "de"
      }
    ],
    "title": "Formal analysis of the Java Collections framework",
    "page": "108",
    "page-first": "108",
    "language": "en-US",
    "_line": "FormalBib.bib:7147"
  },
  "mine_octagon_2007": {
    "id": "mine_octagon_2007",
    "type": "article-journal",
    "author": [
      {
        "family": "Miné",
        "given": "Antoine"
      }
    ],
    "title": "The Octagon Abstract Domain",
    "container-title": "arXiv:cs/0703084",
    "issued": {
      "date-parts": [
        [
          "2007",
          "3",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "8"
        ]
      ]
    },
    "abstract": "This article presents a new numerical abstract domain for static analysis by abstract interpretation. It extends a former numerical abstract domain based on Difference-Bound Matrices and allows us to represent invariants of the form (+/-x+/-y&lt;=c), where x and y are program variables and c is a real constant. We focus on giving an efficient representation based on Difference-Bound Matrices - O(n2) memory cost, where n is the number of variables - and graph-based algorithms for all common abstract operators - O(n3) time cost. This includes a normal form algorithm to test equivalence of representation and a widening operator to compute least fixpoint approximations.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "cs/0703084",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "cs/0703084",
    "URL": "http://arxiv.org/abs/cs/0703084",
    "_line": "FormalBib.bib:7155"
  },
  "watt_two_nodate": {
    "id": "watt_two_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Watt",
        "given": "Conrad"
      },
      {
        "family": "Rao",
        "given": "Xiaojia"
      },
      {
        "family": "Pichon-Pharabod",
        "given": "Jean"
      },
      {
        "family": "Bodin",
        "given": "Martin"
      },
      {
        "family": "Gardner",
        "given": "Philippa"
      }
    ],
    "title": "Two Mechanisations of WebAssembly 1.0",
    "abstract": "WebAssembly (Wasm) is a new bytecode language supported by all major Web browsers, designed primarily to be an eﬃcient compilation target for low-level languages such as C/C++ and Rust. It is unusual in that it is oﬃcially speciﬁed through a formal semantics. An initial draft speciﬁcation was published in 2017 \\[14\\], with an associated mechanised speciﬁcation in Isabelle/HOL published by Watt that found bugs in the original speciﬁcation, ﬁxed before its publication \\[37\\].",
    "page": "19",
    "page-first": "19",
    "language": "en-US",
    "_line": "FormalBib.bib:7169"
  },
  "wang_concurrent_2021": {
    "id": "wang_concurrent_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Wang",
        "given": "Shangbei"
      }
    ],
    "title": "Concurrent matching logic",
    "container-title": "arXiv:2109.00319 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "8"
        ]
      ]
    },
    "abstract": "Abstract. Matching logic cannot handle concurrency. We introduce concurrent matching logic (CML) to reason about fault-free partial correctness of shared-memory concurrent programs. We also present a soundness proof for concurrent matching logic (CML) in terms of operational semantics. Under certain assumptions, the assertion of CSL can be transformed into the assertion of CML. Hence, CSL can be seen as an instance of CML.",
    "keywords": "Computer Science - Logic in Computer Science, Computer Science - Computer Science and Game Theory",
    "URLtext": "2109.00319",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2109.00319",
    "URL": "http://arxiv.org/abs/2109.00319",
    "_line": "FormalBib.bib:7178"
  },
  "raad_local_nodate": {
    "id": "raad_local_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Raad",
        "given": "Azalea"
      },
      {
        "family": "Berdine",
        "given": "Josh"
      },
      {
        "family": "Dang",
        "given": "Hoang-Hai"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      },
      {
        "family": "O’Hearn",
        "given": "Peter"
      },
      {
        "family": "Villard",
        "given": "Jules"
      }
    ],
    "title": "Local Reasoning about the Presence of Bugs: Incorrectness Separation Logic",
    "abstract": "There has been a large body of work on local reasoning for proving the absence of bugs, but none for proving their presence. We present a new formal framework for local reasoning about the presence of bugs, building on two complementary foundations: 1) separation logic and 2) incorrectness logic. We explore the theory of this new incorrectness separation logic (ISL), and use it to derive a begin-anywhere, intraprocedural symbolic execution analysis that has no false positives by construction. In so doing, we take a step towards transferring modular, scalable techniques from the world of program veriﬁcation to bug catching.",
    "page": "41",
    "page-first": "41",
    "note": "This is the full paper with appendix.",
    "language": "en-US",
    "_line": "FormalBib.bib:7192"
  },
  "noauthor_local_nodate": {
    "id": "noauthor_local_nodate",
    "type": "webpage",
    "title": "Local Reasoning About the Presence of Bugs: Incorrectness Separation Logic",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "8"
        ]
      ]
    },
    "URL": "http://plv.mpi-sws.org/ISL/",
    "_line": "FormalBib.bib:7202"
  },
  "ohearn_separation_2019": {
    "id": "ohearn_separation_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "O'Hearn",
        "given": "Peter"
      }
    ],
    "title": "Separation logic",
    "container-title": "Communications of the ACM",
    "container-title-short": "Commun. ACM",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "8"
        ]
      ]
    },
    "issn": "0001-0782",
    "abstract": "Separation logic is a key development in formal reasoning about programs, opening up new lines of attack on longstanding problems.",
    "URL": "https://doi.org/10.1145/3211968",
    "DOI": "10.1145/3211968",
    "page": "86-95",
    "page-first": "86",
    "volume": "62",
    "issue": "2",
    "_line": "FormalBib.bib:7209"
  },
  "ohearn_incorrectness_2019": {
    "id": "ohearn_incorrectness_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "O'Hearn",
        "given": "Peter W."
      }
    ],
    "title": "Incorrectness logic",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2019",
          "12",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "7"
        ]
      ]
    },
    "abstract": "Program correctness and incorrectness are two sides of the same coin. As a programmer, even if you would like to have correctness, you might find yourself spending most of your time reasoning about incorrectness. This includes informal reasoning that people do while looking at or thinking about their code, as well as that supported by automated testing and static analysis tools. This paper describes a simple logic for program incorrectness which is, in a sense, the other side of the coin to Hoare's logic of correctness.",
    "keywords": "none",
    "URL": "https://doi.org/10.1145/3371078",
    "DOI": "10.1145/3371078",
    "page": "10:1-10:32",
    "page-first": "10",
    "volume": "4",
    "_line": "FormalBib.bib:7226"
  },
  "le_finding_nodate": {
    "id": "le_finding_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Le",
        "given": "Quang Loc"
      },
      {
        "family": "Raad",
        "given": "Azalea"
      },
      {
        "family": "Villard",
        "given": "Jules"
      },
      {
        "family": "Berdine",
        "given": "Josh"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      },
      {
        "family": "O'Hearn",
        "given": "Peter W"
      }
    ],
    "title": "Finding Real Bugs in Big Programs with Incorrectness Logic",
    "page": "31",
    "page-first": "31",
    "volume": "1",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:7243"
  },
  "boulme_formally_nodate": {
    "id": "boulme_formally_nodate",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Boulmé",
        "given": "Sylvain"
      }
    ],
    "title": "Formally Veriﬁed Defensive Programming",
    "number-of-pages": "144",
    "language": "en-US",
    "_line": "FormalBib.bib:7253"
  },
  "six_compilation_2021": {
    "id": "six_compilation_2021",
    "type": "thesis",
    "genre": "Theses",
    "author": [
      {
        "family": "Six",
        "given": "Cyril"
      }
    ],
    "title": "Compilation optimisante et formellement prouvée pour un processeur VLIW",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "7"
        ]
      ]
    },
    "publisher": "Université Grenoble Alpes",
    "number-of-pages": "258",
    "keywords": "Optimization, Formal Verification, Compilation, Embarqué, Embedded, Optimisations, Vérification Formelle, VLIW",
    "URL": "https://hal.archives-ouvertes.fr/tel-03326923",
    "_line": "FormalBib.bib:7262"
  },
  "devkota_cfgconf_2021": {
    "id": "devkota_cfgconf_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Devkota",
        "given": "Sabin"
      },
      {
        "family": "Legendre",
        "given": "Matthew"
      },
      {
        "family": "Kunen",
        "given": "Adam"
      },
      {
        "family": "Aschwanden",
        "given": "Pascal"
      },
      {
        "family": "Isaacs",
        "given": "Katherine E."
      }
    ],
    "title": "CFGConf: Supporting high level requirements for visualizing Control Flow Graphs",
    "container-title": "arXiv:2108.03047 \\[cs\\]",
    "container-title-short": "CFGConf",
    "title-short": "CFGConf",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "16"
        ]
      ]
    },
    "abstract": "Control Flow Graphs (CFGs) are directed graphs that represent all possible walks a program can take during its execution. CFGs are used to analyze computer programs for purposes such as compilation, performance, and security. They are commonly drawn using hierarchical layouts. However, the general nature of such layouts may not capture CFG-specific structures, making it more difficult to match the drawing to the domain. Domain-specific drawings often require the help of a graph drawing expert, despite the computing expertise of the target audience. To alleviate these issues, we conduct a survey of drawing conventions and needs for CFGs. We then, through an iterative design process, design a flexible set of representations based on these findings and develop CFGConf, a JSON specification for specifying and drawing these higher-level drawing requirements, thereby allowing users to generate and integrate their own CFG-aware graph drawings. The CFGConf language enables the creation of domain-aware graph drawings of CFGs by increasing the notational efficiency of specifying the requirements while also retaining the expressiveness found in commonly used systems such as dot/graphviz. We evaluate CFGConf in terms of notational efficiency, expressiveness, and accessibility through user study and illustrative examples.",
    "keywords": "Computer Science - Human-Computer Interaction",
    "URLtext": "2108.03047",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2108.03047",
    "URL": "http://arxiv.org/abs/2108.03047",
    "_line": "LanguageTools.bib:272"
  },
  "ringer_proof_nodate": {
    "id": "ringer_proof_nodate",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Ringer",
        "given": "Talia"
      }
    ],
    "title": "Proof Repair - Talia Ringer Thesis",
    "number-of-pages": "158",
    "language": "en-US",
    "_line": "FormalBib.bib:7290"
  },
  "lin_trustworthy_nodate": {
    "id": "lin_trustworthy_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Lin",
        "given": "Zhengyao"
      },
      {
        "family": "Chen",
        "given": "Xiaohong"
      },
      {
        "family": "Trinh",
        "given": "Minh-Thai"
      },
      {
        "family": "Wang",
        "given": "John"
      },
      {
        "family": "Roşu",
        "given": "Grigore"
      }
    ],
    "title": "Trustworthy Program Veriﬁcation via Proof Generation",
    "abstract": "In an ideal language framework, language designers only need to deﬁne the formal semantics of their languages. Deductive program veriﬁers and other language tools are automatically generated by the framework. In this paper, we propose a novel approach to establishing the correctness of these autogenerated veriﬁers via proof generation. Our approach is based on the K language framework and its logical foundation, matching logic. Given a formal language semantics in K, we translate it into a corresponding matching logic theory. Then, we encode formal veriﬁcation tasks as reachability formulas in matching logic. The correctness of one veriﬁcation task is then established, on a case-by-case basis, by automatically generating a rigorous, machine-checkable mathematical proof of the associated reachability formula. Experiments with our proof generation prototype on various veriﬁcation tasks in diﬀerent programming languages show promising performance and attest to the feasibility of the proposed approach.",
    "page": "43",
    "page-first": "43",
    "language": "en-US",
    "_line": "FormalBib.bib:7299"
  },
  "bakhirkin_combining_2017": {
    "id": "bakhirkin_combining_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Bakhirkin",
        "given": "Alexey"
      },
      {
        "family": "Monniaux",
        "given": "David"
      }
    ],
    "editor": [
      {
        "family": "Ranzato",
        "given": "Francesco"
      }
    ],
    "title": "Combining Forward and Backward Abstract Interpretation of Horn Clauses",
    "container-title": "24th International Static Analysis Symposium (SAS)",
    "collection-title": "Static Analysis",
    "issued": {
      "date-parts": [
        [
          "2017",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "7"
        ]
      ]
    },
    "publisher": "Springer",
    "abstract": "Alternation of forward and backward analyses is a standard technique in abstract interpretation of programs, which is in particular useful when we wish to prove unreachability of some undesired program states. The current state-of-the-art technique for combining forward (bottom-up, in logic programming terms) and backward (top-down) abstract interpretation of Horn clauses is query-answer transformation. It transforms a system of Horn clauses, such that standard forward analysis can propagate constraints both forward, and backward from a goal. Query-answer transformation is effective, but has issues that we wish to address. For that, we introduce a new backward collecting semantics, which is suitable for alternating forward and backward abstract interpretation of Horn clauses. We show how the alternation can be used to prove unreachability of the goal and how every subsequent run of an analysis yields a refined model of the system. Experimentally, we observe that combining forward and backward analyses is important for analysing systems that encode questions about reachability in C programs. In particular, the combination that follows our new semantics improves the precision of our own abstract interpreter, including when compared to a forward analysis of a query-answer-transformed system.",
    "keywords": "Horn clauses, Static analysis of programs",
    "URL": "https://hal.archives-ouvertes.fr/hal-01551447",
    "publisher-place": "New York City, United States",
    "note": "Backup Publisher: Francesco Ranzato and Patrick Cousot",
    "_line": "FormalBib.bib:7308"
  },
  "pickard_calculating_2021": {
    "id": "pickard_calculating_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Pickard",
        "given": "Mitchell"
      },
      {
        "family": "Hutton",
        "given": "Graham"
      }
    ],
    "title": "Calculating dependently-typed compilers (functional pearl)",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "GRAHAM HUTTON, University of Nottingham, UK Compilers are difficult to write, and difficult to get right. Bahr and Hutton recently developed a new technique for calculating compilers directly from specifications of their correctness, which ensures that the resulting compilers are correct-by-construction. To date, however, this technique has only been applicable to source languages that are untyped. In this article, we show that moving to a dependently-typed setting allows us to naturally support typed source languages, ensure that all compilation components are type-safe, and make the resulting calculations easier to mechanically check using a proof assistant. CCS Concepts: • Software and its engineering → Compilers; • Theory of computation → Type theory; Logic and verification.",
    "URL": "https://dl.acm.org/doi/10.1145/3473587",
    "DOI": "10.1145/3473587",
    "page": "1-27",
    "page-first": "1",
    "volume": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:7325"
  },
  "baudin_dogged_2021": {
    "id": "baudin_dogged_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Baudin",
        "given": "Patrick"
      },
      {
        "family": "Bobot",
        "given": "François"
      },
      {
        "family": "Bühler",
        "given": "David"
      },
      {
        "family": "Correnson",
        "given": "Loïc"
      },
      {
        "family": "Kirchner",
        "given": "Florent"
      },
      {
        "family": "Kosmatov",
        "given": "Nikolai"
      },
      {
        "family": "Maroneze",
        "given": "André"
      },
      {
        "family": "Perrelle",
        "given": "Valentin"
      },
      {
        "family": "Prevosto",
        "given": "Virgile"
      },
      {
        "family": "Signoles",
        "given": "Julien"
      },
      {
        "family": "Williams",
        "given": "Nicky"
      }
    ],
    "title": "The dogged pursuit of bug-free C programs: the Frama-C software analysis platform",
    "container-title": "Communications of the ACM",
    "container-title-short": "The dogged pursuit of bug-free C programs",
    "title-short": "The dogged pursuit of bug-free C programs",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "26"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "issn": "0001-0782",
    "abstract": "A panoramic view of a popular platform for C program analysis and verification.",
    "URL": "https://doi.org/10.1145/3470569",
    "DOI": "10.1145/3470569",
    "page": "56-68",
    "page-first": "56",
    "volume": "64",
    "issue": "8",
    "_line": "FormalBib.bib:7343"
  },
  "scharager_verified_2021": {
    "id": "scharager_verified_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Scharager",
        "given": "Matias"
      },
      {
        "family": "Cordwell",
        "given": "Katherine"
      },
      {
        "family": "Mitsch",
        "given": "Stefan"
      },
      {
        "family": "Platzer",
        "given": "André"
      }
    ],
    "title": "Verified Quadratic Virtual Substitution for Real Arithmetic",
    "container-title": "arXiv:2105.14183 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "abstract": "This paper presents a formally verified quantifier elimination (QE) algorithm for first-order real arithmetic by linear and quadratic virtual substitution (VS) in Isabelle/HOL. The Tarski-Seidenberg theorem established that the first-order logic of real arithmetic is decidable by QE. However, in practice, QE algorithms are highly complicated and often combine multiple methods for performance. VS is a practically successful method for QE that targets formulas with low-degree polynomials. To our knowledge, this is the first work to formalize VS for quadratic real arithmetic including inequalities. The proofs necessitate various contributions to the existing multivariate polynomial libraries in Isabelle/HOL, including a method for re-indexing variables in a polynomial. Our framework is modularized and easily expandable (to facilitate integrating future optimizations), and could serve as a basis for developing a general-purpose QE algorithm. Further, as our formalization is designed with practicality in mind, we export our development to SML and test the resulting code on 378 benchmarks from the literature, comparing to Redlog, Z3, Mathematica, and SMT-RAT.",
    "keywords": "Computer Science - Logic in Computer Science, F.3.1, F.4.1, 03B35, 03C10, 68V20",
    "URLtext": "2105.14183",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.14183",
    "URL": "http://arxiv.org/abs/2105.14183",
    "_line": "FormalBib.bib:7361"
  },
  "mine_static_nodate": {
    "id": "mine_static_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Miné",
        "given": "Antoine"
      },
      {
        "family": "Mastroeni",
        "given": "Isabella"
      },
      {
        "family": "Møller",
        "given": "Anders"
      },
      {
        "family": "Chailloux",
        "given": "Emmanuel"
      },
      {
        "family": "Logozzo",
        "given": "Francesco"
      },
      {
        "family": "Müller",
        "given": "Peter"
      },
      {
        "family": "Schmitt",
        "given": "Alan"
      },
      {
        "family": "Ouadjaout",
        "given": "Abdelraouf"
      },
      {
        "family": "Verona",
        "given": "Università",
        "dropping-particle": "di"
      }
    ],
    "title": "Static Type and Value Analysis by Abstract Interpretation of Python Programs with Native C Libraries",
    "page": "271",
    "page-first": "271",
    "language": "fr-FR",
    "_line": "LanguageTools.bib:1276"
  },
  "dawes_specifying_nodate": {
    "id": "dawes_specifying_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Dawes",
        "given": "Joshua Heneage"
      }
    ],
    "title": "Specifying Properties over Inter-Procedural, Source Code Level Behaviour of Programs",
    "abstract": "The problem of verifying a program at runtime with respect to some formal speciﬁcation has led to the development of a rich collection of speciﬁcation languages. These languages often have a high level of abstraction and provide sophisticated modal operators, giving a high level of expressiveness. In particular, this makes it possible to express properties concerning the source code level behaviour of programs. However, for many languages, the correspondence between events generated at the source code level and parts of the speciﬁcation in question would have to be carefully deﬁned.",
    "page": "20",
    "page-first": "20",
    "language": "en-US",
    "_line": "FormalBib.bib:7385"
  },
  "chen_boosting_2021": {
    "id": "chen_boosting_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Chen",
        "given": "Tianyi"
      },
      {
        "family": "Heo",
        "given": "Kihong"
      },
      {
        "family": "Raghothaman",
        "given": "Mukund"
      }
    ],
    "title": "Boosting static analysis accuracy with instrumented test executions",
    "container-title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
    "collection-title": "ESEC/FSE 2021",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8562-6",
    "abstract": "The two broad approaches to discover properties of programs&mdash;static and dynamic analyses&mdash;have complementary strengths: static techniques perform exhaustive exploration and prove upper bounds on program behaviors, while the dynamic analysis of test cases provides concrete evidence of these behaviors and promise low false alarm rates. In this paper, we present DynaBoost, a system which uses information obtained from test executions to prioritize the alarms of a static analyzer. We instrument the program to dynamically look for dataflow behaviors predicted by the static analyzer, and use these results to bootstrap a probabilistic alarm ranking system, where the user repeatedly inspects the alarm judged most likely to be a real bug, and where the system re-ranks the remaining alarms in response to user feedback. The combined system is able to exploit information that cannot be easily provided by users, and provides significant improvements in the human alarm inspection burden: by 35&perc; compared to the baseline ranking system, and by 89&perc; compared to an unaided programmer triaging alarm reports.",
    "keywords": "Static analysis, alarm ranking, Bayesian inference, belief networks, dynamic analysis",
    "URL": "https://doi.org/10.1145/3468264.3468626",
    "DOI": "10.1145/3468264.3468626",
    "publisher-place": "New York, NY, USA",
    "page": "1154-1165",
    "page-first": "1154",
    "_line": "FormalBib.bib:7394"
  },
  "pizzuti_generating_2021": {
    "id": "pizzuti_generating_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Pizzuti",
        "given": "Federico"
      },
      {
        "family": "Steuwer",
        "given": "Michel"
      },
      {
        "family": "Dubach",
        "given": "Christophe"
      }
    ],
    "title": "Generating high performance code for irregular data structures using dependent types",
    "container-title": "Proceedings of the 9th ACM SIGPLAN International Workshop on Functional High-Performance and Numerical Computing",
    "collection-title": "FHPNC 2021",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8614-2",
    "abstract": "Parallel architectures offer high performance but are challenging to program. Data parallel functional languages offer a solution by providing a high-level programming model to work with accelerators such as GPUs. Existing languages are designed to work with dense arrays, limiting their usefulness in expressing irregular data structures, such as graphs and sparse matrices important in many application domains. This paper addresses this limitation by extending a data-parallel language with limited dependent types, including position dependent arrays and dependent pairs to model irregular data structures. The approach is demonstrated through three case studies: dense to sparse matrix conversion, sparse matrix-vector multiplication, and parallel breadth-first search. Experimental results show that this approach outperforms state-of-the-art implementations on GPUs. Compared to Nvidia’s cuSparse, our automatically generated code achieves an average speedup of 1.2× for dense to sparse matrix conversion and 1.3× for sparse matrix-vector multiplication.",
    "keywords": "Dependent Types, Irregular Data Structures",
    "URL": "https://doi.org/10.1145/3471873.3472977",
    "DOI": "10.1145/3471873.3472977",
    "publisher-place": "New York, NY, USA",
    "page": "37-49",
    "page-first": "37",
    "_line": "FormalBib.bib:7412"
  },
  "punchihewa_safe_2021": {
    "id": "punchihewa_safe_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Punchihewa",
        "given": "Hashan"
      },
      {
        "family": "Wu",
        "given": "Nicolas"
      }
    ],
    "title": "Safe mutation with algebraic effects",
    "container-title": "Proceedings of the 14th ACM SIGPLAN International Symposium on Haskell",
    "event-title": "ICFP '21: 26th ACM SIGPLAN International Conference on Functional Programming",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-8615-9",
    "abstract": "It can be difficult to write safe concurrent programs which use shared mutable state. Subtle mistakes can lead to data races that manifest as unexpected program behaviour. The prevailing approaches to solving this dilemma are to either eschew mutable state altogether, or design bespoke languages that prevent data races by design. This article introduces a third approach by showing how safe mutation can be integrated into a mainstream functional programming language with algebraic effects. This article produces a framework that tracks the use of mutable state and guarantees data race freedom at compile time.",
    "URL": "https://dl.acm.org/doi/10.1145/3471874.3472988",
    "DOI": "10.1145/3471874.3472988",
    "publisher-place": "Virtual Republic of Korea",
    "page": "122-135",
    "page-first": "122",
    "language": "en-US",
    "_line": "FormalBib.bib:7430"
  },
  "eisenberg_existential_2021": {
    "id": "eisenberg_existential_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Eisenberg",
        "given": "Richard A."
      },
      {
        "family": "Duboc",
        "given": "Guillaume"
      },
      {
        "family": "Weirich",
        "given": "Stephanie"
      },
      {
        "family": "Lee",
        "given": "Daniel"
      }
    ],
    "title": "An existential crisis resolved: type inference for first-class existential types",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "An existential crisis resolved",
    "title-short": "An existential crisis resolved",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "abstract": "Despite the great success of inferring and programming with universal types, their dual—existential types—are much harder to work with. Existential types are useful in building abstract types, working with indexed types, and providing first-class support for refinement types. This paper, set in the context of Haskell, presents a bidirectional type-inference algorithm that infers where to introduce and eliminate existentials without any annotations in terms, along with an explicitly typed, type-safe core language usable as a compilation target. This approach is backward compatible. The key ingredient is to use strong existentials, which support (lazily) projecting out the encapsulated data, not weak existentials accessible only by pattern-matching.",
    "keywords": "Haskell, existential types, type inference",
    "URL": "https://doi.org/10.1145/3473569",
    "DOI": "10.1145/3473569",
    "page": "64:1-64:29",
    "page-first": "64",
    "volume": "5",
    "_line": "FormalBib.bib:7448"
  },
  "zakowski_modular_2021": {
    "id": "zakowski_modular_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Zakowski",
        "given": "Yannick"
      },
      {
        "family": "Beck",
        "given": "Calvin"
      },
      {
        "family": "Yoon",
        "given": "Irene"
      },
      {
        "family": "Zaichuk",
        "given": "Ilia"
      },
      {
        "family": "Zaliva",
        "given": "Vadim"
      },
      {
        "family": "Zdancewic",
        "given": "Steve"
      }
    ],
    "title": "Modular, compositional, and executable formal semantics for LLVM IR",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "abstract": "This paper presents a novel formal semantics, mechanized in Coq, for a large, sequential subset of the LLVM IR. In contrast to previous approaches, which use relationally-specified operational semantics, this new semantics is based on monadic interpretation of interaction trees, a structure that provides a more compositional approach to defining language semantics while retaining the ability to extract an executable interpreter. Our semantics handles many of the LLVM IR's non-trivial language features and is constructed modularly in terms of event handlers, including those that deal with nondeterminism in the specification. We show how this semantics admits compositional reasoning principles derived from the interaction trees equational theory of weak bisimulation, which we extend here to better deal with nondeterminism, and we use them to prove that the extracted reference interpreter faithfully refines the semantic model. We validate the correctness of the semantics by evaluating it on unit tests and LLVM IR programs generated by HELIX.",
    "keywords": "LLVM, Coq, Semantics, Monads, Verified Compilation",
    "URL": "https://doi.org/10.1145/3473572",
    "DOI": "10.1145/3473572",
    "page": "67:1-67:30",
    "page-first": "67",
    "volume": "5",
    "_line": "FormalBib.bib:7466"
  },
  "su_conditional_2021": {
    "id": "su_conditional_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Su",
        "given": "Jie"
      },
      {
        "family": "Tian",
        "given": "Cong"
      },
      {
        "family": "Duan",
        "given": "Zhenhua"
      }
    ],
    "title": "Conditional interpolation: making concurrent program verification more effective",
    "container-title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
    "container-title-short": "Conditional interpolation",
    "title-short": "Conditional interpolation",
    "event-title": "ESEC/FSE '21: 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-8562-6",
    "abstract": "Due to the state-space explosion problem, efficient verification of real-world programs in large scale is still a big challenge. Particularly, thread alternation makes the verification of concurrent programs much more difficult since it aggravates this problem. In this paper, an application of Craig interpolation, namely conditional interpolation, is proposed to work together with CEGAR-based approach to reduce the state-space of concurrent tasks. Specifically, conditional interpolation is formalized to confine the reachable region of states so that infeasible conditional branches could be pruned. Furthermore, the generated conditional interpolants are utilized to shorten the interpolation paths, which makes the time consumed for verification significantly reduced. We have implemented the proposed approach on top of an open-source software model checker. Empirical results show that the conditional interpolation is effective in improving the verification efficiency of concurrent tasks.",
    "URL": "https://dl.acm.org/doi/10.1145/3468264.3468602",
    "DOI": "10.1145/3468264.3468602",
    "publisher-place": "Athens Greece",
    "page": "144-154",
    "page-first": "144",
    "language": "en-US",
    "_line": "FormalBib.bib:7483"
  },
  "chlipala_skipping_nodate": {
    "id": "chlipala_skipping_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Chlipala",
        "given": "Adam"
      }
    ],
    "title": "Skipping the Binder Bureaucracy with Mixed Embeddings in a Semantics Course (Functional Pearl)",
    "abstract": "ADAM CHLIPALA, MIT, USA Rigorous reasoning about programs calls for some amount of bureaucracy in managing details like variable binding, but, in guiding students through big ideas in semantics, we might hope to minimize the overhead. We describe our experiment introducing a range of such ideas, using the Coq proof assistant, without any explicit representation of variables, instead using a higher-order syntax encoding that we dub łmixed embeddingž: it is neither the fully explicit syntax of deep embeddings nor the syntax-free programming of shallow embeddings. Marquee examples include different takes on concurrency reasoning, including in the traditions of model checking (partial-order reduction), program logics (concurrent separation logic), and type checking (session types) ś all presented without any side conditions on variables. CCS Concepts: • Theory of computation → Program semantics; Program reasoning.",
    "page": "28",
    "page-first": "28",
    "volume": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:7502"
  },
  "mcbride_applicative_2008": {
    "id": "mcbride_applicative_2008",
    "type": "article-journal",
    "author": [
      {
        "family": "Mcbride",
        "given": "Conor"
      },
      {
        "family": "Paterson",
        "given": "Ross"
      }
    ],
    "title": "Applicative programming with effects",
    "container-title": "Journal of Functional Programming",
    "container-title-short": "J. Funct. Prog.",
    "issued": {
      "date-parts": [
        [
          "2008",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "9"
        ]
      ]
    },
    "issn": "0956-7968, 1469-7653",
    "abstract": "In this paper, we introduce Applicative functors—an abstract characterisation of an applicative style of eﬀectful programming, weaker than Monads and hence more widespread. Indeed, it is the ubiquity of this programming pattern that drew us to the abstraction. We retrace our steps in this paper, introducing the applicative pattern by diverse examples, then abstracting it to deﬁne the Applicative type class and introducing a bracket notation which interprets the normal application syntax in the idiom of an Applicative functor. Further, we develop the properties of applicative functors and the generic operations they support. We close by identifying the categorical structure of applicative functors and examining their relationship both with Monads and with Arrows.",
    "URL": "http://www.journals.cambridge.org/abstract_S0956796807006326",
    "DOI": "10.1017/S0956796807006326",
    "volume": "18",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:7512"
  },
  "gratzer_multimodal_2021": {
    "id": "gratzer_multimodal_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Gratzer",
        "given": "Daniel"
      },
      {
        "family": "Kavvos",
        "given": "G. A."
      },
      {
        "family": "Nuyts",
        "given": "Andreas"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      }
    ],
    "title": "Multimodal Dependent Type Theory",
    "container-title": "Logical Methods in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "4"
        ]
      ]
    },
    "issn": "1860-5974",
    "abstract": "We introduce MTT, a dependent type theory which supports multiple modalities. MTT is parametrized by a mode theory which speciﬁes a collection of modes, modalities, and transformations between them. We show that diﬀerent choices of mode theory allow us to use the same type theory to compute and reason in many modal situations, including guarded recursion, axiomatic cohesion, and parametric quantiﬁcation. We reproduce examples from prior work in guarded recursion and axiomatic cohesion, thereby demonstrating that MTT constitutes a simple and usable syntax whose instantiations intuitively correspond to previous handcrafted modal type theories. In some cases, instantiating MTT to a particular situation unearths a previously unknown type theory that improves upon prior systems. Finally, we investigate the metatheory of MTT. We prove the consistency of MTT and establish canonicity through an extension of recent type-theoretic gluing techniques. These results hold irrespective of the choice of mode theory, and thus apply to a wide variety of modal situations.",
    "URL": "https://lmcs.episciences.org/7571",
    "DOI": "10.46298/lmcs-17(3:11)2021",
    "page": "7571",
    "page-first": "7571",
    "volume": "Volume 17, Issue 3",
    "language": "en-US",
    "_line": "FormalBib.bib:7529"
  },
  "tarski_lattice-theoretical_1955": {
    "id": "tarski_lattice-theoretical_1955",
    "type": "article-journal",
    "author": [
      {
        "family": "Tarski",
        "given": "Alfred"
      }
    ],
    "title": "A lattice-theoretical fixpoint theorem and its applications.",
    "container-title": "Pacific Journal of Mathematics",
    "issued": {
      "date-parts": [
        [
          "1955",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "3"
        ]
      ]
    },
    "issn": "0030-8730",
    "abstract": "Pacific Journal of Mathematics",
    "keywords": "06.0X",
    "URL": "https://www.projecteuclid.org/journals/pacific-journal-of-mathematics/volume-5/issue-2/A-lattice-theoretical-fixpoint-theorem-and-its-applications/pjm/1103044538.full",
    "page": "285-309",
    "page-first": "285",
    "volume": "5",
    "note": "Publisher: Pacific Journal of Mathematics, A Non-profit Corporation",
    "issue": "2",
    "_line": "FormalBib.bib:7545"
  },
  "maillard_multiverse_nodate": {
    "id": "maillard_multiverse_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Maillard",
        "given": "Kenji"
      },
      {
        "family": "Margulies",
        "given": "Nicolas"
      },
      {
        "family": "Sozeau",
        "given": "Matthieu"
      },
      {
        "family": "Tabareau",
        "given": "Nicolas"
      },
      {
        "family": "Tanter",
        "given": "Éric"
      }
    ],
    "title": "The Multiverse: Logical Modularity for Proof Assistants",
    "page": "28",
    "page-first": "28",
    "language": "en-US",
    "_line": "FormalBib.bib:7562"
  },
  "mosses_fundamental_2021": {
    "id": "mosses_fundamental_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Mosses",
        "given": "Peter D."
      }
    ],
    "title": "Fundamental Constructs in Programming Languages",
    "container-title": "arXiv:2107.10545 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "27"
        ]
      ]
    },
    "abstract": "Specifying the semantics of a programming language formally can have many benefits. However, it can also require a huge effort. The effort can be significantly reduced by translating language syntax to so-called fundamental constructs (funcons). A translation to funcons is easy to update when the language evolves, and it exposes relationships between individual language constructs. The PLanCompS project has developed an initial collection of funcons (primarily for translation of functional and imperative languages). The behaviour of each funcon is defined, once and for all, using a modular variant of structural operational semantics. The definitions are available online. This paper introduces and motivates funcons. It illustrates translation of language constructs to funcons, and how funcons are defined. It also relates funcons to notation used in previous frameworks, including monadic semantics and action semantics.",
    "keywords": "Computer Science - Programming Languages, D.3.3, F.3.2, D.3.1",
    "URLtext": "2107.10545",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2107.10545",
    "URL": "http://arxiv.org/abs/2107.10545",
    "_line": "FormalBib.bib:7570"
  },
  "chang_shape_2020": {
    "id": "chang_shape_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Chang",
        "given": "Bor-Yuh Evan"
      },
      {
        "family": "Drăgoi",
        "given": "Cezara"
      },
      {
        "family": "Manevich",
        "given": "Roman"
      },
      {
        "family": "Rinetzky",
        "given": "Noam"
      },
      {
        "family": "Rival",
        "given": "Xavier"
      }
    ],
    "title": "Shape Analysis",
    "container-title": "Foundations and Trends® in Programming Languages",
    "container-title-short": "FNT in Programming Languages",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "27"
        ]
      ]
    },
    "issn": "2325-1107, 2325-1131",
    "URL": "http://www.nowpublishers.com/article/Details/PGL-037",
    "DOI": "10.1561/2500000037",
    "page": "1-158",
    "page-first": "1",
    "volume": "6",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:7584"
  },
  "chajed_gojournal_nodate": {
    "id": "chajed_gojournal_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Chajed",
        "given": "Tej"
      },
      {
        "family": "Tassarotti",
        "given": "Joseph"
      },
      {
        "family": "Kaashoek",
        "given": "M Frans"
      },
      {
        "family": "Theng",
        "given": "Mark"
      },
      {
        "family": "Jung",
        "given": "Ralf"
      },
      {
        "family": "Zeldovich",
        "given": "Nickolai"
      }
    ],
    "title": "GoJournal: a verified, concurrent, crash-safe journaling system",
    "abstract": "The main contribution of this paper is GoJournal, a verified, concurrent journaling system that provides atomicity for storage applications, together with Perennial 2.0, a framework for formally specifying and verifying concurrent crash-safe systems. GoJournal’s goal is to bring the advantages of journaling for code to specs and proofs. Perennial 2.0 makes this possible by introducing several techniques to formalize GoJournal’s specification and to manage the complexity in the proof of GoJournal’s implementation. Lifting predicates and crash framing make the specification easy to use for developers, and logically atomic crash specifications allow for modular reasoning in GoJournal, making the proof tractable despite complex concurrency and crash interleavings.",
    "page": "17",
    "page-first": "17",
    "language": "en-US",
    "_line": "FormalBib.bib:7601"
  },
  "moskal_programming_2009": {
    "id": "moskal_programming_2009",
    "type": "paper-conference",
    "author": [
      {
        "family": "Moskal",
        "given": "Michał"
      }
    ],
    "title": "Programming with triggers",
    "container-title": "Proceedings of the 7th International Workshop on Satisfiability Modulo Theories",
    "collection-title": "SMT '09",
    "issued": {
      "date-parts": [
        [
          "2009",
          "8",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "27"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-60558-484-3",
    "abstract": "We give a case study for a Satisfiability Modulo Theories (SMT) solver usage in functional verification of a real world operating system. In particular, we present a view of the E-matching pattern annotations on quantified formulas as a kind of logic programming language, used to encode semantics of the programming language undergoing verification. We postulate a few encoding patterns to be benchmark problems for a possible E-matching alternative. We also describe features required from the SMT solver in deductive software verification scenarios.",
    "keywords": "program verification, SMT, axiomatizations, e-matching, triggers",
    "URL": "https://doi.org/10.1145/1670412.1670416",
    "DOI": "10.1145/1670412.1670416",
    "publisher-place": "New York, NY, USA",
    "page": "20-29",
    "page-first": "20",
    "_line": "FormalBib.bib:7610"
  },
  "parkinson_relationship_2012": {
    "id": "parkinson_relationship_2012",
    "type": "article-journal",
    "author": [
      {
        "family": "Parkinson",
        "given": "Matthew J"
      },
      {
        "family": "Summers",
        "given": "Alexander J"
      }
    ],
    "title": "THE RELATIONSHIP BETWEEN SEPARATION LOGIC AND IMPLICIT DYNAMIC FRAMES",
    "container-title": "Logic Methods in Computer Science",
    "container-title-short": "lmcs",
    "issued": {
      "date-parts": [
        [
          "2012",
          "7",
          "31"
        ]
      ]
    },
    "abstract": "Separation logic is a concise method for specifying programs that manipulate dynamically allocated storage. Partially inspired by separation logic, Implicit Dynamic Frames has recently been proposed, aiming at ﬁrst-order tool support. In this paper, we precisely connect the semantics of these two logics. We deﬁne a logic whose syntax subsumes both that of a standard separation logic, and that of implicit dynamic frames as sub-syntaxes. We deﬁne a total heap semantics for our logic, and, for the separation logic subsyntax, prove it equivalent the standard partial heaps model. In order to deﬁne a semantics which works uniformly for both subsyntaxes, we deﬁne the novel concept of a minimal state extension, which provides a diﬀerent (but equivalent) deﬁnition of the semantics of separation logic implication and magic wand connectives, while also giving a suitable semantics for these connectives in implicit dynamic frames. We show that our resulting semantics agrees with the existing deﬁnition of weakest pre-condition semantics for the implicit dynamic frames fragment. Finally, we show that we can encode the separation logic fragment of our logic into the implicit dynamic frames fragment, preserving semantics. For the connectives typically supported by tools, this shows that separation logic can be faithfully encoded in a ﬁrst-order automatic veriﬁcation tool (Chalice).",
    "URL": "https://lmcs.episciences.org/802",
    "DOI": "10.2168/LMCS-8(3:1)2012",
    "page": "54",
    "page-first": "54",
    "volume": "802",
    "issue": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:7628"
  },
  "bahr_monadic_nodate": {
    "id": "bahr_monadic_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Bahr",
        "given": "Patrick"
      },
      {
        "family": "Hutton",
        "given": "Graham"
      }
    ],
    "title": "Monadic Compiler Calculation",
    "page": "27",
    "page-first": "27",
    "language": "en-US",
    "_line": "FormalBib.bib:7644"
  },
  "el-beheiry_smltocoq_2021": {
    "id": "el-beheiry_smltocoq_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "El-Beheiry",
        "given": "Laila"
      },
      {
        "family": "Reis",
        "given": "Giselle"
      },
      {
        "family": "Karkour",
        "given": "Ammar"
      }
    ],
    "title": "SMLtoCoq: Automated Generation of Coq Specifications and Proof Obligations from SML Programs with Contracts",
    "container-title": "Electronic Proceedings in Theoretical Computer Science",
    "container-title-short": "SMLtoCoq",
    "title-short": "SMLtoCoq",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "14"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "26"
        ]
      ]
    },
    "issn": "2075-2180",
    "abstract": "Formally reasoning about functional programs is supposed to be straightforward and elegant, however, it is not typically done as a matter of course. Reasoning in a proof assistant requires \"reimplementing\" the code in those tools, which is far from trivial. SMLtoCoq provides an automatic translation of SML programs and function contracts into Coq. Programs are translated into Coq specifications, and function contracts into theorems, which can then be formally proved. Using the Equations plugin and other well established Coq libraries, SMLtoCoq is able to translate SML programs without side-effects containing partial functions, structures, functors, records, among others. Additionally, we provide a Coq version of many parts of SML's basis library, so that calls to these libraries are kept almost as is.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science, F.3.1",
    "URLtext": "2107.07664",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2107.07664",
    "URL": "http://arxiv.org/abs/2107.07664",
    "DOI": "10.4204/EPTCS.337.6",
    "page": "71-87",
    "page-first": "71",
    "volume": "337",
    "_line": "FormalBib.bib:7652"
  },
  "franceschino_verified_2021": {
    "id": "franceschino_verified_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Franceschino",
        "given": "Lucas"
      },
      {
        "family": "Pichardie",
        "given": "David"
      },
      {
        "family": "Talpin",
        "given": "Jean-Pierre"
      }
    ],
    "title": "Verified Functional Programming of an Abstract Interpreter",
    "container-title": "arXiv:2107.09472 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "26"
        ]
      ]
    },
    "abstract": "Abstract interpreters are complex pieces of software: even if the abstract interpretation theory and companion algorithms are well understood, their implementations are subject to bugs, that might question the soundness of their computations. While some formally verified abstract interpreters have been written in the past, writing and understanding them requires expertise in the use of proof assistants, and requires a non-trivial amount of interactive proofs. This paper presents a formally verified abstract interpreter fully programmed and proved correct in the F\\* verified programming environment. Thanks to F\\* refinement types and SMT prover capabilities we demonstrate a substantial saving in proof effort compared to previous works based on interactive proof assistants. Almost all the code of our implementation, proofs included, written in a functional style, are presented directly in the paper.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2107.09472",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2107.09472",
    "URL": "http://arxiv.org/abs/2107.09472",
    "_line": "FormalBib.bib:7672"
  },
  "sozeau_touring_2021": {
    "id": "sozeau_touring_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Sozeau",
        "given": "Matthieu"
      }
    ],
    "title": "Touring the MetaCoq Project (Invited Paper)",
    "container-title": "Electronic Proceedings in Theoretical Computer Science",
    "container-title-short": "Electron. Proc. Theor. Comput. Sci.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "14"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "26"
        ]
      ]
    },
    "issn": "2075-2180",
    "abstract": "Proof assistants are getting more widespread use in research and industry to provide certified and independently checkable guarantees about theories, designs, systems and implementations. However, proof assistant implementations themselves are seldom verified, although they take a major share of the trusted code base in any such certification effort. In this area, proof assistants based on Higher-Order Logic enjoy stronger guarantees, as self-certified implementations have been available for some years. One cause of this difference is the inherent complexity of dependent type theories together with their extensions with inductive types, universe polymorphism and complex sort systems, and the gap between theory on paper and practical implementations in efficient programming languages. MetaCoq is a collaborative project that aims to tackle these difficulties to provide the first fully-certified realistic implementation of a type checker for the full calculus underlying the Coq proof assistant. To achieve this, we refined the sometimes blurry, if not incorrect, specification and implementation of the system. We show how theoretical tools from this community such as bidirectional type-checking, Tait-Martin-L&bslash;\"of/Takahashi's confluence proof technique and monadic and dependently-typed programming can help construct the following artefacts: a specification of Coq's syntax and type theory, the Polymorphic Cumulative Calculus of (Co)-Inductive Constructions (PCUIC); a monad for the manipulation of raw syntax and interaction with the Coq system; a verification of PCUIC's metatheory, whose main results are the confluence of reduction, type preservation and principality of typing; a realistic, correct and complete type-checker for PCUIC; a sound type and proof erasure procedure from PCUIC to untyped lambda-calculus, i.e., the core of the extraction mechanism of Coq.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "2107.07670",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2107.07670",
    "URL": "http://arxiv.org/abs/2107.07670",
    "DOI": "10.4204/EPTCS.337.2",
    "page": "13-29",
    "page-first": "13",
    "volume": "337",
    "_line": "FormalBib.bib:7686"
  },
  "bhargavan_dy_nodate": {
    "id": "bhargavan_dy_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Bhargavan",
        "given": "Karthikeyan"
      },
      {
        "family": "Bichhawat",
        "given": "Abhishek"
      },
      {
        "family": "Do",
        "given": "Quoc Huy"
      },
      {
        "family": "Hosseyni",
        "given": "Pedram"
      },
      {
        "family": "Küsters",
        "given": "Ralf"
      },
      {
        "family": "Schmitz",
        "given": "Guido"
      },
      {
        "family": "Würtele",
        "given": "Tim"
      }
    ],
    "title": "DY\\* : A Modular Symbolic Veriﬁcation Framework for Executable Cryptographic Protocol Code",
    "abstract": "We present DY , a new formal veriﬁcation framework for the symbolic security analysis of cryptographic protocol code written in the F programming language. Unlike automated symbolic provers, our framework accounts for advanced protocol features like unbounded loops and mutable recursive data structures, as well as low-level implementation details like protocol state machines and message formats, which are often at the root of real-world attacks. Our work extends a long line of research on using dependent type systems for this task, but takes a fundamentally new approach by explicitly modeling the global trace-based semantics within the framework, hence bridging the gap between trace-based and type-based protocol analyses. This approach enables us to uniformly, precisely, and soundly model, for the ﬁrst time using dependent types, long-lived mutable protocol state, equational theories, ﬁne-grained dynamic corruption, and trace-based security properties like forward secrecy and post-compromise security. DY is built as a library of F modules that includes a model of low-level protocol execution, a Dolev-Yao symbolic attacker, and generic security abstractions and lemmas, all veriﬁed using F . The library exposes a high-level API that facilitates succinct security proofs for protocol code. We demonstrate the effectiveness of this approach through a detailed symbolic security analysis of the Signal protocol that is based on an interoperable implementation of the protocol from prior work, and is the ﬁrst mechanized proof of Signal to account for forward and post-compromise security over an unbounded number of protocol rounds.",
    "page": "20",
    "page-first": "20",
    "language": "en-US",
    "_line": "FormalBib.bib:7705"
  },
  "rastogi_programming_nodate": {
    "id": "rastogi_programming_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Rastogi",
        "given": "Aseem"
      }
    ],
    "title": "Programming and Proving with Indexed Effects",
    "page": "28",
    "page-first": "28",
    "language": "en-US",
    "_line": "FormalBib.bib:7714"
  },
  "zhao_formal_2013": {
    "id": "zhao_formal_2013",
    "type": "paper-conference",
    "author": [
      {
        "family": "Zhao",
        "given": "Jianzhou"
      },
      {
        "family": "Nagarakatte",
        "given": "Santosh"
      },
      {
        "family": "Martin",
        "given": "Milo M.K."
      },
      {
        "family": "Zdancewic",
        "given": "Steve"
      }
    ],
    "title": "Formal verification of SSA-based optimizations for LLVM",
    "container-title": "Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation",
    "collection-title": "PLDI '13",
    "issued": {
      "date-parts": [
        [
          "2013",
          "6",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "21"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-2014-6",
    "abstract": "Modern compilers, such as LLVM and GCC, use a static single assignment(SSA) intermediate representation (IR) to simplify and enable many advanced optimizations. However, formally verifying the correctness of SSA-based optimizations is challenging because SSA properties depend on a function's entire control-flow graph. This paper addresses this challenge by developing a proof technique for proving SSA-based program invariants and compiler optimizations. We use this technique in the Coq proof assistant to create mechanized correctness proofs of several \"micro\" transformations that form the building blocks for larger SSA optimizations. To demonstrate the utility of this approach, we formally verify a variant of LLVM's mem2reg transformation in Vellvm, a Coq-based formal semantics of the LLVM IR. The extracted implementation generates code with performance comparable to that of LLVM's unverified implementation.",
    "keywords": "coq, llvm, single static assignment",
    "URL": "https://doi.org/10.1145/2491956.2462164",
    "DOI": "10.1145/2491956.2462164",
    "publisher-place": "New York, NY, USA",
    "page": "175-186",
    "page-first": "175",
    "_line": "FormalBib.bib:7722"
  },
  "jaiswal_unified_2021": {
    "id": "jaiswal_unified_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Jaiswal",
        "given": "Swati"
      },
      {
        "family": "Khedker",
        "given": "Uday P."
      },
      {
        "family": "Mycroft",
        "given": "Alan"
      }
    ],
    "title": "A Unified Model for Context-Sensitive Program Analyses: The Blind Men and the Elephant",
    "container-title": "ACM Computing Surveys",
    "container-title-short": "A Unified Model for Context-Sensitive Program Analyses",
    "title-short": "A Unified Model for Context-Sensitive Program Analyses",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "20"
        ]
      ]
    },
    "issn": "0360-0300",
    "abstract": "Context-sensitive methods of program analysis increase the precision of interprocedural analysis by achieving the effect of call inlining. These methods have been defined using different formalisms and hence appear as algorithms that are very different from each other. Some methods traverse a call graph top-down, whereas some others traverse it bottom-up first and then top-down. Some define contexts explicitly, whereas some do not. Some of them directly compute data flow values, while some first compute summary functions and then use them to compute data flow values. Further, different methods place different kinds of restrictions on the data flow frameworks supported by them. As a consequence, it is difficult to compare the ideas behind these methods in spite of the fact that they solve essentially the same problem. We argue that these incomparable views are similar to those of blind men describing an elephant, called context sensitivity, and make it difficult for a non-expert reader to form a coherent picture of context-sensitive data flow analysis. We bring out this whole-elephant view of context sensitivity in program analysis by proposing a unified model of context sensitivity that provides a clean separation between computation of contexts and computation of data flow values. Our model captures the essence of context sensitivity and defines simple soundness and precision criteria for context-sensitive methods. It facilitates declarative specifications of context-sensitive methods, insightful comparisons between them, and reasoning about their soundness and precision. We demonstrate this by instantiating our model to many known context-sensitive methods.",
    "keywords": "context sensitivity, flow sensitivity, Interprocedural data flow analysis, interprocedurally valid paths",
    "URL": "https://doi.org/10.1145/3456563",
    "DOI": "10.1145/3456563",
    "page": "114:1-114:37",
    "page-first": "114",
    "volume": "54",
    "issue": "6",
    "_line": "FormalBib.bib:7740"
  },
  "loring_practical_nodate": {
    "id": "loring_practical_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Loring",
        "given": "Blake William"
      }
    ],
    "title": "Practical Dynamic Symbolic Execution for JavaScript",
    "page": "222",
    "page-first": "222",
    "language": "en-US",
    "_line": "FormalBib.bib:7759"
  },
  "saborido_verification_nodate": {
    "id": "saborido_verification_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Saborido",
        "given": "Jorge Blázquez"
      }
    ],
    "title": "Verification of linked data structures in Dafny",
    "abstract": "Formal veri cation is gaining adoption in the parts of the software industry where full correctness of a system is needed. Dafny is a programming language with veri cation capabilities that allows the programmer to formally specify their code and have it veri ed by an underlying theorem prover. Veri cation in Dafny, however, is not always an easy task. The programmer must give a detailed description of the behavior of the program.",
    "page": "68",
    "page-first": "68",
    "language": "en-US",
    "_line": "FormalBib.bib:7767"
  },
  "li_deriving_nodate": {
    "id": "li_deriving_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "John M"
      },
      {
        "family": "Appel",
        "given": "Andrew W"
      }
    ],
    "title": "Deriving Efficient Program Transformations from Rewrite Rules",
    "page": "29",
    "page-first": "29",
    "volume": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:7776"
  },
  "paraskevopoulou_compositional_nodate": {
    "id": "paraskevopoulou_compositional_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Paraskevopoulou",
        "given": "Zoe"
      },
      {
        "family": "Li",
        "given": "John M"
      },
      {
        "family": "Appel",
        "given": "Andrew W"
      }
    ],
    "title": "Compositional Optimizations for CertiCoq",
    "page": "30",
    "page-first": "30",
    "volume": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:7785"
  },
  "noauthor_ieee_nodate-3": {
    "id": "noauthor_ieee_nodate-3",
    "type": "webpage",
    "title": "IEEE Xplore Full-Text PDF:",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "16"
        ]
      ]
    },
    "URL": "https://ieeexplore-ieee-org.rsic.army.mil:3443/stamp/stamp.jsp?tp=&arnumber=9470541&tag=1",
    "_line": "FormalBib.bib:7794"
  },
  "monteiro_model_2021": {
    "id": "monteiro_model_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Monteiro",
        "given": "Felipe R."
      },
      {
        "family": "Gadelha",
        "given": "Mikhail R."
      },
      {
        "family": "Cordeiro",
        "given": "Lucas C."
      }
    ],
    "title": "Model Checking C++ Programs",
    "container-title": "arXiv:2107.01093 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "9"
        ]
      ]
    },
    "abstract": "In the last three decades, memory safety issues in system programming languages such as C or C++ have been one of the significant sources of security vulnerabilities. However, there exist only a few attempts with limited success to cope with the complexity of C++ program verification. Here we describe and evaluate a novel verification approach based on bounded model checking (BMC) and satisfiability modulo theories (SMT) to verify C++ programs formally. Our verification approach analyzes bounded C++ programs by encoding into SMT various sophisticated features that the C++ programming language offers, such as templates, inheritance, polymorphism, exception handling, and the Standard C++ Libraries. We formalize these features within our formal verification framework using a decidable fragment of first-order logic and then show how state-of-the-art SMT solvers can efficiently handle that. We implemented our verification approach on top of ESBMC. We compare ESBMC to LLBMC and DIVINE, which are state-of-the-art verifiers to check C++ programs directly from the LLVM bitcode. Experimental results show that ESBMC can handle a wide range of C++ programs, presenting a higher number of correct verification results. At the same time, it reduces the verification time if compared to LLBMC and DIVINE tools. Additionally, ESBMC has been applied to a commercial C++ application in the telecommunication domain and successfully detected arithmetic overflow errors, potentially leading to security vulnerabilities.",
    "keywords": "Computer Science - Software Engineering",
    "URLtext": "2107.01093",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2107.01093",
    "URL": "http://arxiv.org/abs/2107.01093",
    "_line": "FormalBib.bib:7801"
  },
  "watanabe_certifying_nodate": {
    "id": "watanabe_certifying_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Watanabe",
        "given": "Yasunari"
      },
      {
        "family": "College",
        "given": "Yale-NUS"
      },
      {
        "family": "Gopinathan",
        "given": "Kiran"
      },
      {
        "family": "Pîrlea",
        "given": "George"
      },
      {
        "family": "Polikarpova",
        "given": "Nadia"
      },
      {
        "family": "Sergey",
        "given": "Ilya"
      },
      {
        "family": "College",
        "given": "Yale-NUS"
      }
    ],
    "title": "Certifying the Synthesis of Heap-Manipulating Programs",
    "page": "29",
    "page-first": "29",
    "volume": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:7815"
  },
  "tondwalkar_refinements_2021": {
    "id": "tondwalkar_refinements_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Tondwalkar",
        "given": "Anish"
      },
      {
        "family": "Kolosick",
        "given": "Matthew"
      },
      {
        "family": "Jhala",
        "given": "Ranjit"
      }
    ],
    "title": "Refinements of Futures Past: Higher-Order Specification with Implicit Refinement Types (Extended Version)",
    "container-title": "arXiv:2105.01954 \\[cs\\]",
    "container-title-short": "Refinements of Futures Past",
    "title-short": "Refinements of Futures Past",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "9"
        ]
      ]
    },
    "abstract": "Refinement types decorate types with assertions that enable automatic verification. Like assertions, refinements are limited to binders that are in scope, and hence, cannot express higher-order specifications. Ghost variables circumvent this limitation but are prohibitively tedious to use as the programmer must divine and explicate their values at all call-sites. We introduce Implicit Refinement Types which turn ghost variables into implicit pair and function types, in a way that lets the refinement typechecker automatically synthesize their values at compile time. Implicit Refinement Types further take advantage of refinement type information, allowing them to be used as a lightweight verification tool, rather than merely as a technique to automate programming tasks. We evaluate the utility of Implicit Refinement Types by showing how they enable the modular specification and automatic verification of various higher-order examples including stateful protocols, access control, and resource usage.",
    "keywords": "Computer Science - Programming Languages, F.3.1, F.3.3",
    "URLtext": "2105.01954",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.01954",
    "URL": "http://arxiv.org/abs/2105.01954",
    "language": "en-US",
    "_line": "FormalBib.bib:7824"
  },
  "knuppel_how_2021": {
    "id": "knuppel_how_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Knüppel",
        "given": "Alexander"
      },
      {
        "family": "Schaer",
        "given": "Leon"
      },
      {
        "family": "Schaefer",
        "given": "Ina"
      }
    ],
    "title": "How much Specification is Enough? Mutation Analysis for Software Contracts",
    "container-title": "2021 IEEE/ACM 9th International Conference on Formal Methods in Software Engineering (FormaliSE)",
    "container-title-short": "How much Specification is Enough?",
    "title-short": "How much Specification is Enough?",
    "event-title": "2021 IEEE/ACM 9th International Conference on Formal Methods in Software Engineering (FormaliSE)",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5"
        ]
      ]
    },
    "abstract": "Design-by-contract is a light-weight formal development paradigm, in which object-oriented software is specified with so-called software contracts. Contracts are annotations in the source code that explicitly document intended functional behavior and can be used for verifying correctness of a particular implementation or as test oracles during automatic test case generation. As writing strong specifications is an expensive and error-prone activity due to lack of expertise and tool support, developers are often only willing to write simpler specifications, covering only a fraction of all functional properties. As a consequence, software quality is lowered, or even worse, potential bugs remain undetected during software verification. To give developers a sense of specification coverage, we propose a methodology that considers the degree of incomplete specifications by means of mutation analysis. We consider Java programs annotated with JML and employ the deductive program verifier KEY-2.6.3 to show that this approach is applicable to numerous open-source JML projects from the literature.",
    "keywords": "Writing, Java, Tools, Software quality, Computer bugs, Annotations, Design by Contract, Measurement, Mutation Analysis, Software Quality Metrics",
    "DOI": "10.1109/FormaliSE52586.2021.00011",
    "page": "42-53",
    "page-first": "42",
    "note": "ISSN: 2575-5099",
    "_line": "FormalBib.bib:7840"
  },
  "wolff_modular_nodate": {
    "id": "wolff_modular_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Wolff",
        "given": "Fabian"
      }
    ],
    "title": "Modular Specification and Verification of Closures in Rust",
    "abstract": "Closures are a language feature supported by many mainstream languages, combining the ability to package up references to code blocks with the possibility of capturing state from the environment of the closure’s declaration. Closures are powerful, but complicate understanding and formal reasoning, especially when closure invocations may mutate objects reachable from the captured state or from closure arguments. This paper presents a novel technique for the modular specification and verification of closure-manipulating code in Rust. Our technique combines Rust’s type system guarantees and novel specification features to enable formal verification of rich functional properties. It encodes higher-order concerns into a first-order logic, which enables automation via SMT solvers. Our technique is implemented as an extension of the deductive verifier Prusti, with which we have successfully verified many common idioms of closure usage.",
    "page": "27",
    "page-first": "27",
    "language": "en-US",
    "_line": "FormalBib.bib:7855"
  },
  "myreen_cakeml_2021": {
    "id": "myreen_cakeml_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Myreen",
        "given": "Magnus O"
      }
    ],
    "title": "The CakeML Project's Quest for Ever Stronger Correctness Theorems",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "The CakeML project has developed a proof-producing code generation mechanism for the HOL4 theorem prover, a verified compiler for ML and, using these, a number of verified application programs that are proved correct down to the machine code that runs them (in some cases, even down to the underlying hardware). The purpose of this extended abstract is to tell the story of the project and to point curious readers to publications where they can read more about specific contributions.",
    "page": "10",
    "page-first": "10",
    "language": "en-US",
    "_line": "FormalBib.bib:7864"
  },
  "zhang_verifying_2021": {
    "id": "zhang_verifying_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Zhang",
        "given": "Hengchu"
      },
      {
        "family": "Koh",
        "given": "Nicolas"
      },
      {
        "family": "Li",
        "given": "Yishuai"
      },
      {
        "family": "Beringer",
        "given": "Lennart"
      },
      {
        "family": "Pierce",
        "given": "Benjamin"
      },
      {
        "family": "Honoré",
        "given": "Wolf"
      },
      {
        "family": "Li",
        "given": "Yao"
      },
      {
        "family": "Xia",
        "given": "Li-Yao"
      },
      {
        "family": "Mansky",
        "given": "William"
      },
      {
        "family": "Zdancewic",
        "given": "Steve"
      }
    ],
    "title": "Verifying an HTTP Key-Value Server with Interaction Trees and VST",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "We present a networked key-value server, implemented in C and formally verified in Coq. The server interacts with clients using a subset of the HTTP/1.1 protocol and is specified and verified using interaction trees and the Verified Software Toolchain. The codebase includes a reusable and fully verified C string library that provides 17 standard POSIX string functions and 17 general purpose non-POSIX string functions. For the KVServer socket system calls, we establish a refinement relation between specifications at user-space level and at CertiKOS kernel-space level.",
    "page": "19",
    "page-first": "19",
    "language": "en-US",
    "_line": "FormalBib.bib:7874"
  },
  "polikarpova_synthesis_2021": {
    "id": "polikarpova_synthesis_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Polikarpova",
        "given": "Nadia"
      }
    ],
    "editor": [
      {
        "family": "Cohen",
        "given": "Liron"
      },
      {
        "family": "Kaliszyk",
        "given": "Cezary"
      }
    ],
    "title": "Synthesis of Safe Pointer-Manipulating Programs",
    "container-title": "12th International Conference on Interactive Theorem Proving (ITP 2021)",
    "collection-title": "Leibniz International Proceedings in Informatics (LIPIcs)",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "22"
        ]
      ]
    },
    "publisher": "Schloss Dagstuhl – Leibniz-Zentrum für Informatik",
    "isbn": "978-3-95977-188-7",
    "keywords": "Program Synthesis, Separation Logic, Proof Search",
    "URL": "https://drops.dagstuhl.de/opus/volltexte/2021/13897",
    "DOI": "10.4230/LIPIcs.ITP.2021.2",
    "publisher-place": "Dagstuhl, Germany",
    "page": "2:1-2:1",
    "page-first": "2",
    "volume": "193",
    "note": "ISSN: 1868-8969",
    "_line": "FormalBib.bib:7884"
  },
  "barras_sets_2010": {
    "id": "barras_sets_2010",
    "type": "article-journal",
    "author": [
      {
        "family": "Barras",
        "given": "Bruno"
      }
    ],
    "title": "Sets in Coq, Coq in Sets",
    "container-title": "Journal of Formalized Reasoning",
    "container-title-short": "JFR",
    "issued": {
      "date-parts": [
        [
          "2010",
          "10",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "22"
        ]
      ]
    },
    "issn": "1972-5787",
    "URL": "https://jfr.unibo.it/article/view/1695",
    "DOI": "10.6092/issn.1972-5787/1695",
    "page": "29-48",
    "page-first": "29",
    "volume": "3",
    "note": "Number: 1",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:7904"
  },
  "haslbeck_for_nodate": {
    "id": "haslbeck_for_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Haslbeck",
        "given": "Maximilian P L"
      },
      {
        "family": "Lammich",
        "given": "Peter"
      }
    ],
    "title": "For a Few Dollars More: Verified Fine-Grained Algorithm Analysis Down to LLVM",
    "container-title": "J. ACM",
    "abstract": "MAXIMILIAN P. L. HASLBECK, Technische Universität München, Germany PETER LAMMICH, University of Twente, Netherlands We present a framework to verify both, functional correctness and (amortized) worst-case complexity of practically efficient algorithms. We implemented a stepwise refinement approach, using the novel concept of resource currencies to naturally structure the resource analysis along the refinement chain, and allow a fine-grained analysis of operation counts. Our framework targets the LLVM intermediate representation. We extend its semantics from earlier work with a cost model. As case studies, we verify the amortized constant time push operation on dynamic arrays and the O(n log n) introsort algorithm, and refine them down to efficient LLVM implementations. Our sorting algorithm performs on par with the state-of-the-art implementation found in the GNU C++ Library, and provably satisfies the complexity required by the C++ standard. CCS Concepts: • Theory of computation → Logic and verification; Separation logic; Program semantics; Program verification.",
    "page": "35",
    "page-first": "35",
    "volume": "37",
    "issue": "4",
    "language": "en-US",
    "_line": "FormalBib.bib:7923"
  },
  "chen_homotopy_2021": {
    "id": "chen_homotopy_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Chen",
        "given": "Joshua"
      }
    ],
    "title": "Homotopy Type Theory in Isabelle",
    "container-title": "arXiv:2002.09282 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "4",
          "19"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "4",
          "25"
        ]
      ]
    },
    "abstract": "This paper introduces Isabelle/HoTT, the first development of homotopy type theory in the Isabelle proof assistant. Building on earlier work by Paulson, I use Isabelle's existing logical framework infrastructure to implement essential automation, such as type checking and term elaboration, that is usually handled on the source code level of dependently typed systems. I also integrate the propositions-as-types paradigm with the declarative Isar proof language, providing an alternative to the tactic-based proofs of Coq and the proof terms of Agda. The infrastructure developed is then used to formalize foundational results from the Homotopy Type Theory book.",
    "keywords": "Computer Science - Logic in Computer Science",
    "URLtext": "2002.09282",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2002.09282",
    "URL": "http://arxiv.org/abs/2002.09282",
    "_line": "FormalBib.bib:7935"
  },
  "smolka_modeling_nodate": {
    "id": "smolka_modeling_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Smolka",
        "given": "Gert"
      }
    ],
    "title": "Modeling and Proving in Computational Type Theory Using the Coq Proof Assistant",
    "page": "338",
    "page-first": "338",
    "language": "en-US",
    "_line": "FormalBib.bib:7949"
  },
  "koenig_compcerto_2021": {
    "id": "koenig_compcerto_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Koenig",
        "given": "Jérémie"
      },
      {
        "family": "Shao",
        "given": "Zhong"
      }
    ],
    "title": "CompCertO: Compiling Certified Open C Components",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Since the introduction of CompCert, researchers have been refining its language semantics and correctness theorem, and used them as components in software verification efforts. Meanwhile, artifacts ranging from CPU designs to network protocols have been successfully verified, and there is interest in making them interoperable to tackle end-to-end verification at an even larger scale. Recent work shows that a synthesis of game semantics, refinement-based methods, and abstraction layers has the potential to serve as a common theory of certified components. Integrating certified compilers to such a theory is a critical goal. However, none of the existing variants of CompCert meets the requirements we have identified for this task.",
    "page": "15",
    "page-first": "15",
    "language": "en-US",
    "_line": "FormalBib.bib:7957"
  },
  "arasu_fastver_nodate": {
    "id": "arasu_fastver_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Arasu",
        "given": "Arvind"
      },
      {
        "family": "Chandramouli",
        "given": "Badrish"
      },
      {
        "family": "Gehrke",
        "given": "Johannes"
      },
      {
        "family": "Ghosh",
        "given": "Esha"
      },
      {
        "family": "Kossmann",
        "given": "Donald"
      },
      {
        "family": "Protzenko",
        "given": "Jonathan"
      },
      {
        "family": "Ramamurthy",
        "given": "Ravi"
      },
      {
        "family": "Ramananandro",
        "given": "Tahina"
      },
      {
        "family": "Rastogi",
        "given": "Aseem"
      },
      {
        "family": "Setty",
        "given": "Srinath"
      },
      {
        "family": "Swamy",
        "given": "Nikhil"
      }
    ],
    "title": "FastVer: Making Data Integrity a Commodity",
    "abstract": "We present FastVer, a high-performance key-value store with strong data integrity guarantees. FastVer is built as an extension of FASTER, an open-source, high-performance key-value store. It offers the same key-value API as FASTER plus an additional verify() method that detects if an unauthorized attacker tampered with the database and checks whether results of all read operations are consistent with historical updates. FastVer is based on a novel approach that combines the advantages of Merkle trees and deferred memory verification. We show that this approach achieves one to two orders of magnitudes higher throughputs than traditional approaches based on either Merkle trees or memory verification. We have formally proven the correctness of our approach in a proof assistant, ensuring that verify() detects any inconsistencies, except if a collision can be found on a cryptographic hash.",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "FormalBib.bib:7967"
  },
  "li_secure_nodate": {
    "id": "li_secure_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "Shih-Wei"
      },
      {
        "family": "Li",
        "given": "Xupeng"
      },
      {
        "family": "Gu",
        "given": "Ronghui"
      },
      {
        "family": "Nieh",
        "given": "Jason"
      },
      {
        "family": "Hui",
        "given": "John Zhuang"
      }
    ],
    "title": "A Secure and Formally Verified Linux KVM Hypervisor",
    "abstract": "Commodity hypervisors are widely deployed to support virtual machines (VMs) on multiprocessor hardware. Their growing complexity poses a security risk. To enable formal verification over such a large codebase, we introduce microverification, a new approach that decomposes a commodity hypervisor into a small core and a set of untrusted services so that we can prove security properties of the entire hypervisor by verifying the core alone. To verify the multiprocessor hypervisor core, we introduce security-preserving layers to modularize the proof without hiding information leakage so we can prove each layer of the implementation refines its specification, and the top layer specification is refined by all layers of the core implementation. To verify commodity hypervisor features that require dynamically changing information flow, we introduce data oracles to mask intentional information flow. We can then prove noninterference at the top layer specification and guarantee the resulting security properties hold for the entire hypervisor implementation. Using microverification, we retrofitted the Linux KVM hypervisor with only modest modifications to its codebase. Using Coq, we proved that the hypervisor protects the confidentiality and integrity of VM data, while retaining KVM’s functionality and performance. Our work is the first machinechecked security proof for a commodity multiprocessor hypervisor.",
    "page": "18",
    "page-first": "18",
    "language": "en-US",
    "_line": "FormalBib.bib:7976"
  },
  "fromherz_steel_nodate": {
    "id": "fromherz_steel_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Fromherz",
        "given": "Aymeric"
      },
      {
        "family": "Rastogi",
        "given": "Aseem"
      },
      {
        "family": "Swamy",
        "given": "Nikhil"
      },
      {
        "family": "Gibson",
        "given": "Sydney"
      },
      {
        "family": "Martínez",
        "given": "Guido"
      },
      {
        "family": "Merigoux",
        "given": "Denis"
      },
      {
        "family": "Ramananandro",
        "given": "Tahina"
      }
    ],
    "title": "Steel: Proof-oriented Programming in a Dependently Typed Concurrent Separation Logic",
    "page": "27",
    "page-first": "27",
    "volume": "1",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:7985"
  },
  "bourgeat_multipurpose_2021": {
    "id": "bourgeat_multipurpose_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bourgeat",
        "given": "Thomas"
      },
      {
        "family": "Clester",
        "given": "Ian"
      },
      {
        "family": "Erbsen",
        "given": "Andres"
      },
      {
        "family": "Gruetter",
        "given": "Samuel"
      },
      {
        "family": "Wright",
        "given": "Andrew"
      },
      {
        "family": "Chlipala",
        "given": "Adam"
      }
    ],
    "title": "A Multipurpose Formal RISC-V Specification",
    "container-title": "arXiv:2104.00762 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "4",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "4",
          "11"
        ]
      ]
    },
    "abstract": "RISC-V is a relatively new, open instruction set architecture with a mature ecosystem and an official formal machine-readable specification. It is therefore a promising playground for formal-methods research. However, we observe that different formal-methods research projects are interested in different aspects of RISC-V and want to simplify, abstract, approximate, or ignore the other aspects. Often, they also require different encoding styles, resulting in each project starting a new formalization from-scratch. We set out to identify the commonalities between projects and to represent the RISC-V specification as a program with holes that can be instantiated differently by different projects. Our formalization of the RISC-V specification is written in Haskell and leverages existing tools rather than requiring new domain-specific tools, contrary to other approaches. To our knowledge, it is the first RISC-V specification able to serve as the interface between a processor-correctness proof and a compiler-correctness proof, while supporting several other projects with diverging requirements as well.",
    "keywords": "Computer Science - Logic in Computer Science",
    "URLtext": "2104.00762",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2104.00762",
    "URL": "http://arxiv.org/abs/2104.00762",
    "_line": "FormalBib.bib:7995"
  },
  "bhargavan_dy_nodate-1": {
    "id": "bhargavan_dy_nodate-1",
    "type": "article-journal",
    "author": [
      {
        "family": "Bhargavan",
        "given": "Karthikeyan"
      },
      {
        "family": "Bichhawat",
        "given": "Abhishek"
      },
      {
        "family": "Do",
        "given": "Quoc"
      },
      {
        "family": "Hosseyni",
        "given": "Pedram"
      },
      {
        "family": "Küsters",
        "given": "Ralf"
      },
      {
        "family": "Schmitz",
        "given": "Guido"
      },
      {
        "family": "Würtele",
        "given": "Tim"
      }
    ],
    "title": "DY\\*: A Modular Symbolic Verification Framework for Executable Cryptographic Protocol Code",
    "abstract": "We present DY , a new formal veriﬁcation framework for the symbolic security analysis of cryptographic protocol code written in the F programming language. Unlike automated symbolic provers, our framework accounts for advanced protocol features like unbounded loops and mutable recursive data structures, as well as low-level implementation details like protocol state machines and message formats, which are often at the root of real-world attacks. Our work extends a long line of research on using dependent type systems for this task, but takes a fundamentally new approach by explicitly modeling the global trace-based semantics within the framework, hence bridging the gap between trace-based and type-based protocol analyses. This approach enables us to uniformly, precisely, and soundly model, for the ﬁrst time using dependent types, long-lived mutable protocol state, equational theories, ﬁne-grained dynamic corruption, and trace-based security properties like forward secrecy and post-compromise security. DY is built as a library of F modules that includes a model of low-level protocol execution, a Dolev-Yao symbolic attacker, and generic security abstractions and lemmas, all veriﬁed using F . The library exposes a high-level API that facilitates succinct security proofs for protocol code. We demonstrate the effectiveness of this approach through a detailed symbolic security analysis of the Signal protocol that is based on an interoperable implementation of the protocol from prior work, and is the ﬁrst mechanized proof of Signal to account for forward and post-compromise security over an unbounded number of protocol rounds.",
    "page": "21",
    "page-first": "21",
    "language": "en-US",
    "_line": "FormalBib.bib:8009"
  },
  "jung_safe_2021": {
    "id": "jung_safe_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Jung",
        "given": "Ralf"
      },
      {
        "family": "Jourdan",
        "given": "Jacques-Henri"
      },
      {
        "family": "Krebbers",
        "given": "Robbert"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      }
    ],
    "title": "Safe systems programming in Rust",
    "container-title": "Communications of the ACM",
    "container-title-short": "Commun. ACM",
    "issued": {
      "date-parts": [
        [
          "2021",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "31"
        ]
      ]
    },
    "issn": "0001-0782, 1557-7317",
    "abstract": "In order to obtain safety, the Rust type system enforces the discipline that a reference is never both aliased and mutable. Having a value of type T means you “own” it fully. The value of type T can be “borrowed” using a mutable reference (&amp;mut T) or shared reference (&amp;T).",
    "URL": "https://dl.acm.org/doi/10.1145/3418295",
    "DOI": "10.1145/3418295",
    "page": "144-152",
    "page-first": "144",
    "volume": "64",
    "issue": "4",
    "language": "en-US",
    "_line": "FormalBib.bib:8018"
  },
  "merigoux_hacspec_nodate": {
    "id": "merigoux_hacspec_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Merigoux",
        "given": "Denis"
      },
      {
        "family": "Kiefer",
        "given": "Franziskus"
      },
      {
        "family": "Bhargavan",
        "given": "Karthikeyan"
      }
    ],
    "title": "Hacspec: succinct, executable, verifiable specifications for high-assurance cryptography embedded in Rust",
    "abstract": "Despite signiﬁcant progress in the formal veriﬁcation of security-critical components like cryptographic libraries and protocols, the secure integration of these components into larger unveriﬁed applications remains an open challenge. The ﬁrst problem is that any memory safety bug or side-channel leak in the unveriﬁed code can nullify the security guarantees of the veriﬁed code. A second issue is that application developers may misunderstand the speciﬁcation and assumptions of the veriﬁed code and so use it incorrectly. In this paper, we propose a novel veriﬁcation framework that seeks to close these gaps for applications written in Rust. At the heart of this framework is hacspec, a new language for writing succinct, executable, formal speciﬁcations for cryptographic components. Syntactically, hacspec is a purely functional subset of Rust that aims to be readable by developers, cryptographers, and veriﬁcation experts. An application developer can use hacspec to specify and prototype cryptographic components in Rust, and then replace this speciﬁcation with a veriﬁed implementation before deployment. We present the hacspec language, its formal semantics and type system, and describe a translation from hacspec to F . We evaluate the language and its toolchain on a library of popular cryptographic algorithms.",
    "page": "17",
    "page-first": "17",
    "language": "en-US",
    "_line": "FormalBib.bib:8036"
  },
  "derakhshan_session_nodate": {
    "id": "derakhshan_session_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Derakhshan",
        "given": "Farzaneh"
      },
      {
        "family": "Balzer",
        "given": "Stephanie"
      },
      {
        "family": "Jia",
        "given": "Limin"
      }
    ],
    "title": "Session Logical Relations for Noninterference",
    "abstract": "Information ﬂow control type systems statically restrict the propagation of sensitive data to ensure end-toend conﬁdentiality. The property to be shown in particular is noninterference, asserting that an attacker cannot infer any secrets from made observations. Session types delimit the kinds of observations that can be made along a communication channel by imposing a protocol of message exchange. These protocols govern the exchange along a single channel and thus leave unconstrained the propagation along adjacent channels. This paper contributes an information ﬂow control type system for linear session types. The type system stands in close correspondence with intuitionistic linear logic. Intuitionistic linear logic typing not only ensures that process conﬁgurations are acyclic but also form a tree such that client processes are parent nodes and provider processes child nodes. To control the propagation of secret messages, the type system is enriched with secrecy levels and arranges these levels to be aligned with the conﬁguration tree. Two levels are associated with every process: the maximal secrecy denoting the process’ security clearance and the running secrecy denoting the highest level of secret information obtained so far. The computational semantics naturally stratiﬁes process conﬁgurations such that higher-secrecy processes are parents of lower-secrecy ones. This invariant is enforced by conﬁguration typing. Noninterference is then stated in terms of a logical relation that is indexed by the secrecy-level-enriched session types. The logical relation contributes a novel development of logical relations for session typed languages as it considers open conﬁgurations, allowing for any closing substitutions as long as they are related by the logical relation.",
    "page": "31",
    "page-first": "31",
    "language": "en-US",
    "_line": "FormalBib.bib:8045"
  },
  "hance_finding_nodate": {
    "id": "hance_finding_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Hance",
        "given": "Travis"
      },
      {
        "family": "Heule",
        "given": "Marijn"
      },
      {
        "family": "Martins",
        "given": "Ruben"
      },
      {
        "family": "Parno",
        "given": "Bryan"
      }
    ],
    "title": "Finding Invariants of Distributed Systems: It’s a Small (Enough) World After All",
    "abstract": "Today’s distributed systems are increasingly complex, leading to subtle bugs that are difﬁcult to detect with standard testing methods. Formal veriﬁcation can provably rule out such bugs, but historically it has been excessively labor intensive. For distributed systems, recent work shows that, given a correct inductive invariant, nearly all other proof work can be automated; however, the construction of such invariants is still a difﬁcult manual task. In this paper, we demonstrate a new methodology for automating the construction of inductive invariants, given as input a (formal) description of the distributed system and a desired safety condition. Our system performs an exhaustive search within a given space of candidate invariants in order to ﬁnd and verify inductive invariants which sufﬁce to prove the safety condition. Central to our ability to search efﬁciently is our algorithm’s ability to learn from counterexamples whenever a candidate fails to be invariant, allowing us to check the remaining candidates more efﬁciently. We hypothesize that many distributed systems, even complex ones, may have concise invariants that make this approach practical, and in support of this, we show that our system is able to identify and verify inductive invariants for the Paxos protocol, which proved too complex for previous work.",
    "page": "17",
    "page-first": "17",
    "language": "en-US",
    "_line": "FormalBib.bib:8054"
  },
  "gauhar_formal_2021": {
    "id": "gauhar_formal_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Gauhar",
        "given": "Ayesha"
      },
      {
        "family": "Rashid",
        "given": "Adnan"
      },
      {
        "family": "Hasan",
        "given": "Osman"
      },
      {
        "family": "Bispo",
        "given": "João"
      },
      {
        "family": "Cardoso",
        "given": "João M. P."
      }
    ],
    "title": "Formal verification of Matrix based MATLAB models using interactive theorem proving",
    "container-title": "PeerJ Computer Science",
    "container-title-short": "PeerJ Comput. Sci.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "3",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "31"
        ]
      ]
    },
    "issn": "2376-5992",
    "abstract": "MATLAB is a software based analysis environment that supports a high-level programing language and is widely used to model and analyze systems in various domains of engineering and sciences. Traditionally, the analysis of MATLAB models is done using simulation and debugging/testing frameworks. These methods provide limited coverage due to their inherent incompleteness. Formal verification can overcome these limitations, but developing the formal models of the underlying MATLAB models is a very challenging and time-consuming task, especially in the case of higher-order-logic models. To facilitate this process, we present a library of higher-order-logic functions corresponding to the commonly used matrix functions of MATLAB as well as a translator that allows automatic conversion of MATLAB models to higher-order logic. The formal models can then be formally verified in an interactive theorem prover. For illustrating the usefulness of the proposed library and approach, we present the formal analysis of a Finite Impulse Response (FIR) filter, which is quite commonly used in digital signal processing applications, within the sound core of the HOL Light theorem prover.",
    "URL": "https://peerj.com/articles/cs-440",
    "DOI": "10.7717/peerj-cs.440",
    "page": "e440",
    "page-first": "e440",
    "volume": "7",
    "note": "Publisher: PeerJ Inc.",
    "language": "en-US",
    "_line": "FormalBib.bib:8063"
  },
  "li_reasoning_2021": {
    "id": "li_reasoning_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "Yao"
      },
      {
        "family": "Xia",
        "given": "Li-yao"
      },
      {
        "family": "Weirich",
        "given": "Stephanie"
      }
    ],
    "title": "Reasoning about the garden of forking paths",
    "container-title": "arXiv:2103.07543 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "3",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "22"
        ]
      ]
    },
    "abstract": "Lazy evaluation is a powerful tool for functional programmers. It enables the concise expression of on-demand computation and a form of compositionality not available under other evaluation strategies. However, the stateful nature of lazy evaluation makes it hard to analyze a program's computational cost, either informally or formally. In this work, we present a novel and simple framework for formally reasoning about lazy computation costs based on a recent model of lazy evaluation: clairvoyant call-by-value. The key feature of our framework is its simplicity, as expressed by our definition of the clairvoyance monad. This monad is both simple to define (around 20 lines of Coq) and simple to reason about. We show that this monad can be effectively used to mechanically reason about the computational cost of lazy functional programs written in Coq.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2103.07543",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2103.07543",
    "URL": "http://arxiv.org/abs/2103.07543",
    "_line": "FormalBib.bib:8081"
  },
  "schmid_proving_2021": {
    "id": "schmid_proving_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Schmid",
        "given": "Georg"
      },
      {
        "family": "Kunčak",
        "given": "Viktor"
      }
    ],
    "title": "Proving and Disproving Programs with Shared Mutable Data",
    "container-title": "arXiv:2103.07699 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "3",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "22"
        ]
      ]
    },
    "abstract": "We present a tool for verification of deterministic programs with shared mutable references against specifications such as assertions, preconditions, postconditions, and read/write effects. We implement our tool by encoding programs with mutable references into annotated purely functional recursive programs. We then rely on function unfolding and the SMT solver Z3 to prove or disprove safety and to establish program termination. Our tool uses a new translation of programs where frame conditions are encoded using quantifier-free formulas in first-order logic (instead of relying on quantifiers or separation logic). This quantifier-free encoding enables SMT solvers to prove safety or report counterexamples relative to the semantics of procedure specifications. Our encoding is possible thanks to the expressive power of the extended array theory of the Z3 SMT solver. In addition to the ability to report counterexamples, our tool retains efficiency of reasoning about purely functional layers of data structures, providing expressiveness for mutable data but also a significant level of automation for purely functional aspects of software. We illustrate our tool through examples manipulating mutable linked structures and arrays.",
    "keywords": "Computer Science - Logic in Computer Science",
    "URLtext": "2103.07699",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2103.07699",
    "URL": "http://arxiv.org/abs/2103.07699",
    "_line": "FormalBib.bib:8095"
  },
  "demeo_agda_2021": {
    "id": "demeo_agda_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "DeMeo",
        "given": "William"
      }
    ],
    "title": "The Agda Universal Algebra Library, Part 2: Structure",
    "container-title": "arXiv:2103.09092 \\[cs, math\\]",
    "container-title-short": "The Agda Universal Algebra Library, Part 2",
    "title-short": "The Agda Universal Algebra Library, Part 2",
    "issued": {
      "date-parts": [
        [
          "2021",
          "3",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "22"
        ]
      ]
    },
    "abstract": "The Agda Universal Algebra Library (UALib) is a library of types and programs (theorems and proofs) we developed to formalize the foundations of universal algebra in dependent type theory using the Agda programming language and proof assistant. The UALib includes a substantial collection of definitions, theorems, and proofs from universal algebra, equational logic, and model theory, and as such provides many examples that exhibit the power of inductive and dependent types for representing and reasoning about mathematical structures and equational theories. In this paper, we describe the the types and proofs of the UALib that concern homomorphisms, terms, and subalgebras.",
    "keywords": "Computer Science - Logic in Computer Science, F.4.1, Mathematics - Logic, 68V20 (Primary) 03C05 (Secondary)",
    "URLtext": "2103.09092",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2103.09092",
    "URL": "http://arxiv.org/abs/2103.09092",
    "_line": "FormalBib.bib:8109"
  },
  "bauer_extensible_2021": {
    "id": "bauer_extensible_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bauer",
        "given": "Andrej"
      },
      {
        "family": "Petković",
        "given": "Anja"
      }
    ],
    "title": "An extensible equality checking algorithm for dependent type theories",
    "container-title": "arXiv:2103.07397 \\[cs, math\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "3",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "22"
        ]
      ]
    },
    "abstract": "We present a general and user-extensible equality checking algorithm that is applicable to a large class of type theories. The algorithm has a type-directed phase for applying extensionality rules and a normalization phase based on computation rules, where both kinds of rules are defined using the type-theoretic concept of object-invertible rules. We also give sufficient syntactic criteria for recognizing such rules, as well as a simple pattern-matching algorithm for applying them. A third component of the algorithm is a suitable notion of principal arguments, which determines a notion of normal form. By varying these, we obtain known notions, such as weak head-normal and strong normal forms. We prove that our algorithm is sound. We implemented it in the Andromeda&tilde;2 proof assistant, which supports user-definable type theories. The user need only provide the equality rules they wish to use, which the algorithm automatically classifies as computation or extensionality rules, and select appropriate principal arguments.",
    "keywords": "Computer Science - Logic in Computer Science, F.4.1, Mathematics - Logic, 03B38 (Primary), 68Q42 (Secondary)",
    "URLtext": "2103.07397",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2103.07397",
    "URL": "http://arxiv.org/abs/2103.07397",
    "_line": "FormalBib.bib:8124"
  },
  "zakowski_equational_2020": {
    "id": "zakowski_equational_2020",
    "type": "paper-conference",
    "author": [
      {
        "family": "Zakowski",
        "given": "Yannick"
      },
      {
        "family": "He",
        "given": "Paul"
      },
      {
        "family": "Hur",
        "given": "Chung-Kil"
      },
      {
        "family": "Zdancewic",
        "given": "Steve"
      }
    ],
    "title": "An equational theory for weak bisimulation via generalized parameterized coinduction",
    "container-title": "Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "collection-title": "CPP 2020",
    "issued": {
      "date-parts": [
        [
          "2020",
          "1",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "15"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-7097-4",
    "abstract": "Coinductive reasoning about infinitary structures such as streams is widely applicable. However, practical frameworks for developing coinductive proofs and finding reasoning principles that help structure such proofs remain a challenge, especially in the context of machine-checked formalization. This paper gives a novel presentation of an equational theory for reasoning about structures up to weak bisimulation. The theory is both compositional, making it suitable for defining general-purpose lemmas, and also incremental, meaning that the bisimulation can be created interactively. To prove the theory’s soundness, this paper also introduces generalized parameterized coinduction, which addresses expressivity problems of earlier works and provides a practical framework for coinductive reasoning. The paper presents the resulting equational theory for streams, but the technique applies to other structures too. All of the results in this paper have been proved in Coq, and the generalized parameterized coinduction framework is available as a Coq library.",
    "keywords": "Coq, coinduction, equational theory, up-to techniques, weak bisimulation",
    "URL": "https://doi.org/10.1145/3372885.3373813",
    "DOI": "10.1145/3372885.3373813",
    "publisher-place": "New York, NY, USA",
    "page": "71-84",
    "page-first": "71",
    "_line": "FormalBib.bib:8138"
  },
  "xia_interaction_2019": {
    "id": "xia_interaction_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Xia",
        "given": "Li-yao"
      },
      {
        "family": "Zakowski",
        "given": "Yannick"
      },
      {
        "family": "He",
        "given": "Paul"
      },
      {
        "family": "Hur",
        "given": "Chung-Kil"
      },
      {
        "family": "Malecha",
        "given": "Gregory"
      },
      {
        "family": "Pierce",
        "given": "Benjamin C."
      },
      {
        "family": "Zdancewic",
        "given": "Steve"
      }
    ],
    "title": "Interaction trees: representing recursive and impure programs in Coq",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Interaction trees",
    "title-short": "Interaction trees",
    "issued": {
      "date-parts": [
        [
          "2019",
          "12",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "15"
        ]
      ]
    },
    "abstract": "Interaction trees (ITrees) are a general-purpose data structure for representing the behaviors of recursive programs that interact with their environments. A coinductive variant of “free monads,” ITrees are built out of uninterpreted events and their continuations. They support compositional construction of interpreters from event handlers, which give meaning to events by defining their semantics as monadic actions. ITrees are expressive enough to represent impure and potentially nonterminating, mutually recursive computations, while admitting a rich equational theory of equivalence up to weak bisimulation. In contrast to other approaches such as relationally specified operational semantics, ITrees are executable via code extraction, making them suitable for debugging, testing, and implementing software artifacts that are amenable to formal verification. We have implemented ITrees and their associated theory as a Coq library, mechanizing classic domain- and category-theoretic results about program semantics, iteration, monadic structures, and equational reasoning. Although the internals of the library rely heavily on coinductive proofs, the interface hides these details so that clients can use and reason about ITrees without explicit use of Coq’s coinduction tactics. To showcase the utility of our theory, we prove the termination-sensitive correctness of a compiler from a simple imperative source language to an assembly-like target whose meanings are given in an ITree-based denotational semantics. Unlike previous results using operational techniques, our bisimulation proof follows straightforwardly by structural induction and elementary rewriting via an equational theory of combinators for control-flow graphs.",
    "keywords": "Coq, coinduction, compiler correctness, monads",
    "URL": "https://doi.org/10.1145/3371119",
    "DOI": "10.1145/3371119",
    "page": "51:1-51:32",
    "page-first": "51",
    "volume": "4",
    "_line": "FormalBib.bib:8156"
  },
  "demeo_agda_2021-1": {
    "id": "demeo_agda_2021-1",
    "type": "article-journal",
    "author": [
      {
        "family": "DeMeo",
        "given": "William"
      }
    ],
    "title": "The Agda Universal Algebra Library, Part 1: Foundation",
    "container-title": "arXiv:2103.05581 \\[cs, math\\]",
    "container-title-short": "The Agda Universal Algebra Library, Part 1",
    "title-short": "The Agda Universal Algebra Library, Part 1",
    "issued": {
      "date-parts": [
        [
          "2021",
          "3",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "15"
        ]
      ]
    },
    "abstract": "The Agda Universal Algebra Library (UALib) is a library of types and programs (theorems and proofs) we developed to formalize the foundations of universal algebra in dependent type theory using the Agda programming language and proof assistant. The UALib includes a substantial collection of definitions, theorems, and proofs from general algebra and equational logic, including many examples that exhibit the power of inductive and dependent types for representing and reasoning about relations, algebraic structures, and equational theories. In this paper we describe several important aspects of the logical foundations on which the library is built. We also discuss (though sometimes only briefly) all of the types defined in the first 13 modules of the library, with special attention given to those details that seem most interesting or challenging from a type theory or mathematical foundations perspective.",
    "keywords": "Computer Science - Logic in Computer Science, F.4.1, Mathematics - Logic, 68V20 (Primary) 03C05 (Secondary)",
    "URLtext": "2103.05581",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2103.05581",
    "URL": "http://arxiv.org/abs/2103.05581",
    "_line": "FormalBib.bib:8174"
  },
  "russinovich_toward_2021": {
    "id": "russinovich_toward_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Russinovich",
        "given": "Mark"
      },
      {
        "family": "Costa",
        "given": "Manuel"
      },
      {
        "family": "Fournet",
        "given": "Cédric"
      },
      {
        "family": "Chisnall",
        "given": "David"
      },
      {
        "family": "Delignat-Lavaud",
        "given": "Antoine"
      },
      {
        "family": "Clebsch",
        "given": "Sylvan"
      },
      {
        "family": "Vaswani",
        "given": "Kapil"
      },
      {
        "family": "Bhatia",
        "given": "Vikas"
      }
    ],
    "title": "Toward Confidential Cloud Computing: Extending hardware-enforced cryptographic protection to data while in use",
    "container-title": "Queue",
    "container-title-short": "Toward Confidential Cloud Computing",
    "title-short": "Toward Confidential Cloud Computing",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "15"
        ]
      ]
    },
    "issn": "1542-7730",
    "abstract": "Although largely driven by economies of scale, the development of the modern cloud also enables increased security. Large data centers provide aggregate availability, reliability, and security assurances. The operational cost of ensuring that operating systems, databases, and other services have secure configurations can be amortized among all tenants, allowing the cloud provider to employ experts who are responsible for security; this is often unfeasible for smaller businesses, where the role of systems administrator is often conflated with many others.",
    "URL": "https://doi.org/10.1145/3454122.3456125",
    "DOI": "10.1145/3454122.3456125",
    "page": "Pages 20:49-Pages 20:76",
    "page-first": "Pages 20:49--Pages 20:76",
    "volume": "19",
    "issue": "1",
    "_line": "FormalBib.bib:8189"
  },
  "li_verification_2020": {
    "id": "li_verification_2020",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Li",
        "given": "Liyi"
      }
    ],
    "title": "A VERIFICATION FRAMEWORK SUITABLE FOR PROVING LARGE LANGUAGE TRANSLATIONS",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "publisher": "University of Illinois at Urbana-Champaign",
    "number-of-pages": "191",
    "language": "en-US",
    "_line": "FormalBib.bib:8207"
  },
  "gheorghiu_provability_2021": {
    "id": "gheorghiu_provability_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Gheorghiu",
        "given": "Alexander"
      },
      {
        "family": "Docherty",
        "given": "Simon"
      },
      {
        "family": "Pym",
        "given": "David"
      }
    ],
    "title": "Provability in BI's Sequent Calculus is Decidable",
    "container-title": "arXiv:2103.02343 \\[cs, math\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "3",
          "3"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "11"
        ]
      ]
    },
    "abstract": "The logic of Bunched Implications (BI) combines both additive and multiplicative connectives, which include two primitive intuitionistic implications. As a consequence, contexts in the sequent presentation are not lists, nor multisets, but rather tree-like structures called bunches. This additional complexity notwithstanding, the logic has a well-behaved metatheory admitting all the familiar forms of semantics and proof systems. However, the presentation of an effective proof-search procedure has been elusive since the logic's debut. We show that one can reduce the proof-search space for any given sequent to a primitive recursive set, the argument generalizing Gentzen's decidability argument for classical propositional logic and combining key features of Dyckhoff's contraction-elimination argument for intuitionistic logic. An effective proof-search procedure, and hence decidability of provability, follows as a corollary.",
    "keywords": "Computer Science - Logic in Computer Science, F.3, Mathematics - Logic, 03B25 (Primary) 03D99, 68W68, 68Q68 (Secondary), Computer Science - Symbolic Computation, F.0, I.1",
    "URLtext": "2103.02343",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2103.02343",
    "URL": "http://arxiv.org/abs/2103.02343",
    "_line": "FormalBib.bib:8218"
  },
  "birkedal_guarded_2017": {
    "id": "birkedal_guarded_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Birkedal",
        "given": "Lars"
      },
      {
        "family": "Bizjak",
        "given": "Aleš"
      },
      {
        "family": "Clouston",
        "given": "Ranald"
      },
      {
        "family": "Grathwohl",
        "given": "Hans Bugge"
      },
      {
        "family": "Spitters",
        "given": "Bas"
      },
      {
        "family": "Vezzosi",
        "given": "Andrea"
      }
    ],
    "title": "Guarded Cubical Type Theory",
    "container-title": "arXiv:1611.09263 \\[cs, math\\]",
    "issued": {
      "date-parts": [
        [
          "2017",
          "10",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "11"
        ]
      ]
    },
    "abstract": "This paper improves the treatment of equality in guarded dependent type theory (GDTT), by combining it with cubical type theory (CTT). GDTT is an extensional type theory with guarded recursive types, which are useful for building models of program logics, and for programming and reasoning with coinductive types. We wish to implement GDTT with decidable type checking, while still supporting non-trivial equality proofs that reason about the extensions of guarded recursive constructions. CTT is a variation of Martin-L&bslash;\"of type theory in which the identity type is replaced by abstract paths between terms. CTT provides a computational interpretation of functional extensionality, enjoys canonicity for the natural numbers type, and is conjectured to support decidable type-checking. Our new type theory, guarded cubical type theory (GCTT), provides a computational interpretation of extensionality for guarded recursive types. This further expands the foundations of CTT as a basis for formalisation in mathematics and computer science. We present examples to demonstrate the expressivity of our type theory, all of which have been checked using a prototype type-checker implementation. We show that CTT can be given semantics in presheaves on the product of the cube category and a small category with an initial object. We then show that the category of presheaves on the product of the cube category and omega provides semantics for GCTT.",
    "keywords": "Computer Science - Logic in Computer Science, F.3.2, F.4.1, Mathematics - Logic, Mathematics - Category Theory, F.3.3, 03B70, 03B15, 55U35",
    "URLtext": "1611.09263",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1611.09263",
    "URL": "http://arxiv.org/abs/1611.09263",
    "_line": "FormalBib.bib:8232"
  },
  "de_boer_finding_2020": {
    "id": "de_boer_finding_2020",
    "type": "chapter",
    "author": [
      {
        "family": "Fava",
        "given": "Daniel Schnetzer"
      }
    ],
    "editor": [
      {
        "family": "Boer",
        "given": "Frank",
        "dropping-particle": "de"
      },
      {
        "family": "Cerone",
        "given": "Antonio"
      }
    ],
    "title": "Finding and Fixing a Mismatch Between the Go Memory Model and Data-Race Detector: A Story on Applied Formal Methods",
    "container-title": "Software Engineering and Formal Methods",
    "container-title-short": "Finding and Fixing a Mismatch Between the Go Memory Model and Data-Race Detector",
    "title-short": "Finding and Fixing a Mismatch Between the Go Memory Model and Data-Race Detector",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "11"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-58767-3 978-3-030-58768-0",
    "abstract": "Go is an open-source programming language developed at Google. In previous works, we presented formalizations for a weak memory model and a data-race detector inspired by the Go speciﬁcation. In this paper, we describe how our theoretical research guided us in the process of ﬁnding and ﬁxing a concrete bug in the language. Speciﬁcally, we discovered and ﬁxed a discrepancy between the Go memory model and the Go data-race detector implementation—the discrepancy led to the under-reporting of data races in Go programs. Here, we share our experience applying formal methods on software that powers infrastructure used by millions of people.",
    "URL": "http://link.springer.com/10.1007/978-3-030-58768-0_2",
    "DOI": "10.1007/978-3-030-58768-0_2",
    "publisher-place": "Cham",
    "page": "24-40",
    "page-first": "24",
    "volume": "12310",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "FormalBib.bib:8246"
  },
  "nie_roosterize_2021": {
    "id": "nie_roosterize_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Nie",
        "given": "Pengyu"
      },
      {
        "family": "Palmskog",
        "given": "Karl"
      },
      {
        "family": "Li",
        "given": "Junyi Jessy"
      },
      {
        "family": "Gligoric",
        "given": "Milos"
      }
    ],
    "title": "Roosterize: Suggesting Lemma Names for Coq Verification Projects Using Deep Learning",
    "container-title": "arXiv:2103.01346 \\[cs\\]",
    "container-title-short": "Roosterize",
    "title-short": "Roosterize",
    "issued": {
      "date-parts": [
        [
          "2021",
          "3",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "8"
        ]
      ]
    },
    "abstract": "Naming conventions are an important concern in large verification projects using proof assistants, such as Coq. In particular, lemma names are used by proof engineers to effectively understand and modify Coq code. However, providing accurate and informative lemma names is a complex task, which is currently often carried out manually. Even when lemma naming is automated using rule-based tools, generated names may fail to adhere to important conventions not specified explicitly. We demonstrate a toolchain, dubbed Roosterize, which automatically suggests lemma names in Coq projects. Roosterize leverages a neural network model trained on existing Coq code, thus avoiding manual specification of naming conventions. To allow proof engineers to conveniently access suggestions from Roosterize during Coq project development, we integrated the toolchain into the popular Visual Studio Code editor. Our evaluation shows that Roosterize substantially outperforms strong baselines for suggesting lemma names and is useful in practice. The demo video for Roosterize can be viewed at: https://youtu.be/HZ5ac7Q14rc.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Software Engineering, Computer Science - Computation and Language",
    "URLtext": "2103.01346",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2103.01346",
    "URL": "http://arxiv.org/abs/2103.01346",
    "_line": "FormalBib.bib:8267"
  },
  "cavallo_higher_2021": {
    "id": "cavallo_higher_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Cavallo",
        "given": "Evan"
      }
    ],
    "title": "Higher Inductive Types and Internal Parametricity for Cubical Type Theory",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Recent innovation in the design of type theories—foundational systems of mathematics with a focus on constructivity—has produced the concept of interval variable, which can be used to capture relations between objects that carry computational content. We examine two such relationships in type theory: equality, in particular quotients, and arbitrary relations as applied in parametricity interpretations.",
    "page": "322",
    "page-first": "322",
    "language": "en-US",
    "_line": "FormalBib.bib:8282"
  },
  "molina_evospex_2021": {
    "id": "molina_evospex_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Molina",
        "given": "Facundo"
      },
      {
        "family": "Ponzio",
        "given": "Pablo"
      },
      {
        "family": "Aguirre",
        "given": "Nazareno"
      },
      {
        "family": "Frias",
        "given": "Marcelo"
      }
    ],
    "title": "EvoSpex: An Evolutionary Algorithm for Learning Postconditions",
    "container-title": "arXiv:2102.13569 \\[cs\\]",
    "container-title-short": "EvoSpex",
    "title-short": "EvoSpex",
    "issued": {
      "date-parts": [
        [
          "2021",
          "3",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "8"
        ]
      ]
    },
    "abstract": "Software reliability is a primary concern in the construction of software, and thus a fundamental component in the definition of software quality. Analyzing software reliability requires a specification of the intended behavior of the software under analysis, and at the source code level, such specifications typically take the form of assertions. Unfortunately, software many times lacks such specifications, or only provides them for scenario-specific behaviors, as assertions accompanying tests. This issue seriously diminishes the analyzability of software with respect to its reliability. In this paper, we tackle this problem by proposing a technique that, given a Java method, automatically produces a specification of the method's current behavior, in the form of postcondition assertions. This mechanism is based on generating executions of the method under analysis to obtain valid pre/post state pairs, mutating these pairs to obtain (allegedly) invalid ones, and then using a genetic algorithm to produce an assertion that is satisfied by the valid pre/post pairs, while leaving out the invalid ones. The technique, which targets in particular methods of reference-based class implementations, is assessed on a benchmark of open source Java projects, showing that our genetic algorithm is able to generate post-conditions that are stronger and more accurate, than those generated by related automated approaches, as evaluated by an automated oracle assessment tool. Moreover, our technique is also able to infer an important part of manually written rich postconditions in verified classes, and reproduce contracts for methods whose class implementations were automatically synthesized from specifications.",
    "keywords": "Computer Science - Software Engineering",
    "URLtext": "2102.13569",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2102.13569",
    "URL": "http://arxiv.org/abs/2102.13569",
    "_line": "FormalBib.bib:8292"
  },
  "majumder_hir_2021": {
    "id": "majumder_hir_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Majumder",
        "given": "Kingshuk"
      },
      {
        "family": "Bondhugula",
        "given": "Uday"
      }
    ],
    "title": "HIR: An MLIR-based Intermediate Representation for Hardware Accelerator Description",
    "container-title": "arXiv:2103.00194 \\[cs\\]",
    "container-title-short": "HIR",
    "title-short": "HIR",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "8"
        ]
      ]
    },
    "abstract": "The emergence of machine learning, image and audio processing on edge devices has motivated research towards power efficient custom hardware accelerators. Though FPGAs are an ideal target for energy efficient custom accelerators, the difficulty of hardware design and the lack of vendor agnostic, standardized hardware compilation infrastructure has hindered their adoption. This paper introduces HIR, an MLIR-based intermediate representation (IR) to describe hardware accelerator designs. HIR combines high level language features, such as loops and multi-dimensional tensors, with programmer defined explicit scheduling, to provide a high-level IR suitable for DSL compiler pipelines without compromising control over the micro-architecture of the accelerator. HIR's explicit schedules allow it to express fine-grained, synchronization-free parallelism and optimizations such as retiming and pipelining. Built as a dialect in MLIR, it draws from best IR practices learnt from communities like those of LLVM. While offering rich optimization opportunities and a high level abstraction, HIR enables sharing of optimizations, utilities and passes with software compiler infrastructure. Our implementation shows that the code generation time of the HIR code generator is on average 1112x lower than that of Xilinx Vivado HLS on a range of kernels without a compromise on the quality of the generated hardware. We believe that these are significant steps forward in the design of IRs for hardware synthesis and in equipping domain-specific languages with a productive and performing compilation path to custom hardware acceleration.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Hardware Architecture",
    "URLtext": "2103.00194",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2103.00194",
    "URL": "http://arxiv.org/abs/2103.00194",
    "_line": "FormalBib.bib:8307"
  },
  "qian_client-server_nodate": {
    "id": "qian_client-server_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Qian",
        "given": "Zesen"
      },
      {
        "family": "Kavvos",
        "given": "G A"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      }
    ],
    "title": "Client-Server Sessions in Linear Logic",
    "page": "28",
    "page-first": "28",
    "volume": "1",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:8322"
  },
  "birkedal_theorems_2021": {
    "id": "birkedal_theorems_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Birkedal",
        "given": "Lars"
      },
      {
        "family": "Dinsdale-Young",
        "given": "Thomas"
      },
      {
        "family": "Guéneau",
        "given": "Armaël"
      },
      {
        "family": "Jaber",
        "given": "Guilhem"
      },
      {
        "family": "Svendsen",
        "given": "Kasper"
      },
      {
        "family": "Tzevelekos",
        "given": "Nikos"
      }
    ],
    "title": "Theorems for Free from Separation Logic Specifications",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "page": "28",
    "page-first": "28",
    "volume": "1",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:8332"
  },
  "marsik_introducing_2021": {
    "id": "marsik_introducing_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Maršík",
        "given": "Jirka"
      },
      {
        "family": "Amblard",
        "given": "Maxime"
      },
      {
        "family": "Groote",
        "given": "Philippe",
        "dropping-particle": "de"
      }
    ],
    "title": "Introducing ⦇ λ ⦈, a λ-calculus for Effectful Computation",
    "container-title": "Theoretical Computer Science",
    "container-title-short": "Theoretical Computer Science",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "5"
        ]
      ]
    },
    "issn": "0304-3975",
    "abstract": "We present ⦇λ⦈, a calculus with special constructions for dealing with effects and handlers. This is an extension of the simply-typed λ-calculus (STLC). We enrich STLC with a type for representing effectful computations alongside with operations to create and process values of this type. The calculus is motivated by natural language modelling, and especially semantic representation. Traditionally, the meaning of a sentence is calculated using λ-terms, but some semantic phenomena need more flexibility. In this article we introduce the calculus and show that the calculus respects the laws of algebraic structures and it enjoys strong normalisation. To do so, confluence is proven using the Combinatory Reduction Systems (CRSs) of Klop and termination using the Inductive Data Type Systems (IDTSs) of Blanqui.",
    "keywords": "monads, -calculus, CRS, handlers, IDTS, side effects",
    "URL": "https://www.sciencedirect.com/science/article/pii/S0304397521001225",
    "DOI": "10.1016/j.tcs.2021.02.038",
    "language": "en-US",
    "_line": "FormalBib.bib:8343"
  },
  "hickman_certifying_2021": {
    "id": "hickman_certifying_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Hickman",
        "given": "Thomas"
      },
      {
        "family": "Laursen",
        "given": "Christian Pardillo"
      },
      {
        "family": "Foster",
        "given": "Simon"
      }
    ],
    "title": "Certifying Differential Equation Solutions from Computer Algebra Systems in Isabelle/HOL",
    "container-title": "arXiv:2102.02679 \\[cs, math\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "1"
        ]
      ]
    },
    "abstract": "The Isabelle/HOL proof assistant has a powerful library for continuous analysis, which provides the foundation for verification of hybrid systems. However, Isabelle lacks automated proof support for continuous artifacts, which means that verification is often manual. In contrast, Computer Algebra Systems (CAS), such as Mathematica and SageMath, contain a wealth of efficient algorithms for matrices, differential equations, and other related artifacts. Nevertheless, these algorithms are not verified, and thus their outputs cannot, of themselves, be trusted for use in a safety critical system. In this paper we integrate two CAS systems into Isabelle, with the aim of certifying symbolic solutions to ordinary differential equations. This supports a verification technique that is both automated and trustworthy.",
    "keywords": "Computer Science - Logic in Computer Science, Mathematics - Dynamical Systems",
    "URLtext": "2102.02679",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2102.02679",
    "URL": "http://arxiv.org/abs/2102.02679",
    "_line": "FormalBib.bib:8359"
  },
  "balabonski_strong_nodate": {
    "id": "balabonski_strong_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Balabonski",
        "given": "Thibaut"
      },
      {
        "family": "Lanco",
        "given": "Antoine"
      },
      {
        "family": "Melquiond",
        "given": "Guillaume"
      }
    ],
    "title": "A strong call-by-need calculus",
    "page": "22",
    "page-first": "22",
    "language": "en-US",
    "_line": "FormalBib.bib:8373"
  },
  "ongaro_search_2014": {
    "id": "ongaro_search_2014",
    "type": "article-journal",
    "author": [
      {
        "family": "Ongaro",
        "given": "Diego"
      },
      {
        "family": "Ousterhout",
        "given": "John"
      }
    ],
    "title": "In Search of an Understandable Consensus Algorithm",
    "issued": {
      "date-parts": [
        [
          "2014",
          "5",
          "20"
        ]
      ]
    },
    "abstract": "Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efﬁcient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.",
    "page": "18",
    "page-first": "18",
    "language": "en-US",
    "_line": "FormalBib.bib:8381"
  },
  "woos_planning_2016": {
    "id": "woos_planning_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Woos",
        "given": "Doug"
      },
      {
        "family": "Wilcox",
        "given": "James R."
      },
      {
        "family": "Anton",
        "given": "Steve"
      },
      {
        "family": "Tatlock",
        "given": "Zachary"
      },
      {
        "family": "Ernst",
        "given": "Michael D."
      },
      {
        "family": "Anderson",
        "given": "Thomas"
      }
    ],
    "title": "Planning for change in a formal verification of the raft consensus protocol",
    "container-title": "Proceedings of the 5th ACM SIGPLAN Conference on Certified Programs and Proofs",
    "collection-title": "CPP 2016",
    "issued": {
      "date-parts": [
        [
          "2016",
          "1",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "1"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-4127-1",
    "abstract": "We present the first formal verification of state machine safety for the Raft consensus protocol, a critical component of many distributed systems. We connected our proof to previous work to establish an end-to-end guarantee that our implementation provides linearizable state machine replication. This proof required iteratively discovering and proving 90 system invariants. Our verified implementation is extracted to OCaml and runs on real networks. The primary challenge we faced during the verification process was proof maintenance, since proving one invariant often required strengthening and updating other parts of our proof. To address this challenge, we propose a methodology of planning for change during verification. Our methodology adapts classical information hiding techniques to the context of proof assistants, factors out common invariant-strengthening patterns into custom induction principles, proves higher-order lemmas that show any property proved about a particular component implies analogous properties about related components, and makes proofs robust to change using structural tactics. We also discuss how our methodology may be applied to systems verification more broadly.",
    "keywords": "Coq, proof assistants, distributed systems, Formal verification, Raft, Verdi",
    "URL": "https://doi.org/10.1145/2854065.2854081",
    "DOI": "10.1145/2854065.2854081",
    "publisher-place": "New York, NY, USA",
    "page": "154-165",
    "page-first": "154",
    "_line": "FormalBib.bib:8391"
  },
  "tusil_hyperproperties_2021": {
    "id": "tusil_hyperproperties_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Tuˇsil",
        "given": "Jan"
      },
      {
        "family": "Chen",
        "given": "Xiaohong"
      },
      {
        "family": "Rosu",
        "given": "Grigore"
      }
    ],
    "title": "Hyperproperties in Matching Logic",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Matching logic is a uniform logic to specify and reason about programming languages and program properties. Many important logics and/or formal systems have been shown to be deﬁnable in matching logic as logical theories. However, no research has been conducted to studying how hyperproperties can be treated in matching logic. In this paper, we give the ﬁrst theoretical result that shows that HyperLTL (hyper linear temporal logic), which is an important temporal logic designed for specifying and reasoning about hyperproperties, can be completely captured by matching logic. Our result demonstrates that matching logic oﬀers a uniform treatment to handling hyperproperties and to supporting their model checking problems.",
    "page": "70",
    "page-first": "70",
    "language": "en-US",
    "_line": "FormalBib.bib:8409"
  },
  "kachapova_formalizing_2021": {
    "id": "kachapova_formalizing_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Kachapova",
        "given": "Farida"
      }
    ],
    "title": "Formalizing relations in type theory",
    "container-title": "arXiv:2102.08595 \\[cs, math\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "25"
        ]
      ]
    },
    "abstract": "Type theory plays an important role in foundations of mathematics as a framework for formalizing mathematics and a base for proof assistants providing semi-automatic proof checking and construction. Derivation of each theorem in type theory results in a formal term encapsulating the whole proof process. In this paper we use a variant of type theory, namely the Calculus of Constructions with Definitions, to formalize the standard theory of binary relations. This includes basic operations on relations, criteria for special properties of relations, invariance of these properties under the basic operations, equivalence relation, well-ordering, and transfinite induction. Definitions and proofs are presented as flag-style derivations.",
    "keywords": "Computer Science - Logic in Computer Science, Mathematics - Logic, 03B30 (Primary) 03B38 (Secondary)",
    "URLtext": "2102.08595",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2102.08595",
    "URL": "http://arxiv.org/abs/2102.08595",
    "_line": "FormalBib.bib:8419"
  },
  "chin_finding_2021": {
    "id": "chin_finding_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Chin",
        "given": "Janice"
      },
      {
        "family": "Pearce",
        "given": "David J"
      }
    ],
    "title": "Finding Bugs with Speciﬁcation-Based Testing is Easy!",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Automated speciﬁcation-based testing has a long history with several notable tools having emerged. For example, QuickCheck for Haskell focuses on testing against user-provided properties. Others, such as JMLUnit, use speciﬁcations in the form of pre- and post-conditions to drive testing. An interesting (and underexplored) question is how eﬀective this approach is at ﬁnding bugs in practice. In general, one would assume automated testing is less eﬀective at bug ﬁnding than static veriﬁcation. But, how much less eﬀective? To shed light on this question, we consider automated testing of programs written in Whiley — a language with ﬁrst-class support for speciﬁcations. Whilst originally designed with static veriﬁcation in mind, we have anecdotally found automated testing for Whiley surprisingly useful and cost-eﬀective. For example, when an error is detected with automated testing, a counterexample is always provided. This has motivated the more rigorous empirical examination presented in this paper. To that end, we provide a technical discussion of the implementation behind an automated testing tool for Whiley. Here, a key usability concern is the ability to parameterise the input space, and we present novel approaches for references and lambdas. We then report on several large experiments investigating the tool’s eﬀectiveness at bug ﬁnding using a range of benchmarks, including a suite of + mutants. The results indicate the automated testing is eﬀective in many cases, and that sampling oﬀers useful performance beneﬁts with only modest reductions in bug-ﬁnding capability. Finally, we report on some real-world uses of the tool where it has proved eﬀective at ﬁnding bugs (such as in the standard library).",
    "page": "35",
    "page-first": "35",
    "language": "en-US",
    "_line": "FormalBib.bib:8433"
  },
  "passmore_imandra_2020": {
    "id": "passmore_imandra_2020",
    "type": "paper-conference",
    "author": [
      {
        "family": "Passmore",
        "given": "Grant"
      },
      {
        "family": "Cruanes",
        "given": "Simon"
      },
      {
        "family": "Ignatovich",
        "given": "Denis"
      },
      {
        "family": "Aitken",
        "given": "Dave"
      },
      {
        "family": "Bray",
        "given": "Matt"
      },
      {
        "family": "Kagan",
        "given": "Elijah"
      },
      {
        "family": "Kanishev",
        "given": "Kostya"
      },
      {
        "family": "Maclean",
        "given": "Ewen"
      },
      {
        "family": "Mometto",
        "given": "Nicola"
      }
    ],
    "editor": [
      {
        "family": "Peltier",
        "given": "Nicolas"
      },
      {
        "family": "Sofronie-Stokkermans",
        "given": "Viorica"
      }
    ],
    "title": "The Imandra Automated Reasoning System (System Description)",
    "container-title": "Automated Reasoning",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-51054-1",
    "abstract": "We describe Imandra, a modern computational logic theorem prover designed to bridge the gap between decision procedures such as SMT, semi-automatic inductive provers of the Boyer-Moore family like ACL2, and interactive proof assistants for typed higher-order logics. Imandra’s logic is computational, based on a pure subset of OCaml in which all functions are terminating, with restrictions on types and higher-order functions that allow conjectures to be translated into multi-sorted first-order logic with theories, including arithmetic and datatypes. Imandra has novel features supporting large-scale industrial applications, including a seamless integration of bounded and unbounded verification, first-class computable counterexamples, efficiently executable models and a cloud-native architecture supporting live multiuser collaboration. The core reasoning mechanisms of Imandra are (i) a semi-complete procedure for finding models of formulas in the logic mentioned above, centered around the lazy expansion of recursive functions, (ii) an inductive waterfall and simplifier which “lifts” many Boyer-Moore ideas to our typed higher-order setting. These mechanisms are tightly integrated and subject to many forms of user control.",
    "DOI": "10.1007/978-3-030-51054-1_30",
    "publisher-place": "Cham",
    "page": "464-471",
    "page-first": "464",
    "language": "en-US",
    "_line": "FormalBib.bib:8443"
  },
  "blom_correct_2021": {
    "id": "blom_correct_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Blom",
        "given": "S."
      },
      {
        "family": "Darabi",
        "given": "S."
      },
      {
        "family": "Huisman",
        "given": "M."
      },
      {
        "family": "Safari",
        "given": "M."
      }
    ],
    "title": "Correct program parallelisations",
    "container-title": "International Journal on Software Tools for Technology Transfer",
    "container-title-short": "Int J Softw Tools Technol Transfer",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "14"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "22"
        ]
      ]
    },
    "issn": "1433-2787",
    "abstract": "A commonly used approach to develop deterministic parallel programs is to augment a sequential program with compiler directives that indicate which program blocks may potentially be executed in parallel. This paper develops a verification technique to reason about such compiler directives, in particular to show that they do not change the behaviour of the program. Moreover, the verification technique is tool-supported and can be combined with proving functional correctness of the program. To develop our verification technique, we propose a simple intermediate representation (syntax and semantics) that captures the main forms of deterministic parallel programs. This language distinguishes three kinds of basic blocks: parallel, vectorised and sequential blocks, which can be composed using three different composition operators: sequential, parallel and fusion composition. We show how a widely used subset of OpenMP can be encoded into this intermediate representation. Our verification technique builds on the notion of iteration contract to specify the behaviour of basic blocks; we show that if iteration contracts are manually specified for single blocks, then that is sufficient to automatically reason about data race freedom of the composed program. Moreover, we also show that it is sufficient to establish functional correctness on a linearised version of the original program to conclude functional correctness of the parallel program. Finally, we exemplify our approach on an example OpenMP program, and we discuss how tool support is provided.",
    "URL": "https://doi.org/10.1007/s10009-020-00601-z",
    "DOI": "10.1007/s10009-020-00601-z",
    "language": "en-US",
    "_line": "FormalBib.bib:8460"
  },
  "luo_c11tester_2021": {
    "id": "luo_c11tester_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Luo",
        "given": "Weiyu"
      },
      {
        "family": "Demsky",
        "given": "Brian"
      }
    ],
    "title": "C11Tester: A Race Detector for C/C++ Atomics Technical Report",
    "container-title": "arXiv:2102.07901 \\[cs\\]",
    "container-title-short": "C11Tester",
    "title-short": "C11Tester",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "22"
        ]
      ]
    },
    "abstract": "Writing correct concurrent code that uses atomics under the C/C++ memory model is extremely difficult. We present C11Tester, a race detector for the C/C++ memory model that can explore executions in a larger fragment of the C/C++ memory model than previous race detector tools. Relative to previous work, C11Tester's larger fragment includes behaviors that are exhibited by ARM processors. C11Tester uses a new constraint-based algorithm to implement modification order that is optimized to allow C11Tester to make decisions in terms of application-visible behaviors. We evaluate C11Tester on several benchmark applications, and compare C11Tester's performance to both tsan11rec, the state of the art tool that controls scheduling for C/C++; and tsan11, the state of the art tool that does not control scheduling.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2102.07901",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2102.07901",
    "URL": "http://arxiv.org/abs/2102.07901",
    "_line": "FormalBib.bib:8474"
  },
  "dross_verifythis_2020": {
    "id": "dross_verifythis_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Dross",
        "given": "Claire"
      },
      {
        "family": "Furia",
        "given": "Carlo A."
      },
      {
        "family": "Huisman",
        "given": "Marieke"
      },
      {
        "family": "Monahan",
        "given": "Rosemary"
      },
      {
        "family": "Müller",
        "given": "Peter"
      }
    ],
    "title": "VerifyThis 2019: A Program Verification Competition (Extended Report)",
    "container-title": "arXiv:2008.13610 \\[cs\\]",
    "container-title-short": "VerifyThis 2019",
    "title-short": "VerifyThis 2019",
    "issued": {
      "date-parts": [
        [
          "2020",
          "12",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "abstract": "VerifyThis is a series of program verification competitions that emphasize the human aspect: participants tackle the verification of detailed behavioral properties &ndash; something that lies beyond the capabilities of fully automatic verification, and requires instead human expertise to suitably encode programs, specifications, and invariants. This paper describes the 8th edition of VerifyThis, which took place at ETAPS 2019 in Prague. Thirteen teams entered the competition, which consisted of three verification challenges and spanned two days of work. The report analyzes how the participating teams fared on these challenges, reflects on what makes a verification challenge more or less suitable for the typical VerifyThis participants, and outlines the difficulties of comparing the work of teams using wildly different verification approaches in a competition focused on the human aspect.",
    "keywords": "Computer Science - Logic in Computer Science",
    "URLtext": "2008.13610",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2008.13610",
    "URL": "http://arxiv.org/abs/2008.13610",
    "_line": "FormalBib.bib:8489"
  },
  "gross_performance_2021": {
    "id": "gross_performance_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Gross",
        "given": "Jason S"
      }
    ],
    "title": "Performance Engineering of Proof-Based Software Systems at Scale",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2"
        ]
      ]
    },
    "abstract": "Formal verification is increasingly valuable as our world comes to rely more on software for critical infrastructure. A significant and understudied cost of developing mechanized proofs, especially at scale, is the computer performance of proof generation. This dissertation aims to be a partial guide to identifying and resolving performance bottlenecks in dependently typed tactic-driven proof assistants like Coq.",
    "page": "258",
    "page-first": "258",
    "language": "en-US",
    "_line": "FormalBib.bib:8504"
  },
  "li_program_nodate": {
    "id": "li_program_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "Yao"
      },
      {
        "family": "Weirich",
        "given": "Stephanie"
      }
    ],
    "title": "Program Adverbs: Structures for Embedding Eﬀectful Programs",
    "abstract": "We propose a new class of structures called program adverbs designed to abstractly represent eﬀectful programs in theorem provers like Coq. Program adverbs enable formal reasoning about general properties that hold for certain computation patterns regardless of the eﬀects that may occur during computation. Program adverbs are composable, allowing them to be used to model languages containing diﬀerent characteristics.",
    "page": "31",
    "page-first": "31",
    "language": "en-US",
    "_line": "FormalBib.bib:8514"
  },
  "li_mirchecker_nodate": {
    "id": "li_mirchecker_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "Zhuohua"
      },
      {
        "family": "Wang",
        "given": "Jincheng"
      },
      {
        "family": "Sun",
        "given": "Mingshen"
      },
      {
        "family": "Lui",
        "given": "John C S"
      }
    ],
    "title": "MirChecker: Detecting Bugs in Rust Programs via Static Analysis",
    "abstract": "Safe system programming is often a crucial requirement due to its critical role in system software engineering. Conventional lowlevel programming languages such as C and assembly are efficient, but their inherent unsafe nature makes it undesirable for securitycritical scenarios. Recently, Rust has become a promising alternative for safe system-level programming. While giving programmers finegrained hardware control, its strong type system enforces many security properties including memory safety. However, Rust’s security guarantee is not a silver bullet. Runtime crashes and memory-safety errors still harass Rust developers, causing damaging exploitable vulnerabilities, as reported by numerous studies \\[29, 42, 47, 53, 54\\]. In this paper, we present and evaluate MirChecker, a fully automated bug detection framework for Rust programs by performing static analysis on Rust’s Mid-level Intermediate Representation (MIR). Based on the observation of existing bugs found in Rust codebases, our approach keeps track of both numerical and symbolic information, detects potential runtime crashes and memory-safety errors by using constraint solving techniques, and outputs informative diagnostics to users. We evaluate MirChecker on both buggy code snippets extracted from existing Common Vulnerabilities and Exposures (CVE) and real-world Rust codebases. Our experiments show that MirChecker can detect all the issues in our code snippets, and is capable of performing bug finding in realworld scenarios, where it detected a total of 33 previously unknown bugs including 16 memory-safety issues from 12 Rust packages (crates) with an acceptable false-positive rate.",
    "page": "14",
    "page-first": "14",
    "language": "en-US",
    "_line": "FormalBib.bib:8523"
  },
  "zhu_icallee_2021": {
    "id": "zhu_icallee_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Zhu",
        "given": "Wenyu"
      },
      {
        "family": "Feng",
        "given": "Zhiyao"
      },
      {
        "family": "Zhang",
        "given": "Zihan"
      },
      {
        "family": "Ou",
        "given": "Zhijian"
      },
      {
        "family": "Yang",
        "given": "Min"
      },
      {
        "family": "Zhang",
        "given": "Chao"
      }
    ],
    "title": "iCallee: Recovering Call Graphs for Binaries",
    "container-title": "arXiv:2111.01415 \\[cs\\]",
    "container-title-short": "iCallee",
    "title-short": "iCallee",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "8"
        ]
      ]
    },
    "abstract": "Recovering programs' call graphs is crucial for inter-procedural analysis tasks and applications based on them. The core challenge is recognizing targets of indirect calls (i.e., indirect callees). It becomes more challenging if target programs are in binary forms, due to information loss in binaries. Existing indirect callee recognition solutions for binaries all have high false positives and negatives, making call graphs inaccurate. In this paper, we propose a new solution iCallee based on the Siamese Neural Network, inspired by the advances in question-answering applications. The key insight is that, neural networks can learn to answer whether a callee function is a potential target of an indirect callsite by comprehending their contexts, i.e., instructions nearby callsites and of callees. Following this insight, we first preprocess target binaries to extract contexts of callsites and callees. Then, we build a customized Natural Language Processing (NLP) model applicable to assembly language. Further, we collect abundant pairs of callsites and callees, and embed their contexts with the NLP model, then train a Siamese network and a classifier to answer the callsite-callee question. We have implemented a prototype of iCallee and evaluated it on several groups of targets. Evaluation results showed that, our solution could match callsites to callees with an F1-Measure of 93.7&perc;, recall of 93.8&perc;, and precision of 93.5&perc;, much better than state-of-the-art solutions. To show its usefulness, we apply iCallee to two specific applications - binary code similarity detection and binary program hardening, and found that it could greatly improve state-of-the-art solutions.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Software Engineering, Computer Science - Artificial Intelligence",
    "URLtext": "2111.01415",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2111.01415",
    "URL": "http://arxiv.org/abs/2111.01415",
    "_line": "FormalBib.bib:8532"
  },
  "huyghebaert_semi-automatic_2021": {
    "id": "huyghebaert_semi-automatic_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Huyghebaert",
        "given": "Sander"
      },
      {
        "family": "Keuchel",
        "given": "Steven"
      },
      {
        "family": "Devriese",
        "given": "Dominique"
      }
    ],
    "title": "Semi-automatic verification of ISA security guarantees in the form of universal contracts",
    "container-title": "SILM Workshop 2021",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "8"
        ]
      ]
    },
    "abstract": "Where ISA speciﬁcations used to be deﬁned in long prose documents, we have recently seen progress on formal and executable ISA speciﬁcations. However, for now, formal speciﬁcations provide only a functional speciﬁcation of the ISA, without specifying the ISA’s security guarantees. In this paper, we present a novel, general approach to specify an ISA’s security guarantee in a way that (1) can be semi-automatically validated against the ISA semantics, producing a mechanically veriﬁable proof, (2) supports informal and formal reasoning about security-critical software in the presence of adversarial code. Our approach is based on the use of universal contracts: software contracts that express bounds on the authority of arbitrary untrusted code on the ISA. We semi-automatically verify these contracts against existing ISA semantics implemented in Sail using our Katamaran tool: a veriﬁed, semi-automatic separation logic veriﬁer for Sail. For now, in this paper, we will illustrate our approach for MinimalCaps: a simpliﬁed custom-built capability machine ISA. However, we believe our approach has the potential to redeﬁne the formalization of ISA security guarantees and we will sketch our vision and plans.",
    "URL": "https://silm-workshop-2021.inria.fr/wp-content/uploads/2021/09/ISAVerif.pdf",
    "page": "6",
    "page-first": "6",
    "language": "en-US",
    "_line": "FormalBib.bib:8547"
  },
  "qu_relational_2021": {
    "id": "qu_relational_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Qu",
        "given": "Weihao"
      },
      {
        "family": "Gaboardi",
        "given": "Marco"
      },
      {
        "family": "Garg",
        "given": "Deepak"
      }
    ],
    "title": "Relational cost analysis in a functional-imperative setting",
    "container-title": "Journal of Functional Programming",
    "container-title-short": "J. Funct. Prog.",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "8"
        ]
      ]
    },
    "issn": "0956-7968, 1469-7653",
    "abstract": "Relational cost analysis aims at formally establishing bounds on the difference in the evaluation costs of two programs. As a particular case, one can also use relational cost analysis to establish bounds on the difference in the evaluation cost of the same program on two different inputs. One way to perform relational cost analysis is to use a relational type-and-effect system that supports reasoning about relations between two executions of two programs. Building on this basic idea, we present a type-and-effect system, called ARel, for reasoning about the relative cost (the difference in the evaluation cost) of array-manipulating, higher order functional-imperative programs. The key ingredient of our approach is a new lightweight type reﬁnement discipline that we use to track relations (differences) between two mutable arrays. This discipline combined with Hoare-style triples built into the types allows us to express and establish precise relative costs of several interesting programs that imperatively update their data. We have implemented ARel using ideas from bidirectional type checking.",
    "URL": "https://www.cambridge.org/core/product/identifier/S0956796821000071/type/journal_article",
    "DOI": "10.1017/S0956796821000071",
    "page": "e27",
    "page-first": "e27",
    "volume": "31",
    "language": "en-US",
    "_line": "FormalBib.bib:8560"
  },
  "feldman_property-directed_2021": {
    "id": "feldman_property-directed_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Feldman",
        "given": "Yotam M. Y."
      },
      {
        "family": "Sagiv",
        "given": "Mooly"
      },
      {
        "family": "Shoham",
        "given": "Sharon"
      },
      {
        "family": "Wilcox",
        "given": "James R."
      }
    ],
    "title": "Property-Directed Reachability as Abstract Interpretation in the Monotone Theory",
    "container-title": "arXiv:2111.00324 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "8"
        ]
      ]
    },
    "abstract": "Inferring inductive invariants is one of the main challenges of formal verification. The theory of abstract interpretation provides a rich framework to devise invariant inference algorithms. One of the latest breakthroughs in invariant inference is property-directed reachability (PDR), but the research community views PDR and abstract interpretation as mostly unrelated techniques. This paper shows that, surprisingly, propositional PDR can be formulated as an abstract interpretation algorithm in a logical domain. More precisely, we define a version of PDR, called &dollar;&bslash;Lambda&dollar;-PDR, in which all generalizations of counterexamples are used to strengthen a frame. In this way, there is no need to refine frames after their creation, because all the possible supporting facts are included in advance. We analyze this algorithm using notions from Bshouty's monotone theory, originally developed in the context of exact learning. We show that there is an inherent overapproximation between the algorithm's frames that is related to the monotone theory. We then define a new abstract domain in which the best abstract transformer performs this overapproximation, and show that it captures the invariant inference process, i.e., &dollar;&bslash;Lambda&dollar;-PDR corresponds to Kleene iterations with the best transformer in this abstract domain. We provide some sufficient conditions for when this process converges in a small number of iterations, with sometimes an exponential gap from the number of iterations required for naive exact forward reachability. These results provide a firm theoretical foundation for the benefits of how PDR tackles forward reachability.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2111.00324",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2111.00324",
    "URL": "http://arxiv.org/abs/2111.00324",
    "language": "en-US",
    "_line": "FormalBib.bib:8577"
  },
  "new_gradual_2021": {
    "id": "new_gradual_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "New",
        "given": "Max S."
      },
      {
        "family": "Licata",
        "given": "Daniel R."
      },
      {
        "family": "Ahmed",
        "given": "Amal"
      }
    ],
    "title": "Gradual type theory",
    "container-title": "Journal of Functional Programming",
    "container-title-short": "J. Funct. Prog.",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "3"
        ]
      ]
    },
    "issn": "0956-7968, 1469-7653",
    "abstract": "Gradually typed languages are designed to support both dynamically typed and statically typed programming styles while preserving the beneﬁts of each. Sound gradually typed languages dynamically check types at runtime at the boundary between statically typed and dynamically typed modules. However, there is much disagreement in the gradual typing literature over how to enforce complex types such as tuples, lists, functions and objects. In this paper, we propose a new perspective on the design of runtime gradual type enforcement: runtime type casts exist precisely to ensure the correctness of certain type-based refactorings and optimizations. For instance, for simple types, a language designer might desire that beta-eta equality is valid. We show that this perspective is useful by demonstrating that a cast semantics can be derived from beta-eta equality. We do this by providing an axiomatic account program equivalence in a gradual cast calculus in a logic we call gradual type theory (GTT). Based on Levy’s call-by-push-value, GTT allows us to axiomatize both call-byvalue and call-by-name gradual languages. We then show that we can derive the behavior of casts for simple types from the corresponding eta equality principle and the assumption that the language satisﬁes a property called graduality, also known as the dynamic gradual guarantee. Since we can derive the semantics from the assumption of eta equality, we also receive a useful contrapositive: any observably different cast semantics that satisﬁes graduality must violate the eta equality. We show the consistency and applicability of our axiomatic theory by proving that a contract-based implementation using the lazy cast semantics gives a logical relations model of our type theory, where equivalence in GTT implies contextual equivalence of the programs. Since GTT also axiomatizes the dynamic gradual guarantee, our model also establishes this central theorem of gradual typing. The model is parameterized by the implementation of the dynamic types, and so gives a family of implementations that validate type-based optimization and the gradual guarantee.",
    "URL": "https://www.cambridge.org/core/product/identifier/S0956796821000125/type/journal_article",
    "DOI": "10.1017/S0956796821000125",
    "page": "e21",
    "page-first": "e21",
    "volume": "31",
    "language": "en-US",
    "_line": "FormalBib.bib:8592"
  },
  "casteran_hydras_2021": {
    "id": "casteran_hydras_2021",
    "type": "manuscript",
    "author": [
      {
        "family": "Castéran",
        "given": "Pierre"
      },
      {
        "family": "Damour",
        "given": "Jérémy"
      },
      {
        "family": "Palmskog",
        "given": "Karl"
      },
      {
        "family": "Pit-Claudel",
        "given": "Clément"
      },
      {
        "family": "Zimmermann",
        "given": "Théo"
      }
    ],
    "title": "Hydras &amp; Co.: Formalized mathematics in Coq for inspiration and entertainment",
    "container-title-short": "Hydras &amp; Co.",
    "title-short": "Hydras &amp; Co.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "3"
        ]
      ]
    },
    "abstract": "Hydras &amp; Co. is a collaborative library of discrete mathematics for the Coq proof assistant, developed as part of the Coq-community organization on GitHub. The Coq code is accompanied by an electronic book, generated with the help of the Alectryon literate proving tool. We present the evolution of the mathematical contents of the library since former presentations at JFLA meetings. Then, we describe how the structure of the project is determined by two requirements which must be continuously satisfied. First, the Coq code needs to be compatible with its ever-evolving dependencies (the Coq proof assistant and several Coq packages both from inside and outside Coq-community) and reverse dependencies (Coq-community projects that depend on it). Second, the book needs to be consistent with the Coq code, which undergoes frequent changes to improve structure and include new material. We believe Hydras &amp; Co. demonstrates that books on formalized mathematics are not limited to providing exposition of theories and reasoning techniquesthey can also provide inspiration and entertainment that transcends educational goals.",
    "URL": "https://hal.archives-ouvertes.fr/hal-03404668",
    "_line": "FormalBib.bib:8609"
  },
  "bily_flexible_2021": {
    "id": "bily_flexible_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bílý",
        "given": "Aurel"
      },
      {
        "family": "Matheja",
        "given": "Christoph"
      },
      {
        "family": "Müller",
        "given": "Peter"
      }
    ],
    "title": "Flexible Refinement Proofs in Separation Logic",
    "container-title": "arXiv:2110.13559 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "26"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "1"
        ]
      ]
    },
    "abstract": "Refinement transforms an abstract system model into a concrete, executable program, such that properties established for the abstract model carry over to the concrete implementation. Refinement has been used successfully in the development of substantial verified systems. Nevertheless, existing refinement techniques have limitations that impede their practical usefulness. Some techniques generate executable code automatically, which generally leads to implementations with sub-optimal performance. Others employ bottom-up program verification to reason about efficient implementations, but impose strict requirements on the structure of the code, the structure of the refinement proofs, as well as the employed verification logic and tools. In this paper, we present a novel refinement technique that removes these limitations. Our technique uses separation logic to reason about efficient concurrent implementations. It prescribes only a loose coupling between an abstract model and the concrete implementation. It thereby supports a wide range of program structures, data representations, and proof structures. We make only minimal assumptions about the underlying program logic, which allows our technique to be used in combination with a wide range of logics and to be automated using off-the-shelf separation logic verifiers. We formalize the technique, prove the central trace inclusion property, and demonstrate its usefulness on several case studies.",
    "keywords": "Computer Science - Logic in Computer Science, F.3.1, 68Q60",
    "URLtext": "2110.13559",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2110.13559",
    "URL": "http://arxiv.org/abs/2110.13559",
    "_line": "FormalBib.bib:8620"
  },
  "oconnor_cogent_2021": {
    "id": "oconnor_cogent_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "O’Connor",
        "given": "Liam"
      },
      {
        "family": "Chen",
        "given": "Zilin"
      },
      {
        "family": "Rizkallah",
        "given": "Christine"
      },
      {
        "family": "Jackson",
        "given": "Vincent"
      },
      {
        "family": "Amani",
        "given": "Sidney"
      },
      {
        "family": "Klein",
        "given": "Gerwin"
      },
      {
        "family": "Murray",
        "given": "Toby"
      },
      {
        "family": "Sewell",
        "given": "Thomas"
      },
      {
        "family": "Keller",
        "given": "Gabriele"
      }
    ],
    "title": "Cogent: uniqueness types and certifying compilation",
    "container-title": "Journal of Functional Programming",
    "container-title-short": "Cogent",
    "title-short": "Cogent",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "1"
        ]
      ]
    },
    "issn": "0956-7968, 1469-7653",
    "abstract": "Abstract\n   This paper presents a framework aimed at significantly reducing the cost of proving functional correctness for low-level operating systems components. The framework is designed around a new functional programming language, Cogent. A central aspect of the language is its uniqueness type system, which eliminates the need for a trusted runtime or garbage collector while still guaranteeing memory safety, a crucial property for safety and security. Moreover, it allows us to assign two semantics to the language: The first semantics is imperative, suitable for efficient C code generation, and the second is purely functional, providing a user-friendly interface for equational reasoning and verification of higher-level correctness properties. The refinement theorem connecting the two semantics allows the compiler to produce a proof via translation validation certifying the correctness of the generated C code with respect to the semantics of the Cogent source program. We have demonstrated the effectiveness of our framework for implementation and for verification through two file system implementations.",
    "URL": "https://www.cambridge.org/core/product/identifier/S095679682100023X/type/journal_article",
    "DOI": "10.1017/S095679682100023X",
    "page": "e25",
    "page-first": "e25",
    "volume": "31",
    "language": "en-US",
    "_line": "FormalBib.bib:8634"
  },
  "tao_formal_2021": {
    "id": "tao_formal_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Tao",
        "given": "Runzhou"
      },
      {
        "family": "Yao",
        "given": "Jianan"
      },
      {
        "family": "Li",
        "given": "Xupeng"
      },
      {
        "family": "Li",
        "given": "Shih-Wei"
      },
      {
        "family": "Nieh",
        "given": "Jason"
      },
      {
        "family": "Gu",
        "given": "Ronghui"
      }
    ],
    "title": "Formal Verification of a Multiprocessor Hypervisor on Arm Relaxed Memory Hardware",
    "container-title": "Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
    "collection-title": "SOSP '21",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "26"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "1"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8709-5",
    "abstract": "Concurrent systems software is widely-used, complex, and error-prone, posing a significant security risk. We introduce VRM, a new framework that makes it possible for the first time to verify concurrent systems software, such as operating systems and hypervisors, on Arm relaxed memory hardware. VRM defines a set of synchronization and memory access conditions such that a program that satisfies these conditions can be mostly verified on a sequentially consistent hardware model and the proofs will automatically hold on relaxed memory hardware. VRM can be used to verify concurrent kernel code that is not data race free, including code responsible for managing shared page tables in the presence of relaxed MMU hardware. Using VRM, we verify the security guarantees of a retrofitted implementation of the Linux KVM hypervisor on Arm. For multiple versions of KVM, we prove KVM's security properties on a sequentially consistent model, then prove that KVM satisfies VRM's required program conditions such that its security proofs hold on Arm relaxed memory hardware. Our experimental results show that the retrofit and VRM conditions do not adversely affect the scalability of verified KVM, as it performs similar to unmodified KVM when concurrently running many multiprocessor virtual machines with real application workloads on Arm multiprocessor server hardware. Our work is the first machine-checked proof for concurrent systems software on Arm relaxed memory hardware.",
    "keywords": "Formal methods, Arm, hypervisors, KVM, relaxed memory, systems verification",
    "URL": "https://doi.org/10.1145/3477132.3483560",
    "DOI": "10.1145/3477132.3483560",
    "publisher-place": "New York, NY, USA",
    "page": "866-881",
    "page-first": "866",
    "_line": "FormalBib.bib:8653"
  },
  "noauthor_theorem_nodate-1": {
    "id": "noauthor_theorem_nodate-1",
    "type": "webpage",
    "title": "Theorem Proving and the Real Numbers: Overview Proving and the Real Numbers: Overview and Challenges Lawrence C. Paulson Computer Laboratory, University of Cambridge, England lp15@cl.cam.ac.uk - \\[Download PDF\\].vdocuments.mx",
    "container-title-short": "Theorem Proving and the Real Numbers",
    "title-short": "Theorem Proving and the Real Numbers",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "1"
        ]
      ]
    },
    "abstract": "Download theorem proving and the real numbers: overview proving and the real numbers: overview and challenges lawrence c. paulson computer laboratory, university of...",
    "URL": "https://vdocuments.mx/theorem-proving-and-the-real-numbers-overview-proving-and-the-real-numbers.html",
    "language": "en-US",
    "_line": "FormalBib.bib:8671"
  },
  "frumin_reloc_2021": {
    "id": "frumin_reloc_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Frumin",
        "given": "Dan"
      },
      {
        "family": "Krebbers",
        "given": "Robbert"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      }
    ],
    "title": "ReLoC Reloaded: A Mechanized Relational Logic for Fine-Grained Concurrency and Logical Atomicity",
    "container-title": "Logical Methods in Computer Science",
    "container-title-short": "ReLoC Reloaded",
    "title-short": "ReLoC Reloaded",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "26"
        ]
      ]
    },
    "issn": "1860-5974",
    "abstract": "We present a new version of ReLoC: a relational separation logic for proving reﬁnements of programs with higher-order state, ﬁne-grained concurrency, polymorphism and recursive types. The core of ReLoC is its reﬁnement judgment e e : τ , which states that a program e reﬁnes a program e at type τ . ReLoC provides type-directed structural rules and symbolic execution rules in separation-logic style for manipulating the judgment, whereas in prior work on reﬁnements for languages with higher-order state and concurrency, such proofs were carried out by unfolding the judgment into its deﬁnition in the model. ReLoC’s abstract proof rules make it simpler to carry out reﬁnement proofs, and enable us to generalize the notion of logically atomic speciﬁcations to the relational case, which we call logically atomic relational speciﬁcations.",
    "URL": "https://lmcs.episciences.org/6598",
    "DOI": "10.46298/lmcs-17(3:9)2021",
    "page": "6598",
    "page-first": "6598",
    "volume": "Volume 17, Issue 3",
    "language": "en-US",
    "_line": "FormalBib.bib:8682"
  },
  "mirliaz_flow-insensitive-complete_2021": {
    "id": "mirliaz_flow-insensitive-complete_2021",
    "type": "manuscript",
    "author": [
      {
        "family": "Mirliaz",
        "given": "Solène"
      },
      {
        "family": "Pichardie",
        "given": "David"
      }
    ],
    "title": "A Flow-Insensitive-Complete Program Representation",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "26"
        ]
      ]
    },
    "abstract": "When designing a static analysis, choosing between a flowinsensitive or a flow-sensitive analysis often amounts to favor scalability over precision. It is well known than specific program representations can help to reconcile the two objectives at the same time. For example the SSA representation is used in modern compilers to perform a constant propagation analysis flow-insensitively without any loss of precision. This paper proposes a provably correct program transformation that reconciles them for any analysis. We formalize the notion of Flow-Insensitive-Completeness with two collecting semantics and provide a program transformation that permits to analyze a program in a flow insensitive manner without sacrificing the precision we could obtain with a flow sensitive approach.",
    "URL": "https://hal.archives-ouvertes.fr/hal-03384612",
    "_line": "FormalBib.bib:8699"
  },
  "balajorshari_better_nodate": {
    "id": "balajorshari_better_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Balajorshari",
        "given": "Navid Emamdoost"
      },
      {
        "family": "McCamant",
        "given": "Stephen"
      }
    ],
    "title": "Better Program Analysis for Security via Data Flow Tracking and Symbolic Execution",
    "page": "92",
    "page-first": "92",
    "language": "en-US",
    "_line": "FormalBib.bib:8709"
  },
  "yao_program_2021": {
    "id": "yao_program_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Yao",
        "given": "Peisen"
      },
      {
        "family": "Shi",
        "given": "Qingkai"
      },
      {
        "family": "Huang",
        "given": "Heqing"
      },
      {
        "family": "Zhang",
        "given": "Charles"
      }
    ],
    "title": "Program analysis via efficient symbolic abstraction",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "24"
        ]
      ]
    },
    "abstract": "This paper concerns the scalability challenges of symbolic abstraction: given a formula ϕ in a logic L and an abstract domain A, find a most precise element in the abstract domain that over-approximates the meaning of ϕ. Symbolic abstraction is an important point in the space of abstract interpretation, as it allows for automatically synthesizing the best abstract transformers. However, current techniques for symbolic abstraction can have difficulty delivering on its practical strengths, due to performance issues. In this work, we introduce two algorithms for the symbolic abstraction of quantifier-free bit-vector formulas, which apply to the bit-vector interval domain and a certain kind of polyhedral domain, respectively. We implement and evaluate the proposed techniques on two machine code analysis clients, namely static memory corruption analysis and constrained random fuzzing. Using a suite of 57,933 queries from the clients, we compare our approach against a diverse group of state-of-the-art algorithms. The experiments show that our algorithms achieve a substantial speedup over existing techniques and illustrate significant precision advantages for the clients. Our work presents strong evidence that symbolic abstraction of numeric domains can be efficient and practical for large and realistic programs.",
    "keywords": "optimization, Abstract interpretation, interval domain, polyhedral domain, symbolic abstraction",
    "URL": "https://doi.org/10.1145/3485495",
    "DOI": "10.1145/3485495",
    "page": "118:1-118:32",
    "page-first": "118",
    "volume": "5",
    "_line": "FormalBib.bib:8717"
  },
  "paraskevopoulou_compiling_2021": {
    "id": "paraskevopoulou_compiling_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Paraskevopoulou",
        "given": "Zoe"
      },
      {
        "family": "Grover",
        "given": "Anvay"
      }
    ],
    "title": "Compiling with continuations, correctly",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "24"
        ]
      ]
    },
    "abstract": "In this paper we present a novel simulation relation for proving correctness of program transformations that combines syntactic simulations and logical relations. In particular, we establish a new kind of simulation diagram that uses a small-step or big-step semantics in the source language and an untyped, step-indexed logical relation in the target language. Our technique provides a practical solution for proving semantics preservation for transformations that do not preserve reductions in the source language. This is common when transformations generate new binder names, and hence α-conversion must be explicitly accounted for, or when transformations introduce administrative redexes. Our technique does not require reductions in the source language to correspond directly to reductions in the target language. Instead, we enforce a weaker notion of semantic preorder, which suffices to show that semantics are preserved for both whole-program and separate compilation. Because our logical relation is transitive, we can transition between intermediate program states in a small-step fashion and hence the shape of the proof resembles that of a simple small-step simulation. We use this technique to revisit the semantic correctness of a continuation-passing style (CPS) transformation and we demonstrate how it allows us to overcome well-known complications of this proof related to α-conversion and administrative reductions. In addition, by using a logical relation that is indexed by invariants that relate the resource consumption of two programs, we are able show that the transformation preserves diverging behaviors and that our CPS transformation asymptotically preserves the running time of the source program. Our results are formalized in the Coq proof assistant. Our continuation-passing style transformation is part of the CertiCoq compiler for Gallina, the specification language of Coq.",
    "keywords": "logical relations, compiler correctness, continuation-passing style, simulations",
    "URL": "https://doi.org/10.1145/3485491",
    "DOI": "10.1145/3485491",
    "page": "114:1-114:29",
    "page-first": "114",
    "volume": "5",
    "_line": "FormalBib.bib:8734"
  },
  "lanzinger_scalability_2021": {
    "id": "lanzinger_scalability_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Lanzinger",
        "given": "Florian"
      },
      {
        "family": "Weigl",
        "given": "Alexander"
      },
      {
        "family": "Ulbrich",
        "given": "Mattias"
      },
      {
        "family": "Dietl",
        "given": "Werner"
      }
    ],
    "title": "Scalability and precision by combining expressive type systems and deductive verification",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "24"
        ]
      ]
    },
    "abstract": "Type systems and modern type checkers can be used very successfully to obtain formal correctness guarantees with little specification overhead. However, type systems in practical scenarios have to trade precision for decidability and scalability. Tools for deductive verification, on the other hand, can prove general properties in more cases than a typical type checker can, but they do not scale well. We present a method to complement the scalability of expressive type systems with the precision of deductive program verification approaches. This is achieved by translating the type uses whose correctness the type checker cannot prove into assertions in a specification language, which can be dealt with by a deductive verification tool. Type uses whose correctness the type checker can prove are instead turned into assumptions to aid the verification tool in finding a proof.Our novel approach is introduced both conceptually for a simple imperative language, and practically by a concrete implementation for the Java programming language. The usefulness and power of our approach has been evaluated by discharging known false positives from a real-world program and by a small case study.",
    "keywords": "Deductive verification, Pluggable type systems, Refinement types",
    "URL": "https://doi.org/10.1145/3485520",
    "DOI": "10.1145/3485520",
    "page": "143:1-143:29",
    "page-first": "143",
    "volume": "5",
    "_line": "FormalBib.bib:8751"
  },
  "lubin_how_2021": {
    "id": "lubin_how_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Lubin",
        "given": "Justin"
      },
      {
        "family": "Chasins",
        "given": "Sarah E."
      }
    ],
    "title": "How statically-typed functional programmers write code",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "24"
        ]
      ]
    },
    "abstract": "How working statically-typed functional programmers write code is largely understudied. And yet, a better understanding of developer practices could pave the way for the design of more useful and usable tooling, more ergonomic languages, and more effective on-ramps into programming communities. The goal of this work is to address this knowledge gap: to better understand the high-level authoring patterns that statically-typed functional programmers employ. We conducted a grounded theory analysis of 30 programming sessions of practicing statically-typed functional programmers, 15 of which also included a semi-structured interview. The theory we developed gives insight into how the specific affordances of statically-typed functional programming affect domain modeling, type construction, focusing techniques, exploratory and reasoning strategies, and expressions of intent. We conducted a set of quantitative lab experiments to validate our findings, including that statically-typed functional programmers often iterate between editing types and expressions, that they often run their compiler on code even when they know it will not successfully compile, and that they make textual program edits that reliably signal future edits that they intend to make. Lastly, we outline the implications of our findings for language and tool design. The success of this approach in revealing program authorship patterns suggests that the same methodology could be used to study other understudied programmer populations.",
    "keywords": "functional programming, grounded theory, interviews, mixed methods, need-finding, qualitative, quantitative, randomized controlled trial, static types",
    "URL": "https://doi.org/10.1145/3485532",
    "DOI": "10.1145/3485532",
    "page": "155:1-155:30",
    "page-first": "155",
    "volume": "5",
    "_line": "FormalBib.bib:8768"
  },
  "sinkarovs_extracting_2021": {
    "id": "sinkarovs_extracting_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Šinkarovs",
        "given": "Artjoms"
      },
      {
        "family": "Cockx",
        "given": "Jesper"
      }
    ],
    "title": "Extracting the Power of Dependent Types",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Most existing programming languages provide little support to formally state and prove properties about programs. Adding such capabilities is far from trivial, as it requires significant re-engineering of the existing compilers and tools. This paper proposes a novel technique to write correct-byconstruction programs in languages without built-in verification capabilities, while maintaining the ability to use existing tools. This is achieved in three steps. Firstly, we give a shallow embedding of the language (or a subset) into a dependently typed language. Secondly, we write a program in that embedding, and we use dependent types to guarantee correctness properties of interest within the embedding. Thirdly, we extract a program written in the original language, so it can be used with existing compilers and tools.",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "FormalBib.bib:8785"
  },
  "fu_label_2021": {
    "id": "fu_label_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Fu",
        "given": "Weili"
      },
      {
        "family": "Krause",
        "given": "Fabian"
      },
      {
        "family": "Thiemann",
        "given": "Peter"
      }
    ],
    "title": "Label dependent lambda calculus and gradual typing",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "24"
        ]
      ]
    },
    "abstract": "Dependently-typed programming languages are gaining importance, because they can guarantee a wide range of properties at compile time. Their use in practice is often hampered because programmers have to provide very precise types. Gradual typing is a means to vary the level of typing precision between program fragments and to transition smoothly towards more precisely typed programs. The combination of gradual typing and dependent types seems promising to promote the widespread use of dependent types. We investigate a gradual version of a minimalist value-dependent lambda calculus. Compile-time calculations and thus dependencies are restricted to labels, drawn from a generic enumeration type. The calculus supports the usual Pi and Sigma types as well as singleton types and subtyping. It is sufficiently powerful to provide flexible encodings of variant and record types with first-class labels. We provide type checking algorithms for the underlying label-dependent lambda calculus and its gradual extension. The gradual type checker drives the translation into a cast calculus, which extends the original language. The cast calculus comes with several innovations: refined typing for casts in the presence of singletons, type reduction in casts, and fully dependent Sigma types. Besides standard metatheoretical results, we establish the gradual guarantee for the gradual language.",
    "keywords": "dependent types, gradual type systems, subtyping",
    "URL": "https://doi.org/10.1145/3485485",
    "DOI": "10.1145/3485485",
    "page": "108:1-108:29",
    "page-first": "108",
    "volume": "5",
    "_line": "FormalBib.bib:8795"
  },
  "chaliasos_well-typed_2021": {
    "id": "chaliasos_well-typed_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Chaliasos",
        "given": "Stefanos"
      },
      {
        "family": "Sotiropoulos",
        "given": "Thodoris"
      },
      {
        "family": "Drosos",
        "given": "Georgios-Petros"
      },
      {
        "family": "Mitropoulos",
        "given": "Charalambos"
      },
      {
        "family": "Mitropoulos",
        "given": "Dimitris"
      },
      {
        "family": "Spinellis",
        "given": "Diomidis"
      }
    ],
    "title": "Well-typed programs can go wrong: a study of typing-related bugs in JVM compilers",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Well-typed programs can go wrong",
    "title-short": "Well-typed programs can go wrong",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "24"
        ]
      ]
    },
    "abstract": "Despite the substantial progress in compiler testing, research endeavors have mainly focused on detecting compiler crashes and subtle miscompilations caused by bugs in the implementation of compiler optimizations. Surprisingly, this growing body of work neglects other compiler components, most notably the front-end. In statically-typed programming languages with rich and expressive type systems and modern features, such as type inference or a mix of object-oriented with functional programming features, the process of static typing in compiler front-ends is complicated by a high-density of bugs. Such bugs can lead to the acceptance of incorrect programs (breaking code portability or the type system's soundness), the rejection of correct (e.g. well-typed) programs, and the reporting of misleading errors and warnings. We conduct, what is to the best of our knowledge, the first empirical study for understanding and characterizing typing-related compiler bugs. To do so, we manually study 320 typing-related bugs (along with their fixes and test cases) that are randomly sampled from four mainstream JVM languages, namely Java, Scala, Kotlin, and Groovy. We evaluate each bug in terms of several aspects, including their symptom, root cause, bug fix's size, and the characteristics of the bug-revealing test cases. Some representative observations indicate that: (1) more than half of the typing-related bugs manifest as unexpected compile-time errors: the buggy compiler wrongly rejects semantically correct programs, (2) the majority of typing-related bugs lie in the implementations of the underlying type systems and in other core components related to operations on types, (3) parametric polymorphism is the most pervasive feature in the corresponding test cases, (4) one third of typing-related bugs are triggered by non-compilable programs. We believe that our study opens up a new research direction by driving future researchers to build appropriate methods and techniques for a more holistic testing of compilers.",
    "keywords": "Java, compiler bugs, compiler testing, Groovy, Kotlin, Scala, static typing",
    "URL": "https://doi.org/10.1145/3485500",
    "DOI": "10.1145/3485500",
    "page": "123:1-123:30",
    "page-first": "123",
    "volume": "5",
    "_line": "FormalBib.bib:8812"
  },
  "bao_reachability_2021": {
    "id": "bao_reachability_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bao",
        "given": "Yuyan"
      },
      {
        "family": "Wei",
        "given": "Guannan"
      },
      {
        "family": "Bračevac",
        "given": "Oliver"
      },
      {
        "family": "Jiang",
        "given": "Yuxuan"
      },
      {
        "family": "He",
        "given": "Qiyang"
      },
      {
        "family": "Rompf",
        "given": "Tiark"
      }
    ],
    "title": "Reachability types: tracking aliasing and separation in higher-order functional programs",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Reachability types",
    "title-short": "Reachability types",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "24"
        ]
      ]
    },
    "abstract": "Ownership type systems, based on the idea of enforcing unique access paths, have been primarily focused on objects and top-level classes. However, existing models do not as readily reflect the finer aspects of nested lexical scopes, capturing, or escaping closures in higher-order functional programming patterns, which are increasingly adopted even in mainstream object-oriented languages. We present a new type system, λ\\* , which enables expressive ownership-style reasoning across higher-order functions. It tracks sharing and separation through reachability sets, and layers additional mechanisms for selectively enforcing uniqueness on top of it. Based on reachability sets, we extend the type system with an expressive flow-sensitive effect system, which enables flavors of move semantics and ownership transfer. In addition, we present several case studies and extensions, including applications to capabilities for algebraic effects, one-shot continuations, and safe parallelization.",
    "keywords": "type systems, aliasing, effect systems, ownership types, reachability types",
    "URL": "https://doi.org/10.1145/3485516",
    "DOI": "10.1145/3485516",
    "page": "139:1-139:32",
    "page-first": "139",
    "volume": "5",
    "_line": "FormalBib.bib:8830"
  },
  "chaliasos_well-typed_2021-1": {
    "id": "chaliasos_well-typed_2021-1",
    "type": "article-journal",
    "author": [
      {
        "family": "Chaliasos",
        "given": "Stefanos"
      },
      {
        "family": "Sotiropoulos",
        "given": "Thodoris"
      },
      {
        "family": "Drosos",
        "given": "Georgios-Petros"
      },
      {
        "family": "Mitropoulos",
        "given": "Charalambos"
      },
      {
        "family": "Mitropoulos",
        "given": "Dimitris"
      },
      {
        "family": "Spinellis",
        "given": "Diomidis"
      }
    ],
    "title": "Well-typed programs can go wrong: a study of typing-related bugs in JVM compilers",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Well-typed programs can go wrong",
    "title-short": "Well-typed programs can go wrong",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "24"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "STEFANOS CHALIASOS∗, Athens University of Economics and Business, Greece THODORIS SOTIROPOULOS∗, Athens University of Economics and Business, Greece GEORGIOS-PETROS DROSOS, Athens University of Economics and Business, Greece CHARALAMBOS MITROPOULOS, Technical University of Crete, Greece DIMITRIS MITROPOULOS, University of Athens, Greece DIOMIDIS SPINELLIS, Athens University of Economics and Business, Greece and Delft University of Technology, Netherlands Despite the substantial progress in compiler testing, research endeavors have mainly focused on detecting compiler crashes and subtle miscompilations caused by bugs in the implementation of compiler optimizations. Surprisingly, this growing body of work neglects other compiler components, most notably the front-end. In statically-typed programming languages with rich and expressive type systems and modern features, such as type inference or a mix of object-oriented with functional programming features, the process of static typing in compiler front-ends is complicated by a high-density of bugs. Such bugs can lead to the acceptance of incorrect programs (breaking code portability or the type system’s soundness), the rejection of correct (e.g. well-typed) programs, and the reporting of misleading errors and warnings. We conduct, what is to the best of our knowledge, the first empirical study for understanding and characterizing typing-related compiler bugs. To do so, we manually study 320 typing-related bugs (along with their fixes and test cases) that are randomly sampled from four mainstream JVM languages, namely Java, Scala, Kotlin, and Groovy. We evaluate each bug in terms of several aspects, including their symptom, root cause, bug fix’s size, and the characteristics of the bug-revealing test cases. Some representative observations indicate that: (1) more than half of the typing-related bugs manifest as unexpected compile-time errors: the buggy compiler wrongly rejects semantically correct programs, (2) the majority of typing-related bugs lie in the implementations of the underlying type systems and in other core components related to operations on types, (3) parametric polymorphism is the most pervasive feature in the corresponding test cases, (4) one third of typing-related bugs are triggered by non-compilable programs. We believe that our study opens up a new research direction by driving future researchers to build appropriate methods and techniques for a more holistic testing of compilers. CCS Concepts: • Software and its engineering → Compilers; Software testing and debugging.",
    "URL": "https://dl.acm.org/doi/10.1145/3485500",
    "DOI": "10.1145/3485500",
    "page": "1-30",
    "page-first": "1",
    "volume": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:8848"
  },
  "chaliasos_well-typed_2021-2": {
    "id": "chaliasos_well-typed_2021-2",
    "type": "article-journal",
    "author": [
      {
        "family": "Chaliasos",
        "given": "Stefanos"
      },
      {
        "family": "Sotiropoulos",
        "given": "Thodoris"
      },
      {
        "family": "Drosos",
        "given": "Georgios-Petros"
      },
      {
        "family": "Mitropoulos",
        "given": "Charalambos"
      },
      {
        "family": "Mitropoulos",
        "given": "Dimitris"
      },
      {
        "family": "Spinellis",
        "given": "Diomidis"
      }
    ],
    "title": "Well-typed programs can go wrong: a study of typing-related bugs in JVM compilers",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Well-typed programs can go wrong",
    "title-short": "Well-typed programs can go wrong",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "24"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "STEFANOS CHALIASOS∗, Athens University of Economics and Business, Greece THODORIS SOTIROPOULOS∗, Athens University of Economics and Business, Greece GEORGIOS-PETROS DROSOS, Athens University of Economics and Business, Greece CHARALAMBOS MITROPOULOS, Technical University of Crete, Greece DIMITRIS MITROPOULOS, University of Athens, Greece DIOMIDIS SPINELLIS, Athens University of Economics and Business, Greece and Delft University of Technology, Netherlands Despite the substantial progress in compiler testing, research endeavors have mainly focused on detecting compiler crashes and subtle miscompilations caused by bugs in the implementation of compiler optimizations. Surprisingly, this growing body of work neglects other compiler components, most notably the front-end. In statically-typed programming languages with rich and expressive type systems and modern features, such as type inference or a mix of object-oriented with functional programming features, the process of static typing in compiler front-ends is complicated by a high-density of bugs. Such bugs can lead to the acceptance of incorrect programs (breaking code portability or the type system’s soundness), the rejection of correct (e.g. well-typed) programs, and the reporting of misleading errors and warnings. We conduct, what is to the best of our knowledge, the first empirical study for understanding and characterizing typing-related compiler bugs. To do so, we manually study 320 typing-related bugs (along with their fixes and test cases) that are randomly sampled from four mainstream JVM languages, namely Java, Scala, Kotlin, and Groovy. We evaluate each bug in terms of several aspects, including their symptom, root cause, bug fix’s size, and the characteristics of the bug-revealing test cases. Some representative observations indicate that: (1) more than half of the typing-related bugs manifest as unexpected compile-time errors: the buggy compiler wrongly rejects semantically correct programs, (2) the majority of typing-related bugs lie in the implementations of the underlying type systems and in other core components related to operations on types, (3) parametric polymorphism is the most pervasive feature in the corresponding test cases, (4) one third of typing-related bugs are triggered by non-compilable programs. We believe that our study opens up a new research direction by driving future researchers to build appropriate methods and techniques for a more holistic testing of compilers. CCS Concepts: • Software and its engineering → Compilers; Software testing and debugging.",
    "URL": "https://dl.acm.org/doi/10.1145/3485500",
    "DOI": "10.1145/3485500",
    "page": "1-30",
    "page-first": "1",
    "volume": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:8867"
  },
  "he_type_2021": {
    "id": "he_type_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "He",
        "given": "Paul"
      },
      {
        "family": "Westbrook",
        "given": "Eddy"
      },
      {
        "family": "Carmer",
        "given": "Brent"
      },
      {
        "family": "Phifer",
        "given": "Chris"
      },
      {
        "family": "Robert",
        "given": "Valentin"
      },
      {
        "family": "Smeltzer",
        "given": "Karl"
      },
      {
        "family": "Ştefănescu",
        "given": "Andrei"
      },
      {
        "family": "Tomb",
        "given": "Aaron"
      },
      {
        "family": "Wick",
        "given": "Adam"
      },
      {
        "family": "Yacavone",
        "given": "Matthew"
      },
      {
        "family": "Zdancewic",
        "given": "Steve"
      }
    ],
    "title": "A type system for extracting functional specifications from memory-safe imperative programs",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "18"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "PAUL HE, University of Pennsylvania, USA EDDY WESTBROOK, Galois, Inc., USA BRENT CARMER, Galois, Inc., USA CHRIS PHIFER, Galois, Inc., USA VALENTIN ROBERT, Galois, Inc., USA KARL SMELTZER, Galois, Inc., USA ANDREI ŞTEFĂNESCU, Galois, Inc., USA AARON TOMB, Galois, Inc., USA ADAM WICK, Galois, Inc., USA MATTHEW YACAVONE, Galois, Inc., USA STEVE ZDANCEWIC, University of Pennsylvania, USA Verifying imperative programs is hard. A key difficulty is that the specification of what an imperative program does is often intertwined with details about pointers and imperative state. Although there are a number of powerful separation logics that allow the details of imperative state to be captured and managed, these details are complicated and reasoning about them requires significant time and expertise. In this paper, we take a different approach: a memory-safe type system that, as part of type-checking, extracts functional specifications from imperative programs. This disentangles imperative state, which is handled by the type system, from functional specifications, which can be verified without reference to pointers. A key difficulty is that sometimes memory safety depends crucially on the functional specification of a program; e.g., an array index is only memory-safe if the index is in bounds. To handle this case, our specification extraction inserts dynamic checks into the specification. Verification then requires the additional proof that none of these checks fail. However, these checks are in a purely functional language, and so this proof also requires no reasoning about pointers. CCS Concepts: • Software and its engineering → Software verification; • Theory of computation → Program specifications; Type structures.",
    "URL": "https://dl.acm.org/doi/10.1145/3485512",
    "DOI": "10.1145/3485512",
    "page": "1-29",
    "page-first": "1",
    "volume": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:8886"
  },
  "appel_efficient_2021": {
    "id": "appel_efficient_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Appel",
        "given": "Andrew"
      },
      {
        "family": "Leroy",
        "given": "Xavier"
      }
    ],
    "title": "Efficient Extensional Binary Tries",
    "container-title": "arXiv:2110.05063 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "18"
        ]
      ]
    },
    "abstract": "Lookup tables (finite maps) are a ubiquitous data structure. In pure functional languages they are best represented using trees instead of hash tables. In pure functional languages within constructive logic, without a primitive integer type, they are well represented using binary tries instead of search trees. In this work, we introduce canonical binary tries, an improved binary-trie data structure that enjoys a natural extensionality property, quite useful in proofs, and supports sparseness more efficiently. We provide full proofs of correctness in Coq. We provide microbenchmark measurements of canonical binary tries versus several other data structures for finite maps, in a variety of application contexts; as well as measurement of canonical versus original tries in a big, real system. The application context of data structures contained in theorem statements imposes unusual requirements for which canonical tries are particularly well suited.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "2110.05063",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2110.05063",
    "URL": "http://arxiv.org/abs/2110.05063",
    "_line": "FormalBib.bib:8904"
  },
  "yan_secrsl_nodate": {
    "id": "yan_secrsl_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Yan",
        "given": "Pengbo"
      },
      {
        "family": "Murray",
        "given": "Toby"
      }
    ],
    "title": "SecRSL: Security Separation Logic for C11 Release-Acquire Concurrency",
    "page": "26",
    "page-first": "26",
    "volume": "5",
    "language": "en-US",
    "_line": "FormalBib.bib:8918"
  },
  "subramaniam_dependent_2021": {
    "id": "subramaniam_dependent_2021",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Subramaniam",
        "given": "Chaitanya Leena"
      }
    ],
    "title": "From dependent type theory to higher algebraic structures",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "13"
        ]
      ]
    },
    "number-of-pages": "155",
    "abstract": "The first part of this dissertation defines \"dependently typed algebraic theories\", which are a strict subclass of the generalised algebraic theories (GATs) of Cartmell. We characterise dependently typed algebraic theories as finitary monads on certain presheaf categories, generalising a well-known result due to Lawvere, B&bslash;'enabou and Linton for ordinary multisorted algebraic theories. We use this to recognise dependently typed algebraic theories for a number of classes of algebraic structures, such as small categories, n-categories, strict and weak omega-categories, planar coloured operads and opetopic sets. We then show that every locally finitely presentable category is the category of models of some dependently typed algebraic theory. Thus, with respect to their Set-models, these theories are just as expressive as GATs, essentially algebraic theories and finite limit sketches. However, dependently typed algebraic theories admit a good definition of homotopy-models in spaces, via a left Bousfield localisation of a global model structure on simplicial presheaves. Some cases, such as certain \"idempotent opetopic theories\", have a rigidification theorem relating homotopy-models and (strict) simplicial models. The second part of this dissertation concerns localisations of presentable &dollar;(&bslash;infty,1)&dollar;-categories. We give a definition of \"pre-modulator\", and show that every accessible orthogonal factorisation system on a presentable &dollar;(&bslash;infty,1)&dollar;-category can be generated from a pre-modulator by iterating a plus-construction resembling that of sheafification. We give definitions of \"modulator\" and \"left-exact modulator\", and prove that they correspond to those factorisation systems that are modalities and left-exact modalities respectively. Thus every left-exact localisation of an &dollar;&bslash;infty&dollar;-topos is obtained by iterating the plus-construction associated to a left-exact modulator.",
    "keywords": "Mathematics - Category Theory, 18C10, 18C35, 18N40, 18N60",
    "URLtext": "2110.02804",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2110.02804",
    "URL": "http://arxiv.org/abs/2110.02804",
    "_line": "FormalBib.bib:8927"
  },
  "kamburjan_deductive_2021": {
    "id": "kamburjan_deductive_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Kamburjan",
        "given": "Eduard"
      },
      {
        "family": "Wasser",
        "given": "Nathan"
      }
    ],
    "title": "Deductive Verification of Programs with Underspecified Semantics by Model Extraction",
    "container-title": "arXiv:2110.01964 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "12"
        ]
      ]
    },
    "abstract": "We present a novel and well automatable approach to formal veriﬁcation of programs with underspeciﬁed semantics, i.e., a language semantics that leaves open the order of certain evaluations. First, we reduce this problem to non-determinism of distributed systems, automatically extracting a distributed Active Object model from underspeciﬁed, sequential C code. This translation process provides a fully formal semantics for the considered C subset. In the extracted model every nondeterministic choice corresponds to one possible evaluation order. This step also automatically translates speciﬁcations in the ANSI/ISO C Speciﬁcation Language (ACSL) into method contracts and object invariants for Active Objects. We then perform veriﬁcation on the speciﬁed Active Objects model. For this we have implemented a theorem prover Crowbar based on the Behavioral Program Logic (BPL), which veriﬁes the extracted model with respect to the translated speciﬁcation and ensures the original property of the C code for all possible evaluation orders. By using model extraction, we can use standard tools, without designing a new complex program logic to deal with underspeciﬁcation. The case study used is highly underspeciﬁed and cannot be veriﬁed with existing tools for C.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "2110.01964",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2110.01964",
    "URL": "http://arxiv.org/abs/2110.01964",
    "language": "en-US",
    "_line": "FormalBib.bib:8942"
  },
  "lin_interactive_2021": {
    "id": "lin_interactive_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Lin",
        "given": "Zhengyao"
      },
      {
        "family": "Chen",
        "given": "Xiaohong"
      },
      {
        "family": "Roşu",
        "given": "Grigore"
      }
    ],
    "title": "An Interactive Theorem Prover for Matching Logic with Proof Object Generation",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "5"
        ]
      ]
    },
    "abstract": "Matching logic is a uniform logical foundation for K, which is a language semantics framework with the philosophy that all tooling around a language should be automatically generated from a single, rigorous deﬁnition of its formal semantics. In practice, K has been widely used to deﬁne the formal semantics of many real-world languages and to generate their execution and veriﬁcation tools. However, there lacks a generic theorem prover that connects K with its logical foundation—matching logic. In this paper, we present ITPML, which is the ﬁrst interactive theorem prover for matching logic. The main advantage of ITPML is its ability to generate machine-checkable proof objects as certiﬁcates that witness the correctness of its formal reasoning. ITPML is built on top of Metamath, a language to deﬁne formal systems, which allows it to have a small trust base of only 250 lines of code. ITPML supports both backward and forward proofs, allows users to dynamically add intermediate lemmas, and features automated proof tactics for common utilities such as reasoning about notations and proving propositional tautologies.",
    "page": "12",
    "page-first": "12",
    "language": "en-US",
    "_line": "FormalBib.bib:8957"
  },
  "apt_assessing_2021": {
    "id": "apt_assessing_2021",
    "type": "chapter",
    "author": [
      {
        "family": "Apt",
        "given": "Krzysztof R."
      },
      {
        "family": "Olderog",
        "given": "Ernst-Rüdiger"
      }
    ],
    "title": "Assessing the Success and Impact of Hoare's Logic",
    "container-title": "Theories of Programming: The Life and Works of Tony Hoare",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "12"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "edition": "1",
    "isbn": "978-1-4503-8728-6",
    "DOI": "10.1145/3477355.3477359",
    "publisher-place": "New York, NY, USA",
    "page": "41-76",
    "page-first": "41",
    "volume": "39",
    "_line": "FormalBib.bib:8967"
  },
  "muller_first_2021": {
    "id": "muller_first_2021",
    "type": "chapter",
    "author": [
      {
        "family": "Müller",
        "given": "Peter"
      },
      {
        "family": "Shankar",
        "given": "Natarajan"
      }
    ],
    "title": "The First Fifteen Years of the Verified Software Project",
    "container-title": "Theories of Programming: The Life and Works of Tony Hoare",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "12"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "edition": "1",
    "isbn": "978-1-4503-8728-6",
    "DOI": "10.1145/3477355.3477362",
    "publisher-place": "New York, NY, USA",
    "page": "93-124",
    "page-first": "93",
    "volume": "39",
    "_line": "FormalBib.bib:8983"
  },
  "pujet_observational_2022": {
    "id": "pujet_observational_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Pujet",
        "given": "Loïc"
      },
      {
        "family": "Tabareau",
        "given": "Nicolas"
      }
    ],
    "title": "Observational Equality: Now For Good",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "page": "29",
    "page-first": "29",
    "language": "en-US",
    "_line": "FormalBib.bib:8999"
  },
  "koch_mechanizing_nodate": {
    "id": "koch_mechanizing_nodate",
    "type": "thesis",
    "genre": "Bachelor's Thesis",
    "author": [
      {
        "family": "Koch",
        "given": "Mark"
      }
    ],
    "title": "Mechanizing Second-Order Logic in Coq",
    "publisher": "Saarland University",
    "number-of-pages": "102",
    "publisher-place": "8/23/2021",
    "language": "en-US",
    "_line": "FormalBib.bib:9008"
  },
  "watt_mechanising_2021": {
    "id": "watt_mechanising_2021",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Watt",
        "given": "Conrad"
      }
    ],
    "title": "Mechanising and evolving the formal semantics of WebAssembly: the Web's new low-level language",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "5"
        ]
      ]
    },
    "publisher": "St Catharine's College, University of Cambridge",
    "number-of-pages": "176",
    "publisher-place": "Cambridge, UK",
    "language": "en-US",
    "_line": "FormalBib.bib:9019"
  },
  "bornholt_using_2021": {
    "id": "bornholt_using_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bornholt",
        "given": "James"
      },
      {
        "family": "Joshi",
        "given": "Rajeev"
      },
      {
        "family": "Astrauskas",
        "given": "Vytautas"
      },
      {
        "family": "Cully",
        "given": "Brendan"
      },
      {
        "family": "Kragl",
        "given": "Bernhard"
      },
      {
        "family": "Markle",
        "given": "Seth"
      },
      {
        "family": "Sauri",
        "given": "Kyle"
      },
      {
        "family": "Schleit",
        "given": "Drew"
      },
      {
        "family": "Slatton",
        "given": "Grant"
      },
      {
        "family": "Tasiran",
        "given": "Serdar"
      }
    ],
    "title": "Using Lightweight Formal Methods to Validate a Key-Value Storage Node in Amazon S3",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "This paper reports our experience applying lightweight formal methods to validate the correctness of ShardStore, a new key-value storage node implementation for the Amazon S3 cloud object storage service. By “lightweight formal methodsž we mean a pragmatic approach to verifying the correctness of a production storage node that is under ongoing feature development by a full-time engineering team. We do not aim to achieve full formal verification, but instead emphasize automation, usability, and the ability to continually ensure correctness as both software and its specification evolve over time. Our approach decomposes correctness into independent properties, each checked by the most appropriate tool, and develops executable reference models as specifications to be checked against the implementation. Our work has prevented 16 issues from reaching production, including subtle crash consistency and concurrency problems, and has been extended by non-formal-methods experts to check new features and properties as ShardStore has evolved.",
    "page": "15",
    "page-first": "15",
    "language": "en-US",
    "_line": "FormalBib.bib:9031"
  },
  "ferles_practical_2020": {
    "id": "ferles_practical_2020",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Ferles",
        "given": "Kostas"
      }
    ],
    "title": "Practical Formal Methods for Software Analysis and Development",
    "issued": {
      "date-parts": [
        [
          "2020",
          "12"
        ]
      ]
    },
    "publisher": "University of Texas at Austin",
    "number-of-pages": "218",
    "abstract": "Developed tools: Trimmer, Expresso, and CFPChecker.",
    "publisher-place": "Austin, Texas",
    "language": "en-US",
    "_line": "FormalBib.bib:9041"
  },
  "wilcox_compositional_2021": {
    "id": "wilcox_compositional_2021",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Wilcox",
        "given": "James R"
      }
    ],
    "title": "Compositional and Automated Veriﬁcation of Distributed Systems",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "University of Washington",
    "number-of-pages": "160",
    "language": "en-US",
    "_line": "FormalBib.bib:9054"
  },
  "chen_accelerating_nodate": {
    "id": "chen_accelerating_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Chen",
        "given": "Yifan"
      },
      {
        "family": "Yang",
        "given": "Chenyang"
      },
      {
        "family": "Zhang",
        "given": "Xin"
      },
      {
        "family": "Xiong",
        "given": "Yingfei"
      },
      {
        "family": "Tang",
        "given": "Hao"
      },
      {
        "family": "Wang",
        "given": "Xiaoyin"
      },
      {
        "family": "Zhang",
        "given": "Lu"
      }
    ],
    "title": "Accelerating Program Analyses in Datalog by Merging Library Facts",
    "abstract": "Static program analysis uses sensitivity to balance between precision and scalability. However, ﬁner sensitivity does not necessarily lead to more precise results but may reduce scalability. Recently, a number of approaches have been proposed to ﬁnely tune the sensitivity of diﬀerent program parts. However, these approaches are usually designed for speciﬁc program analyses, and their abstraction adjustments are coarse-grained as they directly drop sensitivity elements.",
    "page": "24",
    "page-first": "24",
    "language": "en-US",
    "_line": "FormalBib.bib:9065"
  },
  "timany_trillium_2021": {
    "id": "timany_trillium_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Timany",
        "given": "Amin"
      },
      {
        "family": "Gregersen",
        "given": "Simon Oddershede"
      },
      {
        "family": "Stefanesco",
        "given": "Léo"
      },
      {
        "family": "Gondelman",
        "given": "Léon"
      },
      {
        "family": "Nieto",
        "given": "Abel"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      }
    ],
    "title": "Trillium: Unifying Refinement and Higher-Order Distributed Separation Logic",
    "container-title": "arXiv:2109.07863 \\[cs\\]",
    "container-title-short": "Trillium",
    "title-short": "Trillium",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "24"
        ]
      ]
    },
    "abstract": "We present a unification of refinement and Hoare-style reasoning in a foundational mechanized higher-order distributed separation logic. This unification enables us to prove formally in the Coq proof assistant that concrete implementations of challenging distributed systems refine more abstract models and to combine refinement-style reasoning with Hoare-style program verification. We use our logic to prove correctness of concrete implementations of two-phase commit and single-decree Paxos by showing that they refine their abstract TLA+ specifications. We further use our notion of refinement to transfer fairness assumptions on program executions to model traces and then transfer liveness properties of fair model traces back to program executions, which enables us to prove liveness properties such as strong eventual consistency of a concrete implementation of a Conflict-Free Replicated Data Type and fair termination of a concurrent program.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2109.07863",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2109.07863",
    "URL": "http://arxiv.org/abs/2109.07863",
    "_line": "FormalBib.bib:9074"
  },
  "quines_type_nodate": {
    "id": "quines_type_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Quines",
        "given": "Carl"
      }
    ],
    "title": "Type Theory by Example",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "24"
        ]
      ]
    },
    "URL": "https://www.cjquines.com/files/typetheory.pdf",
    "_line": "FormalBib.bib:9089"
  },
  "garavel_formal_nodate": {
    "id": "garavel_formal_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Garavel",
        "given": "Hubert"
      }
    ],
    "title": "Formal Methos for Safe and Secure Computers Systems",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "24"
        ]
      ]
    },
    "URL": "https://www.bsi.bund.de/SharedDocs/Downloads/DE/BSI/Publikationen/Studien/formal_methods_study_875/formal_methods_study_875.pdf?__blob=publicationFile&v=1",
    "_line": "FormalBib.bib:9097"
  },
  "gleirscher_formal_2020": {
    "id": "gleirscher_formal_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Gleirscher",
        "given": "Mario"
      },
      {
        "family": "Marmsoler",
        "given": "Diego"
      }
    ],
    "title": "Formal methods in dependable systems engineering: a survey of professionals from Europe and North America",
    "container-title": "Empirical Software Engineering",
    "container-title-short": "Formal methods in dependable systems engineering",
    "title-short": "Formal methods in dependable systems engineering",
    "issued": {
      "date-parts": [
        [
          "2020",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "24"
        ]
      ]
    },
    "issn": "1382-3256, 1573-7616",
    "abstract": "Objective We study the use of formal methods in mission-critical software domains, examining industrial and academic views.\nMethod We perform a cross-sectional on-line survey.\nResults Our results indicate an increased intent to apply FMs in industry, suggesting a positively perceived usefulness. But the results also indicate a negatively perceived ease of use. Scalability, skills, and education seem to be among the key challenges to support this intent.\nConclusions We present the largest study of this kind so far (N = 216), and our observations provide valuable insights, highlighting directions for future theoretical and empirical research of formal methods. Our findings are strongly coherent with earlier observations by Austin and Graeme (1993).",
    "URL": "https://link.springer.com/10.1007/s10664-020-09836-5",
    "DOI": "10.1007/s10664-020-09836-5",
    "page": "4473-4546",
    "page-first": "4473",
    "volume": "25",
    "issue": "6",
    "language": "en-US",
    "_line": "FormalBib.bib:9105"
  },
  "abyaneh_ase_nodate": {
    "id": "abyaneh_ase_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Abyaneh",
        "given": "Alireza S"
      },
      {
        "family": "Kirsch",
        "given": "Christoph M"
      }
    ],
    "title": "ASE: A Value Set Decision Procedure for Symbolic Execution",
    "abstract": "A symbolic execution engine regularly queries a Satisﬁability Modulo Theory (SMT) solver to determine reachability of code during execution. Unfortunately, the SMT solver is often the bottleneck of symbolic execution. Inspired by abstract interpretation, we propose an abstract symbolic execution (ASE) engine which aims at querying the SMT solver less often by trying to compute reachability faster through an increasingly weaker abstraction. For this purpose, we have designed and implemented a value set decision procedure based on strided value interval (SVI) sets for efﬁciently determining precise, or under-approximating value sets for variables. Our ASE engine begins reasoning with respect to the SVI abstraction, and then only if needed uses the theory of bit-vectors implemented in SMT solvers. Our ASE engine efﬁciently detects when the former abstraction becomes incomplete to move on and try the next abstraction.",
    "page": "12",
    "page-first": "12",
    "language": "en-US",
    "_line": "FormalBib.bib:9127"
  },
  "bartha_one_2021": {
    "id": "bartha_one_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bartha",
        "given": "Sándor"
      },
      {
        "family": "Cheney",
        "given": "James"
      },
      {
        "family": "Belle",
        "given": "Vaishak"
      }
    ],
    "title": "One Down, 699 to Go: or, synthesising compositional desugarings",
    "container-title": "arXiv:2109.06114 \\[cs\\]",
    "container-title-short": "One Down, 699 to Go",
    "title-short": "One Down, 699 to Go",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "20"
        ]
      ]
    },
    "abstract": "Programming or scripting languages used in real-world systems are seldom designed with a formal semantics in mind from the outset. Therefore, developing well-founded analysis tools for these systems requires reverse-engineering a formal semantics as a first step. This can take months or years of effort. Can we (at least partially) automate this process? Though desirable, automatically reverse-engineering semantics rules from an implementation is very challenging, as found by Krishnamurthi et al. \\[2019\\]. In this paper, we highlight that scaling methods with the size of the language is very difficult due to state space explosion, so we propose to learn semantics incrementally. We give a formalisation of Krishnamurthi et al.'s desugaring learning framework in order to clarify the assumptions necessary for an incremental learning algorithm to be feasible. We show that this reformulation allows us to extend the search space and express rules that Krishnamurthi et al. described as challenging, while still retaining feasibility. We evaluate enumerative synthesis as a baseline algorithm, and demonstrate that, with our reformulation of the problem, it is possible to learn correct desugaring rules for the example source and core languages proposed by Krishnamurthi et al., in most cases identical to the intended rules. In addition, with user guidance, our system was able to synthesize rules for desugaring list comprehensions and try/catch/finally constructs.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2109.06114",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2109.06114",
    "URL": "http://arxiv.org/abs/2109.06114",
    "_line": "FormalBib.bib:9136"
  },
  "meyer_concept_2021": {
    "id": "meyer_concept_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Meyer",
        "given": "Bertrand"
      },
      {
        "family": "Arkadova",
        "given": "Alisa"
      },
      {
        "family": "Kogtenkov",
        "given": "Alexander"
      },
      {
        "family": "Naumchev",
        "given": "Alexandr"
      }
    ],
    "title": "The concept of class invariant in object-oriented programming",
    "container-title": "arXiv:2109.06557 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "14"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "20"
        ]
      ]
    },
    "abstract": "Class invariants &ndash; consistency constraints preserved by every operation on objects of a given type &ndash; are fundamental to building and understanding object-oriented programs. They should also be a key help in verifying them, but turn out instead to raise major verification challenges which have prompted a significant literature with, until now, no widely accepted solution. The present work introduces a general proof rule meant to address invariant-related issues and allow verification tools benefit from invariants. It first clarifies the notion of invariant and identify the three problems: callbacks, furtive access and reference leak. As an example, the 2016 Ethereum DAO bug, in which &bslash;&dollar;50 million were stolen, resulted from a callback invalidating an invariant. The discussion starts with a \"Simple Model\" and an associated proof rule, demonstrating its soundness. It then removes one by one the three assumptions of the Simple Model, each removal bringing up one of the three issues, and introduces the corresponding adaptation to the proof rule. The final version of the rule can tackle tricky examples, including \"challenge problems\" listed in the literature.",
    "keywords": "Computer Science - Programming Languages, F.3, Computer Science - Software Engineering, D.1, D.2, D.3",
    "URLtext": "2109.06557",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2109.06557",
    "URL": "http://arxiv.org/abs/2109.06557",
    "_line": "FormalBib.bib:9151"
  },
  "sanchez-stern_hybrid-neural_nodate": {
    "id": "sanchez-stern_hybrid-neural_nodate",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Sanchez-Stern",
        "given": "Alex"
      }
    ],
    "title": "Hybrid-Neural Synthesis of Machine Checkable Software Correctness Proofs",
    "publisher": "UC San Diego",
    "number-of-pages": "99",
    "URL": "https://escholarship.org/uc/item/5j10b5w8",
    "_line": "FormalBib.bib:9165"
  },
  "patel_verifying_2021": {
    "id": "patel_verifying_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Patel",
        "given": "Nisarg"
      },
      {
        "family": "Krishna",
        "given": "Siddharth"
      },
      {
        "family": "Shasha",
        "given": "Dennis"
      },
      {
        "family": "Wies",
        "given": "Thomas"
      }
    ],
    "title": "Verifying Concurrent Multicopy Search Structures",
    "container-title": "arXiv:2109.05631 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "20"
        ]
      ]
    },
    "abstract": "Multicopy search structures such as log-structured merge (LSM) trees are optimized for high insert/update/delete (collectively known as upsert) performance. In such data structures, an upsert on key &dollar;k&dollar;, which adds &dollar;(k,v)&dollar; where &dollar;v&dollar; can be a value or a tombstone, is added to the root node even if &dollar;k&dollar; is already present in other nodes. Thus there may be multiple copies of &dollar;k&dollar; in the search structure. A search on &dollar;k&dollar; aims to return the value associated with the most recent upsert. We present a general framework for verifying linearizability of concurrent multicopy search structures that abstracts from the underlying representation of the data structure in memory, enabling proof-reuse across diverse implementations. Based on our framework, we propose template algorithms for a) LSM structures forming arbitrary directed acyclic graphs and b) differential file structures, and formally verify these templates in the concurrent separation logic Iris. We also instantiate the LSM template to obtain the first verified concurrent in-memory LSM tree implementation.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "2109.05631",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2109.05631",
    "URL": "http://arxiv.org/abs/2109.05631",
    "_line": "FormalBib.bib:9175"
  },
  "kim_memory-efficient_2020": {
    "id": "kim_memory-efficient_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Kim",
        "given": "Sung Kook"
      },
      {
        "family": "Venet",
        "given": "Arnaud J."
      },
      {
        "family": "Thakur",
        "given": "Aditya V."
      }
    ],
    "title": "Memory-Efficient Fixpoint Computation",
    "container-title": "arXiv:2009.05865 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2020",
          "9",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "16"
        ]
      ]
    },
    "abstract": "Practical adoption of static analysis often requires trading precision for performance. This paper focuses on improving the memory efficiency of abstract interpretation without sacrificing precision or time efficiency. Computationally, abstract interpretation reduces the problem of inferring program invariants to computing a fixpoint of a set of equations. This paper presents a method to minimize the memory footprint in Bourdoncle's iteration strategy, a widely-used technique for fixpoint computation. Our technique is agnostic to the abstract domain used. We prove that our technique is optimal (i.e., it results in minimum memory footprint) for Bourdoncle's iteration strategy while computing the same result. We evaluate the efficacy of our technique by implementing it in a tool called MIKOS, which extends the state-of-the-art abstract interpreter IKOS. When verifying user-provided assertions, MIKOS shows a decrease in peak-memory usage to 4.07&perc; (24.57x) on average compared to IKOS. When performing interprocedural buffer-overflow analysis, MIKOS shows a decrease in peak-memory usage to 43.7&perc; (2.29x) on average compared to IKOS.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2009.05865",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2009.05865",
    "URL": "http://arxiv.org/abs/2009.05865",
    "_line": "FormalBib.bib:9189"
  },
  "kim_deterministic_2020": {
    "id": "kim_deterministic_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Kim",
        "given": "Sung Kook"
      },
      {
        "family": "Venet",
        "given": "Arnaud J."
      },
      {
        "family": "Thakur",
        "given": "Aditya V."
      }
    ],
    "title": "Deterministic Parallel Fixpoint Computation",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2020",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "16"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "Abstract interpretation is a general framework for expressing static program analyses. It reduces the problem of extracting properties of a program to computing an approximation of the least fixpoint of a system of equations. The de facto approach for computing this approximation uses a sequential algorithm based on weak topological order (WTO). This paper presents a deterministic parallel algorithm for fixpoint computation by introducing the notion of weak partial order (WPO). We present an algorithm for constructing a WPO in almost-linear time. Finally, we describe PIKOS, our deterministic parallel abstract interpreter, which extends the sequential abstract interpreter IKOS. We evaluate the performance and scalability of PIKOS on a suite of 1017 C programs. When using 4 cores, PIKOS achieves an average speedup of 2.06x over IKOS, with a maximum speedup of 3.63x. When using 16 cores, PIKOS achieves a maximum speedup of 10.97x.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "1909.05951",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1909.05951",
    "URL": "http://arxiv.org/abs/1909.05951",
    "DOI": "10.1145/3371082",
    "page": "1-33",
    "page-first": "1",
    "volume": "4",
    "_line": "FormalBib.bib:9203"
  },
  "noauthor_efficient_nodate": {
    "id": "noauthor_efficient_nodate",
    "type": "webpage",
    "title": "Efficient Fixpoint Computation for Abstract Interpretation - University of California, Davis",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "16"
        ]
      ]
    },
    "URL": "https://video.ucdavis.edu/media/Efficient+Fixpoint+Computation+for+Abstract+Interpretation/1_1trufm5a",
    "_line": "FormalBib.bib:9223"
  },
  "murphy_validating_2021": {
    "id": "murphy_validating_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Murphy",
        "given": "Logan"
      },
      {
        "family": "Viger",
        "given": "Torin"
      },
      {
        "family": "Sandro",
        "given": "Alessio Di"
      },
      {
        "family": "Shahin",
        "given": "Ramy"
      },
      {
        "family": "Chechik",
        "given": "Marsha"
      }
    ],
    "editor": [
      {
        "family": "Calinescu",
        "given": "Radu"
      },
      {
        "family": "Păsăreanu",
        "given": "Corina S."
      }
    ],
    "title": "Validating Safety Arguments with Lean",
    "container-title": "Software Engineering and Formal Methods",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-92124-8",
    "abstract": "Safety Assurance Cases (ACs) are structured arguments which demonstrate that a system fulfills its safety requirements. However, the reasoning used in ACs is often presented informally and thus difficult to rigorously evaluate. To protect against the acceptance of ACs based on fallacious reasoning, our previous work has proposed a framework for formalizing fragments of ACs and verifying their reasoning using the Lean Theorem Prover. This work expands on the use of Lean to automatically validate fragments of ACs, identifies challenges faced by AC developers who wish the leverage theorem proving software, and demonstrates our approach to mitigating these challenges.",
    "keywords": "Assurance, Lean, Safety cases, Strategies, Theorem proving",
    "DOI": "10.1007/978-3-030-92124-8_2",
    "publisher-place": "Cham",
    "page": "23-43",
    "page-first": "23",
    "language": "en-US",
    "_line": "FormalBib.bib:9230"
  },
  "shao_httpsjhcsjtueducnyutingwangfilespopl22pdf_nodate": {
    "id": "shao_httpsjhcsjtueducnyutingwangfilespopl22pdf_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Shao",
        "given": "Zhong"
      },
      {
        "family": "Koenig",
        "given": "Jérémie"
      }
    ],
    "title": "https://jhc.sjtu.edu.cn/&tilde;yutingwang/files/popl22.pdf",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "7"
        ]
      ]
    },
    "URL": "https://jhc.sjtu.edu.cn/~yutingwang/files/popl22.pdf",
    "_line": "FormalBib.bib:9247"
  },
  "spitters_verified_nodate": {
    "id": "spitters_verified_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Spitters",
        "given": "Bas"
      }
    ],
    "title": "A Verified Pipeline from a Specification Language to Optimized, Safe Rust",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "7"
        ]
      ]
    },
    "URL": "https://cs.au.dk/~spitters/CoqPL22.pdf",
    "_line": "FormalBib.bib:9255"
  },
  "bao_separation_2021": {
    "id": "bao_separation_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bao",
        "given": "Jialu"
      },
      {
        "family": "Gaboardi",
        "given": "Marco"
      },
      {
        "family": "Hsu",
        "given": "Justin"
      },
      {
        "family": "Tassarotti",
        "given": "Joseph"
      }
    ],
    "title": "A Separation Logic for Negative Dependence",
    "container-title": "arXiv:2111.14917 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "29"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "7"
        ]
      ]
    },
    "abstract": "Formal reasoning about hashing-based probabilistic data structures often requires reasoning about random variables where when one variable gets larger (such as the number of elements hashed into one bucket), the others tend to be smaller (like the number of elements hashed into the other buckets). This is an example of negative dependence, a generalization of probabilistic independence that has recently found interesting applications in algorithm design and machine learning. Despite the usefulness of negative dependence for the analyses of probabilistic data structures, existing verification methods cannot establish this property for randomized programs. To fill this gap, we design LINA, a probabilistic separation logic for reasoning about negative dependence. Following recent works on probabilistic separation logic using separating conjunction to reason about the probabilistic independence of random variables, we use separating conjunction to reason about negative dependence. Our assertion logic features two separating conjunctions, one for independence and one for negative dependence. We generalize the logic of bunched implications (BI) to support multiple separating conjunctions, and provide a sound and complete proof system. Notably, the semantics for separating conjunction relies on a non-deterministic, rather than partial, operation for combining resources. By drawing on closure properties for negative dependence, our program logic supports a Frame-like rule for negative dependence and monotone operations. We demonstrate how LINA can verify probabilistic properties of hash-based data structures and balls-into-bins processes.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "2111.14917",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2111.14917",
    "URL": "http://arxiv.org/abs/2111.14917",
    "DOI": "10.1145/3498719",
    "_line": "FormalBib.bib:9263"
  },
  "rosu_guarded_2017": {
    "id": "rosu_guarded_2017",
    "type": "webpage",
    "author": [
      {
        "family": "Rosu",
        "given": "Grigore"
      }
    ],
    "title": "Guarded Matching Logic is Decidable",
    "issued": {
      "date-parts": [
        [
          "2017",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "7"
        ]
      ]
    },
    "URL": "https://www.ideals.illinois.edu/bitstream/handle/2142/112795/rodrigues-chen-rosu-2022-tr-decidability.pdf?sequence=2&isAllowed=y",
    "_line": "FormalBib.bib:9278"
  },
  "dreyer_concurrent_nodate": {
    "id": "dreyer_concurrent_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Dreyer",
        "given": "Derek"
      },
      {
        "family": "O'Hearn",
        "given": "Peter W"
      }
    ],
    "title": "Concurrent Incorrectness Separation Logic",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "7"
        ]
      ]
    },
    "URL": "https://research.fb.com/wp-content/uploads/2021/11/Concurrent-Incorrectness-Separation-Logic.pdf",
    "_line": "FormalBib.bib:9287"
  },
  "mi_general_2021": {
    "id": "mi_general_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Mi",
        "given": "Zeyu"
      },
      {
        "family": "Zhuang",
        "given": "Haoqi"
      },
      {
        "family": "Zang",
        "given": "Binyu"
      },
      {
        "family": "Chen",
        "given": "Haibo"
      }
    ],
    "title": "General and Fast Inter-Process Communication via Bypassing Privileged Software",
    "container-title": "IEEE Transactions on Computers",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "issn": "1557-9956",
    "abstract": "IPC (Inter-Process Communication) is a widely used operating system (OS) technique that allows one process to invoke the services of other processes. The IPC participants may share the same OS (internal IPC) or use a separate OS (external IPC). Even though a long line of researches has optimized the performance of IPC, it is still a major factor of the run-time overhead of IPC-intensive applications. Furthermore, there is no one-size-fits-all solution for both internal and external IPC. This paper presents SkyBridge, a general communication technique designed and optimized for both types of IPC. SkyBridge requires no involvement of the privileged software (the kernel or the hypervisor) and enables a process to directly switch to the virtual address space of the target process, regardless of whether they are running on the same OS or not. We have implemented SkyBridge on two microkernels (seL4 and Google Zircon) as well as an open-source serverless hypervisor (Firecracker). The evaluation results show that SkyBridge improves the latency of internal IPC and external IPC by up to 19.6x and 1265.7x, respectively.",
    "keywords": "Inter-Process Communication, Microkernel, Serverlesss Computing, VMFUNC",
    "DOI": "10.1109/TC.2021.3130751",
    "page": "1-1",
    "page-first": "1",
    "note": "Conference Name: IEEE Transactions on Computers",
    "_line": "FormalBib.bib:9295"
  },
  "krebbers_diaframe_nodate": {
    "id": "krebbers_diaframe_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Krebbers",
        "given": "Robbert"
      }
    ],
    "title": "Diaframe: Automated Verification of Fine-Grained Concurrent Programs in Iris",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "7"
        ]
      ]
    },
    "URL": "https://robbertkrebbers.nl/research/articles/diaframe.pdf",
    "_line": "FormalBib.bib:9309"
  },
  "lin_making_nodate": {
    "id": "lin_making_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Lin",
        "given": "Zhengyao"
      },
      {
        "family": "Chen",
        "given": "Xiaohong"
      },
      {
        "family": "Trinh",
        "given": "Minh-Thai"
      },
      {
        "family": "Wang",
        "given": "John"
      },
      {
        "family": "Roşu",
        "given": "Grigore"
      }
    ],
    "title": "Making Formal Verification Trustworthy via Proof Generation",
    "abstract": "Formal deductive verification aims at proving the correctness of programs via logical deduction. However, the fact that it is usually based on complex program logics makes it error-prone to implement. This paper addresses the important research question of how we can make a deductive verifier trustworthy through a practical approach. We propose a novel technique to generate machine-checkable proof objects to certify each verification task performed by the language-agnostic deductive verifier of K—a semanticsbased language framework. These proof objects encode formal proofs in matching logic—the logical foundation of K. They have a small 240-line trust base and can be directly verified by third-party proof checkers. Our preliminary experiments show promising performance in generating correctness proofs for deductive verification in different programming languages.",
    "page": "15",
    "page-first": "15",
    "language": "en-US",
    "_line": "FormalBib.bib:9317"
  },
  "vale_layered_nodate": {
    "id": "vale_layered_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Vale",
        "given": "Arthur Oliveira"
      }
    ],
    "title": "Layered and Object-Based Game Semantics",
    "page": "49",
    "page-first": "49",
    "volume": "6",
    "language": "en-US",
    "_line": "FormalBib.bib:9326"
  },
  "haase_separation_2021": {
    "id": "haase_separation_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Haase",
        "given": "Darion"
      },
      {
        "family": "Grädel",
        "given": "Erich"
      },
      {
        "family": "Wilke",
        "given": "Richard"
      }
    ],
    "title": "Separation logic and logics with team semantics",
    "container-title": "Annals of Pure and Applied Logic",
    "container-title-short": "Annals of Pure and Applied Logic",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "19"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "26"
        ]
      ]
    },
    "issn": "0168-0072",
    "abstract": "Separation logic is a successful logical system for formal reasoning about programs that mutate their data structures. Team semantics, on the other side, is the basis of modern logics of dependence and independence. Separation logic and team semantics have been introduced with quite different motivations, and are investigated by research communities with rather different backgrounds and objectives. Nevertheless, there are obvious similarities between these formalisms. Both separation logic and logics with team semantics involve the manipulation of second-order objects, such as heaps and teams, by first-order syntax without reference to second-order variables. Moreover, these semantical objects are closely related; it is for instance obvious that a heap can be seen as a team, and the separating conjunction of separation logic is (essentially) the same as the team-semantical disjunction. Based on such similarities, the possible connections between separation logic and team semantics have been raised as a question at several occasions, and lead to informal discussions between these research communities. The objective of this paper is to make this connection precise, and to study its potential but also its obstacles and limitations.",
    "keywords": "Separation logic, Logics of dependence and independence, Team semantics",
    "URL": "https://www.sciencedirect.com/science/article/pii/S0168007221001214",
    "DOI": "10.1016/j.apal.2021.103063",
    "page": "103063",
    "page-first": "103063",
    "language": "en-US",
    "_line": "FormalBib.bib:9335"
  },
  "carbonneaux_applying_2021": {
    "id": "carbonneaux_applying_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Carbonneaux",
        "given": "Quentin"
      },
      {
        "family": "Zilberstein",
        "given": "Noam"
      },
      {
        "family": "Klee",
        "given": "Christoph"
      },
      {
        "family": "O'Hearn",
        "given": "Peter W"
      },
      {
        "family": "Nardelli",
        "given": "Francesco Zappa"
      }
    ],
    "title": "Applying Formal Verification to Microkernel IPC at Meta",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "page": "14",
    "page-first": "14",
    "language": "en-US",
    "_line": "FormalBib.bib:9352"
  },
  "jacobs_connectivity_nodate": {
    "id": "jacobs_connectivity_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Jacobs",
        "given": "Jules"
      },
      {
        "family": "Balzer",
        "given": "Stephanie"
      },
      {
        "family": "Krebbers",
        "given": "Robbert"
      }
    ],
    "title": "Connectivity Graphs: A Method for Proving Deadlock Freedom Based on Separation Logic",
    "page": "33",
    "page-first": "33",
    "volume": "6",
    "language": "en-US",
    "_line": "FormalBib.bib:9361"
  },
  "jang_moebius_2021": {
    "id": "jang_moebius_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Jang",
        "given": "Junyoung"
      },
      {
        "family": "Gélineau",
        "given": "Samuel"
      },
      {
        "family": "Monnier",
        "given": "Stefan"
      },
      {
        "family": "Pientka",
        "given": "Brigitte"
      }
    ],
    "title": "Moebius: Metaprogramming using Contextual Types &ndash; The stage where System F can pattern match on itself (Long Version)",
    "container-title": "arXiv:2111.08099 \\[cs\\]",
    "container-title-short": "Moebius",
    "title-short": "Moebius",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "26"
        ]
      ]
    },
    "abstract": "We describe the foundation of the metaprogramming language, Moebius, which supports the generation of polymorphic code and, more importantly the analysis of polymorphic code via pattern matching. Moebius has two main ingredients: 1) we exploit contextual modal types to describe open code together with the context in which it is meaningful. In Moebius, open code can depend on type and term variables (level 0) whose values are supplied at a later stage, as well as code variables (level 1) that stand for code templates supplied at a later stage. This leads to a multi-level modal lambda-calculus that supports System-F style polymorphism and forms the basis for polymorphic code generation. 2) we extend the multi-level modal lambda-calculus to support pattern matching on code. As pattern matching on polymorphic code may refine polymorphic type variables, we extend our type-theoretic foundation to generate and track typing constraints that arise. We also give an operational semantics and prove type preservation. Our multi-level modal foundation for Moebius provides the appropriate abstractions for both generating and pattern matching on open code without committing to a concrete representation of variable binding and contexts. Hence, our work is a step towards building a general type-theoretic foundation for multi-staged metaprogramming that, on the one hand, enforces strong type guarantees and, on the other hand, makes it easy to generate and manipulate code. This will allow us to exploit the full potential of metaprogramming without sacrificing the reliability of and trust in the code we are producing and running.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2111.08099",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2111.08099",
    "URL": "http://arxiv.org/abs/2111.08099",
    "_line": "FormalBib.bib:9370"
  },
  "griffin_verifying_2021": {
    "id": "griffin_verifying_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Griffin",
        "given": "Matt"
      },
      {
        "family": "Dongol",
        "given": "Brijesh"
      }
    ],
    "editor": [
      {
        "family": "Huisman",
        "given": "Marieke"
      },
      {
        "family": "Păsăreanu",
        "given": "Corina"
      },
      {
        "family": "Zhan",
        "given": "Naijun"
      }
    ],
    "title": "Verifying Secure Speculation in Isabelle/HOL",
    "container-title": "Formal Methods",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-90870-6",
    "abstract": "Secure speculation is an information flow security hyperproperty that prevents transient execution attacks such as Spectre, Meltdown and Foreshadow. Generic compiler mitigations for secure speculation are known to be insufficient for eliminating vulnerabilities. Moreover, these mitigation techniques often overprescribe speculative fences, causing the performance of the programs to suffer. Recently Cheang et al. have developed an operational semantics of program execution capable of characterising speculative executions as well as a new class of information flow hyperproperties named TPOD that ensure secure speculation. This paper presents a framework for verifying TPOD using the Isabelle/HOL proof assistant by encoding the operational semantics of Cheang et al. We provide translation tools for automatically generating the required Isabelle/HOL theory templates from a C-like program syntax, which speeds up verification. Our framework is capable of proving the existence of vulnerabilities and correctness of secure speculation. We exemplify our framework by proving the existence of secure speculation bugs in 15 victim functions for the MSVC compiler as well as correctness of some proposed fixes.",
    "keywords": "Isabelle/HOL, Formal verification, Hyperproperties, Secure speculation, Spectre, Transient execution vulnerabilities",
    "DOI": "10.1007/978-3-030-90870-6_3",
    "publisher-place": "Cham",
    "page": "43-60",
    "page-first": "43",
    "language": "en-US",
    "_line": "FormalBib.bib:9385"
  },
  "wolf_concise_2021": {
    "id": "wolf_concise_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Wolf",
        "given": "Felix A."
      },
      {
        "family": "Schwerhoff",
        "given": "Malte"
      },
      {
        "family": "Müller",
        "given": "Peter"
      }
    ],
    "editor": [
      {
        "family": "Huisman",
        "given": "Marieke"
      },
      {
        "family": "Păsăreanu",
        "given": "Corina"
      },
      {
        "family": "Zhan",
        "given": "Naijun"
      }
    ],
    "title": "Concise Outlines for a Complex Logic: A Proof Outline Checker for TaDA",
    "container-title": "Formal Methods",
    "container-title-short": "Concise Outlines for a Complex Logic",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "Concise Outlines for a Complex Logic",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-90870-6",
    "abstract": "Modern separation logics allow one to prove rich properties of intricate code, e.g. functional correctness and linearizability of non-blocking concurrent code. However, this expressiveness leads to a complexity that makes these logics difficult to apply. Manual proofs or proofs in interactive theorem provers consist of a large number of steps, often with subtle side conditions. On the other hand, automation with dedicated verifiers typically requires sophisticated proof search algorithms that are specific to the given program logic, resulting in limited tool support that makes it difficult to experiment with program logics, e.g. when learning, improving, or comparing them. Proof outline checkers fill this gap. Their input is a program annotated with the most essential proof steps, just like the proof outlines typically presented in papers. The tool then checks automatically that this outline represents a valid proof in the program logic. In this paper, we systematically develop a proof outline checker for the TaDA logic, which reduces the checking to a simpler verification problem, for which automated tools exist. Our approach leads to proof outline checkers that provide substantially more automation than interactive provers, but are much simpler to develop than custom automatic verifiers.",
    "DOI": "10.1007/978-3-030-90870-6_22",
    "publisher-place": "Cham",
    "page": "407-426",
    "page-first": "407",
    "language": "en-US",
    "_line": "FormalBib.bib:9402"
  },
  "ferrando_incrementally_nodate": {
    "id": "ferrando_incrementally_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Ferrando",
        "given": "Angelo"
      }
    ],
    "title": "Incrementally Predictive Runtime Veriﬁcation",
    "abstract": "Runtime Veriﬁcation is a lightweight formal veriﬁcation technique used to verify the runtime behaviour of software (resp. hardware) systems. Given a formal property, one or more monitors are synthesised to verify the latter against a system execution. A monitor can only conclude the violation of a property when it observes such a violation. Unfortunately, in safety-critical scenarios, this might happen too late for the system to react properly. In such scenarios, it is advised to use Predictive Runtime Veriﬁcation, where monitors are capable of anticipating (by using a model of the system) future events before actually observing them. In this work, instead of assuming such a model is given, we describe a runtime veriﬁcation workﬂow where the model is learnt and incrementally reﬁned by using process mining techniques. We present the approach and the resulting prototype tool.",
    "page": "15",
    "page-first": "15",
    "language": "en-US",
    "_line": "FormalBib.bib:9420"
  },
  "lepigre_vip_nodate": {
    "id": "lepigre_vip_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Lepigre",
        "given": "Rodolphe"
      },
      {
        "family": "Sammler",
        "given": "Michael"
      },
      {
        "family": "Memarian",
        "given": "Kayvan"
      },
      {
        "family": "Krebbers",
        "given": "Robbert"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      },
      {
        "family": "Sewell",
        "given": "Peter"
      }
    ],
    "title": "VIP: Verifying Real-World C Idioms with Integer-Pointer Casts",
    "page": "32",
    "page-first": "32",
    "volume": "6",
    "language": "en-US",
    "_line": "FormalBib.bib:9429"
  },
  "bruening_transparent_nodate": {
    "id": "bruening_transparent_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Bruening",
        "given": "Derek"
      },
      {
        "family": "Zhao",
        "given": "Qin"
      },
      {
        "family": "Amarasinghe",
        "given": "Saman"
      }
    ],
    "title": "Transparent Dynamic Instrumentation",
    "abstract": "Process virtualization provides a virtual execution environment within which an unmodiﬁed application can be monitored and controlled while it executes. The provided layer of control can be used for purposes ranging from sandboxing to compatibility to proﬁling. The additional operations required for this layer are performed clandestinely alongside regular program execution. Software dynamic instrumentation is one method for implementing process virtualization which dynamically instruments an application such that the application’s code and the inserted code are interleaved together.",
    "page": "11",
    "page-first": "11",
    "language": "en-US",
    "_line": "FormalBib.bib:9438"
  },
  "vindum_mechanized_2022": {
    "id": "vindum_mechanized_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Vindum",
        "given": "Simon Friis"
      },
      {
        "family": "Frumin",
        "given": "Dan"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      }
    ],
    "title": "Mechanized Verification of a Fine-Grained Concurrent Queue from Meta's Folly Library",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "abstract": "We present the first formal specification and verification of\nthe fine-grained concurrent multi-producer-multi-consumer\nqueue algorithm from Meta’s C++ library Folly of core infrastructure components. The queue is highly optimized,\npractical, and used by Meta in production where it scales to\nthousands of consumer and producer threads. We present\nan implementation of the algorithm in an ML-like language\nand formally prove that it is a contextual refinement of a\nsimple coarse-grained queue (a property that implies that\nthe MPMC queue is linearizable). We use the ReLoC relational logic and the Iris program logic to carry out the proof\nand to mechanize it in the Coq proof assistant. The MPMC\nqueue is implemented using three modules, and our proof\nis similarly modular. By using ReLoC and Iris’s support for\nmodular reasoning we verify each module in isolation and\ncompose these together. A key challenge of the MPMC queue\nis that it has a so-called external linearization point, which\nReLoC has no support for reasoning about. Thus we extend\nReLoC, both on paper and in Coq, with novel support for\nreasoning about external linearization points.",
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "FormalBib.bib:9447"
  },
  "kumar_efficient_2021": {
    "id": "kumar_efficient_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Kumar",
        "given": "Shivam"
      },
      {
        "family": "Agrawal",
        "given": "Anupam"
      },
      {
        "family": "Biswas",
        "given": "Swarnendu"
      }
    ],
    "title": "Efficient Data Race Detection of Async-Finish Programs Using Vector Clocks",
    "container-title": "arXiv:2112.04352 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "12",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "15"
        ]
      ]
    },
    "abstract": "Existing data race detectors for task-based programs incur large run time and space overheads. The overheads arise because of frequent lookups in fine-grained tree data structures to check whether two accesses can happen in parallel. This work shows how to efficiently apply vector clocks for dynamic race detection of async-finish programs with locks. Our proposed technique, FastRacer, builds on the FastTrack algorithm with per-task and per-variable optimizations to reduce the size of vector clocks. FastRacer also exploits the structured parallelism of async-finish programs to use a coarse-grained encoding of the dynamic task inheritance to limit the metadata in the presence of many concurrent readers. Our evaluation shows that FastRacer substantially improves time and space overheads over FastTrack and is competitive with the state-of-the-art data race detectors for async-finish programs with locks.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2112.04352",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2112.04352",
    "URL": "http://arxiv.org/abs/2112.04352",
    "_line": "FormalBib.bib:9474"
  },
  "huisman_formal_2021": {
    "id": "huisman_formal_2021",
    "type": "chapter",
    "author": [
      {
        "family": "Djoudi",
        "given": "Adel"
      },
      {
        "family": "Hána",
        "given": "Martin"
      },
      {
        "family": "Kosmatov",
        "given": "Nikolai"
      }
    ],
    "editor": [
      {
        "family": "Huisman",
        "given": "Marieke"
      },
      {
        "family": "Păsăreanu",
        "given": "Corina"
      },
      {
        "family": "Zhan",
        "given": "Naijun"
      }
    ],
    "title": "Formal Verification of a JavaCard Virtual Machine with Frama-C",
    "container-title": "Formal Methods",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "20"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-90869-0 978-3-030-90870-6",
    "abstract": "Formal veriﬁcation of real-life industrial software remains a challenging task. It provides strong guarantees of correctness, which are particularly important for security-critical products, such as smart cards. Security of a smart card strongly relies on the requirement that the underlying JavaCard virtual machine ensures necessary isolation properties. This case study paper presents a recent formal veriﬁcation of a JavaCard Virtual Machine implementation performed by Thales using the FramaC veriﬁcation toolset. This is the ﬁrst veriﬁcation project for such a large-scale industrial smart card product where deductive veriﬁcation is applied on the real-life C code. The target properties include common security properties such as integrity and conﬁdentiality. The implementation contains over 7,000 lines of C code. After a formal speciﬁcation in the ACSL speciﬁcation language, over 52,000 veriﬁcation conditions were generated and successfully proved. We present several issues identiﬁed during the project, illustrate them by representative examples and present solutions we used to solve them. Finally, we describe proof results, some lessons learned and desired tool improvements.",
    "URL": "https://link.springer.com/10.1007/978-3-030-90870-6_23",
    "DOI": "10.1007/978-3-030-90870-6_23",
    "publisher-place": "Cham",
    "page": "427-444",
    "page-first": "427",
    "volume": "13047",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "FormalBib.bib:9488"
  },
  "zhaohui_verifying_2021": {
    "id": "zhaohui_verifying_2021",
    "type": "book",
    "author": [
      {
        "family": "Zhaohui",
        "given": "Li"
      },
      {
        "family": "Feng",
        "given": "Xinyu"
      }
    ],
    "title": "Verifying Contextual Refinement with Ownership Transfer(Extended Version)",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "3"
        ]
      ]
    },
    "abstract": "Contextual refinement is a compositional approach to compositional verification of concurrent objects. There has been much work designing program logics to prove the contextual refinement between the object implementation and its abstract specification. However, these program logics for contextual refinement verification cannot support objects with resource ownership transfer, which is a common pattern in many concurrent objects, such as the memory management module in OS kernels, which transfers the allocated memory block between the object and clients. In this paper, we propose a new approach to give abstract and implementation independent specifications to concurrent objects with ownership transfer. We also design a program logic to verify contextual refinement of concurrent objects w.r.t their abstract specifications. We have successfully apply our logic to verify an implementation of the memory management module, where the implementation is an appropriately simplified version of the original version from a real-world preemptive OS kernel.",
    "_line": "FormalBib.bib:9508"
  },
  "fasse_code_nodate": {
    "id": "fasse_code_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Fasse",
        "given": "Justus"
      }
    ],
    "title": "Code Transformations to Increase Prepass Scheduling Opportunities in CompCert",
    "page": "53",
    "page-first": "53",
    "language": "en-US",
    "_line": "FormalBib.bib:9516"
  },
  "komel_meta-analysis_2021": {
    "id": "komel_meta-analysis_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Komel",
        "given": "Anja Petković"
      }
    ],
    "title": "META-ANALYSIS OF TYPE THEORIES WITH AN APPLICATION TO THE DESIGN OF FORMAL PROOFS",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "page": "194",
    "page-first": "194",
    "language": "en-US",
    "_line": "FormalBib.bib:9524"
  },
  "vishwanathan_sound_nodate": {
    "id": "vishwanathan_sound_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Vishwanathan",
        "given": "Harishankar"
      },
      {
        "family": "Shachnai",
        "given": "Matan"
      },
      {
        "family": "Narayana",
        "given": "Srinivas"
      },
      {
        "family": "Nagarakatte",
        "given": "Santosh"
      }
    ],
    "title": "Sound, Precise, and Fast Abstract Interpretation with Tristate Numbers",
    "abstract": "Extended Berkeley Packet Filter (BPF) is a language and run-time system that allows non-superusers to extend the Linux and Windows operating systems by downloading user code into the kernel. To ensure that user code is safe to run in kernel context, BPF relies on a static analyzer that proves properties about the code, such as bounded memory access and the absence of operations that crash. The BPF static analyzer checks safety using abstract interpretation with several abstract domains. Among these, the domain of tnums (tristate numbers) is a key domain used to reason about the bitwise uncertainty in program values. This paper formally speciﬁes the tnum abstract domain and its arithmetic operators. We provide the ﬁrst proofs of soundness and optimality of the abstract arithmetic operators for tnum addition and subtraction used in the BPF analyzer. Further, we describe a novel sound algorithm for multiplication of tnums that is more precise and efﬁcient (runs 33&perc; faster on average) than the Linux kernel’s algorithm. Our tnum multiplication is now merged in the Linux kernel.",
    "page": "12",
    "page-first": "12",
    "language": "en-US",
    "_line": "FormalBib.bib:9533"
  },
  "chajed_record_nodate": {
    "id": "chajed_record_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Chajed",
        "given": "Tej"
      }
    ],
    "title": "Record Updates in Coq",
    "abstract": "We describe the implementation of coq-record-update, a library that generates functions to set and update fields of Coq records to complement Coq’s existing support for field projections. The implementation abuses features of Coq typeclasses and is thus fun to describe. The library has industrial and academic users that are not in the authors’ institution, which lends credibility to the assertion that it is useful.",
    "page": "2",
    "page-first": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:9542"
  },
  "yang_verified_2017": {
    "id": "yang_verified_2017",
    "type": "chapter",
    "author": [
      {
        "family": "Guéneau",
        "given": "Armaël"
      },
      {
        "family": "Myreen",
        "given": "Magnus O."
      },
      {
        "family": "Kumar",
        "given": "Ramana"
      },
      {
        "family": "Norrish",
        "given": "Michael"
      }
    ],
    "editor": [
      {
        "family": "Yang",
        "given": "Hongseok"
      }
    ],
    "title": "Verified Characteristic Formulae for CakeML",
    "container-title": "Programming Languages and Systems",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "28"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-662-54433-4 978-3-662-54434-1",
    "abstract": "Characteristic Formulae (CF) oﬀer a productive, principled approach to generating veriﬁcation conditions for higher-order imperative programs, but so far the soundness of CF has only been considered with respect to an informal speciﬁcation of a programming language (OCaml). This leaves a gap between what is established by the veriﬁcation framework and the program that actually runs. We present a fullyﬂedged CF framework for the formally speciﬁed CakeML programming language. Our framework extends the existing CF approach to support exceptions and I/O, thereby covering the full feature set of CakeML, and comes with a formally veriﬁed soundness theorem. Furthermore, it integrates with existing proof techniques for verifying CakeML programs. This validates the CF approach, and allows users to prove end-to-end theorems for higher-order imperative programs, from speciﬁcation to language semantics, within a single theorem prover.",
    "URL": "https://link.springer.com/10.1007/978-3-662-54434-1_22",
    "DOI": "10.1007/978-3-662-54434-1_22",
    "publisher-place": "Berlin, Heidelberg",
    "page": "584-610",
    "page-first": "584",
    "volume": "10201",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "FormalBib.bib:9551"
  },
  "myreen_minimalistic_2021-1": {
    "id": "myreen_minimalistic_2021-1",
    "type": "paper-conference",
    "author": [
      {
        "family": "Myreen",
        "given": "Magnus O."
      }
    ],
    "title": "A minimalistic verified bootstrapped compiler (proof pearl)",
    "container-title": "Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "event-title": "CPP '21: 10th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "28"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-8299-1",
    "abstract": "This paper shows how a small verified bootstrapped compiler can be developed inside an interactive theorem prover (ITP). Throughout, emphasis is put on clarity and minimalism.",
    "URL": "https://dl.acm.org/doi/10.1145/3437992.3439915",
    "DOI": "10.1145/3437992.3439915",
    "publisher-place": "Virtual Denmark",
    "page": "32-45",
    "page-first": "32",
    "language": "en-US",
    "_line": "FormalBib.bib:9571"
  },
  "noauthor_coq_nodate": {
    "id": "noauthor_coq_nodate",
    "type": "webpage",
    "title": "Coq Coq correct! verification of type checking and erasure for Coq, in Coq",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "30"
        ]
      ]
    },
    "URL": "https://dl.acm.org/doi/epdf/10.1145/3371076",
    "DOI": "10.1145/3371076",
    "_line": "FormalBib.bib:9589"
  },
  "beeson_proof-checking_2019": {
    "id": "beeson_proof-checking_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Beeson",
        "given": "Michael"
      },
      {
        "family": "Narboux",
        "given": "Julien"
      },
      {
        "family": "Wiedijk",
        "given": "Freek"
      }
    ],
    "title": "Proof-checking Euclid",
    "container-title": "Annals of Mathematics and Artificial Intelligence",
    "container-title-short": "Ann Math Artif Intell",
    "issued": {
      "date-parts": [
        [
          "2019",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "30"
        ]
      ]
    },
    "issn": "1012-2443, 1573-7470",
    "abstract": "We used computer proof-checking methods to verify the correctness of our proofs of the propositions in Euclid Book I. We used axioms as close as possible to those of Euclid, in a language closely related to that used in Tarski’s formal geometry. We used proofs as close as possible to those given by Euclid, but ﬁlling Euclid’s gaps and correcting errors. Euclid Book I has 48 propositions; we proved 235 theorems. The extras were partly “Book Zero”, preliminaries of a very fundamental nature, partly propositions that Euclid omitted but were used implicitly, partly advanced theorems that we found necessary to ﬁll Euclid’s gaps, and partly just variants of Euclid’s propositions. We wrote these proofs in a simple fragment of ﬁrst-order logic corresponding to Euclid’s logic, debugged them using a custom software tool, and then checked them in the well-known and trusted proof checkers HOL Light and Coq.",
    "URL": "http://link.springer.com/10.1007/s10472-018-9606-x",
    "DOI": "10.1007/s10472-018-9606-x",
    "page": "213-257",
    "page-first": "213",
    "volume": "85",
    "issue": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:9597"
  },
  "bourdoncle_efficient_1993": {
    "id": "bourdoncle_efficient_1993",
    "type": "paper-conference",
    "author": [
      {
        "family": "Bourdoncle",
        "given": "François"
      }
    ],
    "title": "Efficient chaotic iteration strategies with widenings",
    "issued": {
      "date-parts": [
        [
          "1993"
        ]
      ]
    },
    "publisher": "Springer-Verlag",
    "abstract": "Abstract. Abstract interpretation is a formal method that enables the static and automatic determination of run-time properties of programs. This method uses a characterization of program invariants as least and greatest fixed points of continuous functions over complete lattices of program properties. In this paper, we study precise and efficient chaotic iteration strategies for computing such fixed points when lattices are of infinite height and speedup techniques, known as widening and narrowing, have to be used. These strategies are based on a weak topological ordering of the dependency graph of the system of semantic equations associated with the program and minimize the loss in precision due to the use of widening operators. We discuss complexity and implementation issues and give precise upper bounds on the complexity of the intraprocedural and interprocedural abstract interpretation of higher-order programs based on the structure of their control flow graph. 1",
    "page": "128-141",
    "page-first": "128",
    "_line": "FormalBib.bib:9615"
  },
  "de_vilhena_separation_2021": {
    "id": "de_vilhena_separation_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Vilhena",
        "given": "Paulo Emílio",
        "dropping-particle": "de"
      },
      {
        "family": "Pottier",
        "given": "François"
      }
    ],
    "title": "A separation logic for effect handlers",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "5"
        ]
      ]
    },
    "abstract": "User-defined effects and effect handlers are advertised and advocated as a relatively easy-to-understand and modular approach to delimited control. They offer the ability of suspending and resuming a computation and allow information to be transmitted both ways between the computation, which requests a certain service, and the handler, which provides this service. Yet, a key question remains, to this day, largely unanswered: how does one modularly specify and verify programs in the presence of both user-defined effect handlers and primitive effects, such as heap-allocated mutable state? We answer this question by presenting a Separation Logic with built-in support for effect handlers, both shallow and deep. The specification of a program fragment includes a protocol that describes the effects that the program may perform as well as the replies that it can expect to receive. The logic allows local reasoning via a frame rule and a bind rule. It is based on Iris and inherits all of its advanced features, including support for higher-order functions, user-defined ghost state, and invariants. We illustrate its power via several case studies, including (1) a generic formulation of control inversion, which turns a producer that \"pushes\" elements towards a consumer into a producer from which one can \"pull\" elements on demand, and (2) a simple system for cooperative concurrency, where several threads execute concurrently, can spawn new threads, and communicate via promises.",
    "keywords": "program verification, separation logic, effect handlers",
    "URL": "https://doi.org/10.1145/3434314",
    "DOI": "10.1145/3434314",
    "page": "33:1-33:28",
    "page-first": "33",
    "volume": "5",
    "_line": "FormalBib.bib:9625"
  },
  "medina-martinez_database_2021": {
    "id": "medina-martinez_database_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Medina-Martínez",
        "given": "Diego"
      },
      {
        "family": "Bárcenas",
        "given": "Everardo"
      },
      {
        "family": "Molero-Castillo",
        "given": "Guillermo"
      },
      {
        "family": "Velázquez-Mena",
        "given": "Alejandro"
      },
      {
        "family": "Aldeco-Pérez",
        "given": "Rocío"
      }
    ],
    "title": "Database Management System Verification with Separation Logics",
    "container-title": "Programming and Computer Software",
    "container-title-short": "Program Comput Soft",
    "issued": {
      "date-parts": [
        [
          "2021",
          "12",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "5"
        ]
      ]
    },
    "issn": "1608-3261",
    "abstract": "Program verification consists in finding a formal proof that the program satisfies a given specification. This specification can be described as assertions about the input and output a correct program must satisfy. Assertions and programs are traditionally specified in terms of classical first order logic (FOL). FOL reasoners (inference systems) automatically find the correspondent program correctness proof, if any. However, verification of programs with mutable data structures, such as pointers, is currently a major challenge for the FOL assertion based approach. Mutable data structures are often written in terms of syntactically unrelated expressions, whose specification represents a significant defiance for FOL. Separation logics are a family of formal languages with specially-purposed constructors designed to model mutable data structures. In this paper, we formally verify a database management system using separation logics. We focused on the verification of libraries containing programs about heap manipulation. Several detected bugs are described in detail, respective solutions are also provided.",
    "URL": "https://doi.org/10.1134/S036176882108017X",
    "DOI": "10.1134/S036176882108017X",
    "page": "654-672",
    "page-first": "654",
    "volume": "47",
    "issue": "8",
    "language": "en-US",
    "_line": "FormalBib.bib:9642"
  },
  "ambal_certified_nodate": {
    "id": "ambal_certified_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Ambal",
        "given": "Guillaume"
      },
      {
        "family": "Lenglet",
        "given": "Sergueï"
      },
      {
        "family": "Schmitt",
        "given": "Alan"
      }
    ],
    "title": "Certified Abstract Machines for Skeletal Semantics",
    "page": "14",
    "page-first": "14",
    "language": "en-US",
    "_line": "FormalBib.bib:9660"
  },
  "appel_coqs_2022": {
    "id": "appel_coqs_2022",
    "type": "paper-conference",
    "author": [
      {
        "family": "Appel",
        "given": "Andrew W."
      }
    ],
    "title": "Coq’s vibrant ecosystem for verification engineering (invited talk)",
    "container-title": "Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "event-title": "CPP '22: 11th ACM SIGPLAN International Conference on Certified Programs and Proofs",
    "issued": {
      "date-parts": [
        [
          "2022",
          "1",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "14"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-9182-5",
    "abstract": "Program verification in the large is not only a matter of mechanizing a program logic to handle the semantics of your programming language. You must reason in the mathematics of your application domain—and there are many application domains, each with their own community of domain experts. So you will need to import mechanized proof theories from many domains, and they must all interoperate. Such an ecosystem is not only a matter of mathematics, it is a matter of software process engineering and social engineering. Coq’s ecosystem has been maturing nicely in these senses.",
    "URL": "https://dl.acm.org/doi/10.1145/3497775.3503951",
    "DOI": "10.1145/3497775.3503951",
    "publisher-place": "Philadelphia PA USA",
    "page": "2-11",
    "page-first": "2",
    "language": "en-US",
    "_line": "FormalBib.bib:9668"
  },
  "porncharoenwase_formal_2022": {
    "id": "porncharoenwase_formal_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Porncharoenwase",
        "given": "Sorawee"
      },
      {
        "family": "Nelson",
        "given": "Luke"
      },
      {
        "family": "Wang",
        "given": "Xi"
      },
      {
        "family": "Torlak",
        "given": "Emina"
      }
    ],
    "title": "A formal foundation for symbolic evaluation with merging",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2022",
          "1",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "14"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "Reusable symbolic evaluators are a key building block of solver-aided verification and synthesis tools. A reusable evaluator reduces the semantics of all paths in a program to logical constraints, and a client tool uses these constraints to formulate a satisfiability query that is discharged with SAT or SMT solvers. The correctness of the evaluator is critical to the soundness of the tool and the domain properties it aims to guarantee. Yet so far, the trust in these evaluators has been based on an ad-hoc foundation of testing and manual reasoning.\n   This paper presents the first formal framework for reasoning about the behavior of reusable symbolic evaluators. We develop a new symbolic semantics for these evaluators that incorporates state merging. Symbolic evaluators use state merging to avoid path explosion and generate compact encodings. To accommodate a wide range of implementations, our semantics is parameterized by a symbolic factory, which abstracts away the details of merging and creation of symbolic values. The semantics targets a rich language that extends Core Scheme with assumptions and assertions, and thus supports branching, loops, and (first-class) procedures. The semantics is designed to support reusability, by guaranteeing two key properties: legality of the generated symbolic states, and the reducibility of symbolic evaluation to concrete evaluation. Legality makes it simpler for client tools to formulate queries, and reducibility enables testing of client tools on concrete inputs. We use the Lean theorem prover to mechanize our symbolic semantics, prove that it is sound and complete with respect to the concrete semantics, and prove that it guarantees legality and reducibility.\n   To demonstrate the generality of our semantics, we develop Leanette, a reference evaluator written in Lean, and Rosette 4, an optimized evaluator written in Racket. We prove Leanette correct with respect to the semantics, and validate Rosette 4 against Leanette via solver-aided differential testing. To demonstrate the practicality of our approach, we port 16 published verification and synthesis tools from Rosette 3 to Rosette 4. Rosette 3 is an existing reusable evaluator that implements the classic merging semantics, adopted from bounded model checking. Rosette 4 replaces the semantic core of Rosette 3 but keeps its optimized symbolic factory. Our results show that Rosette 4 matches the performance of Rosette 3 across a wide range of benchmarks, while providing a cleaner interface that simplifies the implementation of client tools.",
    "URL": "https://dl.acm.org/doi/10.1145/3498709",
    "DOI": "10.1145/3498709",
    "page": "1-28",
    "page-first": "1",
    "volume": "6",
    "language": "en-US",
    "_line": "FormalBib.bib:9686"
  },
  "coward_formal_nodate": {
    "id": "coward_formal_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Coward",
        "given": "Samuel"
      },
      {
        "family": "Paulson",
        "given": "Lawrence"
      },
      {
        "family": "Drane",
        "given": "Theo"
      },
      {
        "family": "Morini",
        "given": "Emiliano"
      }
    ],
    "title": "Formal Veriﬁcation of Transcendental Fixed and Floating Point Algorithms using an Automatic Theorem Prover",
    "abstract": "We present a method for formal veriﬁcation of transcendental hardware and software algorithms that scales to higher precision without suﬀering an exponential growth in runtimes. A class of implementations using piecewise polynomial approximation to compute the result is veriﬁed using MetiTarski, an automated theorem prover, which veriﬁes a range of inputs for each call. The method was applied to commercial implementations from Cadence Design Systems with signiﬁcant runtime gains over exhaustive testing methods and was successful in proving that the expected accuracy of one implementation was overly optimistic. Reproducing the veriﬁcation of a sine implementation in software, previously done using an alternative theorem proving technique, demonstrates that the MetiTarski approach is a viable competitor. Veriﬁcation of a 52 bit implementation of the square root function highlights the method’s high precision capabilities.",
    "page": "20",
    "page-first": "20",
    "language": "en-US",
    "_line": "FormalBib.bib:9706"
  },
  "bodin_trusted_2014": {
    "id": "bodin_trusted_2014",
    "type": "paper-conference",
    "author": [
      {
        "family": "Bodin",
        "given": "Martin"
      },
      {
        "family": "Chargueraud",
        "given": "Arthur"
      },
      {
        "family": "Filaretti",
        "given": "Daniele"
      },
      {
        "family": "Gardner",
        "given": "Philippa"
      },
      {
        "family": "Maffeis",
        "given": "Sergio"
      },
      {
        "family": "Naudziuniene",
        "given": "Daiva"
      },
      {
        "family": "Schmitt",
        "given": "Alan"
      },
      {
        "family": "Smith",
        "given": "Gareth"
      }
    ],
    "title": "A trusted mechanised JavaScript specification",
    "container-title": "Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "event-title": "POPL '14: The 41st Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "issued": {
      "date-parts": [
        [
          "2014",
          "1",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "20"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-2544-8",
    "abstract": "JavaScript is the most widely used web language for client-side applications. Whilst the development of JavaScript was initially just led by implementation, there is now increasing momentum behind the ECMA standardisation process. The time is ripe for a formal, mechanised speciﬁcation of JavaScript, to clarify ambiguities in the ECMA standards, to serve as a trusted reference for high-level language compilation and JavaScript implementations, and to provide a platform for high-assurance proofs of language properties.",
    "URL": "https://dl.acm.org/doi/10.1145/2535838.2535876",
    "DOI": "10.1145/2535838.2535876",
    "publisher-place": "San Diego California USA",
    "page": "87-100",
    "page-first": "87",
    "language": "en-US",
    "_line": "FormalBib.bib:9715"
  },
  "distefano_scaling_2019": {
    "id": "distefano_scaling_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Distefano",
        "given": "Dino"
      },
      {
        "family": "Fähndrich",
        "given": "Manuel"
      },
      {
        "family": "Logozzo",
        "given": "Francesco"
      },
      {
        "family": "O'Hearn",
        "given": "Peter W."
      }
    ],
    "title": "Scaling static analyses at Facebook",
    "container-title": "Communications of the ACM",
    "container-title-short": "Commun. ACM",
    "issued": {
      "date-parts": [
        [
          "2019",
          "7",
          "24"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "20"
        ]
      ]
    },
    "issn": "0001-0782",
    "abstract": "Key lessons for designing static analyses tools deployed to find bugs in hundreds of millions of lines of code.",
    "URL": "https://doi.org/10.1145/3338112",
    "DOI": "10.1145/3338112",
    "page": "62-70",
    "page-first": "62",
    "volume": "62",
    "issue": "8",
    "_line": "FormalBib.bib:9733"
  },
  "le_finding_nodate-1": {
    "id": "le_finding_nodate-1",
    "type": "article-journal",
    "author": [
      {
        "family": "Le",
        "given": "Quang Loc"
      },
      {
        "family": "Raad",
        "given": "Azalea"
      },
      {
        "family": "Villard",
        "given": "Jules"
      },
      {
        "family": "Berdine",
        "given": "Josh"
      },
      {
        "family": "Dreyer",
        "given": "Derek"
      },
      {
        "family": "O'Hearn",
        "given": "Peter W"
      }
    ],
    "title": "Finding Real Bugs in Big Programs Appendix",
    "page": "6",
    "page-first": "6",
    "volume": "1",
    "issue": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:9750"
  },
  "finkbeiner_loop_2022": {
    "id": "finkbeiner_loop_2022",
    "type": "chapter",
    "author": [
      {
        "family": "Ernst",
        "given": "Gidon"
      }
    ],
    "editor": [
      {
        "family": "Finkbeiner",
        "given": "Bernd"
      },
      {
        "family": "Wies",
        "given": "Thomas"
      }
    ],
    "title": "Loop Verification with Invariants and Contracts",
    "container-title": "Verification, Model Checking, and Abstract Interpretation",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "27"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-94582-4 978-3-030-94583-1",
    "abstract": "Invariants are the predominant approach to verify the correctness of loops. As an alternative, loop contracts, which make explicit the premise and conclusion of the underlying induction proof, can sometimes capture correctness conditions more naturally. But despite this advantage, the second approach receives little attention overall, and the goal of this paper is to lift it out of its niche. We give the ﬁrst comprehensive exposition of the theory of loop contracts, including a characterization of its completeness. We show concrete examples on standard algorithms that showcase their relative merits. Moreover, we demonstrate a novel constructive translation between the two approaches, which decouples the chosen speciﬁcation approach from the veriﬁcation backend.",
    "URL": "https://link.springer.com/10.1007/978-3-030-94583-1_4",
    "DOI": "10.1007/978-3-030-94583-1_4",
    "publisher-place": "Cham",
    "page": "69-92",
    "page-first": "69",
    "volume": "13182",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "FormalBib.bib:9760"
  },
  "bereczky_mechanizing_2022": {
    "id": "bereczky_mechanizing_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Bereczky",
        "given": "Péter"
      },
      {
        "family": "Chen",
        "given": "Xiaohong"
      },
      {
        "family": "Horpácsi",
        "given": "Dániel"
      },
      {
        "family": "Mizsei",
        "given": "Tamás Bálint"
      },
      {
        "family": "Peña",
        "given": "Lucas"
      },
      {
        "family": "Tusil",
        "given": "Jan"
      }
    ],
    "title": "Mechanizing Matching Logic in Coq",
    "container-title": "arXiv:2201.05716 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2022",
          "1",
          "19"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "27"
        ]
      ]
    },
    "abstract": "Matching logic is a formalism for specifying and reasoning about structures using patterns and pattern matching. Growing in popularity, matching logic has been used to define many logical systems such as separation logic with recursive definitions and linear-temporal logic. Despite this, there is no way for a user to define his or her own matching logic theories using a theorem prover, with maximal assurance of the properties being proved. Hence, in this work, we formalized a version of matching logic using the Coq proof assistant. Specifically, we create a new version of matching logic that uses a locally nameless representation, where quantified variables are unnamed in order to aid verification. We formalize the syntax, semantics, and proof system of this representation of matching logic using the Coq proof assistant. Crucially, we also verify the soundness of the formalized proof system, thereby guaranteeing that any matching logic properties proved in our Coq formalization are indeed correct. We believe this work provides a previously unexplored avenue for defining and proving matching logic theories and properties.",
    "keywords": "Computer Science - Logic in Computer Science",
    "URLtext": "2201.05716",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2201.05716",
    "URL": "http://arxiv.org/abs/2201.05716",
    "_line": "FormalBib.bib:9780"
  },
  "zeng_static_2012": {
    "id": "zeng_static_2012",
    "type": "report",
    "genre": "Technical Report",
    "author": [
      {
        "family": "Zeng",
        "given": "Bin"
      }
    ],
    "title": "Static Analysis on Binary Code",
    "issued": {
      "date-parts": [
        [
          "2012"
        ]
      ]
    },
    "publisher": "Lehigh University",
    "abstract": "As the number and sophistication of attacks increase, static analysis gains attention. Since it is binary code that is executed directly on the bare-metal, binary-level static analysis oﬀers root-cause approaches to security problems such as malware detection. In this survey, we start with the challenges to do binary-level static analysis and then transfer to the advantages of carrying out static analysis on binary code. After that, we introduce some typical binary-level static analysis algorithms including disassembly, control ﬂow graph construction, dataﬂow analysis, alias analysis algorithms et al. Based on the current situation and approaches, we express our own opinions on the future work and propose some preliminary ideas to solve these problems.",
    "publisher-place": "Bethlehem, PA",
    "page": "19",
    "page-first": "19",
    "language": "en-US",
    "_line": "FormalBib.bib:9794"
  },
  "fornaia_jscan_2019": {
    "id": "fornaia_jscan_2019",
    "type": "book",
    "author": [
      {
        "family": "Fornaia",
        "given": "Andrea"
      },
      {
        "family": "Scafiti",
        "given": "Stefano"
      },
      {
        "family": "Tramontana",
        "given": "Emiliano"
      }
    ],
    "title": "JSCAN: Designing an Easy to use LLVM-Based Static Analysis Framework",
    "container-title-short": "JSCAN",
    "title-short": "JSCAN",
    "issued": {
      "date-parts": [
        [
          "2019",
          "6",
          "1"
        ]
      ]
    },
    "number-of-pages": "237",
    "DOI": "10.1109/WETICE.2019.00058",
    "note": "Pages: 242",
    "_line": "FormalBib.bib:9807"
  },
  "bora_openmp_2021": {
    "id": "bora_openmp_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bora",
        "given": "Utpal"
      },
      {
        "family": "Vaishay",
        "given": "Shraiysh"
      },
      {
        "family": "Joshi",
        "given": "Saurabh"
      },
      {
        "family": "Upadrasta",
        "given": "Ramakrishna"
      }
    ],
    "title": "OpenMP aware MHP Analysis for Improved Static Data-Race Detection",
    "container-title": "arXiv:2111.04259 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "31"
        ]
      ]
    },
    "abstract": "Data races, a major source of bugs in concurrent programs, can result in loss of manpower and time as well as data loss due to system failures. OpenMP, the de facto shared memory parallelism framework used in the HPC community, also suffers from data races. To detect race conditions in OpenMP programs and improve turnaround time and/or developer productivity, we present a data flow analysis based, fast, static data race checker in the LLVM compiler framework. Our tool can detect races in the presence or absence of explicit barriers, with implicit or explicit synchronization. In addition, our tool effectively works for the OpenMP target offloading constructs and also supports the frequently used OpenMP constructs. We formalize and provide a data flow analysis framework to perform Phase Interval Analysis (PIA) of OpenMP programs. Phase intervals are then used to compute the MHP (and its complement NHP) sets for the programs, which, in turn, are used to detect data races statically. We evaluate our work using multiple OpenMP race detection benchmarks and real world applications. Our experiments show that the checker is comparable to the state-of-the-art in various performance metrics with around 90&perc; accuracy, almost perfect recall, and significantly lower runtime and memory footprint.",
    "keywords": "Computer Science - Programming Languages, D.2.4, D.1.3, D.3.4, Computer Science - Software Engineering, D.2.5",
    "URLtext": "2111.04259",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2111.04259",
    "URL": "http://arxiv.org/abs/2111.04259",
    "_line": "FormalBib.bib:9818"
  },
  "ballabriga_static_2019": {
    "id": "ballabriga_static_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Ballabriga",
        "given": "Clément"
      },
      {
        "family": "Forget",
        "given": "Julien"
      },
      {
        "family": "Gonnord",
        "given": "Laure"
      },
      {
        "family": "Lipari",
        "given": "Giuseppe"
      },
      {
        "family": "Ruiz",
        "given": "Jordy"
      }
    ],
    "title": "Static Analysis Of Binary Code With Memory Indirections Using Polyhedra",
    "container-title": "VMCAI'19 - International Conference on Verification, Model Checking, and Abstract Interpretation",
    "collection-title": "LNCS",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "31"
        ]
      ]
    },
    "publisher": "Springer",
    "abstract": "In this paper we propose a new abstract domain for staticanalysis of binary code. Our motivation stems from the need to im-prove the precision of the estimation of the Worst-Case Execution Time(WCET) of safety-critical real-time code. WCET estimation requirescomputing information such as upper bounds on the number of loopiterations, unfeasible execution paths, etc. These estimations are usuallyperformed on binary code, mainly to avoid making assumptions on howthe compiler works. Our abstract domain, based on polyhedra and ontwo mapping functions that associate polyhedra variables with registersand memory, targets the precise computation of such information. Weprove the correctness of the method, and demonstrate its effectivenesson benchmarks and examples from typical embedded code.",
    "keywords": "binary analysis, polyhedra, Worst-case Execution Time WCET",
    "URL": "https://hal.archives-ouvertes.fr/hal-01939659",
    "DOI": "10.1007/978-3-030-11245-5_6",
    "publisher-place": "Cascais, Portugal",
    "page": "114-135",
    "page-first": "114",
    "volume": "11388",
    "_line": "FormalBib.bib:9832"
  },
  "dral_verified_2022": {
    "id": "dral_verified_2022",
    "type": "thesis",
    "genre": "Masters",
    "author": [
      {
        "family": "Dral",
        "given": "Joris"
      }
    ],
    "title": "Verified Compiler Optimisations",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "publisher": "Utrecht University",
    "number-of-pages": "62",
    "abstract": "This thesis explores how to formally verify the correctness of a certifying compiler where the correctness of individual compiler runs is specified by translation relations, each of which characterises the admissible behaviour of a single\ncompiler pass. Whereas the correctness of compiler passes follows from the fact that we can recognise compiler behaviour, there is no guarantee that the recognised behaviour is correct. This motivates the need for formal guarantees\nthat express that translation relations themselves are well-behaved.\n\n...",
    "language": "en-US",
    "_line": "FormalBib.bib:9850"
  },
  "echenim_proof_2022": {
    "id": "echenim_proof_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Echenim",
        "given": "Mnacho"
      },
      {
        "family": "Peltier",
        "given": "Nicolas"
      }
    ],
    "title": "A Proof Procedure For Separation Logic With Inductive Definitions and Theory Reasoning",
    "container-title": "arXiv:2201.13227 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2022",
          "1",
          "31"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "8"
        ]
      ]
    },
    "abstract": "A proof procedure, in the spirit of the sequent calculus, is proposed to check the validity of entailments between Separation Logic formulas combining inductively defined predicates denoted structures of bounded tree width and theory reasoning. The calculus is sound and complete, in the sense that a sequent is valid iff it admits a (possibly infinite) proof tree. We show that the procedure terminates in the two following cases: (i) When the inductive rules that define the predicates occurring on the left-hand side of the entailment terminate, in which case the proof tree is always finite. (ii) When the theory is empty, in which case every valid sequent admits a rational proof tree, where the total number of pairwise distinct sequents occurring in the proof tree is doubly exponential w.r.t.&bslash; the size of the end-sequent. We also show that the validity problem is undecidable for a wide class of theories, even with a very low expressive power.",
    "keywords": "Computer Science - Logic in Computer Science, F.4.1, 03B70",
    "URLtext": "2201.13227",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2201.13227",
    "URL": "http://arxiv.org/abs/2201.13227",
    "_line": "FormalBib.bib:9866"
  },
  "konecny_extracting_2022": {
    "id": "konecny_extracting_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Konečný",
        "given": "Michal"
      },
      {
        "family": "Park",
        "given": "Sewon"
      },
      {
        "family": "Thies",
        "given": "Holger"
      }
    ],
    "title": "Extracting efficient exact real number computation from proofs in constructive type theory",
    "container-title": "arXiv:2202.00891 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "8"
        ]
      ]
    },
    "abstract": "Exact real computation is an alternative to floating-point arithmetic where operations on real numbers are performed exactly, without the introduction of rounding errors. When proving the correctness of an implementation, one can focus solely on the mathematical properties of the problem without thinking about the subtleties of representing real numbers. We propose a new axiomatization of the real numbers in a dependent type theory with the goal of extracting certified exact real computation programs from constructive proofs. Our formalization differs from similar approaches, in that we formalize the reals in a conceptually similar way as some mature implementations of exact real computation. Primitive operations on reals can be extracted directly to the corresponding operations in such an implementation, producing more efficient programs. We particularly focus on the formalization of partial and nondeterministic computation, which is essential in exact real computation. We prove the soundness of our formalization with regards of the standard realizability interpretation from computable analysis and show how to relate our theory to a classical formalization of the reals. We demonstrate the feasibility of our theory by implementing it in the Coq proof assistant and present several natural examples. From the examples we have automatically extracted Haskell programs that use the exact real computation framework AERN for efficiently performing exact operations on real numbers. In experiments, the extracted programs behave similarly to native implementations in AERN in terms of running time and memory usage.",
    "keywords": "Computer Science - Logic in Computer Science",
    "URLtext": "2202.00891",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2202.00891",
    "URL": "http://arxiv.org/abs/2202.00891",
    "_line": "FormalBib.bib:9880"
  },
  "stefano_verification_nodate": {
    "id": "stefano_verification_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Stefano",
        "given": "Luca Di"
      }
    ],
    "title": "Verification of Distributed Systems via Sequential Emulation",
    "abstract": "LUCA DI STEFANO, Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP∗, LIG, France ROCCO DE NICOLA, IMT School of Advanced Studies, Italy OMAR INVERSO, Gran Sasso Science Institute (GSSI), Italy Sequential emulation is a semantics-based technique to automatically reduce property checking of distributed systems to the analysis of sequential programs. An automated procedure takes as input a formal specification of a distributed system, a property of interest and the structural operational semantics of the specification language and generates a sequential program whose execution traces emulate the possible evolutions of the considered system. The problem as to whether the property of interest holds for the system can then be expressed either as a reachability or as a termination query on the program. This allows to immediately adapt mature verification techniques developed for general-purpose languages to domain-specific languages, and to effortlessly integrate new techniques as soon as they become available. We test our approach on a selection of concurrent systems originated from different contexts from population protocols to models of flocking behaviour. By combining a comprehensive range of program verification techniques, from traditional symbolic execution to modern inductive-based methods such as property-directed reachability, we are able to draw consistent and correct verification verdicts for the considered systems. CCS Concepts: • General and reference → Verification; • Software and its engineering → Automated static analysis; • Theory of computation → Process calculi.",
    "page": "42",
    "page-first": "42",
    "language": "en-US",
    "_line": "FormalBib.bib:9894"
  },
  "li_formal_2022": {
    "id": "li_formal_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "Liyi"
      },
      {
        "family": "Liu",
        "given": "Yiyun"
      },
      {
        "family": "Postol",
        "given": "Deena L."
      },
      {
        "family": "Lampropoulos",
        "given": "Leonidas"
      },
      {
        "family": "Van Horn",
        "given": "David"
      },
      {
        "family": "Hicks",
        "given": "Michael"
      }
    ],
    "title": "A Formal Model of Checked C",
    "container-title": "arXiv:2201.13394 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2022",
          "1",
          "31"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "8"
        ]
      ]
    },
    "abstract": "We present a formal model of Checked C, a dialect of C that aims to enforce spatial memory safety. Our model pays particular attention to the semantics of dynamically sized, potentially null-terminated arrays. We formalize this model in Coq, and prove that any spatial memory safety errors can be blamed on portions of the program labeled unchecked; this is a Checked C feature that supports incremental porting and backward compatibility. While our model's operational semantics uses annotated (\"fat\") pointers to enforce spatial safety, we show that such annotations can be safely erased: Using PLT Redex we formalize an executable version of our model and a compilation procedure from it to an untyped C-like language, and use randomized testing to validate that generated code faithfully simulates the original. Finally, we develop a custom random generator for well-typed and almost-well-typed terms in our Redex model, and use it to search for inconsistencies between our model and the Clang Checked C implementation. We find these steps to be a useful way to co-develop a language (Checked C is still in development) and a core model of it.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Software Engineering, D.3.1",
    "URLtext": "2201.13394",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2201.13394",
    "URL": "http://arxiv.org/abs/2201.13394",
    "_line": "FormalBib.bib:9903"
  },
  "kanabar_taming_nodate": {
    "id": "kanabar_taming_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Kanabar",
        "given": "Hrutvik"
      },
      {
        "family": "Fox",
        "given": "Anthony C J"
      },
      {
        "family": "Myreen",
        "given": "Magnus O"
      }
    ],
    "title": "Taming an Authoritative Armv8 ISA Specification: L3 Validation and CakeML Compiler Verification",
    "abstract": "Machine-readable specifications for the Armv8 instruction set architecture have become publicly available as part of Arm’s release processes, providing an official and unambiguous source of truth for the semantics of Arm instructions. To date, compiler and machine code verification efforts have made use of unofficial theorem proving friendly specifications of Armv8, e.g. CakeML uses an L3-based specification. The validity of these verification efforts hinges upon their unofficial ISA specifications being valid with respect to the official Arm specification.",
    "page": "21",
    "page-first": "21",
    "language": "en-US",
    "_line": "FormalBib.bib:9917"
  },
  "batz_foundations_2022": {
    "id": "batz_foundations_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Batz",
        "given": "Kevin"
      },
      {
        "family": "Fesefeldt",
        "given": "Ira"
      },
      {
        "family": "Jansen",
        "given": "Marvin"
      },
      {
        "family": "Katoen",
        "given": "Joost-Pieter"
      },
      {
        "family": "Keßler",
        "given": "Florian"
      },
      {
        "family": "Matheja",
        "given": "Christoph"
      },
      {
        "family": "Noll",
        "given": "Thomas"
      }
    ],
    "title": "Foundations for Entailment Checking in Quantitative Separation Logic (extended version)",
    "container-title": "arXiv:2201.11464 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2022",
          "1",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "8"
        ]
      ]
    },
    "abstract": "Quantitative separation logic (QSL) is an extension of separation logic (SL) for the verification of probabilistic pointer programs. In QSL, formulae evaluate to real numbers instead of truth values, e.g., the probability of memory-safe termination in a given symbolic heap. As with &bslash;SL, one of the key problems when reasoning with QSL is &bslash;emph&lcurly;entailment&rcurly;: does a formula f entail another formula g? We give a generic reduction from entailment checking in QSL to entailment checking in SL. This allows to leverage the large body of SL research for the automated verification of probabilistic pointer programs. We analyze the complexity of our approach and demonstrate its applicability. In particular, we obtain the first decidability results for the verification of such programs by applying our reduction to a quantitative extension of the well-known symbolic-heap fragment of separation logic.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "2201.11464",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2201.11464",
    "URL": "http://arxiv.org/abs/2201.11464",
    "_line": "FormalBib.bib:9926"
  },
  "pit-claudel_relational_nodate": {
    "id": "pit-claudel_relational_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Pit-Claudel",
        "given": "Clément"
      }
    ],
    "title": "Relational compilation: functional-to-imperative code generation for performance-critical applications",
    "abstract": "Purely functional programs verified using interactive theorem provers typically need to be translated to run: either by extracting them to a similar language (like Coq to OCaml) or by proving them equivalent to deeply embedded implementations (like C programs). Traditionally, the first approach is automated but produces unverified programs with average performance, and the second approach is manual but produces verified, highperformance programs. This thesis shows how to recast program extraction as a proof-search problem to automatically derive correct-by-construction, high-performance code from shallowly embedded functional programs. It introduces a unifying framework, relational compilation, to capture and extend recent developments in program extraction, with a focus on modularity and sound extensibility. To demonstrate the value of this approach, it then presents Rupicola, a relational compiler-construction toolkit designed to extract fast, verified, idiomatic low-level code from annotated functional models. The originality of this approach lies in its combination of foundational proofs, extensibility, and performance, backed by an unconventional take on compiler extensions: unlike traditional compilers, Rupicola generates good code not because of clever built-in optimizations, but because it allows expert users to plug in domain- and sometimes programspecific extensions that allow them to generate exactly the low-level code that they want. This thesis demonstrates the benefits of this approach through case studies and performance benchmarks that highlight how easy Rupicola makes it to create domain-specific compilers that generate code with performance comparable to that of handwritten C programs.",
    "page": "160",
    "page-first": "160",
    "language": "en-US",
    "_line": "FormalBib.bib:9940"
  },
  "choudhury_towards_nodate": {
    "id": "choudhury_towards_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Choudhury",
        "given": "Pritam"
      }
    ],
    "title": "Towards a Formalization of Nominal Sets in Coq",
    "page": "3",
    "page-first": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:9949"
  },
  "appel_c-language_2020": {
    "id": "appel_c-language_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Appel",
        "given": "Andrew W"
      }
    ],
    "title": "C-language ﬂoating-point proofs layered with VST and Flocq",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "FormalBib.bib:9957"
  },
  "darais_constructive_2016": {
    "id": "darais_constructive_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Darais",
        "given": "David"
      },
      {
        "family": "Van Horn",
        "given": "David"
      }
    ],
    "title": "Constructive Galois Connections: Taming the Galois Connection Framework for Mechanized Metatheory",
    "container-title": "arXiv:1511.06965 \\[cs\\]",
    "container-title-short": "Constructive Galois Connections",
    "title-short": "Constructive Galois Connections",
    "issued": {
      "date-parts": [
        [
          "2016",
          "10",
          "26"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "11"
        ]
      ]
    },
    "abstract": "Galois connections are a foundational tool for structuring abstraction in semantics and their use lies at the heart of the theory of abstract interpretation. Yet, mechanization of Galois connections remains limited to restricted modes of use, preventing their general application in mechanized metatheory and certified programming. This paper presents constructive Galois connections, a variant of Galois connections that is effective both on paper and in proof assistants; is complete with respect to a large subset of classical Galois connections; and enables more general reasoning principles, including the \"calculational\" style advocated by Cousot. To design constructive Galois connection we identify a restricted mode of use of classical ones which is both general and amenable to mechanization in dependently-typed functional programming languages. Crucial to our metatheory is the addition of monadic structure to Galois connections to control a \"specification effect\". Effectful calculations may reason classically, while pure calculations have extractable computational content. Explicitly moving between the worlds of specification and implementation is enabled by our metatheory. To validate our approach, we provide two case studies in mechanizing existing proofs from the literature: one uses calculational abstract interpretation to design a static analyzer, the other forms a semantic basis for gradual typing. Both mechanized proofs closely follow their original paper-and-pencil counterparts, employ reasoning principles not captured by previous mechanization approaches, support the extraction of verified algorithms, and are novel.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "1511.06965",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1511.06965",
    "URL": "http://arxiv.org/abs/1511.06965",
    "_line": "FormalBib.bib:9966"
  },
  "pichardie_building_2008": {
    "id": "pichardie_building_2008",
    "type": "article-journal",
    "author": [
      {
        "family": "Pichardie",
        "given": "David"
      }
    ],
    "title": "Building Certified Static Analysers by Modular Construction of Well-founded Lattices",
    "container-title": "Electronic Notes in Theoretical Computer Science",
    "container-title-short": "Electronic Notes in Theoretical Computer Science",
    "issued": {
      "date-parts": [
        [
          "2008",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "11"
        ]
      ]
    },
    "issn": "15710661",
    "abstract": "This paper presents ﬁxpoint calculations on lattice structures as example of highly modular programming in a dependently typed functional language. We propose a library of Coq module functors for constructing complex lattices using eﬃcient data structures. The lattice signature contains a well-foundedness proof obligation which ensures termination of generic ﬁxpoint iteration algorithms. With this library, complex well-foundedness proofs can hence be constructed in a functorial fashion. This paper demonstrates the ability of the recent Coq module system in manipulating algebraic structures and extracting eﬃcient Ocaml implementations from them. The second contribution of this work is a generic result, based on the constructive notion of accessibility predicate, about preservation of accessibility properties when combining relations.",
    "URL": "https://linkinghub.elsevier.com/retrieve/pii/S157106610800279X",
    "DOI": "10.1016/j.entcs.2008.04.064",
    "page": "225-239",
    "page-first": "225",
    "volume": "212",
    "language": "en-US",
    "_line": "FormalBib.bib:9981"
  },
  "theng_gotxn_2022": {
    "id": "theng_gotxn_2022",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Theng",
        "given": "Mark"
      }
    ],
    "title": "GoTxn: Verifying a Crash-Safe, Concurrent Transaction System",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2"
        ]
      ]
    },
    "publisher": "MIT",
    "number-of-pages": "71",
    "abstract": "Bugs related to concurrency and crash safety are infamous for being subtle and hard to reproduce. Formal veriőcation provides a way to combat such bugs through the use of machine-checked proofs about program behavior. However, reasoning about concurrency and crashes can be tricky, especially when scaling up to larger systems that must also have good performance.",
    "language": "en-US",
    "_line": "FormalBib.bib:9998"
  },
  "chajed_verifying_nodate": {
    "id": "chajed_verifying_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Chajed",
        "given": "Tej"
      },
      {
        "family": "Tassarotti",
        "given": "Joseph"
      },
      {
        "family": "Kaashoek",
        "given": "M Frans"
      },
      {
        "family": "Zeldovich",
        "given": "Nickolai"
      }
    ],
    "title": "Verifying concurrent Go code in Coq with Goose",
    "abstract": "This paper describes Goose, a system for writing code in Go and translating it to a model in Coq. The Coq model plugs into Iris for concurrency proofs, giving an end-to-end system for writing and verifying concurrent systems. We have used Goose as part of our work on Perennial to verify a concurrent, crash-safe mail server that gets good performance.",
    "page": "3",
    "page-first": "3",
    "language": "en-US",
    "_line": "FormalBib.bib:10010"
  },
  "chajed_verifying_2019": {
    "id": "chajed_verifying_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Chajed",
        "given": "Tej"
      },
      {
        "family": "Tassarotti",
        "given": "Joseph"
      },
      {
        "family": "Kaashoek",
        "given": "M. Frans"
      },
      {
        "family": "Zeldovich",
        "given": "Nickolai"
      }
    ],
    "title": "Verifying concurrent, crash-safe systems with Perennial",
    "container-title": "Proceedings of the 27th ACM Symposium on Operating Systems Principles",
    "event-title": "SOSP '19: ACM SIGOPS 27th Symposium on Operating Systems Principles",
    "issued": {
      "date-parts": [
        [
          "2019",
          "10",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "13"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-6873-5",
    "abstract": "This paper introduces Perennial, a framework for verifying concurrent, crash-safe systems. Perennial extends the Iris concurrency framework with three techniques to enable crash-safety reasoning: recovery leases, recovery helping, and versioned memory. To ease development and deployment of applications, Perennial provides Goose, a subset of Go and a translator from that subset to a model in Perennial with support for reasoning about Go threads, data structures, and file-system primitives. We implemented and verified a crash-safe, concurrent mail server using Perennial and Goose that achieves speedup on multiple cores. Both Perennial and Iris use the Coq proof assistant, and the mail server and the framework’s proofs are machine checked.",
    "URL": "https://dl.acm.org/doi/10.1145/3341301.3359632",
    "DOI": "10.1145/3341301.3359632",
    "publisher-place": "Huntsville Ontario Canada",
    "page": "243-258",
    "page-first": "243",
    "language": "en-US",
    "_line": "FormalBib.bib:10019"
  },
  "dubut_fixed_2022": {
    "id": "dubut_fixed_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Dubut",
        "given": "Jérémy"
      },
      {
        "family": "Yamada",
        "given": "Akihisa"
      }
    ],
    "title": "Fixed Points Theorems for Non-Transitive Relations",
    "container-title": "Logical Methods in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "13"
        ]
      ]
    },
    "issn": "1860-5974",
    "abstract": "In this paper, we develop an Isabelle/HOL library of order-theoretic ﬁxedpoint theorems. We keep our formalization as general as possible: we reprove several well-known results about complete orders, often with only antisymmetry or attractivity, a mild condition implied by either antisymmetry or transitivity. In particular, we generalize various theorems ensuring the existence of a quasi-ﬁxed point of monotone maps over complete relations, and show that the set of (quasi-)ﬁxed points is itself complete. This result generalizes and strengthens theorems of Knaster–Tarski, Bourbaki–Witt, Kleene, Markowsky, Pataraia, Mashburn, Bhatta–George, and Stouti–Maaden.",
    "URL": "https://lmcs.episciences.org/6809",
    "DOI": "10.46298/lmcs-18(1:30)2022",
    "page": "6809",
    "page-first": "6809",
    "volume": "Volume 18, Issue 1",
    "language": "en-US",
    "_line": "FormalBib.bib:10037"
  },
  "di_pierro_galois_2020": {
    "id": "di_pierro_galois_2020",
    "type": "chapter",
    "author": [
      {
        "family": "Al-Sibahi",
        "given": "Ahmad Salim"
      },
      {
        "family": "Jensen",
        "given": "Thomas"
      },
      {
        "family": "Møgelberg",
        "given": "Rasmus Ejlers"
      },
      {
        "family": "Wąsowski",
        "given": "Andrzej"
      }
    ],
    "editor": [
      {
        "family": "Di Pierro",
        "given": "Alessandra"
      },
      {
        "family": "Malacaria",
        "given": "Pasquale"
      },
      {
        "family": "Nagarajan",
        "given": "Rajagopal"
      }
    ],
    "title": "Galois Connections for Recursive Types",
    "container-title": "From Lambda Calculus to Cybersecurity Through Program Analysis",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "15"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-41102-2 978-3-030-41103-9",
    "abstract": "Building a static analyser for a real language involves modeling of large domains capturing the many available data types. To scale domain design and support eﬃcient development of project-speciﬁc analyzers, it is desirable to be able to build, extend, and change abstractions in a systematic and modular fashion. We present a framework for modular design of abstract domains for recursive types and higher-order functions, based on the theory of solving recursive domain equations. We show how to relate computable abstract domains to our framework, and illustrate the potential of the construction by modularizing a monolithic domain for regular tree grammars. A prototype implementation in the dependently typed functional language Agda shows how the theoretical solution can be used in practice to construct static analysers.",
    "URL": "http://link.springer.com/10.1007/978-3-030-41103-9_4",
    "DOI": "10.1007/978-3-030-41103-9_4",
    "publisher-place": "Cham",
    "page": "105-131",
    "page-first": "105",
    "volume": "12065",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "FormalBib.bib:10053"
  },
  "khan_executable_2022": {
    "id": "khan_executable_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Khan",
        "given": "Wilayat"
      },
      {
        "family": "Hou",
        "given": "Zhe"
      },
      {
        "family": "Sanan",
        "given": "David"
      },
      {
        "family": "Nebhen",
        "given": "Jamel"
      },
      {
        "family": "Liu",
        "given": "Yang"
      },
      {
        "family": "Tiu",
        "given": "Alwen"
      }
    ],
    "title": "An Executable Formal Model of the VHDL in Isabelle/HOL",
    "container-title": "arXiv:2202.04192 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "15"
        ]
      ]
    },
    "abstract": "In the hardware design process, hardware components are usually described in a hardware description language. Most of the hardware description languages, such as Verilog and VHDL, do not have mathematical foundation and hence are not fit for formal reasoning about the design. To enable formal reasoning in one of the most commonly used description language VHDL, we define a formal model of the VHDL language in Isabelle/HOL. Our model targets the functional part of VHDL designs used in industry, specifically the design of the LEON3 processor's integer unit. We cover a wide range of features in the VHDL language that are usually not modelled in the literature and define a novel operational semantics for it. Furthermore, our model can be exported to OCaml code for execution, turning the formal model into a VHDL simulator. We have tested our simulator against simple designs used in the literature, as well as the div32 module in the LEON3 design. The Isabelle/HOL code is publicly available: https://zhehou.github.io/apps/VHDLModel.zip",
    "keywords": "Computer Science - Logic in Computer Science, Computer Science - Formal Languages and Automata Theory, Computer Science - Computation and Language",
    "URLtext": "2202.04192",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2202.04192",
    "URL": "http://arxiv.org/abs/2202.04192",
    "_line": "FormalBib.bib:10073"
  },
  "sakaguchi_reflexive_2022": {
    "id": "sakaguchi_reflexive_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Sakaguchi",
        "given": "Kazuhiko"
      }
    ],
    "title": "Reflexive tactics for algebra, revisited",
    "container-title": "arXiv:2202.04330 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "15"
        ]
      ]
    },
    "abstract": "Computational reflection allows us to turn verified decision procedures into efficient automated reasoning tools in proof assistants. The typical applications of such methodology include mathematical structures that have decidable theory fragments, e.g., equational theories of commutative rings and lattices. However, such existing tools are known not to cooperate with packed classes, a methodology to define mathematical structures in dependent type theory, that allows for the sharing of vocabulary across the inheritance hierarchy. Additionally, such tools do not support homomorphisms whose domain and codomain types may differ. This paper demonstrates how to implement reflexive tactics that support packed classes and homomorphisms. As applications of our methodology, we adapt the ring and field tactics of Coq to the commutative ring and field structures of the Mathematical Components library, and apply the resulting tactics to the formal proof of the irrationality of &dollar;&bslash;zeta(3)&dollar; by Chyzak, Mahboubi, and Sibut-Pinote, to bring more proof automation.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "2202.04330",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2202.04330",
    "URL": "http://arxiv.org/abs/2202.04330",
    "_line": "FormalBib.bib:10087"
  },
  "wang_k-st_2022": {
    "id": "wang_k-st_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Wang",
        "given": "Kun"
      },
      {
        "family": "Wang",
        "given": "Jingyi"
      },
      {
        "family": "Poskitt",
        "given": "Christopher M."
      },
      {
        "family": "Chen",
        "given": "Xiangxiang"
      },
      {
        "family": "Sun",
        "given": "Jun"
      },
      {
        "family": "Cheng",
        "given": "Peng"
      }
    ],
    "title": "K-ST: A Formal Executable Semantics of PLC Structured Text Language",
    "container-title": "arXiv:2202.04076 \\[cs\\]",
    "container-title-short": "K-ST",
    "title-short": "K-ST",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "15"
        ]
      ]
    },
    "abstract": "Programmable Logic Controllers (PLCs) are responsible for automating process control in many industrial systems (e.g. in manufacturing and public infrastructure), and thus it is critical to ensure that they operate correctly and safely. The majority of PLCs are programmed in languages such as Structured Text (ST). However, a lack of formal semantics makes it difficult to ascertain the correctness of their translators and compilers, which vary from vendor-to-vendor. In this work, we develop K-ST, a formal executable semantics for ST in the K framework. Defined with respect to the IEC 61131-3 standard and PLC vendor manuals, K-ST is a high-level reference semantics that can be used to evaluate the correctness and consistency of different ST implementations. We validate K-ST by executing 509 ST programs extracted from Github and comparing the results against existing commercial compilers (i.e., CODESYS, CX-Programmer, and GX Works2). We then apply K-ST to validate the implementation of the open source OpenPLC platform, comparing the executions of several test programs to uncover five bugs and nine functional defects in the compiler.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Software Engineering",
    "URLtext": "2202.04076",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2202.04076",
    "URL": "http://arxiv.org/abs/2202.04076",
    "_line": "FormalBib.bib:10101"
  },
  "bhat_lambda_2022": {
    "id": "bhat_lambda_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Bhat",
        "given": "Siddharth"
      },
      {
        "family": "Grosser",
        "given": "Tobias"
      }
    ],
    "title": "Lambda the Ultimate SSA: Optimizing Functional Programs in SSA",
    "container-title-short": "Lambda the Ultimate SSA",
    "title-short": "Lambda the Ultimate SSA",
    "issued": {
      "date-parts": [
        [
          "2022",
          "1",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "22"
        ]
      ]
    },
    "abstract": "Static Single Assignment (SSA) is the workhorse of modern optimizing compilers for imperative programming languages. However, functional languages have been slow to adopt SSA and prefer to use intermediate representations based on minimal lambda calculi due to SSA's inability to express higher order constructs. We exploit a new SSA construct &ndash; regions &ndash; in order to express functional optimizations via classical SSA based reasoning. Region optimization currently relies on ad-hoc analyses and transformations on imperative programs. These ad-hoc transformations are sufficient for imperative languages as regions are used in a limited fashion. In contrast, we use regions pervasively to model sub-expressions in our functional IR. This motivates us to systematize region optimizations. We extend classical SSA reasoning to regions for functional-style analyses and transformations. We implement a new SSA+regions based backend for LEAN4, a theorem prover that implements a purely functional, dependently typed programming language. Our backend is feature-complete and handles all constructs of LEAN4's functional intermediate representation &lcurly;&bslash;lambda&rcurly;rc within the SSA framework. We evaluate our proposed region optimizations by optimizing &lcurly;&bslash;lambda&rcurly;rc within an SSA+regions based framework implemented in MLIR and demonstrating performance parity with the current LEAN4 backend. We believe our work will pave the way for a unified optimization framework capable of representing, analyzing, and optimizing both functional and imperative languages.",
    "URL": "https://arxiv.org/abs/2201.07272v1",
    "language": "en-US",
    "_line": "FormalBib.bib:10116"
  },
  "grannan_rest_nodate": {
    "id": "grannan_rest_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Grannan",
        "given": "Zachary"
      },
      {
        "family": "Vazou",
        "given": "Niki"
      },
      {
        "family": "Darulova",
        "given": "Eva"
      },
      {
        "family": "Summers",
        "given": "Alexander J."
      }
    ],
    "title": "REST: Integrating Term Rewriting with Program Verification",
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "22"
        ]
      ]
    },
    "abstract": "We introduce REST, a novel term rewriting technique for theorem proving that uses online termination\nchecking and can be integrated with existing program verifiers. REST enables flexible but terminating\nterm rewriting for theorem proving by: (1) exploiting newly-introduced term orderings that are more\npermissive than standard rewrite simplification orderings; (2) dynamically and iteratively selecting\norderings based on the path of rewrites taken so far; and (3) integrating external oracles that allow\nsteps that cannot be justified with rewrite rules. Our REST approach is designed around an easily\nimplementable core algorithm, parameterizable by choices of term orderings and their implementations; in this way our approach can be easily integrated into existing tools. We implemented\nREST as a Haskell library and incorporated it into Liquid Haskell’s evaluation strategy, extending\nLiquid Haskell with rewriting rules. We evaluated our REST implementation by comparing it against\nboth existing rewriting techniques and E-matching and by showing that it can be used to supplant\nmanual lemma application in many existing Liquid Haskell proofs.",
    "URL": "https://arxiv.org/pdf/2202.05872.pdf",
    "_line": "FormalBib.bib:10128"
  },
  "groote_tools_2021": {
    "id": "groote_tools_2021",
    "type": "book",
    "editor": [
      {
        "family": "Groote",
        "given": "Jan Friso"
      },
      {
        "family": "Larsen",
        "given": "Kim Guldstrand"
      }
    ],
    "title": "Tools and Algorithms for the Construction and Analysis of Systems: 27th International Conference, TACAS 2021, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2021, Luxembourg City, Luxembourg, March 27 – April 1, 2021, Proceedings, Part II",
    "container-title-short": "Tools and Algorithms for the Construction and Analysis of Systems",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "Tools and Algorithms for the Construction and Analysis of Systems",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "22"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-72012-4 978-3-030-72013-1",
    "URL": "http://link.springer.com/10.1007/978-3-030-72013-1",
    "DOI": "10.1007/978-3-030-72013-1",
    "publisher-place": "Cham",
    "volume": "12652",
    "language": "en-US",
    "_line": "FormalBib.bib:10146"
  },
  "tan_cake_lpr_2021": {
    "id": "tan_cake_lpr_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Tan",
        "given": "Yong Kiam"
      },
      {
        "family": "Heule",
        "given": "Marijn J. H."
      },
      {
        "family": "Myreen",
        "given": "Magnus O."
      }
    ],
    "editor": [
      {
        "family": "Groote",
        "given": "Jan Friso"
      },
      {
        "family": "Larsen",
        "given": "Kim Guldstrand"
      }
    ],
    "title": "cake&underscore;lpr: Verified Propagation Redundancy Checking in CakeML",
    "container-title": "Tools and Algorithms for the Construction and Analysis of Systems",
    "container-title-short": "cake&underscore;lpr",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "cake&underscore;lpr",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-72013-1",
    "abstract": "Modern SAT solvers can emit independently checkable proof certificates to validate their results. The state-of-the-art proof system that allows for compact proof certificates is propagation redundancy (PR). However, the only existing method to validate proofs in this system with a formally verified tool requires a transformation to a weaker proof system, which can result in a significant blowup in the size of the proof and increased proof validation time. This paper describes the first approach to formally verify PR proofs on a succinct representation; we present (i) a new Linear PR (LPR) proof format, (ii) a tool to efficiently convert PR proofs into LPR format, and (iii) cake&underscore;lpr, a verified LPR proof checker developed in CakeML. The LPR format is backwards compatible with the existing LRAT format, but extends the latter with support for the addition of PR clauses. Moreover, cake&underscore;lpr is verified using CakeML ’s binary code extraction toolchain, which yields correctness guarantees for its machine code (binary) implementation. This further distinguishes our clausal proof checker from existing ones because unverified extraction and compilation tools are removed from its trusted computing base. We experimentally show that LPR provides efficiency gains over existing proof formats and that the strong correctness guarantees are obtained without significant sacrifice in the performance of the verified executable.",
    "keywords": "binary code extraction, linear propagation redundancy",
    "DOI": "10.1007/978-3-030-72013-1_12",
    "publisher-place": "Cham",
    "page": "223-241",
    "page-first": "223",
    "language": "en-US",
    "_line": "FormalBib.bib:10163"
  },
  "gorjiara_yashme_2022": {
    "id": "gorjiara_yashme_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Gorjiara",
        "given": "Hamed"
      },
      {
        "family": "Xu",
        "given": "Guoqing Harry"
      },
      {
        "family": "Demsky",
        "given": "Brian"
      }
    ],
    "title": "Yashme: Detecting Persistency Races",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "abstract": "Persistent memory (PM) or Non-Volatile Random-Access Memory (NVRAM) hardware such as Intel’s Optane memory product promises to transform how programs store and manipulate information. Ensuring that persistent memory programs are crash consistent is a major challenge. We present a novel class of crash consistency bugs for persistent memory programs, which we call persistency races. Persistency races can cause non-atomic stores to be made partially persistent. Persistency races arise due to the interaction of standard compiler optimizations with persistent memory semantics. We present Yashme, the first detector for persistency races. A major challenge is that in order to detect persistency races, the execution must crash in a very narrow window between a store with a persistency race and its corresponding cache flush operation, making it challenging for naïve techniques to be effective. Yashme overcomes this challenge with a novel technique for detecting races in executions that are prefixes of the pre-crash execution. This technique enables Yashme to effectively find persistency races even if the injected crashes do not fall into that window. We have evaluated Yashme on a range of persistent memory benchmarks and have found 24 real persistency races that have never been reported before.",
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "FormalBib.bib:10182"
  },
  "forster_constructive_nodate": {
    "id": "forster_constructive_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Forster",
        "given": "Yannick"
      },
      {
        "family": "Jahn",
        "given": "Felix"
      },
      {
        "family": "Smolka",
        "given": "Gert"
      }
    ],
    "title": "A Constructive and Synthetic Theory of Reducibility: Myhill's Isomorphism Theorem and Post's Problem for Many-one and Truth-table Reducibility in Coq (Full Version)",
    "abstract": "We present a constructive analysis and machine-checked synthetic approach to the theory of one-one, many-one, and truth-table reductions carried out in the Calculus of Inductive Constructions, the type theory underlying the proof assistant Coq. In synthetic computability, one assumes axioms allowing to carry out computability theory with all deﬁnitions and proofs purely in terms of functions of the type theory with no mention of a model of computation. Our synthetic proof of Myhill’s isomorphism theorem that one-one equivalence yields a computational isomorphism makes a compelling case for synthetic computability due to its simplicity without sacriﬁcing formality.",
    "page": "26",
    "page-first": "26",
    "language": "en-US",
    "_line": "FormalBib.bib:10192"
  },
  "first_diversity-driven_2022": {
    "id": "first_diversity-driven_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "First",
        "given": "Emily"
      }
    ],
    "title": "Diversity-Driven Automated Formal Verification",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "abstract": "Formally verified correctness is one of the most desirable properties of software systems. But despite great progress made via interactive theorem provers, such as Coq, writing proof scripts for verification remains one of the most effort-intensive (and often prohibitively difficult) software development activities. Recent work has created tools that automatically synthesize proofs or proof scripts. For example, CoqHammer can prove 26.6&perc; of theorems completely automatically by reasoning using precomputed facts, while TacTok and ASTactic, which use machine learning to model proof scripts and then perform biased search through the proof-script space, can prove 12.9&perc; and 12.3&perc; of the theorems, respectively. Further, these three tools are highly complementary; together, they can prove 30.4&perc; of the theorems fully automatically. Our key insight is that control over the learning process can produce a diverse set of models, and that, due to the unique nature of proof synthesis (the existence of the theorem prover, an oracle that infallibly judges a proof’s correctness), this diversity can significantly improve these tools’ proving power. Accordingly, we develop Diva, which uses a diverse set of models with TacTok’s and ASTactic’s search mechanism to prove 21.7&perc; of the theorems. That is, Diva proves 68&perc; more theorems than TacTok and 77&perc; more than ASTactic. Complementary to CoqHammer, Diva proves 781 theorems (27&perc; added value) that CoqHammer does not, and 364 theorems no existing tool has proved automatically. Together with CoqHammer, Diva proves 33.8&perc; of the theorems, the largest fraction to date. We explore nine dimensions for learning diverse models, and identify which dimensions lead to the most useful diversity. Further, we develop an optimization to speed up Diva’s execution by 40×. Our study introduces a completely new idea for using diversity in machine learning to improve the power of state-of-the-art proof-script synthesis techniques, and empirically demonstrates that the improvement is significant on a dataset of 68K theorems from 122 open-source software projects.",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "FormalBib.bib:10201"
  },
  "gratzer_stratified_nodate": {
    "id": "gratzer_stratified_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Gratzer",
        "given": "Daniel"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      }
    ],
    "title": "A stratified approach to Löb induction",
    "abstract": "Guarded type theory extends type theory with a handful of modalities and constants to encode productive recursion. While these theories have seen widespread use, the metatheory of guarded type theories, particularly guarded dependent type theories remains underdeveloped. We show that integrating Löb induction is the key obstruction to unifying guarded recursion and dependence in a well-behaved type theory and prove a no-go theorem sharply bounding such type theories.",
    "page": "23",
    "page-first": "23",
    "language": "en-US",
    "_line": "FormalBib.bib:10211"
  },
  "ullrich_beyond_2020": {
    "id": "ullrich_beyond_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Ullrich",
        "given": "Sebastian"
      },
      {
        "family": "Moura",
        "given": "Leonardo",
        "dropping-particle": "de"
      }
    ],
    "title": "Beyond Notations: Hygienic Macro Expansion for Theorem Proving Languages",
    "container-title": "arXiv:2001.10490 \\[cs\\]",
    "container-title-short": "Beyond Notations",
    "title-short": "Beyond Notations",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "3",
          "2"
        ]
      ]
    },
    "abstract": "In interactive theorem provers (ITPs), extensible syntax is not only crucial to lower the cognitive burden of manipulating complex mathematical objects, but plays a critical role in developing reusable abstractions in libraries. Most ITPs support such extensions in the form of restrictive \"syntax sugar\" substitutions and other ad hoc mechanisms, which are too rudimentary to support many desirable abstractions. As a result, libraries are littered with unnecessary redundancy. Tactic languages in these systems are plagued by a seemingly unrelated issue: accidental name capture, which often produces unexpected and counterintuitive behavior. We take ideas from the Scheme family of programming languages and solve these two problems simultaneously by proposing a novel hygienic macro system custom-built for ITPs. We further describe how our approach can be extended to cover type-directed macro expansion resulting in a single, uniform system offering multiple abstraction levels that range from supporting simplest syntax sugars to elaboration of formerly baked-in syntax. We have implemented our new macro system and integrated it into the new version of the Lean theorem prover, Lean 4. Despite its expressivity, the macro system is simple enough that it can easily be integrated into other systems.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2001.10490",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2001.10490",
    "URL": "http://arxiv.org/abs/2001.10490",
    "DOI": "10.1007/978-3-030-51054-1_10",
    "page": "167-182",
    "page-first": "167",
    "volume": "12167",
    "_line": "FormalBib.bib:10220"
  },
  "beyer_decomposing_2022": {
    "id": "beyer_decomposing_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Beyer",
        "given": "Dirk"
      },
      {
        "family": "Haltermann",
        "given": "Jan"
      },
      {
        "family": "Lemberger",
        "given": "Thomas"
      },
      {
        "family": "Wehrheim",
        "given": "Heike"
      }
    ],
    "title": "Decomposing Software Verification into Off-the-Shelf Components: An Application to CEGAR",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "abstract": "Techniques for software verification are typically realized as cohesive units of software with tightly coupled components. This makes it difficult to re-use components, and the potential for workload distribution is limited. Innovations in software verification might find their way into practice faster if provided in smaller, more specialized components.",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "FormalBib.bib:10238"
  },
  "guo_precise_2022": {
    "id": "guo_precise_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Guo",
        "given": "Yiyuan"
      },
      {
        "family": "Zhou",
        "given": "Jinguo"
      },
      {
        "family": "Yao",
        "given": "Peisen"
      },
      {
        "family": "Shi",
        "given": "Qingkai"
      },
      {
        "family": "Zhang",
        "given": "Charles"
      }
    ],
    "title": "Precise Divide-By-Zero Detection with Affirmative Evidence",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "abstract": "The static detection of divide-by-zero, a common programming error, is particularly prone to false positives because conventional static analysis reports a divide-by-zero bug whenever it cannot prove the safety property — the divisor variable is not zero in all executions. When reasoning the program semantics over a large number of under-constrained variables, conventional static analyses significantly loose the bounds of divisor variables, which easily fails the safety proof and leads to a massive number of false positives. We propose a static analysis to detect divide-by-zero bugs taking additional evidence for under-constrained variables into consideration. Based on an extensive empirical study of known divide-by-zero bugs, we no longer arbitrarily report a bug once the safety verification fails. Instead, we actively look for affirmative evidences, namely source evidence and bound evidence, that imply a high possibility of the bug to be triggerable at runtime. When applying our tool Wit to the real-world software such as the Linux kernel, we have found 72 new divide-by-zero bugs with a low false positive rate of 22&perc;.",
    "page": "12",
    "page-first": "12",
    "language": "en-US",
    "_line": "FormalBib.bib:10248"
  },
  "bosamiya_provably-safe_nodate": {
    "id": "bosamiya_provably-safe_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Bosamiya",
        "given": "Jay"
      },
      {
        "family": "Lim",
        "given": "Wen Shih"
      },
      {
        "family": "Parno",
        "given": "Bryan"
      }
    ],
    "title": "Provably-Safe Multilingual Software Sandboxing using WebAssembly",
    "abstract": "Many applications, from the Web to smart contracts, need to safely execute untrusted code. We observe that WebAssembly (Wasm) is ideally positioned to support such applications, since it promises safety and performance, while serving as a compiler target for many high-level languages. However, Wasm’s safety guarantees are only as strong as the implementation that enforces them. Hence, we explore two distinct approaches to producing provably sandboxed Wasm code. One draws on traditional formal methods to produce mathematical, machine-checked proofs of safety. The second carefully embeds Wasm semantics in safe Rust code such that the Rust compiler can emit safe executable code with good performance. Our implementation and evaluation of these two techniques indicate that leveraging Wasm gives us provablysafe multilingual sandboxing with performance comparable to standard, unsafe approaches.",
    "page": "18",
    "page-first": "18",
    "language": "en-US",
    "_line": "FormalBib.bib:10258"
  },
  "din_lagc_2022": {
    "id": "din_lagc_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Din",
        "given": "Crystal Chang"
      },
      {
        "family": "Hähnle",
        "given": "Reiner"
      },
      {
        "family": "Henrio",
        "given": "Ludovic"
      },
      {
        "family": "Johnsen",
        "given": "Einar Broch"
      },
      {
        "family": "Pun",
        "given": "Violet Ka I."
      },
      {
        "family": "Tarifa",
        "given": "Silvia Lizeth Tapia"
      }
    ],
    "title": "LAGC Semantics of Concurrent Programming Languages",
    "container-title": "arXiv:2202.12195 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "24"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "3",
          "3"
        ]
      ]
    },
    "abstract": "Formal, mathematically rigorous programming language semantics are the essential prerequisite for the design of logics and calculi that permit automated reasoning about concurrent programs. We propose a novel modular semantics designed to align smoothly with program logics used in deductive verification and formal specification of concurrent programs. Our semantics separates local evaluation of expressions and statements performed in an abstract, symbolic environment from their composition into global computations, at which point they are concretised. This makes incremental addition of new language concepts possible, without the need to revise the framework. The basis is a generalisation of the notion of a program trace as a sequence of evolving states that we enrich with event descriptors and trailing continuation markers. This allows to postpone scheduling constraints from the level of local evaluation to the global composition stage, where well-formedness predicates over the event structure declaratively characterise a wide range of concurrency models. We also illustrate how a sound program logic and calculus can be defined for this semantics.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2202.12195",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2202.12195",
    "URL": "http://arxiv.org/abs/2202.12195",
    "_line": "FormalBib.bib:10267"
  },
  "noauthor_temps_nodate": {
    "id": "noauthor_temps_nodate",
    "type": "article-journal",
    "title": "Le Temps des Cerises: Efficient Temporal Stack Safety on Capability Machines using Directed Capabilities",
    "page": "30",
    "page-first": "30",
    "volume": "1",
    "language": "en-US",
    "_line": "FormalBib.bib:10281"
  },
  "aguirre_step-indexed_2017": {
    "id": "aguirre_step-indexed_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Aguirre",
        "given": "Alejandro"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      }
    ],
    "title": "Step-Indexed Logical Relations for Nondeterministic and Probabilistic Choice",
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "abstract": "Developing denotational models for higher-order languages that combine probabilistic and nondeterministic choice is known to be very challenging. In this paper, we propose an alternative approach based on operational techniques. We study a higher-order language combining parametric polymorphism, recursive types, discrete probabilistic choice and countable nondeterminism. We define probabilistic generalizations of may- and must-termination as the optimal and pessimal probabilities of termination. Then we define stepindexed logical relations and show that they are sound and complete with respect to the induced contextual preorders. For may-equivalence we use step-indexing over the natural numbers whereas for must-equivalence we index over the countable ordinals. We then show than the probabilities of may- and must-termination coincide with the maximal and minimal probabilities of termination under all schedulers. Finally we derive the equational theory induced by contextual equivalence and show that it validates the combination of the algebraic theories for probabilistic and nondeterministic choice and the distributive property between them.",
    "page": "27",
    "page-first": "27",
    "language": "en-US",
    "_line": "FormalBib.bib:10289"
  },
  "masuda_unied_nodate": {
    "id": "masuda_unied_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Masuda",
        "given": "Masahiro"
      },
      {
        "family": "Kameyama",
        "given": "Yukiyoshi"
      }
    ],
    "title": "Uniﬁed Program Generation and Veriﬁcation: A Case Study on Number-Theoretic Transform",
    "abstract": "Giving correctness assurance to the generated code in the context of generative programming is a poorly explored problem. Such assurance is particularly desired for applications where correctness of the optimized code is far from obvious, such as cryptography.",
    "page": "19",
    "page-first": "19",
    "language": "en-US",
    "_line": "FormalBib.bib:10299"
  },
  "li_path-sensitive_2022": {
    "id": "li_path-sensitive_2022",
    "type": "paper-conference",
    "author": [
      {
        "family": "Li",
        "given": "Tuo"
      },
      {
        "family": "Bai",
        "given": "Jia-Ju"
      },
      {
        "family": "Sui",
        "given": "Yulei"
      },
      {
        "family": "Hu",
        "given": "Shi-Min"
      }
    ],
    "title": "Path-sensitive and alias-aware typestate analysis for detecting OS bugs",
    "container-title": "Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
    "collection-title": "ASPLOS 2022",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "3",
          "3"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-9205-1",
    "abstract": "Operating system (OS) is the cornerstone for modern computer systems. It manages devices and provides fundamental service for user-level applications. Thus, detecting bugs in OSes is important to improve reliability and security of computer systems. Static typestate analysis is a common technique for detecting different types of bugs, but it is often inaccurate or unscalable for large-size OS code, due to imprecision of identifying alias relationships as well as high costs of typestate tracking and path-feasibility validation. In this paper, we present PATA, a novel path-sensitive and aliasaware typestate analysis framework to detect OS bugs. To improve the precision of identifying alias relationships in OS code, PATA performs a path-based alias analysis based on control-flow paths and access paths. With these alias relationships, PATA reduces the costs of typestate tracking and path-feasibility validation, to boost the efficiency of path-sensitive typestate analysis for bug detection. We have evaluated PATA on the Linux kernel and three popular IoT OSes (Zephyr, RIOT and TencentOS-tiny) to detect three common types of bugs (null-pointer dereferences, uninitialized variable accesses and memory leaks). PATA finds 574 real bugs with a false positive rate of 28&perc;. 206 of these bugs have been confirmed by the developers of the four OSes.We also compare PATA to seven state-of-the-art static approaches (Cppcheck, Coccinelle, Smatch,CSA, Infer, Saber and SVF). PATA finds many real bugs missed by them, with a lower false positive rate.",
    "keywords": "bug detection, operation system, static analysis",
    "URL": "https://doi.org/10.1145/3503222.3507770",
    "DOI": "10.1145/3503222.3507770",
    "publisher-place": "New York, NY, USA",
    "page": "859-872",
    "page-first": "859",
    "_line": "FormalBib.bib:10308"
  },
  "utture_fast_2022": {
    "id": "utture_fast_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Utture",
        "given": "Akshay"
      }
    ],
    "title": "Fast and Precise Application Code Analysis using a Partial Library",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "abstract": "Long analysis times are a key bottleneck for the widespread adoption of whole-program static analysis tools. Fortunately, however, a user is often only interested in finding errors in the application code, which constitutes a small fraction of the whole program. Current application-focused analysis tools overapproximate the effect of the library and hence reduce the precision of the analysis results. However, empirical studies have shown that users have high expectations on precision and will ignore tool results that don’t meet these expectations. In this paper, we introduce the first tool QueryMax that significantly speeds up an application code analysis without dropping any precision. QueryMax acts as a pre-processor to an existing analysis tool to select a partial library that is most relevant to the analysis queries in the application code. The selected partial library plus the application is given as input to the existing static analysis tool, with the remaining library pointers treated as the bottom element in the abstract domain. This achieves a significant speedup over a whole-program analysis, at the cost of a few lost errors, and with no loss in precision. We instantiate and run experiments on QueryMax for a cast-check analysis and a null-pointer analysis. For a particular configuration, QueryMax enables these two analyses to achieve, relative to a whole-program analysis, an average recall of 87&perc;, a precision of 100&perc; and a geometric mean speedup of 10x.",
    "page": "12",
    "page-first": "12",
    "language": "en-US",
    "_line": "FormalBib.bib:10326"
  },
  "benzmuller_simplified_2022": {
    "id": "benzmuller_simplified_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Benzmüller",
        "given": "Christoph"
      }
    ],
    "title": "A Simplified Variant of G&bslash;\"odel's Ontological Argument",
    "container-title": "arXiv:2202.06264 \\[cs, math\\]",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "3",
          "8"
        ]
      ]
    },
    "abstract": "A simplified variant of G&bslash;\"odel's ontological argument is presented. The simplified argument is valid already in basic modal logics K or KT, it does not suffer from modal collapse, and it avoids the rather complex predicates of essence (Ess.) and necessary existence (NE) as used by G&bslash;\"odel. The variant presented has been obtained as a side result of a series of theory simplification experiments conducted in interaction with a modern proof assistant system. The starting point for these experiments was the computer encoding of G&bslash;\"odel's argument, and then automated reasoning techniques were systematically applied to arrive at the simplified variant presented. The presented work thus exemplifies a fruitful human-computer interaction in computational metaphysics. Whether the presented result increases or decreases the attractiveness and persuasiveness of the ontological argument is a question I would like to pass on to philosophy and theology.",
    "keywords": "Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Logic in Computer Science, Mathematics - Logic",
    "URLtext": "2202.06264",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2202.06264",
    "URL": "http://arxiv.org/abs/2202.06264",
    "_line": "FormalBib.bib:10336"
  },
  "forster_synthetic_nodate": {
    "id": "forster_synthetic_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Forster",
        "given": "Yannick"
      },
      {
        "family": "Kunze",
        "given": "Fabian"
      },
      {
        "family": "Lauermann",
        "given": "Nils"
      }
    ],
    "title": "Synthetic Kolmogorov Complexity in Coq",
    "abstract": "We present a generalised, constructive, and machine-checked approach to Kolmogorov complexity in the constructive type theory underlying the Coq proof assistant. By proving that nonrandom numbers form a simple predicate, we obtain elegant proofs of undecidability for random and nonrandom numbers and a proof of uncomputability of Kolmogorov complexity.",
    "page": "20",
    "page-first": "20",
    "language": "en-US",
    "_line": "FormalBib.bib:10350"
  },
  "rastogi_wys_2019": {
    "id": "rastogi_wys_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Rastogi",
        "given": "Aseem"
      },
      {
        "family": "Swamy",
        "given": "Nikhil"
      },
      {
        "family": "Hicks",
        "given": "Michael"
      }
    ],
    "title": "Wys\\*: A DSL for Verified Secure Multi-party Computations",
    "container-title": "Principles of Security and Trust (POST 2019)",
    "issued": {
      "date-parts": [
        [
          "2019",
          "4"
        ]
      ]
    },
    "abstract": "Secure multi-party computation (MPC) enables a set of mutually distrusting parties to cooperatively compute, using a cryptographic protocol, a function over their private data. This paper presents Wys\\*, a new domain-specific language (DSL) for writing MPCs. Wys\\* is an embedded DSL hosted in F\\*, a verification-oriented, effectful programming language. Wys\\* source programs are essentially F\\* programs written in a custom MPC effect, meaning that the programmers can use F\\*'s logic to verify the correctness and security properties of their programs. To reason about the distributed runtime semantics of these programs, we formalize a deep embedding of Wys\\*, also in F\\*. We mechanize the necessary metatheory to prove that the properties verified for the Wys\\* source programs carry over to the distributed, multi-party semantics. Finally, we use F\\*'s extraction mechanism to extract an interpreter that we have proved matches this semantics, yielding a verified implementation. Wys\\* is the first DSL to enable formal verification of source MPC programs, and also the first MPC DSL to provide a verified implementation. With Wys\\* we have implemented several MPC protocols, including private set intersection, joint median, and an MPC-based card dealing application, and have verified their security and correctness.",
    "URL": "https://www.microsoft.com/en-us/research/publication/wys-a-dsl-for-verified-secure-multi-party-computations/",
    "_line": "FormalBib.bib:10359"
  },
  "bornebusch_coq_nodate": {
    "id": "bornebusch_coq_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Bornebusch",
        "given": "Fritjof"
      }
    ],
    "title": "COQ meets Clash: PROPOSING A HARDWARE DESIGN SYNTHESIS FLOW THAT COMBINES PROOF ASSISTANTS WITH FUNCTIONAL HARDWARE DESCRIPTION LANGUAGES",
    "page": "186",
    "page-first": "186",
    "language": "en-US",
    "_line": "Hardware.bib:2"
  },
  "altamirano_formal_nodate": {
    "id": "altamirano_formal_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Altamirano",
        "given": "Christian"
      }
    ],
    "title": "Formal Verification of an Implementation of the Roughtime Server",
    "abstract": "Formal verification has been used in the past few decades to prove correctness of programs. This thesis provides a verification of a simpler implementation of Roughtime \\[1\\], a protocol that consists of securely querying the current time via a client-server interaction. The tool that was used is Bedrock2 \\[3\\], a work-in-progress Coq framework suitable for reasoning about low-level code, developed in the Programming Languages and Verification group at MIT CSAIL.",
    "page": "46",
    "page-first": "46",
    "language": "en-US",
    "_line": "Hardware.bib:10"
  },
  "perez-lopez_puppetmaster_nodate": {
    "id": "perez-lopez_puppetmaster_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Perez-Lopez",
        "given": "Áron Ricardo"
      }
    ],
    "title": "Puppetmaster: a certified hardware architecture for task parallelism",
    "abstract": "This thesis presents Puppetmaster, a hardware accelerator for transactional workloads. Existing software and hardware frameworks for transactional memory and online transaction processing are not able to scale to hundreds or thousands of cores unless the rate of conflicts between transactions is very low. Puppetmaster aims to improve upon the scalability of concurrency control by requiring transactions to declare their read and write sets in advance and uses this information to only run transactions concurrently when they are known not to conflict. In this thesis, I present and evaluate the design of Puppetmaster in a high-level model, in cycle-accurate simulations, and on real reconfigurable hardware.",
    "page": "49",
    "page-first": "49",
    "language": "en-US",
    "_line": "Hardware.bib:19"
  },
  "dagand_formal_2022": {
    "id": "dagand_formal_2022",
    "type": "report",
    "genre": "Technical Report",
    "author": [
      {
        "family": "Dagand",
        "given": "Pierre-Evariste"
      },
      {
        "family": "Berthou",
        "given": "Gautier"
      },
      {
        "family": "Demange",
        "given": "Delphine"
      },
      {
        "family": "Risset",
        "given": "Tanguy"
      }
    ],
    "title": "A Formal Model of Interrupt-based Checkpointing with Peripherals",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "9"
        ]
      ]
    },
    "publisher": "IRIF ; IRISA ; INSA RENNES",
    "abstract": "Transiently-powered systems featuring non-volatile memory as well as external peripherals enable the development of new low-power sensor applications. However, as programmers, we are ill-equipped to reason about systems where power failures are the norm rather than the exception. A first challenge consists in being able to capture all the volatile state of the application-external peripherals included-to ensure progress. A second, more fundamental, challenge consists in specifying how power failures may interact with peripheral operations. In this paper, we propose a formal specification of intermittent computing with peripherals, an axiomatic model of interrupt-based checkpointing as well as its proof of correctness, machine-checked in the Coq proof assistant. We state the correctness of the checkpointing mechanism as a trace refinement property between the model and the specification, which accounts for peripheral device operations replays due to power failures. Our proof methodology relies on intermediate oracle semantics to tame the non-determinism of power failures scenarios.",
    "URL": "https://hal.archives-ouvertes.fr/hal-03557760",
    "page": "1-36",
    "page-first": "1",
    "_line": "Hardware.bib:28"
  },
  "yang_c2aadl_reverse_2021": {
    "id": "yang_c2aadl_reverse_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Yang",
        "given": "Zhibin"
      },
      {
        "family": "Qiu",
        "given": "Zhikai"
      },
      {
        "family": "Zhou",
        "given": "Yong"
      },
      {
        "family": "Huang",
        "given": "Zhiqiu"
      },
      {
        "family": "Bodeveix",
        "given": "Jean-Paul"
      },
      {
        "family": "Filali",
        "given": "Mamoun"
      }
    ],
    "title": "C2AADL&underscore;Reverse: A model-driven reverse engineering approach to development and verification of safety-critical software",
    "container-title": "Journal of Systems Architecture",
    "container-title-short": "C2AADL&underscore;Reverse",
    "title-short": "C2AADL&underscore;Reverse",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "14"
        ]
      ]
    },
    "issn": "1383-7621",
    "abstract": "The safety-critical system communities have been struggling to manage and maintain their legacy softwaresystems because upgrading such systems has been a complex challenge. To overcome or reduce this problem, reverse engineering has been increasingly used in safety-critical systems. This paper proposes C2AADL&underscore;Reverse, a model-driven reverse engineering approach for safety-critical software development and verification. C2AADL&underscore;Reverse takes multi-task C source code as input, and generates AADL (Architecture Analysis and Design Language) model of the legacy software systems. Compared with the existing works, this paper considers more reversed construction including AADL component structure, behavior, and multi-threaded run-time information. Moreover, two types of activities are proposed to ensure the correctness of C2AADL&underscore;Reverse. First, it is necessary to validate the reverse engineering process. Second, the generated AADL models should conform to desired critical properties. We propose the verification of the reverse-engineered AADL model by using UPPAAL to establish component-level properties and the Assume Guarantee REasoning Environment (AGREE) to perform compositional verification of the architecture. This combination of verification tools allows us to iteratively explore design and verification of detailed behavioral models, and to scale formal analysis to large models. In addition, the prototype tool and the evaluation of C2AADL&underscore;Reverse using a real-world aerospace case study are presented.",
    "keywords": "AADL, Compositional verification, Model-driven development, Model-driven reverse engineering, Safety-critical systems",
    "URL": "https://www.sciencedirect.com/science/article/pii/S1383762121001454",
    "DOI": "10.1016/j.sysarc.2021.102202",
    "page": "102202",
    "page-first": "102202",
    "volume": "118",
    "language": "en-US",
    "_line": "LanguageTools.bib:2"
  },
  "wolf_gobra_2021": {
    "id": "wolf_gobra_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Wolf",
        "given": "Felix A."
      },
      {
        "family": "Arquint",
        "given": "Linard"
      },
      {
        "family": "Clochard",
        "given": "Martin"
      },
      {
        "family": "Oortwijn",
        "given": "Wytse"
      },
      {
        "family": "Pereira",
        "given": "João C."
      },
      {
        "family": "Müller",
        "given": "Peter"
      }
    ],
    "title": "Gobra: Modular Specification and Verification of Go Programs (extended version)",
    "container-title": "arXiv:2105.13840 \\[cs\\]",
    "container-title-short": "Gobra",
    "title-short": "Gobra",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "7"
        ]
      ]
    },
    "abstract": "Go is an increasingly-popular systems programming language targeting, especially, concurrent and distributed systems. Go differentiates itself from other imperative languages by offering structural subtyping and lightweight concurrency through goroutines with message-passing communication. This combination of features poses interesting challenges for static verification, most prominently the combination of a mutable heap and advanced concurrency primitives. We present Gobra, a modular, deductive program verifier for Go that proves memory safety, crash safety, data-race freedom, and user-provided specifications. Gobra is based on separation logic and supports a large subset of Go. Its implementation translates an annotated Go program into the Viper intermediate verification language and uses an existing SMT-based verification backend to compute and discharge proof obligations.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2105.13840",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.13840",
    "URL": "http://arxiv.org/abs/2105.13840",
    "_line": "LanguageTools.bib:21"
  },
  "noauthor_perceus_2020": {
    "id": "noauthor_perceus_2020",
    "type": "article-journal",
    "title": "Perceus: Garbage Free Reference Counting with ReuseMicrosoft Technical Report, MSR-TR-2020-42, Jan 11, 2021, v3.",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "abstract": "We introduce Perceus, an algorithm for precise reference counting with reuse and specialization. Starting from a functional core language with explicit control-flow, Perceus emits precise reference counting instructions such that programs are garbage free, where only live references are retained. This enables further optimizations, like reuse analysis that allows for guaranteed in-place updates at runtime. This in turn enables a novel programming paradigm that we call functional but in-place (FBIP). Much like tail-call optimization enables writing loops with regular function calls, reuse analysis enables writing in-place mutating algorithms in a purely functional way. We give a novel formalization of reference counting in a linear resource calculus, and prove that Perceus is sound and garbage free. We show evidence that Perceus, as implemented in Koka, has good performance and is competitive with other state-of-the-art memory collectors.",
    "page": "41",
    "page-first": "41",
    "language": "en-US",
    "_line": "LanguageTools.bib:36"
  },
  "zuo_chianina_2021": {
    "id": "zuo_chianina_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Zuo",
        "given": "Zhiqiang"
      },
      {
        "family": "Zhang",
        "given": "Yiyu"
      },
      {
        "family": "Pan",
        "given": "Qiuhong"
      },
      {
        "family": "Lu",
        "given": "Shenming"
      },
      {
        "family": "Li",
        "given": "Yue"
      },
      {
        "family": "Wang",
        "given": "Linzhang"
      },
      {
        "family": "Li",
        "given": "Xuandong"
      },
      {
        "family": "Xu",
        "given": "Guoqing Harry"
      }
    ],
    "title": "Chianina: An Evolving Graph System for Flow- and Context-Sensitive Analyses of Million Lines of C Code",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Sophisticated static analysis techniques often have complicated implementations, much of which provides logic for tuning and scaling rather than basic analysis functionalities. This tight coupling of basic algorithms with special treatments for scalability makes an analysis implementation hard to (1) make correct, (2) understand/work with, and (3) reuse for other clients. This paper presents Chianina, a graph system we developed for fully context- and flow-sensitive analysis of large C programs. Chianina overcomes these challenges by allowing the developer to provide only the basic algorithm of an analysis and pushing the tuning/scaling work to the underlying system. Key to the success of Chianina is (1) an evolving graph formulation of flow sensitivity and (2) the leverage of out-of-core, disk support to deal with memory blowup resulting from context sensitivity. We implemented three context- and flow-sensitive analyses on top of Chianina and scaled them to large C programs like Linux (17M LoC) on a single commodity PC. CCS Concepts: • Computer systems organization → Special purpose systems; Reliability; • Theory of computation → Program analysis.",
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "LanguageTools.bib:45"
  },
  "parr_adaptive_nodate": {
    "id": "parr_adaptive_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Parr",
        "given": "Terence"
      },
      {
        "family": "Harwell",
        "given": "Sam"
      },
      {
        "family": "Fisher",
        "given": "Kathleen"
      }
    ],
    "title": "Adaptive LL(\\*) Parsing: The Power of Dynamic Analysis",
    "abstract": "Despite the advances made by modern parsing strategies such as PEG, LL(\\*), GLR, and GLL, parsing is not a solved problem. Existing approaches suffer from a number of weaknesses, including difﬁculties supporting side-effecting embedded actions, slow and/or unpredictable performance, and counterintuitive matching strategies. This paper introduces the ALL(\\*) parsing strategy that combines the simplicity, efﬁciency, and predictability of conventional top-down LL(k) parsers with the power of a GLR-like mechanism to make parsing decisions. The critical innovation is to move grammar analysis to parsetime, which lets ALL(\\*) handle any non-left-recursive contextfree grammar. ALL(\\*) is O(n4) in theory but consistently performs linearly on grammars used in practice, outperforming general strategies such as GLL and GLR by orders of magnitude. ANTLR 4 generates ALL(\\*) parsers and supports direct left-recursion through grammar rewriting. Widespread ANTLR 4 use (5000 downloads/month in 2013) provides evidence that ALL(\\*) is effective for a wide variety of applications.",
    "page": "19",
    "page-first": "19",
    "language": "en-US",
    "_line": "LanguageTools.bib:55"
  },
  "noauthor_writing_2021": {
    "id": "noauthor_writing_2021",
    "type": "webpage",
    "title": "Writing Compiler Front-Ends for LLVM with Lua using Mewa.CodeProject",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "26"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "1"
        ]
      ]
    },
    "abstract": "In this article we see how a very primitive compiler is written in Lua using Mewa and how to compile and run a simple demo program in the shell.",
    "URL": "https://www.codeproject.com/Articles/5301384/Writing-Compiler-Front-Ends-for-LLVM-with-Lua-usin",
    "language": "en-US",
    "_line": "LanguageTools.bib:64"
  },
  "sammler_refinedc_2021": {
    "id": "sammler_refinedc_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Sammler",
        "given": "Michael"
      },
      {
        "family": "Lepigre",
        "given": "Rodolphe"
      },
      {
        "family": "Krebbers",
        "given": "Robbert"
      }
    ],
    "title": "RefinedC: Automating the Foundational Verification of C Code with Refined Ownership Types",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Given the central role that C continues to play in systems software, and the difficulty of writing safe and correct C code, it remains a grand challenge to develop effective formal methods for verifying C programs. In this paper, we propose a new approach to this problem: a type system we call RefinedC, which combines ownership types (for modular reasoning about shared state and concurrency) with refinement types (for encoding precise invariants on C data types and Hoare-style specifications for C functions).",
    "page": "17",
    "page-first": "17",
    "language": "en-US",
    "_line": "LanguageTools.bib:75"
  },
  "huihui_optimizing_nodate": {
    "id": "huihui_optimizing_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Huihui",
        "given": "Cheng"
      },
      {
        "family": "Hongwei",
        "given": "Zeng"
      }
    ],
    "title": "Optimizing demand-driven null dereference verification via merging branches",
    "container-title": "Expert Systems",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "1"
        ]
      ]
    },
    "issn": "1468-0394",
    "abstract": "Null dereference is a common type of runtime failure in Java programs, and it is necessary to verify whether a dereference in the program is safe. However, previous works often have redundant path exploration and high false positive rate. In this paper, we propose a merged null dereference verification (MNDV) approach. MNDV employs a backward, path-sensitive inter-procedural analysis technique to verify a given dereference as safe or potentially unsafe. It uses a branch merging strategy to remove redundant paths, and a method call's relevance to the null references is checked to determine whether it is necessary to explore the internal codes of the method. We have evaluated the approach in some standard benchmark programs. Compared with some existing approaches, our approach reduces false alarm rate and effectively reduce time and memory consumption.",
    "keywords": "backward analysis, data-flow analysis, null dereference, static analysis",
    "URL": "https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12707",
    "DOI": "10.1111/exsy.12707",
    "page": "e12707",
    "page-first": "e12707",
    "volume": "n/a",
    "note": "&underscore;eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12707",
    "language": "en-US",
    "_line": "LanguageTools.bib:85"
  },
  "vu_secure_2020": {
    "id": "vu_secure_2020",
    "type": "paper-conference",
    "author": [
      {
        "family": "Vu",
        "given": "Son Tuan"
      },
      {
        "family": "Heydemann",
        "given": "Karine"
      },
      {
        "family": "Grandmaison",
        "given": "Arnaud",
        "dropping-particle": "de"
      },
      {
        "family": "Cohen",
        "given": "Albert"
      }
    ],
    "title": "Secure delivery of program properties through optimizing compilation",
    "container-title": "Proceedings of the 29th International Conference on Compiler Construction",
    "collection-title": "CC 2020",
    "issued": {
      "date-parts": [
        [
          "2020",
          "2",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "27"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-7120-9",
    "abstract": "Annotations and assertions capturing static program properties are ubiquitous, from robust software engineering to safety-critical or secure code. These may be functional or non-functional properties of control and data flow, memory usage, I/O and real time. We propose an approach to encode, translate, and preserve the semantics of both functional and non-functional properties along the optimizing compilation of C to machine code. The approach involves (1) capturing and translating source-level properties through lowering passes and intermediate representations, such that data and control flow optimizations will preserve their consistency with the transformed program, and (2) carrying properties and their translation as debug information down to machine code. Our experiments using LLVM validate the soundness, expressiveness and efficiency of the approach, considering a reference suite of functional properties as well as established security properties and applications hardened against side-channel attacks.",
    "keywords": "Annotation, Compiler, LLVM, Optimization, Security",
    "URL": "https://doi.org/10.1145/3377555.3377897",
    "DOI": "10.1145/3377555.3377897",
    "publisher-place": "New York, NY, USA",
    "page": "14-26",
    "page-first": "14",
    "_line": "LanguageTools.bib:103"
  },
  "blouin_interacto_2021": {
    "id": "blouin_interacto_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Blouin",
        "given": "Arnaud"
      },
      {
        "family": "Jezequel",
        "given": "Jean-Marc"
      }
    ],
    "title": "Interacto: A Modern User Interaction Processing Model",
    "container-title": "IEEE Transactions on Software Engineering",
    "container-title-short": "Interacto",
    "title-short": "Interacto",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "26"
        ]
      ]
    },
    "issn": "0098-5589, 1939-3520, 2326-3881",
    "URL": "https://ieeexplore.ieee.org/document/9440800/",
    "DOI": "10.1109/TSE.2021.3083321",
    "page": "1-1",
    "page-first": "1",
    "language": "en-US",
    "_line": "LanguageTools.bib:121"
  },
  "giallorenzo_multiparty_2021": {
    "id": "giallorenzo_multiparty_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Giallorenzo",
        "given": "Saverio"
      },
      {
        "family": "Montesi",
        "given": "Fabrizio"
      },
      {
        "family": "Peressotti",
        "given": "Marco"
      },
      {
        "family": "Richter",
        "given": "David"
      },
      {
        "family": "Salvaneschi",
        "given": "Guido"
      },
      {
        "family": "Weisenburger",
        "given": "Pascal"
      }
    ],
    "title": "Multiparty Languages: The Choreographic and Multitier Cases",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Choreographic languages aim to express multiparty communication protocols, by providing primitives that make interaction manifest. Multitier languages enable programming computation that spans across several tiers of a distributed system, by supporting primitives that allow computation to change the location of execution. Rooted into different theoretical underpinnings—respectively process calculi and lambda calculus—the two paradigms have been investigated independently by different research communities with little or no contact. As a result, the link between the two paradigms has remained hidden for long.",
    "page": "28",
    "page-first": "28",
    "language": "en-US",
    "_line": "LanguageTools.bib:137"
  },
  "egolf_verbatim_nodate": {
    "id": "egolf_verbatim_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Egolf",
        "given": "Derek"
      },
      {
        "family": "Lasser",
        "given": "Sam"
      },
      {
        "family": "Fisher",
        "given": "Kathleen"
      }
    ],
    "title": "Verbatim: A Veriﬁed Lexer Generator",
    "abstract": "Lexers and parsers are often used as front ends to connect input from the outside world with the internals of a larger software system. These front ends are natural targets for attackers who wish to compromise the larger system. A formally veriﬁed tool that performs mechanized lexical analysis would render attacks on these front ends less effective.",
    "page": "9",
    "page-first": "9",
    "language": "en-US",
    "_line": "LanguageTools.bib:147"
  },
  "ernst_deductive_nodate": {
    "id": "ernst_deductive_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Ernst",
        "given": "Gidon"
      },
      {
        "family": "Blau",
        "given": "Johannes"
      },
      {
        "family": "Murray",
        "given": "Toby"
      }
    ],
    "title": "Deductive Verification via the Debug Adapter Protocol",
    "page": "8",
    "page-first": "8",
    "language": "en-US",
    "_line": "LanguageTools.bib:156"
  },
  "kj_specication_nodate": {
    "id": "kj_specication_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Kj",
        "given": "Jonas"
      },
      {
        "family": "Madsen",
        "given": "Frederik Palludan"
      },
      {
        "family": "Battle",
        "given": "Nick"
      }
    ],
    "title": "The Speciﬁcation Language Server Protocol: A Proposal for Standardised LSP Extensions",
    "container-title": "F-IDE2021",
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "LanguageTools.bib:164"
  },
  "shi_path-sensitive_2021": {
    "id": "shi_path-sensitive_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Shi",
        "given": "Qingkai"
      },
      {
        "family": "Yao",
        "given": "Peisen"
      },
      {
        "family": "Wu",
        "given": "Rongxin"
      },
      {
        "family": "Zhang",
        "given": "Charles"
      }
    ],
    "title": "Path-Sensitive Sparse Analysis withoutPath Conditions",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Sparse program analysis is fast as it propagates data �ow facts via data dependence, skipping unnecessary control �ows. However, when path-sensitively checking millions of lines of code, it is still prohibitively expensive because a huge number of path conditions have to be computed and solved via an SMT solver. This paper presents Fusion, a fused approach to inter-procedurally path-sensitive sparse analysis. In Fusion, the SMT solver does not work as a standalone tool on path conditions but directly on the program together with the sparse analysis. Such a fused design allows us to determine the path feasibility without explicitly computing path conditions, not only saving the cost of computing path conditions but also providing an opportunity to enhance the SMT solving algorithm. To the best of our knowledge, Fusion, for the �rst time, enables whole program bug detection on millions of lines of code in a common personal computer, with the precision of inter-procedural path-sensitivity. Compared to two state-of-the-art tools, Fusion is 10⇥ faster but consumes only 10&perc; of memory on average. Fusion has detected over a hundred bugs in mature open-source software, some of which have even been assigned CVE identi�ers due to their security impact.",
    "page": "14",
    "page-first": "14",
    "language": "en-US",
    "_line": "LanguageTools.bib:173"
  },
  "cai_canary_2021": {
    "id": "cai_canary_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Cai",
        "given": "Yuandao"
      },
      {
        "family": "Yao",
        "given": "Peisen"
      },
      {
        "family": "Zhang",
        "given": "Charles"
      }
    ],
    "title": "Canary: Practical Static Detection of  Inter-thread Value-Flow Bugs",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Concurrent programs are still prone to bugs arising from the subtle interleavings of threads. Traditional static analysis for concurrent programs, such as data-�ow analysis and symbolic execution, has to explicitly explore redundant control states, leading to prohibitive computational complexity. This paper presents a value �ow analysis framework for concurrent programs called C����� that is practical to statically �nd diversi�ed inter-thread value-�ow bugs. Our work is the �rst to convert the concurrency bug detection to a source-sink reachability problem, e�ectively reducing redundant thread interleavings. Speci�cally, we propose a scalable thread-modular algorithm to capture data and interference dependence in a value-�ow graph. The relevant edges of value �ows are annotated with execution constraints as guards to describe the conditions of value �ows. C����� then traverses the graph to detect concurrency defects via tracking the source-sink properties and solving the aggregated guards of value �ows with an SMT solver to decide the realizability of interleaving executions. Experiments show that C����� is precise, scalable and practical, detecting over eighteen previously unknown concurrency bugs in large, widely-used software systems with low false positives.",
    "page": "15",
    "page-first": "15",
    "language": "en-US",
    "_line": "LanguageTools.bib:183"
  },
  "dietz_understanding_nodate": {
    "id": "dietz_understanding_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Dietz",
        "given": "Will"
      },
      {
        "family": "Li",
        "given": "Peng"
      },
      {
        "family": "Regehr",
        "given": "John"
      },
      {
        "family": "Adve",
        "given": "Vikram"
      }
    ],
    "title": "Understanding Integer Overﬂow in C/C++",
    "abstract": "Integer overﬂow bugs in C and C++ programs are difﬁcult to track down and may lead to fatal errors or exploitable vulnerabilities. Although a number of tools for ﬁnding these bugs exist, the situation is complicated because not all overﬂows are bugs. Better tools need to be constructed—but a thorough understanding of the issues behind these errors does not yet exist. We developed IOC, a dynamic checking tool for integer overﬂows, and used it to conduct the ﬁrst detailed empirical study of the prevalence and patterns of occurrence of integer overﬂows in C and C++ code. Our results show that intentional uses of wraparound behaviors are more common than is widely believed; for example, there are over 200 distinct locations in the SPEC CINT2000 benchmarks where overﬂow occurs. Although many overﬂows are intentional, a large number of accidental overﬂows also occur. Orthogonal to programmers’ intent, overﬂows are found in both welldeﬁned and undeﬁned ﬂavors. Applications executing undeﬁned operations can be, and have been, broken by improvements in compiler optimizations. Looking beyond SPEC, we found and reported undeﬁned integer overﬂows in SQLite, PostgreSQL, SafeInt, GNU MPC and GMP, Firefox, GCC, LLVM, Python, BIND, and OpenSSL; many of these have since been ﬁxed. Our results show that integer overﬂow issues in C and C++ are subtle and complex, that they are common even in mature, widely used programs, and that they are widely misunderstood by developers.",
    "page": "11",
    "page-first": "11",
    "language": "en-US",
    "_line": "Security.bib:392"
  },
  "sun_taming_2021": {
    "id": "sun_taming_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Sun",
        "given": "Xiaoyu"
      },
      {
        "family": "Li",
        "given": "Li"
      },
      {
        "family": "Bissyandé",
        "given": "Tegawendé F."
      },
      {
        "family": "Klein",
        "given": "Jacques"
      },
      {
        "family": "Octeau",
        "given": "Damien"
      },
      {
        "family": "Grundy",
        "given": "John"
      }
    ],
    "title": "Taming Reflection: An Essential Step Toward Whole-program Analysis of Android Apps",
    "container-title": "ACM Transactions on Software Engineering and Methodology",
    "container-title-short": "Taming Reflection",
    "title-short": "Taming Reflection",
    "issued": {
      "date-parts": [
        [
          "2021",
          "4",
          "23"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "4"
        ]
      ]
    },
    "issn": "1049-331X",
    "abstract": "Android developers heavily use reflection in their apps for legitimate reasons. However, reflection is also significantly used for hiding malicious actions. Unfortunately, current state-of-the-art static analysis tools for Android are challenged by the presence of reflective calls, which they usually ignore. Thus, the results of their security analysis, e.g., for private data leaks, are incomplete, given the measures taken by malware writers to elude static detection. We propose a new instrumentation-based approach to address this issue in a non-invasive way. Specifically, we introduce to the community a prototype tool called DroidRA, which reduces the resolution of reflective calls to a composite constant propagation problem and then leverages the COAL solver to infer the values of reflection targets. After that, it automatically instruments the app to replace reflective calls with their corresponding Java calls in a traditional paradigm. Our approach augments an app so that it can be more effectively statically analyzable, including by such static analyzers that are not reflection-aware. We evaluate DroidRA on benchmark apps as well as on real-world apps, and we demonstrate that it can indeed infer the target values of reflective calls and subsequently allow state-of-the-art tools to provide more sound and complete analysis results.",
    "keywords": "static analysis, Android, DroidRA, reflection",
    "URL": "https://doi.org/10.1145/3440033",
    "DOI": "10.1145/3440033",
    "page": "32:1-32:36",
    "page-first": "32",
    "volume": "30",
    "issue": "3",
    "_line": "LanguageTools.bib:202"
  },
  "wang_find_2021": {
    "id": "wang_find_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Wang",
        "given": "Junjie"
      },
      {
        "family": "Huang",
        "given": "Yuchao"
      },
      {
        "family": "Wang",
        "given": "Song"
      },
      {
        "family": "Wang",
        "given": "Qing"
      }
    ],
    "title": "Find Bugs in Static Bug Finders",
    "container-title": "arXiv:2109.02245 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "13"
        ]
      ]
    },
    "abstract": "Static bug finders have been widely-adopted by developers to find bugs in real world software projects. They leverage predefined heuristic static analysis rules to scan source code or binary code of a software project, and report violations to these rules as warnings to be verified. However, the advantages of static bug finders are overshadowed by such issues as uncovered obvious bugs, false positives, etc. To improve these tools, many techniques have been proposed to filter out false positives reported or design new static analysis rules. Nevertheless, the under-performance of bug finders can also be caused by the incorrectness of current rules contained in the static bug finders, which is not explored yet. In this work, we propose a differential testing approach to detect bugs in the rules of four widely-used static bug finders, i.e., SonarQube, PMD, SpotBugs, and ErrorProne, and conduct a qualitative study about the bugs found. To retrieve paired rules across static bug finders for differential testing, we design a heuristic-based rule mapping method which combines the similarity in rules description and the overlap in warning information reported by the tools. The experiment on 2,728 open source projects reveals 46 bugs in the static bug finders, among which 24 are fixed or confirmed and the left are awaiting confirmation. We also summarize 13 bug patterns in the static analysis rules based on their context and root causes, which can serve as the checklist for designing and implementing other rules and or in other tools. This study indicates that the commonly-used static bug finders are not as reliable as they might have been envisaged. It not only demonstrates the effectiveness of our approach, but also highlights the need to continue improving the reliability of the static bug finders.",
    "keywords": "Computer Science - Software Engineering",
    "URLtext": "2109.02245",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2109.02245",
    "URL": "http://arxiv.org/abs/2109.02245",
    "_line": "LanguageTools.bib:221"
  },
  "willis_design_2021": {
    "id": "willis_design_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Willis",
        "given": "Jamie"
      },
      {
        "family": "Wu",
        "given": "Nicolas"
      }
    ],
    "title": "Design patterns for parser combinators (functional pearl)",
    "container-title": "Proceedings of the 14th ACM SIGPLAN International Symposium on Haskell",
    "event-title": "ICFP '21: 26th ACM SIGPLAN International Conference on Functional Programming",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-8615-9",
    "abstract": "Parser combinators are a popular and elegant approach for parsing in functional languages. The design and implementation of such libraries are well discussed, but having a welldesigned library is only one-half of the story. In this paper we explore several reusable approaches to writing parsers in combinator style, focusing on easy to apply patterns to keep parsing code simple, separated, and maintainable.",
    "URL": "https://dl.acm.org/doi/10.1145/3471874.3472984",
    "DOI": "10.1145/3471874.3472984",
    "publisher-place": "Virtual Republic of Korea",
    "page": "71-84",
    "page-first": "71",
    "language": "en-US",
    "_line": "LanguageTools.bib:235"
  },
  "vintila_mesh_2021": {
    "id": "vintila_mesh_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Vintila",
        "given": "Emanuel Q."
      },
      {
        "family": "Zieris",
        "given": "Philipp"
      },
      {
        "family": "Horsch",
        "given": "Julian"
      }
    ],
    "title": "MESH: A Memory-Efficient Safe Heap for C/C++",
    "container-title": "The 16th International Conference on Availability, Reliability and Security",
    "container-title-short": "MESH",
    "collection-title": "ARES 2021",
    "title-short": "MESH",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-9051-4",
    "abstract": "While memory corruption bugs stemming from the use of unsafe programming languages are an old and well-researched problem, the resulting vulnerabilities still dominate real-world exploitation today. Various mitigations have been proposed to alleviate the problem, mainly in the form of language dialects, static program analysis, and code or binary instrumentation. Solutions like AdressSanitizer (ASan) and Softbound/CETS have proven that the latter approach is very promising, being able to achieve memory safety without requiring manual source code adaptions, albeit suffering substantial performance and memory overheads. While performance overhead can be seen as a flexible constraint, extensive memory overheads can be prohibitive for the use of such solutions in memory-constrained environments. To address this problem, we propose MESH, a highly memory-efficient safe heap for C/C++. With its constant, very small memory overhead (configurable up to 2 MB on x86-64) and constant complexity for pointer access checking, MESH offers efficient, byte-precise spatial and temporal memory safety for memory-constrained scenarios. Without jeopardizing the security of safe heap objects, MESH is fully compatible with existing code and uninstrumented libraries, making it practical to use in heterogeneous environments. We show the feasibility of our approach with a full LLVM-based prototype supporting both major architectures, i.e., x86-64 and ARM64, in a Linux runtime environment. Our prototype evaluation shows that, compared to ASan and Softbound/CETS, MESH can achieve huge memory savings while preserving similar execution performance.",
    "keywords": "memory safety, buffer overflows, dangling pointers, pointer tagging, unsafe programming languages, use-after-free",
    "URL": "https://doi.org/10.1145/3465481.3465760",
    "DOI": "10.1145/3465481.3465760",
    "publisher-place": "New York, NY, USA",
    "page": "1-10",
    "page-first": "1",
    "_line": "LanguageTools.bib:253"
  },
  "costea_hippodrome_2021": {
    "id": "costea_hippodrome_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Costea",
        "given": "Andreea"
      },
      {
        "family": "Tiwari",
        "given": "Abhishek"
      },
      {
        "family": "Chianasta",
        "given": "Sigmund"
      },
      {
        "family": "R",
        "given": "Kishore"
      },
      {
        "family": "Roychoudhury",
        "given": "Abhik"
      },
      {
        "family": "Sergey",
        "given": "Ilya"
      }
    ],
    "title": "HIPPODROME: Data Race Repair using Static Analysis Summaries",
    "container-title": "arXiv:2108.02490 \\[cs\\]",
    "container-title-short": "HIPPODROME",
    "title-short": "HIPPODROME",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "11"
        ]
      ]
    },
    "abstract": "Implementing bug-free concurrent programs is a challenging task in modern software development. State-of-the-art static analyses find hundreds of concurrency bugs in production code, scaling to large codebases. Yet, fixing these bugs in constantly changing codebases represents a daunting effort for programmers, particularly because a fix in the concurrent code can introduce other bugs in a subtle way. In this work, we show how to harness compositional static analysis for concurrency bug detection, to enable a new Automated Program Repair (APR) technique for data races in large concurrent Java codebases. The key innovation of our work is an algorithm that translates procedure summaries inferred by the analysis tool for the purpose of bug reporting, into small local patches that fix concurrency bugs (without introducing new ones). This synergy makes it possible to extend the virtues of compositional static concurrency analysis to APR, making our approach effective (it can detect and fix many more bugs than existing tools for data race repair), scalable (it takes seconds to analyse and suggest fixes for sizeable codebases), and usable (generally, it does not require annotations from the users and can perform continuous automated repair). Our study conducted on popular open-source projects has confirmed that our tool automatically produces concurrency fixes similar to those proposed by the developers in the past.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Software Engineering",
    "URLtext": "2108.02490",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2108.02490",
    "URL": "http://arxiv.org/abs/2108.02490",
    "_line": "LanguageTools.bib:287"
  },
  "zuo_systemizing_2021": {
    "id": "zuo_systemizing_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Zuo",
        "given": "Zhiqiang"
      },
      {
        "family": "Wang",
        "given": "Kai"
      },
      {
        "family": "Hussain",
        "given": "Aftab"
      },
      {
        "family": "Sani",
        "given": "Ardalan Amiri"
      },
      {
        "family": "Zhang",
        "given": "Yiyu"
      },
      {
        "family": "Lu",
        "given": "Shenming"
      },
      {
        "family": "Dou",
        "given": "Wensheng"
      },
      {
        "family": "Wang",
        "given": "Linzhang"
      },
      {
        "family": "Li",
        "given": "Xuandong"
      },
      {
        "family": "Wang",
        "given": "Chenxi"
      },
      {
        "family": "Xu",
        "given": "Guoqing Harry"
      }
    ],
    "title": "Systemizing Interprocedural Static Analysis of Large-scale Systems Code with Graspan",
    "container-title": "ACM Transactions on Computer Systems",
    "container-title-short": "ACM Trans. Comput. Syst.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "29"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "4"
        ]
      ]
    },
    "issn": "0734-2071",
    "abstract": "There is more than a decade-long history of using static analysis to find bugs in systems such as Linux. Most of the existing static analyses developed for these systems are simple checkers that find bugs based on pattern matching. Despite the presence of many sophisticated interprocedural analyses, few of them have been employed to improve checkers for systems code due to their complex implementations and poor scalability. In this article, we revisit the scalability problem of interprocedural static analysis from a “Big Data” perspective. That is, we turn sophisticated code analysis into Big Data analytics and leverage novel data processing techniques to solve this traditional programming language problem. We propose Graspan, a disk-based parallel graph system that uses an edge-pair centric computation model to compute dynamic transitive closures on very large program graphs. We develop two backends for Graspan, namely, Graspan-C running on CPUs and Graspan-G on GPUs, and present their designs in the article. Graspan-C can analyze large-scale systems code on any commodity PC, while, if GPUs are available, Graspan-G can be readily used to achieve orders of magnitude speedup by harnessing a GPU’s massive parallelism. We have implemented fully context-sensitive pointer/alias and dataflow analyses on Graspan. An evaluation of these analyses on large codebases written in multiple languages such as Linux and Apache Hadoop demonstrates that their Graspan implementations are language-independent, scale to millions of lines of code, and are much simpler than their original implementations. Moreover, we show that these analyses can be used to uncover many real-world bugs in large-scale systems code.",
    "keywords": "static analysis, disk-based systems, graph processing",
    "URL": "https://doi.org/10.1145/3466820",
    "DOI": "10.1145/3466820",
    "page": "4:1-4:39",
    "page-first": "4",
    "volume": "38",
    "issue": "1",
    "_line": "LanguageTools.bib:302"
  },
  "swierstra_data_2008": {
    "id": "swierstra_data_2008",
    "type": "article-journal",
    "author": [
      {
        "family": "Swierstra",
        "given": "Wouter"
      }
    ],
    "title": "Data types à la carte",
    "container-title": "Journal of Functional Programming",
    "container-title-short": "J. Funct. Prog.",
    "issued": {
      "date-parts": [
        [
          "2008",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "30"
        ]
      ]
    },
    "issn": "0956-7968, 1469-7653",
    "abstract": "This paper describes a technique for assembling both data types and functions from isolated individual components. We also explore how the same technology can be used to combine free monads and, as a result, structure Haskell’s monolithic IO monad.",
    "URL": "http://www.journals.cambridge.org/abstract_S0956796808006758",
    "DOI": "10.1017/S0956796808006758",
    "volume": "18",
    "issue": "4",
    "language": "en-US",
    "_line": "LanguageTools.bib:320"
  },
  "panchenko_bolt_2019": {
    "id": "panchenko_bolt_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Panchenko",
        "given": "Maksim"
      },
      {
        "family": "Auler",
        "given": "Rafael"
      },
      {
        "family": "Nell",
        "given": "Bill"
      },
      {
        "family": "Ottoni",
        "given": "Guilherme"
      }
    ],
    "title": "BOLT: a practical binary optimizer for data centers and beyond",
    "container-title": "Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization",
    "container-title-short": "BOLT",
    "collection-title": "CGO 2019",
    "title-short": "BOLT",
    "issued": {
      "date-parts": [
        [
          "2019",
          "2",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "20"
        ]
      ]
    },
    "publisher": "IEEE Press",
    "isbn": "978-1-72811-436-1",
    "abstract": "Abstract — Performance optimization for large-scale applications has recently become more important as computation continues to move towards data centers. Data-center applications are generally very large and complex, which makes code layout an important optimization to improve their performance. This has motivated recent investigation of practical techniques to improve code layout at both compile time and link time. Although post-link optimizers had some success in the past, no recent work has explored their benefits in the context of modern data-center applications. In this paper, we present BOLT, an open-source post-link optimizer built on top of the LLVM framework. Utilizing sample-based profiling, BOLT boosts the performance of real-world applications even for highly optimized binaries built with both feedback-driven optimizations (FDO) and link-time optimizations (LTO). We demonstrate that post-link performance improvements are complementary to conventional compiler optimizations, even when the latter are done at a whole-program level and in the presence of profile information. We evaluated BOLT on both Facebook data-center workloads and open-source compilers. For data-center applications, BOLT achieves up to 7.0&perc; performance speedups on top of profile-guided function reordering and LTO. For the GCC and Clang compilers, our evaluation shows that BOLT speeds up their binaries by up to 20.4&perc; on top of FDO and LTO, and up to 52.1&perc; if the binaries are built without FDO and LTO.",
    "publisher-place": "Washington, DC, USA",
    "page": "2-14",
    "page-first": "2",
    "_line": "LanguageTools.bib:337"
  },
  "six_certified_2020": {
    "id": "six_certified_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Six",
        "given": "Cyril"
      },
      {
        "family": "Boulmé",
        "given": "Sylvain"
      },
      {
        "family": "Monniaux",
        "given": "David"
      }
    ],
    "title": "Certified and efficient instruction scheduling: application to interlocked VLIW processors",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Certified and efficient instruction scheduling",
    "title-short": "Certified and efficient instruction scheduling",
    "issued": {
      "date-parts": [
        [
          "2020",
          "11",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "30"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "CYRIL SIX, Kalray S.A., France and Univ. Grenoble Alpes, CNRS, Grenoble INP, Verimag, France SYLVAIN BOULMÉ, Univ. Grenoble Alpes, CNRS, Grenoble INP, Verimag, France DAVID MONNIAUX, Univ. Grenoble Alpes, CNRS, Grenoble INP, Verimag, France CompCert is a moderately optimizing C compiler with a formal, machine-checked, proof of correctness: after successful compilation, the assembly code has a behavior faithful to the source code. Previously, it only supported target instruction sets with sequential semantics, and did not attempt reordering instructions for optimization. We present here a CompCert backend for a VLIW core (i.e. with explicit parallelism at the instruction level), the first CompCert backend providing scalable and efficient instruction scheduling. Furthermore, its highly modular implementation can be easily adapted to other VLIW or non-VLIW pipelined processors. CCS Concepts: • Software and its engineering → Formal software verification; Retargetable compilers; • Theory of computation → Scheduling algorithms; • General and reference → Performance; • Computer systems organization → Superscalar architectures; Very long instruction word.",
    "URL": "https://dl.acm.org/doi/10.1145/3428197",
    "DOI": "10.1145/3428197",
    "page": "1-29",
    "page-first": "1",
    "volume": "4",
    "language": "en-US",
    "_line": "LanguageTools.bib:353"
  },
  "lasser_costar_2021": {
    "id": "lasser_costar_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Lasser",
        "given": "Sam"
      },
      {
        "family": "Casinghino",
        "given": "Chris"
      },
      {
        "family": "Fisher",
        "given": "Kathleen"
      },
      {
        "family": "Roux",
        "given": "Cody"
      }
    ],
    "title": "CoStar: a verified ALL(\\*) parser",
    "container-title": "Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
    "container-title-short": "CoStar",
    "collection-title": "PLDI 2021",
    "title-short": "CoStar",
    "issued": {
      "date-parts": [
        [
          "2021",
          "6",
          "19"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "25"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8391-2",
    "abstract": "Parsers are security-critical components of many software systems, and verified parsing therefore has a key role to play in secure software design. However, existing verified parsers for context-free grammars are limited in their expressiveness, termination properties, or performance characteristics. They are only compatible with a restricted class of grammars, they are not guaranteed to terminate on all inputs, or they are not designed to be performant on grammars for real-world programming languages and data formats. In this work, we present CoStar, a verified parser that addresses these limitations. The parser is implemented with the Coq Proof Assistant and is based on the ALL(\\*) parsing algorithm. CoStar is sound and complete for all non-left-recursive grammars; it produces a correct parse tree for its input whenever such a tree exists, and it correctly detects ambiguous inputs. CoStar also provides strong termination guarantees; it terminates without error on all inputs when applied to a non-left-recursive grammar. Finally, CoStar achieves linear-time performance on a range of unambiguous grammars for commonly used languages and data formats.",
    "keywords": "interactive theorem proving, parsing",
    "URL": "https://doi.org/10.1145/3453483.3454053",
    "DOI": "10.1145/3453483.3454053",
    "publisher-place": "New York, NY, USA",
    "page": "420-434",
    "page-first": "420",
    "_line": "LanguageTools.bib:372"
  },
  "liu_when_2021": {
    "id": "liu_when_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Liu",
        "given": "Bozhen"
      },
      {
        "family": "Liu",
        "given": "Peiming"
      },
      {
        "family": "Li",
        "given": "Yanze"
      },
      {
        "family": "Tsai",
        "given": "Chia-Che"
      },
      {
        "family": "Da Silva",
        "given": "Dilma"
      },
      {
        "family": "Huang",
        "given": "Jeff"
      }
    ],
    "title": "When threads meet events: efficient and precise static race detection with origins",
    "container-title": "Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
    "container-title-short": "When threads meet events",
    "collection-title": "PLDI 2021",
    "title-short": "When threads meet events",
    "issued": {
      "date-parts": [
        [
          "2021",
          "6",
          "19"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "25"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8391-2",
    "abstract": "Data races are among the worst bugs in software in that they exhibit non-deterministic symptoms and are notoriously difficult to detect. The problem is exacerbated by interactions between threads and events in real-world applications. We present a novel static analysis technique, O2, to detect data races in large complex multithreaded and event-driven software. O2 is powered by “origins”, an abstraction that unifies threads and events by treating them as entry points of code paths attributed with data pointers. Origins in most cases are inferred automatically, but can also be specified by developers. More importantly, origins provide an efficient way to precisely reason about shared memory and pointer aliases. Together with several important design choices for race detection, we have implemented O2 for both C/C++ and Java/Android applications and applied it to a wide range of open-source software. O2 has found new races in every single real-world code base we evaluated with, including Linux kernel, Redis, OVS, Memcached, Hadoop, Tomcat, ZooKeeper and Firefox Android. Moreover, O2 scales to millions of lines of code in a few minutes, on average 70x faster (up to 568x) compared to an existing static analysis tool from our prior work, and reduces false positives by 77&perc;. We also compared O2 with the state-of-the-art static race detection tool, RacerD, showing highly promising results. At the time of writing, O2 has revealed more than 40 unique previously unknown races that have been confirmed or fixed by developers.",
    "keywords": "Data Race Detection, Origins, Pointer Analysis, Static Analysis",
    "URL": "https://doi.org/10.1145/3453483.3454073",
    "DOI": "10.1145/3453483.3454073",
    "publisher-place": "New York, NY, USA",
    "page": "725-739",
    "page-first": "725",
    "_line": "LanguageTools.bib:391"
  },
  "athaiya_data_2021": {
    "id": "athaiya_data_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Athaiya",
        "given": "Snigdha"
      },
      {
        "family": "Komondoor",
        "given": "Raghavan"
      },
      {
        "family": "Kumar",
        "given": "K. Narayan"
      }
    ],
    "title": "Data Flow Analysis of Asynchronous Systems using Infinite Abstract Domains",
    "container-title": "Programming Languages and Systems",
    "container-title-short": "Programming Languages and Systems",
    "issued": {
      "date-parts": [
        [
          "2021",
          "3",
          "23"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "4",
          "15"
        ]
      ]
    },
    "abstract": "Asynchronous message-passing systems are employed frequently to implement distributed mechanisms, protocols, and processes. This paper addresses the problem of precise data flow analysis for such systems. To obtain good precision, data flow analysis needs to somehow skip execution paths that read more messages than the number of messages sent so far in the path, as such paths are infeasible at run time. Existing data flow analysis techniques do elide a subset of such infeasible paths, but have the restriction that they admit only finite abstract analysis domains. In this paper we propose a generalization of these approaches to admit infinite abstract analysis domains, as such domains are commonly used in practice to obtain high precision. We have implemented our approach, and have analyzed its performance on a set of 14 benchmarks. On these benchmarks our tool obtains significantly higher precision compared to a baseline approach that does not elide any infeasible paths and to another baseline that elides infeasible paths but admits only finite abstract domains.",
    "URLtext": "PMC7984527",
    "URLpretext": "[PMCID:]{.etype}",
    "eprint-type": "pmcid",
    "eprint": "PMC7984527",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7984527/",
    "DOI": "10.1007/978-3-030-72019-3_2",
    "page": "30-58",
    "page-first": "30",
    "volume": "12648",
    "_line": "LanguageTools.bib:410"
  },
  "barbar_flow-sensitive_2020": {
    "id": "barbar_flow-sensitive_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Barbar",
        "given": "Mohamad"
      },
      {
        "family": "Sui",
        "given": "Yulei"
      },
      {
        "family": "Chen",
        "given": "Shiping"
      }
    ],
    "title": "Flow-Sensitive Type-Based Heap Cloning",
    "container-title": "34th European Conference on Object-Oriented Programming",
    "container-title-short": "ECOOP",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "abstract": "By respecting program control-ﬂow, ﬂow-sensitive pointer analysis promises more precise results than its ﬂow-insensitive counterpart. However, existing heap abstractions for C and C++ ﬂow-sensitive pointer analyses model the heap by creating a single abstract heap object for each memory allocation. Two runtime heap objects which originate from the same allocation site are imprecisely modelled using one abstract object, which makes them share the same imprecise points-to sets and thus reduces the beneﬁt of analysing heap objects ﬂow-sensitively. On the other hand, equipping ﬂow-sensitive analysis with context-sensitivity, whereby an abstract heap object would be created (cloned) per calling context, can yield a more precise heap model, but at the cost of uncontrollable analysis overhead when analysing larger programs.",
    "page": "26",
    "page-first": "26",
    "language": "en-US",
    "_line": "LanguageTools.bib:427"
  },
  "sui_loop-oriented_2018": {
    "id": "sui_loop-oriented_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Sui",
        "given": "Yulei"
      },
      {
        "family": "Fan",
        "given": "Xiaokang"
      },
      {
        "family": "Zhou",
        "given": "Hao"
      },
      {
        "family": "Xue",
        "given": "Jingling"
      }
    ],
    "title": "Loop-Oriented Pointer Analysis for Automatic SIMD Vectorization",
    "container-title": "ACM Transactions on Embedded Computing Systems",
    "container-title-short": "ACM Trans. Embed. Comput. Syst.",
    "issued": {
      "date-parts": [
        [
          "2018",
          "1",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "18"
        ]
      ]
    },
    "issn": "1539-9087",
    "abstract": "Compiler-based vectorization represents a promising solution to automatically generate code that makes efficient use of modern CPUs with SIMD extensions. Two main auto-vectorization techniques, superword-level parallelism vectorization (SLP) and loop-level vectorization (LLV), require precise dependence analysis on arrays and structs to vectorize isomorphic scalar instructions (in the case of SLP) and reduce dynamic dependence checks at runtime (in the case of LLV). The alias analyses used in modern vectorizing compilers are either intra-procedural (without tracking inter-procedural data-flows) or inter-procedural (by using field-sensitive models, which are too imprecise in handling arrays and structs). This article proposes an inter-procedural Loop-oriented Pointer Analysis for C, called Lpa, for analyzing arrays and structs to support aggressive SLP and LLV optimizations effectively. Unlike field-insensitive solutions that pre-allocate objects for each memory allocation site, our approach uses a lazy memory model to generate access-based location sets based on how structs and arrays are accessed. Lpa can precisely analyze arrays and nested aggregate structures to enable SIMD optimizations for large programs. By separating the location set generation as an independent concern from the rest of the pointer analysis, Lpa is designed so that existing points-to resolution algorithms (e.g., flow-insensitive and flow-sensitive pointer analysis) can be reused easily. We have implemented Lpa fully in the LLVM compiler infrastructure (version 3.8.0). We evaluate Lpa by considering SLP and LLV, the two classic vectorization techniques, on a set of 20 C and Fortran CPU2000/2006 benchmarks. For SLP, Lpa outperforms LLVM’s BasicAA and ScevAA by discovering 139 and 273 more vectorizable basic blocks, respectively, resulting in the best speedup of 2.95&perc; for 173.applu. For LLV, LLVM introduces totally 551 and 652 static bound checks under BasicAA and ScevAA, respectively. In contrast, Lpa has reduced these static checks to 220, with an average of 15.7 checks per benchmark, resulting in the best speedup of 7.23&perc; for 177.mesa.",
    "keywords": "Pointer analysis, compiler optimisation, loop-oriented, SIMD",
    "URL": "https://doi.org/10.1145/3168364",
    "DOI": "10.1145/3168364",
    "page": "56:1-56:31",
    "page-first": "56",
    "volume": "17",
    "issue": "2",
    "_line": "LanguageTools.bib:439"
  },
  "sui_flow2vec_2020": {
    "id": "sui_flow2vec_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Sui",
        "given": "Yulei"
      },
      {
        "family": "Cheng",
        "given": "Xiao"
      },
      {
        "family": "Zhang",
        "given": "Guanqin"
      },
      {
        "family": "Wang",
        "given": "Haoyu"
      }
    ],
    "title": "Flow2Vec: value-flow-based precise code embedding",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Flow2Vec",
    "title-short": "Flow2Vec",
    "issued": {
      "date-parts": [
        [
          "2020",
          "11",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "18"
        ]
      ]
    },
    "issn": "2475-1421",
    "URL": "https://dl.acm.org/doi/10.1145/3428301",
    "DOI": "10.1145/3428301",
    "page": "1-27",
    "page-first": "1",
    "volume": "4",
    "language": "en-US",
    "_line": "LanguageTools.bib:457"
  },
  "moller_static_nodate": {
    "id": "moller_static_nodate",
    "type": "book",
    "author": [
      {
        "family": "Møller",
        "given": "Anders"
      },
      {
        "family": "Schwartzbach",
        "given": "Michael I"
      }
    ],
    "title": "Static Program Analysis",
    "number-of-pages": "176",
    "language": "en-US",
    "_line": "LanguageTools.bib:475"
  },
  "barbar_object_2021": {
    "id": "barbar_object_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Barbar",
        "given": "Mohamad"
      },
      {
        "family": "Sui",
        "given": "Yulei"
      },
      {
        "family": "Chen",
        "given": "Shiping"
      }
    ],
    "title": "Object Versioning for Flow-Sensitive Pointer Analysis",
    "container-title": "2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)",
    "event-title": "2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "18"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-72818-613-9",
    "abstract": "Flow-sensitive points-to analysis provides better precision than its ﬂow-insensitive counterpart. Traditionally performed on the control-ﬂow graph, it incurs heavy analysis overhead. For performance, staged ﬂow-sensitive analysis (SFS) is conducted on a pre-computed def-use (value-ﬂow) graph where points-to sets of variables are propagated across def-use chains sparsely rather than across control-ﬂow in the control-ﬂow graph. SFS makes the propagation of different objects’ pointsto sets sparse (multiple-object sparsity), however, it suffers from redundant propagation between instructions of the same object’s points-to sets (single-object sparsity). The points-to set of an object is often duplicated, resulting in redundant propagation and storage, especially in real-world heap-intensive programs.",
    "URL": "https://ieeexplore.ieee.org/document/9370334/",
    "DOI": "10.1109/CGO51591.2021.9370334",
    "publisher-place": "Seoul, Korea (South)",
    "page": "222-235",
    "page-first": "222",
    "language": "en-US",
    "_line": "LanguageTools.bib:483"
  },
  "sui_value-flow-based_2020": {
    "id": "sui_value-flow-based_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Sui",
        "given": "Y."
      },
      {
        "family": "Xue",
        "given": "J."
      }
    ],
    "title": "Value-Flow-Based Demand-Driven Pointer Analysis for C and C++",
    "container-title": "IEEE Transactions on Software Engineering",
    "issued": {
      "date-parts": [
        [
          "2020",
          "8"
        ]
      ]
    },
    "issn": "1939-3520",
    "abstract": "We present Supa, a value-flow-based demand-driven flow- and context-sensitive pointer analysis with strong updates for C and C++ programs. Supa enables computing points-to information via value-flow refinement, in environments with small time and memory budgets. We formulate Supa by solving a graph-reachability problem on an inter-procedural value-flow graph representing a program's def-use chains, which are pre-computed efficiently but over-approximately. To answer a client query (a request for a variable's points-to set), Supa reasons about the flow of values along the pre-computed def-use chains sparsely (rather than across all program points), by performing only the work necessary for the query (rather than analyzing the whole program). In particular, strong updates are performed to filter out spurious def-use chains through value-flow refinement as long as the total budget is not exhausted. We have implemented Supa on top of LLVM (4.0.0) together with a comprehensive micro-benchmark suite after a years-long effort (consisting of around 400 test cases, including hand-written ones and the ones extracted from real programs). We have evaluated Supa by choosing uninitialized pointer detection and C++ virtual table resolution as two major clients, using 24 real-world programs including 18 open-source C programs and 6 large CPU2000/2006 C++ benchmarks. For uninitialized pointer client, Supa achieves improved precision as the analysis budget increases, with its flow-sensitive (context-insensitive) analysis reaching 97.4 percent of that achieved by whole-program Sparse Flow-Sensitive analysis (SFS) by consuming about 0.18 seconds and 65 KB of memory per query, on average (with a budget of at most 10,000 value-flow edges per query). With context-sensitivity also considered, Supa becomes more precise for some programs but also incurs more analysis times. To further demonstrate the effectiveness of Supa, we have also evaluated Supa in resolving C++ virtual tables by querying the function pointers at every virtual callsite. Compared to analysis without strong updates for heap objects, Supa's demand-driven context-sensitive strong update analysis reduces 7.35 percent spurious virtual table targets with only 0.4 secs per query, on average.",
    "keywords": "Resource management, optimising compilers, program diagnostics, flow sensitivity, C program, C++ languages, C++ program, context-sensitive pointer analysis, data flow analysis, data structures, efficiency 7.35 percent, efficiency 97.4 percent, flow graphs, Instruction sets, inter-procedural value-flow graph, memory size 65.0 KByte, object-oriented programming, Open source software, pointer analysis, pointer client, pre-computed def-use chains, reachability analysis, Reachability analysis, Registers, Sensitivity, sensitivity analysis, sparse flow-sensitive analysis, Strong updates, Supa demand-driven context-sensitive strong update analysis, temperature 2006.0 C, time 0.4 s, value flow, value-flow refinement, value-flow-based demand-driven flow, value-flow-based demand-driven pointer analysis",
    "DOI": "10.1109/TSE.2018.2869336",
    "page": "812-835",
    "page-first": "812",
    "volume": "46",
    "note": "Conference Name: IEEE Transactions on Software Engineering",
    "issue": "8",
    "_line": "LanguageTools.bib:501"
  },
  "xu_vfix_2019": {
    "id": "xu_vfix_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Xu",
        "given": "Xuezheng"
      },
      {
        "family": "Sui",
        "given": "Yulei"
      },
      {
        "family": "Yan",
        "given": "Hua"
      },
      {
        "family": "Xue",
        "given": "Jingling"
      }
    ],
    "title": "VFix: value-flow-guided precise program repair for null pointer dereferences",
    "container-title": "Proceedings of the 41st International Conference on Software Engineering",
    "container-title-short": "VFix",
    "collection-title": "ICSE '19",
    "title-short": "VFix",
    "issued": {
      "date-parts": [
        [
          "2019",
          "5",
          "25"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "17"
        ]
      ]
    },
    "publisher": "IEEE Press",
    "abstract": "Automated Program Repair (APR) faces a key challenge in efficiently generating correct patches from a potentially infinite solution space. Existing approaches, which attempt to reason about the entire solution space, can be ineffective (by often producing no plausible patches at all) and imprecise (by often producing plausible but incorrect patches). We present VFix, a new value-flow-guided APR approach, to fix null pointer exception (NPE) bugs by considering a substantially reduced solution space in order to greatly increase the number of correct patches generated. By reasoning about the data and control dependences in the program, VFix can identify bug-relevant repair statements more accurately and generate more correct repairs than before. VFix outperforms a set of 8 state-of-the-art APR tools in fixing the NPE bugs in Defects4j in terms of both precision (by correctly fixing 3 times as many bugs as the most precise one and 50&perc; more than all the bugs correctly fixed by these 8 tools altogether) and efficiency (by producing a correct patch in minutes instead of hours).",
    "keywords": "null dereference, static analysis, program repair",
    "URL": "https://doi.org/10.1109/ICSE.2019.00063",
    "DOI": "10.1109/ICSE.2019.00063",
    "publisher-place": "Montreal, Quebec, Canada",
    "page": "512-523",
    "page-first": "512",
    "_line": "LanguageTools.bib:517"
  },
  "ye_accelerating_2014": {
    "id": "ye_accelerating_2014",
    "type": "paper-conference",
    "author": [
      {
        "family": "Ye",
        "given": "Ding"
      },
      {
        "family": "Sui",
        "given": "Yulei"
      },
      {
        "family": "Xue",
        "given": "Jingling"
      }
    ],
    "title": "Accelerating Dynamic Detection of Uses of Undefined Values with Static Value-Flow Analysis",
    "container-title": "Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization",
    "collection-title": "CGO '14",
    "issued": {
      "date-parts": [
        [
          "2014",
          "2",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "17"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-2670-4",
    "abstract": "Uninitialized variables can cause system crashes when used and security vulnerabilities when exploited. With source rather than binary instrumentation, dynamic analysis tools such as MSan can detect uninitialized memory uses at significantly reduced overhead but are still costly. In this paper, we introduce a static value-flow analysis, called Usher, to guide and accelerate the dynamic analysis performed by such tools. Usher reasons about the definedness of values using a value-flow graph (VFG) that captures def-use chains for both top-level and address-taken variables interprocedurally and removes unnecessary instrumentation by solving a graph reachability problem. Usher works well with any pointer analysis (done a priori) and facilitates advanced instrumentation-reducing optimizations (with two demonstrated here). Implemented in LLVM and evaluated using all the 15 SPEC2000 C programs, Usher can reduce the slowdown of MSan from 212&perc; &ndash; 302&perc; to 123&perc; &ndash; 140&perc; for a number of configurations tested.",
    "keywords": "static and dynamic analysis, Undefined values",
    "URL": "https://doi.org/10.1145/2581122.2544154",
    "DOI": "10.1145/2581122.2544154",
    "publisher-place": "New York, NY, USA",
    "page": "154-164",
    "page-first": "154",
    "_line": "LanguageTools.bib:535"
  },
  "sui_sparse_2016": {
    "id": "sui_sparse_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Sui",
        "given": "Yulei"
      },
      {
        "family": "Di",
        "given": "Peng"
      },
      {
        "family": "Xue",
        "given": "Jingling"
      }
    ],
    "title": "Sparse flow-sensitive pointer analysis for multithreaded programs",
    "container-title": "Proceedings of the 2016 International Symposium on Code Generation and Optimization",
    "collection-title": "CGO '16",
    "issued": {
      "date-parts": [
        [
          "2016",
          "2",
          "29"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "17"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-3778-6",
    "abstract": "For C programs, flow-sensitivity is important to enable pointer analysis to achieve highly usable precision. Despite significant recent advances in scaling flow-sensitive pointer analysis sparsely for sequential C programs, relatively little progress has been made for multithreaded C programs. In this paper, we present FSAM, a new Flow-Sensitive pointer Analysis that achieves its scalability for large Multithreaded C programs by performing sparse analysis on top of a series of thread interference analysis phases. We evaluate FSAM with 10 multithreaded C programs (with more than 100K lines of code for the largest) from Phoenix-2.0, Parsec-3.0 and open-source applications. For two programs, raytrace and x264, the traditional data-flow-based flow-sensitive pointer analysis is un- scalable (under two hours) but our analysis spends just under 5 minutes on raytrace and 9 minutes on x264. For the rest, our analysis is 12x faster and uses 28x less memory.",
    "keywords": "Pointer Analysis, Flow-Sensitivity, Sparse Analysis",
    "URL": "https://doi.org/10.1145/2854038.2854043",
    "DOI": "10.1145/2854038.2854043",
    "publisher-place": "New York, NY, USA",
    "page": "160-170",
    "page-first": "160",
    "_line": "LanguageTools.bib:553"
  },
  "sui_-demand_2016": {
    "id": "sui_-demand_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Sui",
        "given": "Yulei"
      },
      {
        "family": "Xue",
        "given": "Jingling"
      }
    ],
    "title": "On-demand strong update analysis via value-flow refinement",
    "container-title": "Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering",
    "collection-title": "FSE 2016",
    "issued": {
      "date-parts": [
        [
          "2016",
          "11",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "17"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-4218-6",
    "abstract": "We present a new Strong UPdate Analysis for C programs, called Supa, that enables computing points-to information on-demand via value-flow refinement, in environments with small time and memory budgets such as IDEs. We formulate Supa by solving a graph-reachability problem on a value- flow graph representation of the program, so that strong updates are performed where needed, as long as the total analysis budget is not exhausted. Supa facilitates efficiency and precision tradeoffs by allowing different pointer analyses to be applied in a hybrid multi-stage analysis framework. We have implemented Supa in LLVM with its artifact available at \\[1\\]. We evaluate Supa by choosing uninitialized pointer detection as a major client on 12 open-source C programs. As the analysis budget increases, Supa achieves improved precision, with its single-stage flow-sensitive analysis reaching 97&perc; of that achieved by whole-program flow- sensitive analysis by consuming about 0.19 seconds and 36KB of memory per query, on average (with a budget of at most 10000 value-flow edges per query).",
    "keywords": "flow sensitivity, pointer analysis, value flow, strong updates",
    "URL": "https://doi.org/10.1145/2950290.2950296",
    "DOI": "10.1145/2950290.2950296",
    "publisher-place": "New York, NY, USA",
    "page": "460-473",
    "page-first": "460",
    "_line": "LanguageTools.bib:571"
  },
  "di_accelerating_2016": {
    "id": "di_accelerating_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Di",
        "given": "Peng"
      },
      {
        "family": "Sui",
        "given": "Yulei"
      }
    ],
    "title": "Accelerating Dynamic Data Race Detection Using Static Thread Interference Analysis",
    "container-title": "Proceedings of the 7th International Workshop on Programming Models and Applications for Multicores and Manycores",
    "collection-title": "PMAM'16",
    "issued": {
      "date-parts": [
        [
          "2016",
          "3",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "17"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-4196-7",
    "abstract": "Precise dynamic race detectors report an error if and only if more than one thread concurrently exhibits conflict on a memory access. They insert instrumentations at compile-time to perform runtime checks on all memory accesses to ensure that all races are captured and no spurious warnings are generated. However, a dynamic race check for a particular memory access statement is guaranteed to be redundant if the statement can be statically identified as thread interference-free. Despite significant recent advances in dynamic detection techniques, the redundant check remains a critical factor that leads to prohibitive overhead of dynamic race detection for multithreaded programs. In this paper, we present a new framework that eliminates redundant race check and boosts the dynamic race detection by performing static optimizations on top of a series of thread interference analysis phases. Our framework is implemented on top of LLVM 3.5.0 and evaluated with an industry dynamic race detector TSAN which is available as a part of LLVM tool chain. 11 benchmarks from SPLASH2 are used to evaluate the effectiveness of our approach in accelerating TSAN by eliminating redundant interference-free checks. The experimental result demonstrates our new approach achieves from 1.4x to 4.0x (2.4x on average) speedup over original TSAN under 4 threads setting, and achieves from 1.3x to 4.6x (2.6x on average) speedup under 16 threads setting.",
    "keywords": "static analysis, concurrent program, data race, multithreaded",
    "URL": "https://doi.org/10.1145/2883404.2883405",
    "DOI": "10.1145/2883404.2883405",
    "publisher-place": "New York, NY, USA",
    "page": "30-39",
    "page-first": "30",
    "_line": "LanguageTools.bib:589"
  },
  "pereira_wave_2009": {
    "id": "pereira_wave_2009",
    "type": "paper-conference",
    "author": [
      {
        "family": "Pereira",
        "given": "Fernando Magno Quintao"
      },
      {
        "family": "Berlin",
        "given": "Daniel"
      }
    ],
    "title": "Wave Propagation and Deep Propagation for Pointer Analysis",
    "container-title": "Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization",
    "collection-title": "CGO '09",
    "issued": {
      "date-parts": [
        [
          "2009",
          "3",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "15"
        ]
      ]
    },
    "publisher": "IEEE Computer Society",
    "isbn": "978-0-7695-3576-0",
    "abstract": "This paper describes two new algorithms for solving inclusion based points-to analysis. The first algorithm, the Wave Propagation Method, is a modified version of an early technique presented by Pearce et al; however, it greatly improves on the running time of its predecessor. The second algorithm, the Deep Propagation Method, is a more light-weighted analysis, that requires less memory. We have compared these algorithms with three state-of-the-art techniques by Hardekopf-Lin, Heintze-Tardieu and Pearce-Kelly-Hankin. Our experiments show that Deep Propagation has the best average execution time across a suite of 17 well-known benchmarks, the lowest memory requirements in absolute numbers, and the fastest absolute times for benchmarks under 100,000 lines of code. The memory-hungry Wave Propagation has the fastest absolute running times in a memory rich execution environment, matching the speed of the best known points-to analysis algorithms in large benchmarks.",
    "keywords": "Pointer analysis, Context insensitive, Cycle detection, Inclusion based",
    "URL": "https://doi.org/10.1109/CGO.2009.9",
    "DOI": "10.1109/CGO.2009.9",
    "publisher-place": "USA",
    "page": "126-135",
    "page-first": "126",
    "_line": "LanguageTools.bib:607"
  },
  "andersen_program_1994": {
    "id": "andersen_program_1994",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Andersen",
        "given": "Lars Ole"
      }
    ],
    "title": "Program Analysis and Specialization for the C Programming Language",
    "issued": {
      "date-parts": [
        [
          "1994"
        ]
      ]
    },
    "publisher": "University of Copenhagen",
    "number-of-pages": "311",
    "abstract": "Software engineers are faced with a dilemma. They want to write general and wellstructured programs that are flexible and easy to maintain. On the other hand, generality has a price: efficiency. A specialized program solving a particular problem is often significantly faster than a general program. However, the development of specialized software is time-consuming, and is likely to exceed the production of today’s programmers. New techniques are required to solve this so-called software crisis. Partial evaluation is a program specialization technique that reconciles the benefits of generality with efficiency. This thesis presents an automatic partial evaluator for the Ansi C programming language. The content of this thesis is analysis and transformation of C programs. We develop several analyses that support the transformation of a program into its generating extension. A generating extension is a program that produces specialized programs when executed on parts of the input. The thesis contains the following main results.",
    "_line": "LanguageTools.bib:625"
  },
  "liew_floating-point_2017": {
    "id": "liew_floating-point_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Liew",
        "given": "Daniel"
      },
      {
        "family": "Schemmel",
        "given": "Daniel"
      },
      {
        "family": "Cadar",
        "given": "Cristian"
      },
      {
        "family": "Donaldson",
        "given": "Alastair F."
      },
      {
        "family": "Zahl",
        "given": "Rafael"
      },
      {
        "family": "Wehrle",
        "given": "Klaus"
      }
    ],
    "title": "Floating-point symbolic execution: A case study in N-version programming",
    "container-title": "2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)",
    "container-title-short": "Floating-point symbolic execution",
    "title-short": "Floating-point symbolic execution",
    "event-title": "2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)",
    "issued": {
      "date-parts": [
        [
          "2017",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "10"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-5386-2684-9",
    "abstract": "Symbolic execution is a well-known program analysis technique for testing software, which makes intensive use of constraint solvers. Recent support for ﬂoating-point constraint solving has made it feasible to support ﬂoating-point reasoning in symbolic execution tools. In this paper, we present the experience of two research teams that independently added ﬂoating-point support to KLEE, a popular symbolic execution engine. Since the two teams independently developed their extensions, this created the rare opportunity to conduct a rigorous comparison between the two implementations, essentially a modern case study on Nversion programming. As part of our comparison, we report on the different design and implementation decisions taken by each team, and show their impact on a rigorously assembled and tested set of benchmarks, itself a contribution of the paper.",
    "URL": "http://ieeexplore.ieee.org/document/8115670/",
    "DOI": "10.1109/ASE.2017.8115670",
    "publisher-place": "Urbana, IL",
    "page": "601-612",
    "page-first": "601",
    "language": "en-US",
    "_line": "LanguageTools.bib:636"
  },
  "korencik_decompiling_2019": {
    "id": "korencik_decompiling_2019",
    "type": "thesis",
    "genre": "Diploma",
    "author": [
      {
        "family": "Korencik",
        "given": "Lukáš"
      },
      {
        "family": "University",
        "given": "Masaryk"
      }
    ],
    "title": "Decompiling Binaries into LLVM IR Using McSema and Dyninst",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "publisher": "Masaryk University",
    "number-of-pages": "78",
    "abstract": "This thesis provides alternative implementation to the proprietary component of McSema which uses open-source Dyninst disassembler. With\nthis new implementation McSema can be used without proprietary software. The performance of the open source version is demonstrated on\na set of programs and the results are compared with already existing\ncomponents.",
    "URL": "https://is.muni.cz/th/pxe1j/thesis.pdf",
    "language": "en-US",
    "_line": "LanguageTools.bib:655"
  },
  "sui_detecting_2014": {
    "id": "sui_detecting_2014",
    "type": "article-journal",
    "author": [
      {
        "family": "Sui",
        "given": "Yulei"
      },
      {
        "family": "Ye",
        "given": "Ding"
      },
      {
        "family": "Xue",
        "given": "Jingling"
      }
    ],
    "title": "Detecting Memory Leaks Statically with Full-Sparse Value-Flow Analysis",
    "container-title": "IEEE Transactions on Software Engineering",
    "container-title-short": "IIEEE Trans. Software Eng.",
    "issued": {
      "date-parts": [
        [
          "2014",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "4"
        ]
      ]
    },
    "issn": "0098-5589, 1939-3520",
    "abstract": "We introduce a static detector, SABER, for detecting memory leaks in C programs. Leveraging recent advances on sparse pointer analysis, SABER is the ﬁrst to use a full-sparse value-ﬂow analysis for detecting memory leaks statically. SABER tracks the ﬂow of values from allocation to free sites using a sparse value-ﬂow graph (SVFG) that captures def-use chains and value ﬂows via assignments for all memory locations represented by both top-level and address-taken pointers. By exploiting ﬁeld-, ﬂow- and contextsensitivity during different phases of the analysis, SABER detects memory leaks in a program by solving a graph reachability problem on its SVFG. SABER, which is fully implemented in Open64, is effective at detecting 254 leaks in the 15 SPEC2000 C programs and seven applications, while keeping the false positive rate at 18.3 percent. SABER compares favorably with several static leak detectors in terms of accuracy (leaks and false alarms reported) and scalability (LOC analyzed per second). In particular, compared with FASTCHECK (which analyzes allocated objects ﬂowing only into top-level pointers) using the 15 SPEC2000 C programs, SABER detects 44.1 percent more leaks at a slightly higher false positive rate but is only a few times slower.",
    "URL": "http://ieeexplore.ieee.org/document/6720116/",
    "DOI": "10.1109/TSE.2014.2302311",
    "page": "107-122",
    "page-first": "107",
    "volume": "40",
    "issue": "2",
    "language": "en-US",
    "_line": "LanguageTools.bib:671"
  },
  "sui_svf_2016": {
    "id": "sui_svf_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Sui",
        "given": "Yulei"
      },
      {
        "family": "Xue",
        "given": "Jingling"
      }
    ],
    "title": "SVF: interprocedural static value-flow analysis in LLVM",
    "container-title": "Proceedings of the 25th International Conference on Compiler Construction",
    "container-title-short": "SVF",
    "collection-title": "CC 2016",
    "title-short": "SVF",
    "issued": {
      "date-parts": [
        [
          "2016",
          "3",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "4"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-4241-4",
    "abstract": "This paper presents SVF, a tool that enables scalable and precise interprocedural Static Value-Flow analysis for C programs by leveraging recent advances in sparse analysis. SVF, which is fully implemented in LLVM, allows value-flow construction and pointer analysis to be performed in an iterative manner, thereby providing increasingly improved precision for both. SVF accepts points- to information generated by any pointer analysis (e.g., Andersen’s analysis) and constructs an interprocedural memory SSA form, in which the def-use chains of both top-level and address-taken variables are captured. Such value-flows can be subsequently exploited to support various forms of program analysis or enable more precise pointer analysis (e.g., flow-sensitive analysis) to be performed sparsely. By dividing a pointer analysis into three loosely coupled components: Graph, Rules and Solver, SVF provides an extensible interface for users to write their own solutions easily. SVF is publicly available at http://unsw-corg.github.io/SVF.",
    "keywords": "Pointer Analysis, SVF, Value-Flow",
    "URL": "https://doi.org/10.1145/2892208.2892235",
    "DOI": "10.1145/2892208.2892235",
    "publisher-place": "New York, NY, USA",
    "page": "265-266",
    "page-first": "265",
    "_line": "LanguageTools.bib:689"
  },
  "curry_bi-abduction_2020": {
    "id": "curry_bi-abduction_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Curry",
        "given": "Christopher"
      },
      {
        "family": "Le",
        "given": "Quang Loc"
      }
    ],
    "title": "Bi-Abduction for Shapes with Ordered Data",
    "container-title": "arXiv:2006.10439 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2020",
          "6",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "26"
        ]
      ]
    },
    "abstract": "Shape analysis is of great importance for the verification of the correctness and memory-safety of heap-manipulating programs, yet such analyses have been shown to be highly difficult problems. The integration of separation logic into shape analyses has improved the effectiveness of the techniques, but the most significant advancement in this area is bi-abductive inference. Enabled by separation logic, bi-abduction - a combination of abductive inference and frame inference - is the key enabler for compositional reasoning, helping to scale up verification significantly. Indeed, the success of bi-abduction has led to the development of Infer, the tool used daily to verify Facebook's codebase of millions of lines of code. However, this success currently stays largely within the shape domain. To extend this impact towards the combination of shape and arithmetic domains, in this work, we present a novel one-stage bi-abductive procedure for a combination of data structures and ordering values. The procedure is designed in the spirit of the Unfold-and-Match paradigm where the inference is utilized to derive any mismatched portion. We have also implemented a prototype solver, based on the Cyclist library, and demonstrate its capabilities over a range of benchmarks from the SL-COMP competition. The experimental results show that our proposal shows promise for the specification inference in an automated verification of heap-manipulating programs.",
    "keywords": "Computer Science - Logic in Computer Science",
    "URLtext": "2006.10439",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2006.10439",
    "URL": "http://arxiv.org/abs/2006.10439",
    "_line": "LanguageTools.bib:708"
  },
  "hutchison_bi-abduction_2013": {
    "id": "hutchison_bi-abduction_2013",
    "type": "chapter",
    "author": [
      {
        "family": "Trinh",
        "given": "Minh-Thai"
      },
      {
        "family": "Le",
        "given": "Quang Loc"
      },
      {
        "family": "David",
        "given": "Cristina"
      },
      {
        "family": "Chin",
        "given": "Wei-Ngan"
      }
    ],
    "editor": [
      {
        "family": "Shan",
        "given": "Chung-chieh"
      }
    ],
    "title": "Bi-Abduction with Pure Properties for Specification Inference",
    "container-title": "Programming Languages and Systems",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "26"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-319-03541-3 978-3-319-03542-0",
    "abstract": "Separation logic is a state-of-the-art logic for dealing with the heap. Using its frame rule, initial works have strived towards automated modular veriﬁcation for heap-manipulating programs against user-supplied speciﬁcations. Since manually writing speciﬁcations is a tedious and error-prone engineering process, the so-called bi-abduction (a combination of the frame rule and abductive inference) is proposed to automatically infer pre/post speciﬁcations on data structure shapes. However, it has omitted the inference of pure properties of data structures such as their size, sum, height, content and min/max, which are needed to express a higher level of program correctness. In this paper, we propose a novel approach, called pure bi-abduction, for inferring pure information for pre/post speciﬁcations, using the result from a prior shape analysis step. Additionally, we design a predicate extension mechanism to systematically extend shape predicates with pure properties. We have implemented our inference mechanism and evaluated its utility on a benchmark of programs. We show that pure properties are prerequisite to allow the correctness of about 20&perc; of analyzed procedures to be captured and veriﬁed.",
    "URL": "http://link.springer.com/10.1007/978-3-319-03542-0_8",
    "DOI": "10.1007/978-3-319-03542-0_8",
    "publisher-place": "Cham",
    "page": "107-123",
    "page-first": "107",
    "volume": "8301",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "LanguageTools.bib:722"
  },
  "brotherston_biabduction_2016": {
    "id": "brotherston_biabduction_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Brotherston",
        "given": "James"
      },
      {
        "family": "Gorogiannis",
        "given": "Nikos"
      },
      {
        "family": "Kanovich",
        "given": "Max"
      }
    ],
    "title": "Biabduction (and Related Problems) in Array Separation Logic",
    "container-title": "arXiv:1607.01993 \\[cs, math\\]",
    "issued": {
      "date-parts": [
        [
          "2016",
          "11",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "26"
        ]
      ]
    },
    "abstract": "We investigate array separation logic (ASL), a variant of symbolic-heap separation logic in which the data structures are either pointers or arrays, i.e., contiguous blocks of allocated memory. This logic provides a language for compositional memory safety proofs of imperative array programs. We focus on the biabduction problem for this logic, which has been established as the key to automatic specification inference at the industrial scale. We present an NP decision procedure for biabduction in ASL that produces solutions of reasonable quality, and we also show that the problem of finding a consistent solution is NP-hard. Along the way, we study satisfiability and entailment in our logic, giving decision procedures and complexity bounds for both problems. We show satisfiability to be NP-complete, and entailment to be decidable with high complexity. The somewhat surprising fact that biabduction is much simpler than entailment is explained by the fact that, as we show, the element of choice over biabduction solutions enables us to dramatically reduce the search space.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science, D.2.4, F.3.1, Mathematics - Logic, Computer Science - Data Structures and Algorithms, F.2",
    "URLtext": "1607.01993",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1607.01993",
    "URL": "http://arxiv.org/abs/1607.01993",
    "_line": "LanguageTools.bib:762"
  },
  "liu_kubo_nodate": {
    "id": "liu_kubo_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Liu",
        "given": "Changming"
      },
      {
        "family": "Chen",
        "given": "Yaohui"
      },
      {
        "family": "Lu",
        "given": "Long"
      }
    ],
    "title": "KUBO: Precise and Scalable Detection of User-triggerable Undeﬁned Behavior Bugs in OS Kernel",
    "abstract": "Undeﬁned Behavior bugs (UB) often refer to a wide range of programming errors that mainly reside in software implemented in relatively low-level programming languages e.g., C/C++. OS kernels are particularly plagued by UB due to their close interactions with the hardware. A triggered UB can often lead to exploitation from unprivileged userspace programs and cause critical security and reliability issues inside the OS. The previous works on detecting UB in kernels had to sacriﬁce precision for scalability, and in turn, suffered from extremely high false positives which severely impaired their usability.",
    "page": "15",
    "page-first": "15",
    "language": "en-US",
    "_line": "LanguageTools.bib:776"
  },
  "lattner_mlir_2020": {
    "id": "lattner_mlir_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Lattner",
        "given": "Chris"
      },
      {
        "family": "Amini",
        "given": "Mehdi"
      },
      {
        "family": "Bondhugula",
        "given": "Uday"
      },
      {
        "family": "Cohen",
        "given": "Albert"
      },
      {
        "family": "Davis",
        "given": "Andy"
      },
      {
        "family": "Pienaar",
        "given": "Jacques"
      },
      {
        "family": "Riddle",
        "given": "River"
      },
      {
        "family": "Shpeisman",
        "given": "Tatiana"
      },
      {
        "family": "Vasilache",
        "given": "Nicolas"
      },
      {
        "family": "Zinenko",
        "given": "Oleksandr"
      }
    ],
    "title": "MLIR: A Compiler Infrastructure for the End of Moore's Law",
    "container-title": "arXiv:2002.11054 \\[cs\\]",
    "container-title-short": "MLIR",
    "title-short": "MLIR",
    "issued": {
      "date-parts": [
        [
          "2020",
          "2",
          "29"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "25"
        ]
      ]
    },
    "abstract": "This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and also across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, and identifying the challenges and opportunities posed by this novel design point in design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Machine Learning",
    "URLtext": "2002.11054",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2002.11054",
    "URL": "http://arxiv.org/abs/2002.11054",
    "_line": "LanguageTools.bib:785"
  },
  "arusoaie_proof-carrying_2021": {
    "id": "arusoaie_proof-carrying_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Arusoaie",
        "given": "Andrei"
      },
      {
        "family": "Lucanu",
        "given": "Dorel"
      }
    ],
    "title": "Proof-Carrying Parameters in Certified Symbolic Execution: The Case Study of Antiunification",
    "container-title": "arXiv:2110.11700 \\[cs\\]",
    "container-title-short": "Proof-Carrying Parameters in Certified Symbolic Execution",
    "title-short": "Proof-Carrying Parameters in Certified Symbolic Execution",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "1"
        ]
      ]
    },
    "abstract": "Unification and antiunification are essential algorithms used by symbolic execution engines and verification tools. Complex frameworks for defining programming languages, such as K, aim to generate various tools (e.g., interpreters, symbolic execution engines, deductive verifiers, etc.) using only the formal definition of a language. K is the best effort implementation of Matching Logic, a logical framework for defining languages. When used at an industrial scale, a tool like the K framework is constantly updated, and in the same time it is required to be trustworthy. Ensuring the correctness of such a framework is practically impossible. A solution is to generate proof objects as correctness certificates that can be checked by an external trusted checker. In K, symbolic execution makes intensive use of unification and antiunification algorithms to handle conjunctions and disjunctions of term patterns. Conjunctions and disjunctions of formulae have to be automatically normalised and the generation of proof objects needs to take such normalisations into account. The executions of these algorithms can be seen as parameters of the symbolic execution steps and they have to provide proof objects that are used then to generate the proof object for the program execution step. We show in this paper that Plotkin's antiunification can be used to normalise disjunctions and to generate the corresponding proof objects. We provide a prototype implementation of our proof object generation technique and a checker for certifying the generated objects.",
    "keywords": "Computer Science - Logic in Computer Science",
    "URLtext": "2110.11700",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2110.11700",
    "URL": "http://arxiv.org/abs/2110.11700",
    "_line": "LanguageTools.bib:800"
  },
  "bae_rudra_2021": {
    "id": "bae_rudra_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Bae",
        "given": "Yechan"
      },
      {
        "family": "Kim",
        "given": "Youngsuk"
      },
      {
        "family": "Askar",
        "given": "Ammar"
      },
      {
        "family": "Lim",
        "given": "Jungwon"
      },
      {
        "family": "Kim",
        "given": "Taesoo"
      }
    ],
    "title": "Rudra: Finding Memory Safety Bugs in Rust at the Ecosystem Scale",
    "container-title": "Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
    "container-title-short": "Rudra",
    "collection-title": "SOSP '21",
    "title-short": "Rudra",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "26"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "1"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8709-5",
    "abstract": "Rust is a promising system programming language that guarantees memory safety at compile time. To support diverse requirements for system software such as accessing low-level hardware, Rust allows programmers to perform operations that are not protected by the Rust compiler with the unsafe keyword. However, Rust's safety guarantee relies on the soundness of all unsafe code in the program as well as the standard and external libraries, making it hard to reason about their correctness. In other words, a single bug in any unsafe code breaks the whole program's safety guarantee. In this paper, we introduce RUDRA, a program that analyzes and reports potential memory safety bugs in unsafe Rust. Since a bug in unsafe code threatens the foundation of Rust's safety guarantee, our primary focus is to scale our analysis to all the packages hosted in the Rust package registry. RUDRA can scan the entire registry (43k packages) in 6.5 hours and identified 264 previously unknown memory safety bugs&mdash;leading to 76 CVEs and 112 RustSec advisories being filed, which represent 51.6&perc; of memory safety bugs reported to RustSec since 2016. The new bugs RUDRA found are non-trivial, subtle, and often made by Rust experts: two in the Rust standard library, one in the official futures library, and one in the Rust compiler. RUDRA is open-source, and part of its algorithm is integrated into the official Rust linter.",
    "keywords": "Rust, Memory-safety, Program analysis",
    "URL": "https://doi.org/10.1145/3477132.3483570",
    "DOI": "10.1145/3477132.3483570",
    "publisher-place": "New York, NY, USA",
    "page": "84-99",
    "page-first": "84",
    "_line": "LanguageTools.bib:815"
  },
  "emre_translating_2021": {
    "id": "emre_translating_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Emre",
        "given": "Mehmet"
      },
      {
        "family": "Schroeder",
        "given": "Ryan"
      },
      {
        "family": "Dewey",
        "given": "Kyle"
      },
      {
        "family": "Hardekopf",
        "given": "Ben"
      }
    ],
    "title": "Translating C to safer Rust",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "26"
        ]
      ]
    },
    "abstract": "Rust is a relatively new programming language that targets efficient and safe systems-level applications. It includes a sophisticated type system that allows for provable memory- and thread-safety, and is explicitly designed to take the place of unsafe languages such as C and C++ in the coding ecosystem. There is a large existing C and C++ codebase (many of which have been affected by bugs and security vulnerabilities due to unsafety) that would benefit from being rewritten in Rust to remove an entire class of potential bugs. However, porting these applications to Rust manually is a daunting task. In this paper we investigate the problem of automatically translating C programs into safer Rust programs&ndash;that is, Rust programs that improve on the safety guarantees of the original C programs. We conduct an in-depth study into the underlying causes of unsafety in translated programs and the relative impact of fixing each cause. We also describe a novel technique for automatically removing a particular cause of unsafety and evaluate its effectiveness and impact. This paper presents the first empirical study of unsafety in translated Rust programs (as opposed to programs originally written in Rust) and also the first technique for automatically removing causes of unsafety in translated Rust programs.",
    "keywords": "C, Rust, Automatic Translation, Empirical Study, Memory-Safety",
    "URL": "https://doi.org/10.1145/3485498",
    "DOI": "10.1145/3485498",
    "page": "121:1-121:29",
    "page-first": "121",
    "volume": "5",
    "_line": "LanguageTools.bib:834"
  },
  "shen_toward_2021": {
    "id": "shen_toward_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Shen",
        "given": "Gan"
      },
      {
        "family": "Kuper",
        "given": "Lindsey"
      }
    ],
    "title": "Toward SMT-Based Refinement Types in Agda",
    "container-title": "arXiv:2110.05771 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "18"
        ]
      ]
    },
    "abstract": "Dependent types offer great versatility and power, but developing proofs with them can be tedious and requires considerable human guidance. We propose to integrate Satisfiability Modulo Theories (SMT)-based refinement types into the dependently-typed language Agda in an effort to ease some of the burden of programming with dependent types and combine the strengths of the two approaches to mechanized theorem proving.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2110.05771",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2110.05771",
    "URL": "http://arxiv.org/abs/2110.05771",
    "_line": "LanguageTools.bib:851"
  },
  "redmond_toward_2021": {
    "id": "redmond_toward_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Redmond",
        "given": "Patrick"
      },
      {
        "family": "Shen",
        "given": "Gan"
      },
      {
        "family": "Kuper",
        "given": "Lindsey"
      }
    ],
    "title": "Toward Hole-Driven Development with Liquid Haskell",
    "container-title": "arXiv:2110.04461 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "18"
        ]
      ]
    },
    "abstract": "Liquid Haskell is an extension to the Haskell programming language that adds support for refinement types: data types augmented with SMT-decidable logical predicates that refine the set of values that can inhabit a type. Furthermore, Liquid Haskell's support for refinement reflection enables the use of Haskell for general-purpose mechanized theorem proving. A growing list of large-scale mechanized proof developments in Liquid Haskell take advantage of this capability. Adding theorem-proving capabilities to a \"legacy\" language like Haskell lets programmers directly verify properties of real-world Haskell programs (taking advantage of the existing highly tuned compiler, run-time system, and libraries), just by writing Haskell. However, more established proof assistants like Agda and Coq offer far better support for interactive proof development and insight into the proof state (for instance, what subgoals still need to be proved to finish a partially-complete proof). In contrast, Liquid Haskell provides only coarse-grained feedback to the user &ndash; either it reports a type error, or not &ndash; unfortunately hindering its usability as a theorem prover. In this paper, we propose improving the usability of Liquid Haskell by extending it with support for Agda-style typed holes and interactive editing commands that take advantage of them. In Agda, typed holes allow programmers to indicate unfinished parts of a proof, and incrementally complete the proof in a dialogue with the compiler. While GHC Haskell already has its own Agda-inspired support for typed holes, we posit that typed holes would be especially powerful and useful if combined with Liquid Haskell's refinement types and SMT automation. We discuss how typed holes might work in Liquid Haskell, and we consider possible implementation approaches and next steps.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2110.04461",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2110.04461",
    "URL": "http://arxiv.org/abs/2110.04461",
    "_line": "LanguageTools.bib:865"
  },
  "griesemer_featherweight_2020": {
    "id": "griesemer_featherweight_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Griesemer",
        "given": "Robert"
      },
      {
        "family": "Hu",
        "given": "Raymond"
      },
      {
        "family": "Kokke",
        "given": "Wen"
      },
      {
        "family": "Lange",
        "given": "Julien"
      },
      {
        "family": "Taylor",
        "given": "Ian Lance"
      },
      {
        "family": "Toninho",
        "given": "Bernardo"
      },
      {
        "family": "Wadler",
        "given": "Philip"
      },
      {
        "family": "Yoshida",
        "given": "Nobuko"
      }
    ],
    "title": "Featherweight go",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2020",
          "11",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "18"
        ]
      ]
    },
    "issn": "2475-1421",
    "abstract": "ROBERT GRIESEMER, Google, USA RAYMOND HU, University of Hertfordshire, UK WEN KOKKE, University of Edinburgh, UK JULIEN LANGE, Royal Holloway, University of London, UK IAN LANCE TAYLOR, Google, USA BERNARDO TONINHO, NOVA-LINCS, FCT-NOVA, Universidade Nova de Lisboa, Portugal PHILIP WADLER, University of Edinburgh, UK NOBUKO YOSHIDA, Imperial College London, UK We describe a design for generics in Go inspired by previous work on Featherweight Java by Igarashi, Pierce, and Wadler. Whereas subtyping in Java is nominal, in Go it is structural, and whereas generics in Java are defined via erasure, in Go we use monomorphisation. Although monomorphisation is widely used, we are one of the first to formalise it. Our design also supports a solution to The Expression Problem. CCS Concepts: • Theory of computation → Program semantics; Type structures; • Software and its engineering → Polymorphism.",
    "URL": "https://dl.acm.org/doi/10.1145/3428217",
    "DOI": "10.1145/3428217",
    "page": "1-29",
    "page-first": "1",
    "volume": "4",
    "language": "en-US",
    "_line": "LanguageTools.bib:879"
  },
  "yodaiken_how_2021": {
    "id": "yodaiken_how_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Yodaiken",
        "given": "Victor"
      }
    ],
    "title": "How ISO C became unusable for operating systems development",
    "container-title": "Proceedings of the 11th Workshop on Programming Languages and Operating Systems",
    "event-title": "SOSP '21: ACM SIGOPS 28th Symposium on Operating Systems Principles",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "25"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "18"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-8707-1",
    "abstract": "The C programming language was developed in the 1970s as a fairly unconventional systems and operating systems development tool, but has, through the course of the ISO Standards process, added many attributes of more conventional programming languages and become less suitable for operating systems development. Operating system programming continues to be done in non-ISO dialects of C. The differences provide a glimpse of operating system requirements for programming languages.",
    "URL": "https://dl.acm.org/doi/10.1145/3477113.3487274",
    "DOI": "10.1145/3477113.3487274",
    "publisher-place": "Virtual Event Germany",
    "page": "84-90",
    "page-first": "84",
    "language": "en-US",
    "_line": "LanguageTools.bib:897"
  },
  "barbar_compacting_nodate": {
    "id": "barbar_compacting_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Barbar",
        "given": "Mohamad"
      },
      {
        "family": "Sui",
        "given": "Yulei"
      }
    ],
    "title": "Compacting Points-To Sets through Object Clustering",
    "page": "27",
    "page-first": "27",
    "volume": "5",
    "language": "en-US",
    "_line": "LanguageTools.bib:915"
  },
  "alomari_clone_2021": {
    "id": "alomari_clone_2021",
    "type": "book",
    "author": [
      {
        "family": "Alomari",
        "given": "Hakam"
      },
      {
        "family": "Stephan",
        "given": "Matthew"
      }
    ],
    "title": "Clone Detection through srcClone: A Program Slicing Based Approach",
    "container-title-short": "Clone Detection through srcClone",
    "title-short": "Clone Detection through srcClone",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "28"
        ]
      ]
    },
    "abstract": "Software clone detection is an often used approach to understand and maintain software systems. One category of clones that is challenging to detect but very useful is semantic clones, which are similar in semantics but differ in syntax significantly. Semantic clone detectors have trouble scaling to larger systems and sometimes struggle with recall and precision. To address this, we developed a slice-based scalable approach that detects both syntactic and semantic code clones, srcClone. srcClone ascertains code segment similarity by assessing the similarity of their corresponding program slices. We employ a lightweight, publicly-available, scalable program slicer within our clone detection approach. Using dependency analysis to detect and assess cloned components, we discover insights about code components that can be affected by a clone pair or set. These elements are critical in impact analysis. It can also be used by program analysts to run on non-compilable and incomplete source code, which serves comprehension and maintenance tasks very well. We first evaluate srcClone by comparing it to six state-of-the-art tools and two additional semantic clone detectors in performance, recall, and precision. We use the BigCloneBench real clones benchmark to facilitate comparison. We use an additional 16 established benchmark scenarios to perform a qualitative comparison between srcClone and 44 clone detection approaches in their capabilities to detect these scenarios. To further measure scalability, we evaluate srcClone on 191 versions of Linux kernel, containing approximately 87 MLOC. In our evaluations, we illustrate our approach is both relatively scalable and accurate. While its precision is slightly less than some other techniques, it makes up for it in higher recall including semantic clones unable to be found by any existing techniques. We contend our approach is an important advancement in software cloning that it is both demonstrably scalable and can detect more types of clones than existing work, thus providing developers greater information into their software.",
    "_line": "LanguageTools.bib:924"
  },
  "gong_snowboard_nodate": {
    "id": "gong_snowboard_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Gong",
        "given": "Sishuai"
      },
      {
        "family": "Altınbüken",
        "given": "Deniz"
      },
      {
        "family": "Fonseca",
        "given": "Pedro"
      },
      {
        "family": "Maniatis",
        "given": "Petros"
      }
    ],
    "title": "Snowboard: Finding Kernel Concurrency Bugs through Systematic Inter-thread Communication Analysis",
    "abstract": "Kernel concurrency bugs are challenging to find because they depend on very specific thread interleavings and test inputs. While separately exploring kernel thread interleavings or test inputs has been closely examined, jointly exploring interleavings and test inputs has received little attention, in part due to the resulting vast search space. Using precious, limited testing resources to explore this search space and execute just the right concurrent tests in the proper order is critical.",
    "page": "18",
    "page-first": "18",
    "language": "en-US",
    "_line": "LanguageTools.bib:932"
  },
  "ma_detecting_nodate": {
    "id": "ma_detecting_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Ma",
        "given": "Xutong"
      },
      {
        "family": "Yan",
        "given": "Jiwei"
      },
      {
        "family": "Wang",
        "given": "Wei"
      },
      {
        "family": "Yan",
        "given": "Jun"
      },
      {
        "family": "Zhang",
        "given": "Jian"
      },
      {
        "family": "Qiu",
        "given": "Zongyan"
      }
    ],
    "title": "Detecting Memory-Related Bugs by Tracking Heap Memory Management of C++ Smart Pointers",
    "abstract": "The smart pointer mechanism, which is improved in the continuous versions of the C++ standards over the last decade, is designed to prevent memory-leak bugs by automatically deallocating the managed memory blocks. However, not all kinds of memory errors can be immunized by adopting this mechanism. For example, dereferencing a null smart pointer will lead to a software failure. Due to the lack of specialized support for smart pointers, the off-the-shelf C++ static analyzers cannot effectively reveal these bugs.",
    "page": "12",
    "page-first": "12",
    "language": "en-US",
    "_line": "LanguageTools.bib:941"
  },
  "ocallahan_engineering_2017": {
    "id": "ocallahan_engineering_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "O'Callahan",
        "given": "Robert"
      },
      {
        "family": "Jones",
        "given": "Chris"
      },
      {
        "family": "Froyd",
        "given": "Nathan"
      },
      {
        "family": "Huey",
        "given": "Kyle"
      },
      {
        "family": "Noll",
        "given": "Albert"
      },
      {
        "family": "Partush",
        "given": "Nimrod"
      }
    ],
    "title": "Engineering Record And Replay For Deployability: Extended Technical Report",
    "container-title": "arXiv:1705.05937 \\[cs\\]",
    "container-title-short": "Engineering Record And Replay For Deployability",
    "title-short": "Engineering Record And Replay For Deployability",
    "issued": {
      "date-parts": [
        [
          "2017",
          "5",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "6"
        ]
      ]
    },
    "abstract": "The ability to record and replay program executions with low overhead enables many applications, such as reverse-execution debugging, debugging of hard-to-reproduce test failures, and \"black box\" forensic analysis of failures in deployed systems. Existing record-and-replay approaches limit deployability by recording an entire virtual machine (heavyweight), modifying the OS kernel (adding deployment and maintenance costs), requiring pervasive code instrumentation (imposing significant performance and complexity overhead), or modifying compilers and runtime systems (limiting generality). We investigated whether it is possible to build a practical record-and-replay system avoiding all these issues. The answer turns out to be yes - if the CPU and operating system meet certain non-obvious constraints. Fortunately modern Intel CPUs, Linux kernels and user-space frameworks do meet these constraints, although this has only become true recently. With some novel optimizations, our system 'rr' records and replays real-world low-parallelism workloads with low overhead, with an entirely user-space implementation, using stock hardware, compilers, runtimes and operating systems. \"rr\" forms the basis of an open-source reverse-execution debugger seeing significant use in practice. We present the design and implementation of 'rr', describe its performance on a variety of workloads, and identify constraints on hardware and operating system design required to support our approach.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "1705.05937",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1705.05937",
    "URL": "http://arxiv.org/abs/1705.05937",
    "_line": "LanguageTools.bib:950"
  },
  "wickerson_pearl_2020": {
    "id": "wickerson_pearl_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Wickerson",
        "given": "John"
      },
      {
        "family": "Brunet",
        "given": "Paul"
      }
    ],
    "title": "Pearl: Diagrams for Composing Compilers",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "abstract": "T-diagrams (or ‘tombstone diagrams’) are widely used in teaching for explaining how compilers and interpreters can be composed together to build and execute software. In this Pearl, we revisit these diagrams, and show how they can be redesigned for better readability. We demonstrate how they can be applied to explain compiler concepts including bootstrapping and cross-compilation. We provide a formal semantics for our redesigned diagrams, based on binary trees. Finally, we suggest how our diagrams could be used to analyse the performance of a compilation system.",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "LanguageTools.bib:965"
  },
  "honore_much_nodate": {
    "id": "honore_much_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Honoré",
        "given": "Wolf"
      },
      {
        "family": "Kim",
        "given": "Jieung"
      },
      {
        "family": "Shin",
        "given": "Ji-Yong"
      },
      {
        "family": "Shao",
        "given": "Zhong"
      }
    ],
    "title": "Much ADO about Failures: A Fault-Aware Model for Compositional Verification of Strongly Consistent Distributed Systems",
    "abstract": "Despite recent advances, guaranteeing the correctness of large-scale distributed applications without compromising performance remains a challenging problem. Network and node failures are inevitable and, for some applications, careful control over how they are handled is essential. Unfortunately, existing approaches either completely hide these failures behind an atomic state machine replication (SMR) interface, or expose all of the network-level details, sacrificing atomicity. We propose a novel, compositional, atomic distributed object (ADO) model for strongly consistent distributed systems that combines the best of both options. The object-oriented API abstracts over protocol-specific details and decouples high-level correctness reasoning from implementation choices. At the same time, it intentionally exposes an abstract view of certain key distributed failure cases, thus allowing for more fine-grained control over them than SMR-like models. We demonstrate that proving properties even of composite distributed systems can be straightforward with our Coq verification framework, Advert, thanks to the ADO model. We also show that a variety of common protocols including multi-Paxos and Chain Replication refine the ADO semantics, which allows one to freely choose among them for an application’s implementation without modifying ADO-level correctness proofs. CCS Concepts: • Software and its engineering → Software verification; Distributed programming languages; • Theory of computation → Distributed computing models; Object oriented constructs.",
    "page": "42",
    "page-first": "42",
    "volume": "5",
    "language": "en-US",
    "_line": "LanguageTools.bib:975"
  },
  "yao_efficient_2021": {
    "id": "yao_efficient_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Yao",
        "given": "Peisen"
      },
      {
        "family": "Zhou",
        "given": "Jinguo"
      },
      {
        "family": "Xiao",
        "given": "Xiao"
      },
      {
        "family": "Shi",
        "given": "Qingkai"
      },
      {
        "family": "Wu",
        "given": "Rongxin"
      },
      {
        "family": "Zhang",
        "given": "Charles"
      }
    ],
    "title": "Efficient Path-Sensitive Data-Dependence Analysis",
    "container-title": "arXiv:2109.07923 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "24"
        ]
      ]
    },
    "abstract": "This paper presents a scalable path- and context-sensitive data-dependence analysis. The key is to address the aliasing-path-explosion problem via a sparse, demand-driven, and fused approach that piggybacks the computation of pointer information with the resolution of data dependence. Specifically, our approach decomposes the computational efforts of disjunctive reasoning into 1) a context- and semi-path-sensitive analysis that concisely summarizes data dependence as the symbolic and storeless value-flow graphs, and 2) a demand-driven phase that resolves transitive data dependence over the graphs. We have applied the approach to two clients, namely thin slicing and value flow analysis. Using a suite of 16 programs ranging from 13 KLoC to 8 MLoC, we compare our techniques against a diverse group of state-of-the-art analyses, illustrating significant precision and scalability advantages of our approach.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Software Engineering",
    "URLtext": "2109.07923",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2109.07923",
    "URL": "http://arxiv.org/abs/2109.07923",
    "_line": "LanguageTools.bib:985"
  },
  "hutchison_practical_2010": {
    "id": "hutchison_practical_2010",
    "type": "chapter",
    "author": [
      {
        "family": "Naeem",
        "given": "Nomair A."
      },
      {
        "family": "Lhoták",
        "given": "Ondřej"
      },
      {
        "family": "Rodriguez",
        "given": "Jonathan"
      }
    ],
    "editor": [
      {
        "family": "Gupta",
        "given": "Rajiv"
      }
    ],
    "title": "Practical Extensions to the IFDS Algorithm",
    "container-title": "Compiler Construction",
    "issued": {
      "date-parts": [
        [
          "2010"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "23"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-11969-9 978-3-642-11970-5",
    "abstract": "This paper presents four extensions to the Interprocedural Finite Distributive Subset (IFDS) algorithm that make it applicable to a wider class of analysis problems. IFDS is a dynamic programming algorithm that implements context-sensitive ﬂow-sensitive interprocedural dataﬂow analysis. The ﬁrst extension constructs the nodes of the supergraph on demand as the analysis requires them, eliminating the need to build a full supergraph before the analysis. The second extension provides the procedure-return ﬂow function with additional information about the program state before the procedure was called. The third extension improves the precision with which φ instructions are modelled when analyzing a program in SSA form. The fourth extension speeds up the algorithm on domains in which some of the dataﬂow facts subsume each other. These extensions are often necessary when applying the IFDS algorithm to non-separable (i.e. non-bit-vector) problems. We have found them necessary for alias set analysis and multi-object typestate analysis. In this paper, we illustrate and evaluate the extensions on a simpler problem, a variation of variable type analysis.",
    "URL": "http://link.springer.com/10.1007/978-3-642-11970-5_8",
    "DOI": "10.1007/978-3-642-11970-5_8",
    "publisher-place": "Berlin, Heidelberg",
    "page": "124-144",
    "page-first": "124",
    "volume": "6011",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "LanguageTools.bib:999"
  },
  "kruger_cognicrypt_2017": {
    "id": "kruger_cognicrypt_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Kruger",
        "given": "Stefan"
      },
      {
        "family": "Nadi",
        "given": "Sarah"
      },
      {
        "family": "Reif",
        "given": "Michael"
      },
      {
        "family": "Ali",
        "given": "Karim"
      },
      {
        "family": "Mezini",
        "given": "Mira"
      },
      {
        "family": "Bodden",
        "given": "Eric"
      },
      {
        "family": "Gopfert",
        "given": "Florian"
      },
      {
        "family": "Gunther",
        "given": "Felix"
      },
      {
        "family": "Weinert",
        "given": "Christian"
      },
      {
        "family": "Demmler",
        "given": "Daniel"
      },
      {
        "family": "Kamath",
        "given": "Ram"
      }
    ],
    "title": "CogniCrypt: Supporting developers in using cryptography",
    "container-title": "2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)",
    "container-title-short": "CogniCrypt",
    "title-short": "CogniCrypt",
    "event-title": "2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)",
    "issued": {
      "date-parts": [
        [
          "2017",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "23"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-5386-2684-9",
    "abstract": "Previous research suggests that developers often struggle using low-level cryptographic APIs and, as a result, produce insecure code. When asked, developers desire, among other things, more tool support to help them use such APIs. In this paper, we present CogniCrypt, a tool that supports developers with the use of cryptographic APIs. CogniCrypt assists the developer in two ways. First, for a number of common cryptographic tasks, CogniCrypt generates code that implements the respective task in a secure manner. Currently, CogniCrypt supports tasks such as data encryption, communication over secure channels, and long-term archiving. Second, CogniCrypt continuously runs static analyses in the background to ensure a secure integration of the generated code into the developer’s workspace. This video demo showcases the main features of CogniCrypt: youtube.com/watch?v=JUq5mRHfAWY.",
    "URL": "http://ieeexplore.ieee.org/document/8115707/",
    "DOI": "10.1109/ASE.2017.8115707",
    "publisher-place": "Urbana, IL",
    "page": "931-936",
    "page-first": "931",
    "language": "en-US",
    "_line": "LanguageTools.bib:1021"
  },
  "bodden_secret_2018": {
    "id": "bodden_secret_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Bodden",
        "given": "Eric"
      }
    ],
    "title": "The secret sauce in efficient and precise static analysis: the beauty of distributive, summary-based static analyses (and how to master them)",
    "container-title": "Companion Proceedings for the ISSTA/ECOOP 2018 Workshops",
    "container-title-short": "The secret sauce in efficient and precise static analysis",
    "collection-title": "ISSTA '18",
    "title-short": "The secret sauce in efficient and precise static analysis",
    "issued": {
      "date-parts": [
        [
          "2018",
          "7",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "22"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-5939-9",
    "abstract": "In this paper I report on experiences gained from more than five years of extensively designing static code analysis tools- in particular such ones with a focus on security- to scale to real-world projects within an industrial context. Within this time frame, my team and I were able to design static-analysis algorithms that yield both largely improved precision and performance compared to previous approaches. I will give a number of insights regarding important design decisions that made this possible. In particular, I argue that summary-based static-analysis techniques for distributive problems, such as IFDS, IDE and WPDS have been unduly under-appreciated. As my experience shows, those techniques can tremendously benefit both precision and performance, if one uses them in a well-informed way, using carefully designed abstract domains. As one example, I will explain how in previous work on BOOMERANG we were able to decompose pointer analysis, a static analysis problem that is actually not distributive, into sub-problems that are distributive. This yields an implementation that is both highly precise and efficient. This breakthrough, along with the use of a demand-driven program-analysis design, has recently allowed us to implement practical static analysis tools such as the crypto-misuse checker CogniCrypt, which can analyze the entire Maven-Central repository with more than 200.000 binaries in under five days, although its analysis is flow-sensitive, field-sensitive, and fully context-sensitive.",
    "keywords": "static analysis, abstract domains, performance, precision, summarization",
    "URL": "https://doi.org/10.1145/3236454.3236500",
    "DOI": "10.1145/3236454.3236500",
    "publisher-place": "New York, NY, USA",
    "page": "85-93",
    "page-first": "85",
    "_line": "LanguageTools.bib:1040"
  },
  "bodden_inter-procedural_2012": {
    "id": "bodden_inter-procedural_2012",
    "type": "paper-conference",
    "author": [
      {
        "family": "Bodden",
        "given": "Eric"
      }
    ],
    "title": "Inter-procedural data-flow analysis with IFDS/IDE and Soot",
    "container-title": "Proceedings of the ACM SIGPLAN International Workshop on State of the Art in Java Program analysis - SOAP '12",
    "event-title": "the ACM SIGPLAN International Workshop",
    "issued": {
      "date-parts": [
        [
          "2012"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "23"
        ]
      ]
    },
    "publisher": "ACM Press",
    "isbn": "978-1-4503-1490-9",
    "abstract": "The IFDS and IDE frameworks by Reps, Horwitz and Sagiv are two general frameworks for the inter-procedural analysis of data-ﬂow problems with distributive ﬂow functions over ﬁnite domains. Many data-ﬂow problems do have distributive ﬂow functions and are thus expressible as IFDS or IDE problems, reaching from basic analyses like truly-live variables to complex analyses for problems from the current literature such as typestate and secure information-ﬂow.",
    "URL": "http://dl.acm.org/citation.cfm?doid=2259051.2259052",
    "DOI": "10.1145/2259051.2259052",
    "publisher-place": "Beijing, China",
    "page": "3-8",
    "page-first": "3",
    "language": "en-US",
    "_line": "LanguageTools.bib:1059"
  },
  "li_fast_2020": {
    "id": "li_fast_2020",
    "type": "paper-conference",
    "author": [
      {
        "family": "Li",
        "given": "Yuanbo"
      },
      {
        "family": "Zhang",
        "given": "Qirun"
      },
      {
        "family": "Reps",
        "given": "Thomas"
      }
    ],
    "title": "Fast graph simplification for interleaved Dyck-reachability",
    "container-title": "Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation",
    "collection-title": "PLDI 2020",
    "issued": {
      "date-parts": [
        [
          "2020",
          "6",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "22"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-7613-6",
    "abstract": "Many program-analysis problems can be formulated as graph-reachability problems. Interleaved Dyck language reachability. Interleaved Dyck language reachability (InterDyck-reachability) is a fundamental framework to express a wide variety of program-analysis problems over edge-labeled graphs. The InterDyck language represents an intersection of multiple matched-parenthesis languages (i.e., Dyck languages). In practice, program analyses typically leverage one Dyck language to achieve context-sensitivity, and other Dyck languages to model data dependences, such as field-sensitivity and pointer references/dereferences. In the ideal case, an InterDyck-reachability framework should model multiple Dyck languages simultaneously. Unfortunately, precise InterDyck-reachability is undecidable. Any practical solution must over-approximate the exact answer. In the literature, a lot of work has been proposed to over-approximate the InterDyck-reachability formulation. This paper offers a new perspective on improving both the precision and the scalability of InterDyck-reachability: we aim to simplify the underlying input graph G. Our key insight is based on the observation that if an edge is not contributing to any InterDyck-path, we can safely eliminate it from G. Our technique is orthogonal to the InterDyck-reachability formulation, and can serve as a pre-processing step with any over-approximating approaches for InterDyck-reachability. We have applied our graph simplification algorithm to pre-processing the graphs from a recent InterDyck-reachability-based taint analysis for Android. Our evaluation on three popular InterDyck-reachability algorithms yields promising results. In particular, our graph-simplification method improves both the scalability and precision of all three InterDyck-reachability algorithms, sometimes dramatically.",
    "keywords": "Static Analysis, CFL-Reachability",
    "URL": "https://doi.org/10.1145/3385412.3386021",
    "DOI": "10.1145/3385412.3386021",
    "publisher-place": "New York, NY, USA",
    "page": "780-793",
    "page-first": "780",
    "_line": "LanguageTools.bib:1077"
  },
  "spath_synchronized_2019": {
    "id": "spath_synchronized_2019",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Spath",
        "given": "Johannes"
      }
    ],
    "title": "Synchronized Pushdown Systems for Pointer and Data-Flow Analysis",
    "issued": {
      "date-parts": [
        [
          "2019",
          "3",
          "15"
        ]
      ]
    },
    "publisher": "Paderborn University",
    "number-of-pages": "152",
    "publisher-place": "Paderborn",
    "language": "en-US",
    "_line": "LanguageTools.bib:1095"
  },
  "spath_context-_2019": {
    "id": "spath_context-_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Späth",
        "given": "Johannes"
      },
      {
        "family": "Ali",
        "given": "Karim"
      },
      {
        "family": "Bodden",
        "given": "Eric"
      }
    ],
    "title": "Context-, flow-, and field-sensitive data-flow analysis using synchronized Pushdown systems",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "22"
        ]
      ]
    },
    "abstract": "Precise static analyses are context-, field- and flow-sensitive. Context- and field-sensitivity are both expressible as context-free language (CFL) reachability problems. Solving both CFL problems along the same data-flow path is undecidable, which is why most flow-sensitive data-flow analyses over-approximate field-sensitivity through k-limited access-path, or through access graphs. Unfortunately, as our experience and this paper show, both representations do not scale very well when used to analyze programs with recursive data structures. Any single CFL-reachability problem is efficiently solvable, by means of a pushdown system. This work thus introduces the concept of synchronized pushdown systems (SPDS). SPDS encode both procedure calls/returns and field stores/loads as separate but “synchronized” CFL reachability problems. An SPDS solves both individual problems precisely, and approximation occurs only in corner cases that are apparently rare in practice: at statements where both problems are satisfied but not along the same data-flow path. SPDS are also efficient: formal complexity analysis shows that SPDS shift the complexity from &bar;F&bar;3k under k-limiting to &bar;S&bar;&bar;F&bar;2, where F is the set of fields and S the set of statements involved in a data-flow. Our evaluation using DaCapo shows this shift to pay off in practice: SPDS are almost as efficient as k-limiting with k=1 although their precision equals k=∞. For a typestate analysis SPDS accelerate the analysis up to 83× for data-flows of objects that involve many field accesses but span rather few methods. We conclude that SPDS can provide high precision and further improve scalability, in particularly when used in analyses that expose rather local data flows.",
    "keywords": "static analysis, aliasing, access paths, data-flow, pushdown system",
    "URL": "https://doi.org/10.1145/3290361",
    "DOI": "10.1145/3290361",
    "page": "48:1-48:29",
    "page-first": "48",
    "volume": "3",
    "_line": "LanguageTools.bib:1107"
  },
  "spath_ideal_2017": {
    "id": "spath_ideal_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Späth",
        "given": "Johannes"
      },
      {
        "family": "Ali",
        "given": "Karim"
      },
      {
        "family": "Bodden",
        "given": "Eric"
      }
    ],
    "title": "IDEal: efficient and precise alias-aware dataflow analysis",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "IDE$^{\\textrm{\\textit{al}}}$",
    "title-short": "IDE$^{\\textrm{\\textit{al}}}$",
    "issued": {
      "date-parts": [
        [
          "2017",
          "10",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "22"
        ]
      ]
    },
    "abstract": "Program analyses frequently track objects throughout a program, which requires reasoning about aliases. Most dataflow analysis frameworks, however, delegate the task of handling aliases to the analysis clients, which causes a number of problems. For instance, custom-made extensions for alias analysis are complex and cannot easily be reused. On the other hand, due to the complex interfaces involved, off-the-shelf alias analyses are hard to integrate precisely into clients. Lastly, for precision many clients require strong updates, and alias abstractions supporting strong updates are often relatively inefficient. In this paper, we present IDEal, an alias-aware extension to the framework for Interprocedural Distributive Environment (IDE) problems. IDEal relieves static-analysis authors completely of the burden of handling aliases by automatically resolving alias queries on-demand, both efficiently and precisely. IDEal supports a highly precise analysis using strong updates by resorting to an on-demand, flow-sensitive, and context-sensitive all-alias analysis. Yet, it achieves previously unseen efficiency by propagating aliases individually, creating highly reusable per-pointer summaries. We empirically evaluate IDEal by comparing TSf, a state-of-the-art typestate analysis, to TSal, an IDEal-based typestate analysis. Our experiments show that the individual propagation of aliases within IDEal enables TSal to propagate 10.4x fewer dataflow facts and analyze 10.3x fewer methods when compared to TSf. On the DaCapo benchmark suite, TSal is able to efficiently compute precise results.",
    "keywords": "static analysis, aliasing, dataflow",
    "URL": "https://doi.org/10.1145/3133923",
    "DOI": "10.1145/3133923",
    "page": "99:1-99:27",
    "page-first": "99",
    "volume": "1",
    "_line": "LanguageTools.bib:1124"
  },
  "spath_boomerang_2016": {
    "id": "spath_boomerang_2016",
    "type": "article-journal",
    "author": [
      {
        "family": "Späth",
        "given": "Johannes"
      },
      {
        "family": "Do",
        "given": "Lisa Nguyen Quang"
      },
      {
        "family": "Ali",
        "given": "Karim"
      },
      {
        "family": "Bodden",
        "given": "Eric"
      }
    ],
    "title": "Boomerang: Demand-Driven Flow- and Context-Sensitive Pointer Analysis for Java",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "abstract": "Many current program analyses require highly precise pointer information about small, targeted parts of a given program. This motivates the need for demand-driven pointer analyses that compute information only where required. Pointer analyses generally compute points-to sets of program variables or answer boolean alias queries. However, many client analyses require richer pointer information. For example, taint and typestate analyses often need to know the set of all aliases of a given variable under a certain calling context. With most current pointer analyses, clients must compute such information through repeated points-to or alias queries, increasing complexity and computation time for them. This paper presents Boomerang, a demand-driven, ﬂow-, ﬁeld-, and context-sensitive pointer analysis for Java programs. Boomerang computes rich results that include both the possible allocation sites of a given pointer (points-to information) and all pointers that can point to those allocation sites (alias information). For increased precision and scalability, clients can query Boomerang with respect to particular calling contexts of interest.",
    "page": "26",
    "page-first": "26",
    "language": "en-US",
    "_line": "LanguageTools.bib:1142"
  },
  "schubert_know_2019": {
    "id": "schubert_know_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Schubert",
        "given": "Philipp Dominik"
      },
      {
        "family": "Leer",
        "given": "Richard"
      },
      {
        "family": "Hermann",
        "given": "Ben"
      },
      {
        "family": "Bodden",
        "given": "Eric"
      }
    ],
    "title": "Know your analysis: how instrumentation aids understanding static analysis",
    "container-title": "Proceedings of the 8th ACM SIGPLAN International Workshop on State Of the Art in Program Analysis",
    "container-title-short": "Know your analysis",
    "collection-title": "SOAP 2019",
    "title-short": "Know your analysis",
    "issued": {
      "date-parts": [
        [
          "2019",
          "6",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "22"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-6720-2",
    "abstract": "The development of a high-quality data-flow analysis&mdash;one that is precise and scalable&mdash;is a challenging task. A concrete client analysis not only requires data-flow but, in addition, type-hierarchy, points-to, and call-graph information, all of which need to be obtained by wisely chosen and correctly parameterized algorithms. Therefore, many static analysis frameworks have been developed that provide analysis writers with generic data-flow solvers as well as those additional pieces of information. Such frameworks ease the development of an analysis by requiring only a description of the data-flow problem to be solved and a set of framework parameters. Yet, analysis writers often struggle when an analysis does not behave as expected on real-world code. It is usually not apparent what causes a failure due to the complex interplay of the several algorithms and the client analysis code within such frameworks. In this work, we present some of the insights we gained by instrumenting the LLVM-based static analysis framework PhASAR for C/C++ code and show the broad area of applications at which flexible instrumentation supports analysis and framework developers. We present five cases in which instrumentation gave us valuable insights to debug and improve both, the concrete analyses and the underlying PhASAR framework.",
    "keywords": "Static analysis, C/C++, framework, instrumentation",
    "URL": "https://doi.org/10.1145/3315568.3329965",
    "DOI": "10.1145/3315568.3329965",
    "publisher-place": "New York, NY, USA",
    "page": "8-13",
    "page-first": "8",
    "_line": "LanguageTools.bib:1152"
  },
  "li_complexity_2021": {
    "id": "li_complexity_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "Yuanbo"
      },
      {
        "family": "Zhang",
        "given": "Qirun"
      },
      {
        "family": "Reps",
        "given": "Thomas"
      }
    ],
    "title": "On the complexity of bidirected interleaved Dyck-reachability",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "22"
        ]
      ]
    },
    "abstract": "Many program analyses need to reason about pairs of matching actions, such as call/return, lock/unlock, or set-field/get-field. The family of Dyck languages &lcurly;Dk&rcurly;, where Dk has k kinds of parenthesis pairs, can be used to model matching actions as balanced parentheses. Consequently, many program-analysis problems can be formulated as Dyck-reachability problems on edge-labeled digraphs. Interleaved Dyck-reachability (InterDyck-reachability), denoted by Dk ⊙ Dk-reachability, is a natural extension of Dyck-reachability that allows one to formulate program-analysis problems that involve multiple kinds of matching-action pairs. Unfortunately, the general InterDyck-reachability problem is undecidable. In this paper, we study variants of InterDyck-reachability on bidirected graphs, where for each edge ⟨ p, q ⟩ labeled by an open parenthesis ”(a”, there is an edge ⟨ q, p ⟩ labeled by the corresponding close parenthesis ”)a”, and vice versa. Language-reachability on a bidirected graph has proven to be useful both (1) in its own right, as a way to formalize many program-analysis problems, such as pointer analysis, and (2) as a relaxation method that uses a fast algorithm to over-approximate language-reachability on a directed graph. However, unlike its directed counterpart, the complexity of bidirected InterDyck-reachability still remains open. We establish the first decidable variant (i.e., D1 ⊙ D1-reachability) of bidirected InterDyck-reachability. In D1 ⊙ D1-reachability, each of the two Dyck languages is restricted to have only a single kind of parenthesis pair. In particular, we show that the bidirected D1 ⊙ D1 problem is in PTIME. We also show that when one extends each Dyck language to involve k different kinds of parentheses (i.e., Dk ⊙ Dk-reachability with k ≥ 2), the problem is NP-hard (and therefore much harder). We have implemented the polynomial-time algorithm for bidirected D1 ⊙ D1-reachability. Dk ⊙ Dk-reachability provides a new over-approximation method for bidirected Dk ⊙ Dk-reachability in the sense that Dk⊙ Dk-reachability can first be relaxed to bidirected D1 ⊙ D1-reachability, and then the resulting bidirected D1 ⊙ D1-reachability problem is solved precisely. We compare this D1 ⊙ D1-reachability-based approach against another known over-approximating Dk ⊙ Dk-reachability algorithm. Surprisingly, we found that the over-approximation approach based on bidirected D1 ⊙ D1-reachability computes more precise solutions, even though the D1⊙ D1 formalism is inherently less expressive than the Dk⊙ Dk formalism.",
    "keywords": "Static Analysis, Complexity, Formal Language Graph Reachability, Interleaved-Dyck Language",
    "URL": "https://doi.org/10.1145/3434340",
    "DOI": "10.1145/3434340",
    "page": "59:1-59:28",
    "page-first": "59",
    "volume": "5",
    "_line": "LanguageTools.bib:1171"
  },
  "reps_precise_nodate": {
    "id": "reps_precise_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Reps",
        "given": "Thomas"
      },
      {
        "family": "Horwitz",
        "given": "Susan"
      },
      {
        "family": "Sagiv",
        "given": "Mooly"
      }
    ],
    "title": "Precise Interprocedural Dataflow Analysis with Applications to Constant Propagation",
    "container-title-short": "PII",
    "title-short": "PII",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "20"
        ]
      ]
    },
    "URL": "https://reader.elsevier.com/reader/sd/pii/0304397596000722?token=15CB6A19CA3A743944564BDD3617A93CE015B932950E8BD0E9FBBD0AEC87E0476ACCE1E93B67F2D1AE1DCE570B05F8DA&originRegion=us-east-1&originCreation=20210920234905",
    "DOI": "10.1016/0304-3975(96)00072-2",
    "note": "ISSN: 0304-3975",
    "language": "en-US",
    "_line": "LanguageTools.bib:1188"
  },
  "reps_precise_1995": {
    "id": "reps_precise_1995",
    "type": "paper-conference",
    "author": [
      {
        "family": "Reps",
        "given": "Thomas"
      },
      {
        "family": "Horwitz",
        "given": "Susan"
      },
      {
        "family": "Sagiv",
        "given": "Mooly"
      }
    ],
    "title": "Precise interprocedural dataflow analysis via graph reachability",
    "container-title": "Proceedings of the 22nd ACM SIGPLAN-SIGACT symposium on Principles of programming languages",
    "collection-title": "POPL '95",
    "issued": {
      "date-parts": [
        [
          "1995",
          "1",
          "25"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "20"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-0-89791-692-9",
    "abstract": "The paper shows how a large class of interprocedural dataflow-analysis problems can be solved precisely in polynomial time by transforming them into a special kind of graph-reachability problem. The only restrictions are that the set of dataflow facts must be a finite set, and that the dataflow functions must distribute over the confluence operator (either union or intersection). This class of probable problems includes—but is not limited to—the classical separable problems (also known as “gen/kill” or “bit-vector” problems)—e.g., reaching definitions, available expressions, and live variables. In addition, the class of problems that our techniques handle includes many non-separable problems, including truly-live variables, copy constant propagation, and possibly-uninitialized variables. Results are reported from a preliminary experimental study of C programs (for the problem of finding possibly-uninitialized variables).",
    "URL": "https://doi.org/10.1145/199448.199462",
    "DOI": "10.1145/199448.199462",
    "publisher-place": "New York, NY, USA",
    "page": "49-61",
    "page-first": "49",
    "_line": "LanguageTools.bib:1200"
  },
  "schubert_modeling_nodate": {
    "id": "schubert_modeling_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Schubert",
        "given": "Philipp Dominik"
      },
      {
        "family": "Sattler",
        "given": "Florian"
      },
      {
        "family": "Schiebel",
        "given": "Fabian"
      },
      {
        "family": "Hermann",
        "given": "Ben"
      },
      {
        "family": "Bodden",
        "given": "Eric"
      }
    ],
    "title": "Modeling the Effects of Global Variables in Data-Flow Analysis for C/C++",
    "abstract": "Global variables make software systems hard to maintain and debug, and break local reasoning. They also impose a non-trivial challenge to static analysis which needs to model its effects to obtain sound analysis results. However, global variable initialization, codes of corresponding constructors and destructors as well as dynamic library code executed during load and unload not only affect control ﬂows but data ﬂows, too. The PhASAR static data-ﬂow analysis framework does not handle these special cases and also does not provide any functionalities to model the effects of globals. Analysis writers are forced to model the desired effects in an ad-hoc manner increasing an analysis’ complexity and imposing an additional repetitive task. In this paper, we present the challenges of modeling globals, elaborate on the impact they have on analysis information, and present a suitable model to capture their effects, allowing for an easier development of global-aware static data-ﬂow analyses. We present an implementation of our model within the PhASAR framework and show its usefulness for an IDE-based linearconstant propagation that crucially requires correct modeling of globals for correctness.",
    "page": "6",
    "page-first": "6",
    "language": "en-US",
    "_line": "LanguageTools.bib:1217"
  },
  "vojnar_phasar_2019": {
    "id": "vojnar_phasar_2019",
    "type": "chapter",
    "author": [
      {
        "family": "Schubert",
        "given": "Philipp Dominik"
      },
      {
        "family": "Hermann",
        "given": "Ben"
      },
      {
        "family": "Bodden",
        "given": "Eric"
      }
    ],
    "editor": [
      {
        "family": "Vojnar",
        "given": "Tomáš"
      },
      {
        "family": "Zhang",
        "given": "Lijun"
      }
    ],
    "title": "PhASAR: An Inter-procedural Static Analysis Framework for C/C++",
    "container-title": "Tools and Algorithms for the Construction and Analysis of Systems",
    "container-title-short": "PhASAR",
    "title-short": "PhASAR",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "20"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-17464-4 978-3-030-17465-1",
    "abstract": "Static program analysis is used to automatically determine program properties, or to detect bugs or security vulnerabilities in programs. It can be used as a stand-alone tool or to aid compiler optimization as an intermediary step. Developing precise, inter-procedural static analyses, however, is a challenging task, due to the algorithmic complexity, implementation eﬀort, and the threat of state explosion which leads to unsatisfactory performance. Software written in C and C++ is notoriously hard to analyze because of the deliberately unsafe type system, unrestricted use of pointers, and (for C++) virtual dispatch. In this work, we describe the design and implementation of the LLVM-based static analysis framework PhASAR for C/C++ code. PhASAR allows data-ﬂow problems to be solved in a fully automated manner. It provides class hierarchy, call-graph, points-to, and data-ﬂow information, hence requiring analysis developers only to specify a deﬁnition of the data-ﬂow problem. PhASAR thus hides the complexity of static analysis behind a high-level API, making static program analysis more accessible and easy to use. PhASAR is available as an open-source project. We evaluate PhASAR’s scalability during whole-program analysis. Analyzing 12 real-world programs using a taint analysis written in PhASAR, we found PhASAR’s abstractions and their implementations to provide a whole-program analysis that scales well to real-world programs. Furthermore, we peek into the details of analysis runs, discuss our experience in developing static analyses for C/C++, and present possible future improvements. Data or code related to this paper is available at: \\[34\\].",
    "URL": "http://link.springer.com/10.1007/978-3-030-17465-1_22",
    "DOI": "10.1007/978-3-030-17465-1_22",
    "publisher-place": "Cham",
    "page": "393-410",
    "page-first": "393",
    "volume": "11428",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "LanguageTools.bib:1226"
  },
  "schubert_into_nodate": {
    "id": "schubert_into_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Schubert",
        "given": "Philipp Dominik"
      },
      {
        "family": "Leer",
        "given": "Richard"
      },
      {
        "family": "Hermann",
        "given": "Ben"
      },
      {
        "family": "Bodden",
        "given": "Eric"
      }
    ],
    "title": "Into the Woods: Experiences from Building a Dataﬂow Analysis Framework for C/C++",
    "abstract": "While traditional static analysis—albeit its complexity—is a topic that is well understood, we especially struggle when it comes to implementing its concepts. Designing and modeling software that implements static analysis is a challenging task. However, developing usable static analysis implementations and providing toolboxes to researchers and practitioners is key to advance the overall progress in this ﬁeld. This paper reports on the lessons learned from developing the PhASAR and Soot static data-ﬂow analysis frameworks. We present some of the key mistakes of our ﬁrst implementations of PhASAR and their corrections. From those corrections we distill guidelines that will be helpful to static analysis developers and their users. In our work, we identiﬁed modularity as the key guiding principle supported—directly or indirectly—by virtually all other guidelines.",
    "page": "6",
    "page-first": "6",
    "language": "en-US",
    "_line": "LanguageTools.bib:1247"
  },
  "barbar_hash_nodate": {
    "id": "barbar_hash_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Barbar",
        "given": "Mohamad"
      },
      {
        "family": "Sui",
        "given": "Yulei"
      }
    ],
    "title": "Hash Consed Points-To Sets",
    "abstract": "Points-to analysis is a fundamental static analysis, on which many other analyses and optimisations are built. The goal of points-to analysis is to statically approximate the set of abstract objects that a pointer can point to at runtime. Due to the nature of static analysis, points-to analysis introduces much redundancy which can result in duplicate points-to sets and duplicate set union operations, particularly when analysing large programs precisely. To improve performance, there has been extensive eﬀort in mitigating duplication at the algorithmic level through, for example, cycle elimination and variable substitution.",
    "page": "24",
    "page-first": "24",
    "language": "en-US",
    "_line": "LanguageTools.bib:1256"
  },
  "lahav_whats_nodate": {
    "id": "lahav_whats_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Lahav",
        "given": "Ori"
      },
      {
        "family": "Boker",
        "given": "Udi"
      }
    ],
    "title": "What's Decidable about Causally Consistent Shared Memory?",
    "abstract": "CCS Concepts: • Software and its engineering → Software verification; Concurrent programming languages; • Theory of computation → Concurrency; Logic and verification; Program verification; • Information systems → Distributed database transactions.",
    "page": "54",
    "page-first": "54",
    "volume": "0",
    "issue": "0",
    "language": "en-US",
    "_line": "LanguageTools.bib:1265"
  },
  "lin_translation_nodate": {
    "id": "lin_translation_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Lin",
        "given": "Zhengyao"
      },
      {
        "family": "Kasampalis",
        "given": "Theodoros"
      },
      {
        "family": "Adve",
        "given": "Vikram"
      }
    ],
    "title": "A Translation Validation Algorithm for LLVM Register Allocators",
    "abstract": "Register allocation is a crucial and complex phase of any modern production compiler. In this work, we present a translation validation algorithm that veriﬁes each single instance of register allocation. Our algorithm is external to the compiler and independent of the register allocation algorithm. We support all major register allocation optimizations such as live range splitting, register coalescing, and rematerialization. We developed a prototype of this approach for the production-quality register allocation phase of LLVM. We evaluated this prototype for compiling the source code of SPECint 2006 benchmark, and we were able to verify the register allocation of over 88&perc; of supported functions in the benchmark, using all 4 register allocators available in LLVM 5.0.2.",
    "page": "11",
    "page-first": "11",
    "language": "en-US",
    "_line": "LanguageTools.bib:1284"
  },
  "wang_reassembleable_2015": {
    "id": "wang_reassembleable_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Wang",
        "given": "Shuai"
      },
      {
        "family": "Wang",
        "given": "Pei"
      },
      {
        "family": "Wu",
        "given": "Dinghao"
      }
    ],
    "title": "Reassembleable Disassembling",
    "event-title": "24th USENIX Security Symposium (USENIX Security 15)",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "27"
        ]
      ]
    },
    "isbn": "978-1-939133-11-3",
    "URL": "https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/wang-shuai",
    "page": "627-642",
    "page-first": "627",
    "language": "en-US",
    "_line": "LanguageTools.bib:1293"
  },
  "eklind_compositional_2015": {
    "id": "eklind_compositional_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Eklind",
        "given": "Robin"
      }
    ],
    "title": "Compositional Decompilation using LLVM IR",
    "issued": {
      "date-parts": [
        [
          "2015",
          "4",
          "21"
        ]
      ]
    },
    "abstract": "Decompilation or reverse compilation is the process of translating low-level machine-readable code into high-level human-readable code. The problem is nontrivial due to the amount of information lost during compilation, but it can be divided into several smaller problems which may be solved independently. This report explores the feasibility of composing a decompilation pipeline from independent components, and the potential of exposing those components to the end-user. The components of the decompilation pipeline are conceptually grouped into three modules. Firstly, the front-end translates a source language (e.g. x86 assembly) into LLVM IR; a platform-independent low-level intermediate representation. Secondly, the middle-end structures the LLVM IR by identifying high-level control ﬂow primitives (e.g. pre-test loops, 2-way conditionals). Lastly, the back-end translates the structured LLVM IR into a high-level target programming language (e.g. Go). The control ﬂow analysis stage of the middle-end uses subgraph isomorphism search algorithms to locate control ﬂow primitives in CFGs, both of which are described using Graphviz DOT ﬁles.",
    "page": "100",
    "page-first": "100",
    "language": "en-US",
    "_line": "LanguageTools.bib:1306"
  },
  "moll_decompilation_nodate": {
    "id": "moll_decompilation_nodate",
    "type": "thesis",
    "genre": "Bachelor's Thesis",
    "author": [
      {
        "family": "Moll",
        "given": "Simon"
      }
    ],
    "title": "Decompilation of LLVM IR",
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "27"
        ]
      ]
    },
    "publisher": "Saarland University",
    "number-of-pages": "69",
    "URL": "https://compilers.cs.uni-saarland.de/publications/theses/moll_bsc.pdf",
    "_line": "LanguageTools.bib:1316"
  },
  "schulte_gtirb_2020": {
    "id": "schulte_gtirb_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Schulte",
        "given": "Eric"
      },
      {
        "family": "Dorn",
        "given": "Jonathan"
      },
      {
        "family": "Flores-Montoya",
        "given": "Antonio"
      },
      {
        "family": "Ballman",
        "given": "Aaron"
      },
      {
        "family": "Johnson",
        "given": "Tom"
      }
    ],
    "title": "GTIRB: Intermediate Representation for Binaries",
    "container-title": "arXiv:1907.02859 \\[cs\\]",
    "container-title-short": "GTIRB",
    "title-short": "GTIRB",
    "issued": {
      "date-parts": [
        [
          "2020",
          "4",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "27"
        ]
      ]
    },
    "abstract": "GTIRB is an intermediate representation for binary analysis and rewriting tools including disassemblers, lifters, analyzers, rewriters, and pretty-printers. GTIRB is designed to enable communication between tools in a format that provides the basic information necessary for analysis and rewriting while making no further assumptions about domain (e.g., malware vs. cleanware, or PE vs. ELF) or semantic interpretation (functional vs. operational semantics). This design supports the goals of (1) encouraging tool modularization and re-use allowing researchers and developers to focus on a single aspect of binary analysis and rewriting without committing to any single tool chain and (2) facilitating communication and comparison between tools.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "1907.02859",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1907.02859",
    "URL": "http://arxiv.org/abs/1907.02859",
    "_line": "LanguageTools.bib:1327"
  },
  "wang_uroboros_2016": {
    "id": "wang_uroboros_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Wang",
        "given": "Shuai"
      },
      {
        "family": "Wang",
        "given": "Pei"
      },
      {
        "family": "Wu",
        "given": "Dinghao"
      }
    ],
    "title": "UROBOROS: Instrumenting Stripped Binaries with Static Reassembling",
    "container-title": "2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)",
    "container-title-short": "UROBOROS",
    "title-short": "UROBOROS",
    "event-title": "2016 IEEE 23rd International Conference on Software Analysis, Evolution and Reengineering (SANER)",
    "issued": {
      "date-parts": [
        [
          "2016",
          "3"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "27"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-5090-1855-0",
    "abstract": "Software instrumentation techniques are widely used in program analysis tasks such as program proﬁling, vulnerability discovering, and security-oriented transforming. In this paper, we present an instrumentation tool called UROBOROS, which supports static instrumentation on stripped binaries. Due to the lack of relocation and debug information, reverse engineering of stripped binaries is challenging. Compared with the previous work, UROBOROS can provide complete, easy-touse, transparent, and efﬁcient static instrumentation on stripped binaries. UROBOROS supports complete instrumentation by statically recovering the relocatable program (including both code and data sections) and the control ﬂow structures from binary code. UROBOROS provides a rich API to access and manipulate different levels of the program structure. The instrumentation facilities of UROBOROS are easy-to-use, users with no binary rewriting and patching skills can directly manipulate stripped binaries to perform smooth program transformations. Distinguished from most instrumentation tools that need to patch the instrumentation code as new sections, UROBOROS can directly inline the instrumentation code into the disassembled program, which provides transparent instrumentation on stripped binaries. For efﬁciency, in the rewritten output of existing tools, frequent control transfers between the attached and original sections can incur a considerable performance penalty. However, the output from UROBOROS incurs no extra cost because the original and instrumentation code are connected by “fall-through” transfers. We perform comparative evaluations between UROBOROS and the state-of-the-art binary instrumentation tools, including DynInst and Pin. To demonstrate the versatility of UROBOROS, we also implement two real-world reengineering tasks which could be challenging for other instrumentation tools to accomplish. Our experimental results show that UROBOROS outperforms the existing binary instrumentation tools with better performance, lower labor cost, and a broader scope of applications.",
    "URL": "http://ieeexplore.ieee.org/document/7476646/",
    "DOI": "10.1109/SANER.2016.106",
    "publisher-place": "Suita",
    "page": "236-247",
    "page-first": "236",
    "language": "en-US",
    "_line": "LanguageTools.bib:1342"
  },
  "kiaei_rewrite_2020": {
    "id": "kiaei_rewrite_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Kiaei",
        "given": "Pantea"
      },
      {
        "family": "Breunesse",
        "given": "Cees-Bart"
      },
      {
        "family": "Ahmadi",
        "given": "Mohsen"
      },
      {
        "family": "Schaumont",
        "given": "Patrick"
      },
      {
        "family": "Woudenberg",
        "given": "Jasper",
        "dropping-particle": "van"
      }
    ],
    "title": "Rewrite to Reinforce: Rewriting the Binary to Apply Countermeasures against Fault Injection",
    "container-title": "arXiv:2011.14067 \\[cs\\]",
    "container-title-short": "Rewrite to Reinforce",
    "title-short": "Rewrite to Reinforce",
    "issued": {
      "date-parts": [
        [
          "2020",
          "11",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "27"
        ]
      ]
    },
    "abstract": "Fault injection attacks can cause errors in software for malicious purposes. Oftentimes, vulnerable points of a program are detected after its development. It is therefore critical for the user of the program to be able to apply last-minute security assurance to the executable file without having access to the source code. In this work, we explore two methodologies based on binary rewriting that aid in injecting countermeasures in the binary file. The first approach injects countermeasures by reassembling the disassembly whereas the second approach leverages a full translation to a high-level IR and lowering that back to the target architecture.",
    "keywords": "Computer Science - Cryptography and Security",
    "URLtext": "2011.14067",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2011.14067",
    "URL": "http://arxiv.org/abs/2011.14067",
    "_line": "LanguageTools.bib:1361"
  },
  "jain_bird_2021": {
    "id": "jain_bird_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Jain",
        "given": "Ridhi"
      },
      {
        "family": "Purandare",
        "given": "Rahul"
      },
      {
        "family": "Sharma",
        "given": "Subodh"
      }
    ],
    "title": "BiRD: Race Detection in Software Binaries under Relaxed Memory Models",
    "container-title": "ACM Transactions on Software Engineering and Methodology",
    "container-title-short": "BiRD",
    "title-short": "BiRD",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "8"
        ]
      ]
    },
    "issn": "1049-331X",
    "abstract": "Instruction reordering and interleavings in program execution under relaxed memory semantics result in non-intuitive behaviors, making it difficult to provide assurances about program correctness. Studies have shown that up to 90&perc; of the concurrency bugs reported by state-of-the-art static analyzers are false alarms. As a result, filtering false alarms and detecting real concurrency bugs is a challenging problem. Unsurprisingly, this problem has attracted the interest of the research community over the past few decades. Nonetheless, many of the existing techniques rely on analyzing source code, rarely consider the effects introduced by compilers, and assume a sequentially consistent memory model. In a practical setting, however, developers often do not have access to the source code, and even commodity architectures such as x86 and ARM are not sequentially consistent. In this work, we present Bird, a prototype tool, to dynamically detect harmful data races in x86 binaries under relaxed memory models, TSO and PSO. Bird employs source-DPOR to explore all distinct feasible interleavings for a multithreaded application. Our evaluation of Bird on 42 publicly available benchmarks and its comparison with the state-of-the-art tools indicate Bird’s potential in effectively detecting data races in software binaries.",
    "keywords": "Dynamic race detection, Relaxed memory models, Software binaries, TSO and PSO",
    "URL": "https://doi.org/10.1145/3498538",
    "DOI": "10.1145/3498538",
    "note": "Just Accepted",
    "_line": "LanguageTools.bib:1376"
  },
  "suchy_carat_2022": {
    "id": "suchy_carat_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Suchy",
        "given": "Brian"
      },
      {
        "family": "Ghosh",
        "given": "Souradip"
      },
      {
        "family": "Kersnar",
        "given": "Drew"
      },
      {
        "family": "Chai",
        "given": "Siyuan"
      },
      {
        "family": "Huang",
        "given": "Zhen"
      },
      {
        "family": "Nelson",
        "given": "Aaron"
      },
      {
        "family": "Cuevas",
        "given": "Michael"
      },
      {
        "family": "Bernat",
        "given": "Alex"
      },
      {
        "family": "Chaudhary",
        "given": "Gaurav"
      },
      {
        "family": "Hardavellas",
        "given": "Nikos"
      },
      {
        "family": "Campanoni",
        "given": "Simone"
      },
      {
        "family": "Dinda",
        "given": "Peter"
      }
    ],
    "title": "CARAT CAKE: Replacing Paging via Compiler/Kernel Cooperation",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "abstract": "Virtual memory, specifically paging, is undergoing significant innovation due to being challenged by new demands from modern workloads. Recent work has demonstrated an alternative softwareonly design that can result in simplified hardware requirements, even supporting purely physical addressing. While we have made the case for this Compiler- And Runtime-based Address Translation (CARAT) concept, its evaluation was based on a user-level prototype. We now report on incorporating CARAT into a kernel, forming Compiler- And Runtime-based Address Translation for CollAborative Kernel Environments (CARAT CAKE). In our implementation, a Linux-compatible x64 process abstraction can be based either on CARAT CAKE, or on a sophisticated paging implementation. Implementing CARAT CAKE involves kernel changes and compiler optimizations/transformations that must work on all code in the system, including kernel code. We evaluate CARAT CAKE in comparison with paging and find that CARAT CAKE is able to achieve the functionality of paging (protection, mapping, and movement properties) with minimal overhead. In turn, CARAT CAKE allows significant new benefits for systems including energy savings, larger L1 caches, and arbitrary granularity memory management.",
    "page": "18",
    "page-first": "18",
    "language": "en-US",
    "_line": "LanguageTools.bib:1393"
  },
  "vasilache_composable_2022": {
    "id": "vasilache_composable_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Vasilache",
        "given": "Nicolas"
      },
      {
        "family": "Zinenko",
        "given": "Oleksandr"
      },
      {
        "family": "Bik",
        "given": "Aart J. C."
      },
      {
        "family": "Ravishankar",
        "given": "Mahesh"
      },
      {
        "family": "Raoux",
        "given": "Thomas"
      },
      {
        "family": "Belyaev",
        "given": "Alexander"
      },
      {
        "family": "Springer",
        "given": "Matthias"
      },
      {
        "family": "Gysi",
        "given": "Tobias"
      },
      {
        "family": "Caballero",
        "given": "Diego"
      },
      {
        "family": "Herhut",
        "given": "Stephan"
      },
      {
        "family": "Laurenzo",
        "given": "Stella"
      },
      {
        "family": "Cohen",
        "given": "Albert"
      }
    ],
    "title": "Composable and Modular Code Generation in MLIR: A Structured and Retargetable Approach to Tensor Compiler Construction",
    "container-title": "arXiv:2202.03293 \\[cs\\]",
    "container-title-short": "Composable and Modular Code Generation in MLIR",
    "title-short": "Composable and Modular Code Generation in MLIR",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "13"
        ]
      ]
    },
    "abstract": "Despite significant investment in software infrastructure, machine learning systems, runtimes and compilers do not compose properly. We propose a new design aiming at providing unprecedented degrees of modularity, composability and genericity. This paper discusses a structured approach to the construction of domain-specific code generators for tensor compilers, with the stated goal of improving the productivity of both compiler engineers and end-users. The approach leverages the natural structure of tensor algebra. It has been the main driver for the design of progressive lowering paths in &bslash;MLIR. The proposed abstractions and transformations span data structures and control flow with both functional (SSA form) and imperative (side-effecting) semantics. We discuss the implications of this infrastructure on compiler construction and present preliminary experimental results.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2202.03293",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2202.03293",
    "URL": "http://arxiv.org/abs/2202.03293",
    "_line": "LanguageTools.bib:1403"
  },
  "zhang_heterogen_2022": {
    "id": "zhang_heterogen_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Zhang",
        "given": "Qian"
      },
      {
        "family": "Wang",
        "given": "Jiyuan"
      },
      {
        "family": "Xu",
        "given": "Guoqing Harry"
      },
      {
        "family": "Kim",
        "given": "Miryung"
      }
    ],
    "title": "HeteroGen: Transpiling C to Heterogeneous HLS Code with Automated Test Generation and Program Repair",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "abstract": "Despite the trend of incorporating heterogeneity and specialization in hardware, the development of heterogeneous applications is limited to a handful of engineers with deep hardware expertise. We propose HeteroGen that takes C/C++ code as input and automatically generates an HLS version with test behavior preservation and better performance. Key to the success of HeteroGen is adapting the idea of search-based program repair to the heterogeneous computing domain, while addressing two technical challenges. First, the turn-around time of HLS compilation and simulation is much longer than the usual C/C++ compilation and execution time; therefore, HeteroGen applies pattern-oriented program edits guided by common fix patterns and their dependences. Second, behavior and performance checking requires testing, but test cases are often unavailable. Thus, HeteroGen auto-generates test inputs suitable for checking C to HLS-C conversion errors, while providing high branch coverage for the original C code.",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "LanguageTools.bib:1418"
  },
  "christensen_programming_2021": {
    "id": "christensen_programming_2021",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Christensen",
        "given": "Michael Alexandre"
      }
    ],
    "title": "Programming Language Techniques for Improving ISA and HDL Design",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "13"
        ]
      ]
    },
    "publisher": "UC Santa Barbara",
    "number-of-pages": "243",
    "URL": "https://escholarship.org/content/qt7sz5r3vd/qt7sz5r3vd.pdf",
    "publisher-place": "Santa Barbara, CA",
    "_line": "LanguageTools.bib:1428"
  },
  "bartell_optimizing_2021": {
    "id": "bartell_optimizing_2021",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Bartell",
        "given": "Sean"
      }
    ],
    "title": "Optimizing Whole Programs for Code Size",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "University of Illinois Urbana–Champaign",
    "number-of-pages": "145",
    "publisher-place": "Urbana, Illinois",
    "language": "en-US",
    "_line": "LanguageTools.bib:1441"
  },
  "chen_tree_2022": {
    "id": "chen_tree_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Chen",
        "given": "Yanju"
      },
      {
        "family": "Liu",
        "given": "Junrui"
      },
      {
        "family": "Feng",
        "given": "Yu"
      },
      {
        "family": "Bodik",
        "given": "Rastislav"
      }
    ],
    "title": "Tree Traversal Synthesis Using Domain-SpecificSymbolic Compilation",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "abstract": "Efficient computation on tree data structures is important in compilers, numeric computations, and web browser layout engines. Efficiency is achieved by statically scheduling the computation into a small number of tree traversals and by performing the traversals in parallel when possible. Manual design of such traversals leads to bugs, as observed in web browsers. Automatic schedulers avoid these bugs but they currently cannot explore a space of legal traversals, which prevents exploring the trade-offs between parallelism and minimizing the number of traversals. We describe Hecate, a synthesizer of tree traversals that can produce both serial and parallel traversals. A key feature is that the synthesizer is extensible by the programmer who can define a template for new kinds of traversals. Hecate is constructed as a solver-aided domain-specific language, meaning that the synthesizer is generated automatically by translating the tree traversal DSL to an SMT solver that synthesizes the traversals. We improve on the general-purpose solver-aided architecture with a schedulingspecific symbolic evaluation that maintains the engineering advantages solver-aided design but generates efficient ILP encoding that is much more efficient to solve than SMT constraints. On the set of Grafter problems, Hecate synthesizes traversals that trade off traversal fusion to exploit parallelism. Additionally, Hecate allows defining a tree data structure with an arbitrary number of children. Together, parallelism and data structure improvements accelerate the computation 2× on a tree rendering problem. Finally, Hecate’s domain-specific symbolic compilation accelerates synthesis 3× compared to the general-purpose compilation to an SMT solver; when scheduling a CSS engine traversal, this ILP-based synthesis executes orders of magnitude faster.",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "LanguageTools.bib:1453"
  },
  "riganelli_proactive_2022": {
    "id": "riganelli_proactive_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Riganelli",
        "given": "Oliviero"
      },
      {
        "family": "Fagadau",
        "given": "Ionut Daniel"
      },
      {
        "family": "Micucci",
        "given": "Daniela"
      },
      {
        "family": "Mariani",
        "given": "Leonardo"
      }
    ],
    "title": "Proactive Libraries: Enforcing Correct Behaviors in Android Apps",
    "container-title": "arXiv:2202.11999 \\[cs\\]",
    "container-title-short": "Proactive Libraries",
    "title-short": "Proactive Libraries",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "24"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "3",
          "3"
        ]
      ]
    },
    "abstract": "The Android framework provides a rich set of APIs that can be exploited by developers to build their apps. However, the rapid evolution of these APIs jointly with the specific characteristics of the lifecycle of the Android components challenge developers, who may release apps that use APIs incorrectly. In this demo, we present Proactive Libraries, a tool that can be used to decorate regular libraries with the capability of proactively detecting and healing API misuses at runtime. Proactive Libraries blend libraries with multiple proactive modules that collect data, check the compliance of API usages with correctness policies, and heal executions as soon as the possible violation of a policy is detected. The results of our evaluation with 27 possible API misuses show the effectiveness of Proactive Libraries in correcting API misuses with negligible runtime overhead.",
    "keywords": "Computer Science - Software Engineering",
    "URLtext": "2202.11999",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2202.11999",
    "URL": "http://arxiv.org/abs/2202.11999",
    "DOI": "10.1145/3510454.3516837",
    "_line": "LanguageTools.bib:1463"
  },
  "theodoridis_finding_2022": {
    "id": "theodoridis_finding_2022",
    "type": "paper-conference",
    "author": [
      {
        "family": "Theodoridis",
        "given": "Theodoros"
      },
      {
        "family": "Rigger",
        "given": "Manuel"
      },
      {
        "family": "Su",
        "given": "Zhendong"
      }
    ],
    "title": "Finding missed optimizations through the lens of dead code elimination",
    "container-title": "Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
    "event-title": "ASPLOS '22: 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "3",
          "8"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-9205-1",
    "abstract": "Compilers are foundational software development tools and incorporate increasingly sophisticated optimizations. Due to their complexity, it is difficult to systematically identify opportunities for improving them. Indeed, the automatic discovery of missed optimizations has been an important and significant challenge. The few existing approaches either cannot accurately pinpoint missed optimizations or target only specific analyses. This paper tackles this challenge by introducing a novel, effective approach that — in a simple and general manner — automatically identifies a wide range of missed optimizations. Our core insight is to leverage dead code elimination (DCE) to both analyze how well compilers optimize code and identify missed optimizations: (1) insert “optimization markers” in the basic blocks of a given program, (2) compute the program’s live/dead basic blocks using the “optimization markers”, and (3) identify missed optimizations from how well compilers eliminate dead blocks. We essentially exploit that, since DCE heavily depends on the rest of the optimization pipeline, through the lens of DCE, one can systematically quantify how well compilers optimize code. We conduct an extensive analysis of GCC and LLVM using our approach, which (1) provides quantitative and qualitative insights regarding their optimization capabilities, and (2) uncovers a diverse set of missed optimizations. Our results also lead to 84 bug reports for GCC and LLVM, of which 62 have already been confirmed or fixed, demonstrating our work’s strong practical utility. We expect that the simplicity and generality of our approach will make it widely applicable for understanding compiler performance and finding missed optimizations. This work opens and initiates this promising direction.",
    "URL": "https://dl.acm.org/doi/10.1145/3503222.3507764",
    "DOI": "10.1145/3503222.3507764",
    "publisher-place": "Lausanne Switzerland",
    "page": "697-709",
    "page-first": "697",
    "language": "en-US",
    "_line": "LanguageTools.bib:1479"
  },
  "peng_how_2021": {
    "id": "peng_how_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Peng",
        "given": "Dinglan"
      },
      {
        "family": "Zheng",
        "given": "Shuxin"
      },
      {
        "family": "Li",
        "given": "Yatao"
      },
      {
        "family": "Ke",
        "given": "Guolin"
      },
      {
        "family": "He",
        "given": "Di"
      },
      {
        "family": "Liu",
        "given": "Tie-Yan"
      }
    ],
    "title": "How could Neural Networks understand Programs?",
    "container-title": "arXiv:2105.04297 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "19"
        ]
      ]
    },
    "abstract": "Semantic understanding of programs is a fundamental problem for programming language processing (PLP). Recent works that learn representations of code based on pre-training techniques in NLP have pushed the frontiers in this direction. However, the semantics of PL and NL have essential differences. These being ignored, we believe it is difficult to build a model to better understand programs, by either directly applying off-the-shelf NLP pre-training techniques to the source code, or adding features to the model by the heuristic. In fact, the semantics of a program can be rigorously defined by formal semantics in PL theory. For example, the operational semantics, describes the meaning of a valid program as updating the environment (i.e., the memory address-value function) through fundamental operations, such as memory I/O and conditional branching. Inspired by this, we propose a novel program semantics learning paradigm, that the model should learn from information composed of (1) the representations which align well with the fundamental operations in operational semantics, and (2) the information of environment transition, which is indispensable for program understanding. To validate our proposal, we present a hierarchical Transformer-based pre-training model called OSCAR to better facilitate the understanding of programs. OSCAR learns from intermediate representation (IR) and an encoded representation derived from static analysis, which are used for representing the fundamental operations and approximating the environment transitions respectively. OSCAR empirically shows the outstanding capability of program semantics understanding on many practical software engineering tasks.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Machine Learning, Computer Science - Software Engineering",
    "URLtext": "2105.04297",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.04297",
    "URL": "http://arxiv.org/abs/2105.04297",
    "_line": "MachineLearning.bib:2"
  },
  "urban_review_2021": {
    "id": "urban_review_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Urban",
        "given": "Caterina"
      },
      {
        "family": "Miné",
        "given": "Antoine"
      }
    ],
    "title": "A Review of Formal Methods applied to Machine Learning",
    "container-title": "arXiv:2104.02466 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "4",
          "21"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "abstract": "We review state-of-the-art formal methods applied to the emerging field of the verification of machine learning systems. Formal methods can provide rigorous correctness guarantees on hardware and software systems. Thanks to the availability of mature tools, their use is well established in the industry, and in particular to check safety-critical applications as they undergo a stringent certification process. As machine learning is becoming more popular, machine-learned components are now considered for inclusion in critical systems. This raises the question of their safety and their verification. Yet, established formal methods are limited to classic, i.e. non machine-learned software. Applying formal methods to verify systems that include machine learning has only been considered recently and poses novel challenges in soundness, precision, and scalability. We first recall established formal methods and their current use in an exemplar safety-critical field, avionic software, with a focus on abstract interpretation based techniques as they provide a high level of scalability. This provides a golden standard and sets high expectations for machine learning verification. We then provide a comprehensive and detailed review of the formal methods developed so far for machine learning, highlighting their strengths and limitations. The large majority of them verify trained neural networks and employ either SMT, optimization, or abstract interpretation techniques. We also discuss methods for support vector machines and decision tree ensembles, as well as methods targeting training and data preparation, which are critical but often neglected aspects of machine learning. Finally, we offer perspectives for future research directions towards the formal verification of machine learning systems.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science, Computer Science - Machine Learning",
    "URLtext": "2104.02466",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2104.02466",
    "URL": "http://arxiv.org/abs/2104.02466",
    "_line": "MachineLearning.bib:16"
  },
  "sabour_dynamic_2017": {
    "id": "sabour_dynamic_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Sabour",
        "given": "Sara"
      },
      {
        "family": "Frosst",
        "given": "Nicholas"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey E."
      }
    ],
    "title": "Dynamic Routing Between Capsules",
    "container-title": "arXiv:1710.09829 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2017",
          "11",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "1"
        ]
      ]
    },
    "abstract": "A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.",
    "keywords": "Computer Science - Computer Vision and Pattern Recognition",
    "URLtext": "1710.09829",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1710.09829",
    "URL": "http://arxiv.org/abs/1710.09829",
    "_line": "MachineLearning.bib:30"
  },
  "hinton_transforming_2011": {
    "id": "hinton_transforming_2011",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hinton",
        "given": "Geoffrey E."
      },
      {
        "family": "Krizhevsky",
        "given": "Alex"
      },
      {
        "family": "Wang",
        "given": "Sida D."
      }
    ],
    "editor": [
      {
        "family": "Honkela",
        "given": "Timo"
      },
      {
        "family": "Duch",
        "given": "Włodzisław"
      },
      {
        "family": "Girolami",
        "given": "Mark"
      },
      {
        "family": "Kaski",
        "given": "Samuel"
      }
    ],
    "title": "Transforming Auto-Encoders",
    "container-title": "Artificial Neural Networks and Machine Learning – ICANN 2011",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2011"
        ]
      ]
    },
    "publisher": "Springer",
    "isbn": "978-3-642-21735-7",
    "abstract": "The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT \\[6\\], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.",
    "keywords": "auto-encoder, Invariance, shape representation",
    "DOI": "10.1007/978-3-642-21735-7_6",
    "publisher-place": "Berlin, Heidelberg",
    "page": "44-51",
    "page-first": "44",
    "language": "en-US",
    "_line": "MachineLearning.bib:44"
  },
  "hinton_next_2020": {
    "id": "hinton_next_2020",
    "type": "chapter",
    "author": [
      {
        "family": "Hinton",
        "given": "Geoffrey"
      }
    ],
    "title": "The Next Generation of Neural Networks",
    "container-title": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "issued": {
      "date-parts": [
        [
          "2020",
          "7",
          "25"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "22"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8016-4",
    "abstract": "The most important unsolved problem with artificial neural networks is how to do unsupervised learning as effectively as the brain. There are currently two main approaches to unsupervised learning. In the first approach, exemplified by BERT and Variational Autoencoders, a deep neural network is used to reconstruct its input. This is problematic for images because the deepest layers of the network need to encode the fine details of the image. An alternative approach, introduced by Becker and Hinton in 1992, is to train two copies of a deep neural network to produce output vectors that have high mutual information when given two different crops of the same image as their inputs. This approach was designed to allow the representations to be untethered from irrelevant details of the input. The method of optimizing mutual information used by Becker and Hinton was flawed (for a subtle reason that I will explain) so Pacannaro and Hinton (2001) replaced it by a discriminative objective in which one vector representation must select a corresponding vector representation from among many alternatives. With faster hardware, contrastive learning of representations has recently become very popular and is proving to be very effective, but it suffers from a major flaw: To learn pairs of representation vectors that have N bits of mutual information we need to contrast the correct corresponding vector with about 2N incorrect alternatives. I will describe a novel and effective way of dealing with this limitation. I will also show that this leads to a simple way of implementing perceptual learning in cortex.",
    "keywords": "deep learning, neural networks, unsupervised learning",
    "DOI": "10.1145/3397271.3402425",
    "publisher-place": "New York, NY, USA",
    "page": "1",
    "page-first": "1",
    "_line": "Security.bib:1035"
  },
  "suneja_towards_2021": {
    "id": "suneja_towards_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Suneja",
        "given": "Sahil"
      },
      {
        "family": "Zheng",
        "given": "Yunhui"
      },
      {
        "family": "Zhuang",
        "given": "Yufan"
      },
      {
        "family": "Laredo",
        "given": "Jim A."
      },
      {
        "family": "Morari",
        "given": "Alessandro"
      }
    ],
    "title": "Towards Reliable AI for Source Code Understanding",
    "container-title": "Proceedings of the ACM Symposium on Cloud Computing",
    "collection-title": "SoCC '21",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "3"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8638-8",
    "abstract": "Cloud maturity and popularity have resulted in Open source software (OSS) proliferation. And, in turn, managing OSS code quality has become critical in ensuring sustainable Cloud growth. On this front, AI modeling has gained popularity in source code understanding tasks, promoted by the ready availability of large open codebases. However, we have been observing certain peculiarities with these black-boxes, motivating a call for their reliability to be verified before offsetting traditional code analysis. In this work, we highlight and organize different reliability issues affecting AI-for-code into three stages of an AI pipeline- data collection, model training, and prediction analysis. We highlight the need for concerted efforts from the research community to ensure credibility, accountability, and traceability for AI-for-code. For each stage, we discuss unique opportunities afforded by the source code and software engineering setting to improve AI reliability.",
    "keywords": "explainability, machine learning, reliability, signal awareness",
    "URL": "https://doi.org/10.1145/3472883.3486995",
    "DOI": "10.1145/3472883.3486995",
    "publisher-place": "New York, NY, USA",
    "page": "403-411",
    "page-first": "403",
    "_line": "MachineLearning.bib:78"
  },
  "torfah_formal_2021": {
    "id": "torfah_formal_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Torfah",
        "given": "Hazem"
      },
      {
        "family": "Junges",
        "given": "Sebastian"
      },
      {
        "family": "Fremont",
        "given": "Daniel J."
      },
      {
        "family": "Seshia",
        "given": "Sanjit A."
      }
    ],
    "editor": [
      {
        "family": "Feng",
        "given": "Lu"
      },
      {
        "family": "Fisman",
        "given": "Dana"
      }
    ],
    "title": "Formal Analysis of AI-Based Autonomy: From Modeling to Runtime Assurance",
    "container-title": "Runtime Verification",
    "container-title-short": "Formal Analysis of AI-Based Autonomy",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "Formal Analysis of AI-Based Autonomy",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-88494-9",
    "abstract": "Autonomous systems are increasingly deployed in safety-critical applications and rely more on high-performance components based on artificial intelligence (AI) and machine learning (ML). Runtime monitors play an important role in raising the level of assurance in AI/ML-based autonomous systems by ensuring that the autonomous system stays safe within its operating environment. In this tutorial, we present VerifAI, an open-source toolkit for the formal design and analysis of systems that include AI/ML components. VerifAI provides features supporting a variety of use cases including formal modeling of the autonomous system and its environment, automatic falsification of system-level specifications as well as other simulation-based verification and testing methods, automated diagnosis of errors, and automatic specification-driven parameter and component synthesis. In particular, we describe the use of VerifAI for generating runtime monitors that capture the safe operational environment of systems with AI/ML components. We illustrate the advantages and applicability of VerifAI in real-life applications using a case study from the domain of autonomous aviation.",
    "URL": "https://link.springer.com/chapter/10.1007/978-3-030-88494-9_19",
    "DOI": "10.1007/978-3-030-88494-9_19",
    "publisher-place": "Cham",
    "page": "311-330",
    "page-first": "311",
    "language": "en-US",
    "_line": "MachineLearning.bib:96"
  },
  "mirman_provable_2020": {
    "id": "mirman_provable_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Mirman",
        "given": "Matthew"
      },
      {
        "family": "Singh",
        "given": "Gagandeep"
      },
      {
        "family": "Vechev",
        "given": "Martin"
      }
    ],
    "title": "A Provable Defense for Deep Residual Networks",
    "container-title": "arXiv:1903.12519 \\[cs, stat\\]",
    "issued": {
      "date-parts": [
        [
          "2020",
          "1",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "26"
        ]
      ]
    },
    "abstract": "We present a training system, which can provably defend significantly larger neural networks than previously possible, including ResNet-34 and DenseNet-100. Our approach is based on differentiable abstract interpretation and introduces two novel concepts: (i) abstract layers for fine-tuning the precision and scalability of the abstraction, (ii) a flexible domain specific language (DSL) for describing training objectives that combine abstract and concrete losses with arbitrary specifications. Our training method is implemented in the DiffAI system.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning",
    "URLtext": "1903.12519",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1903.12519",
    "URL": "http://arxiv.org/abs/1903.12519",
    "_line": "MachineLearning.bib:114"
  },
  "mirman_differentiable_nodate": {
    "id": "mirman_differentiable_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Mirman",
        "given": "Matthew"
      },
      {
        "family": "Gehr",
        "given": "Timon"
      },
      {
        "family": "Vechev",
        "given": "Martin"
      }
    ],
    "title": "Differentiable Abstract Interpretation for Provably Robust Neural Networks",
    "abstract": "We introduce a scalable method for training robust neural networks based on abstract interpretation. We present several abstract transformers which balance efﬁciency with precision and show these can be used to train large neural networks that are certiﬁably robust to adversarial perturbations.",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "MachineLearning.bib:128"
  },
  "yang_accelerating_nodate": {
    "id": "yang_accelerating_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Yang",
        "given": "Mingyue"
      },
      {
        "family": "Lie",
        "given": "David"
      },
      {
        "family": "Papernot",
        "given": "Nicolas"
      }
    ],
    "title": "Accelerating Symbolic Analysis for Android Apps",
    "abstract": "While tools based on symbolic execution are commonly used to analyze mobile applications, these tools can suffer from path explosion when real-world applications have more paths than available computing resources can handle. However, many of the paths are unsatisﬁable, that is, no input exists that can satisfy all the path constraints and cause the path to execute. Unfortunately, analysis tools cannot determine this without constraint collection and constraint solving, which are expensive to perform. As a result, analysis tools waste valuable computational resources on unsatisﬁable paths.",
    "page": "6",
    "page-first": "6",
    "language": "en-US",
    "_line": "MachineLearning.bib:137"
  },
  "cito_counterfactual_2021": {
    "id": "cito_counterfactual_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Cito",
        "given": "Jürgen"
      },
      {
        "family": "Dillig",
        "given": "Isil"
      },
      {
        "family": "Murali",
        "given": "Vijayaraghavan"
      },
      {
        "family": "Chandra",
        "given": "Satish"
      }
    ],
    "title": "Counterfactual Explanations for Models of Code",
    "container-title": "arXiv:2111.05711 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "26"
        ]
      ]
    },
    "abstract": "Machine learning (ML) models play an increasingly prevalent role in many software engineering tasks. However, because most models are now powered by opaque deep neural networks, it can be difficult for developers to understand why the model came to a certain conclusion and how to act upon the model's prediction. Motivated by this problem, this paper explores counterfactual explanations for models of source code. Such counterfactual explanations constitute minimal changes to the source code under which the model \"changes its mind\". We integrate counterfactual explanation generation to models of source code in a real-world setting. We describe considerations that impact both the ability to find realistic and plausible counterfactual explanations, as well as the usefulness of such explanation to the user of the model. In a series of experiments we investigate the efficacy of our approach on three different models, each based on a BERT-like architecture operating over source code.",
    "keywords": "Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Artificial Intelligence",
    "URLtext": "2111.05711",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2111.05711",
    "URL": "http://arxiv.org/abs/2111.05711",
    "_line": "MachineLearning.bib:146"
  },
  "daggitt_vehicle_2022": {
    "id": "daggitt_vehicle_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Daggitt",
        "given": "Matthew L."
      },
      {
        "family": "Kokke",
        "given": "Wen"
      },
      {
        "family": "Atkey",
        "given": "Robert"
      },
      {
        "family": "Arnaboldi",
        "given": "Luca"
      },
      {
        "family": "Komendantskya",
        "given": "Ekaterina"
      }
    ],
    "title": "Vehicle: Interfacing Neural Network Verifiers with Interactive Theorem Provers",
    "container-title": "arXiv:2202.05207 \\[cs\\]",
    "container-title-short": "Vehicle",
    "title-short": "Vehicle",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "15"
        ]
      ]
    },
    "abstract": "Verification of neural networks is currently a hot topic in automated theorem proving. Progress has been rapid and there are now a wide range of tools available that can verify properties of networks with hundreds of thousands of nodes. In theory this opens the door to the verification of larger control systems that make use of neural network components. However, although work has managed to incorporate the results of these verifiers to prove larger properties of individual systems, there is currently no general methodology for bridging the gap between verifiers and interactive theorem provers (ITPs). In this paper we present Vehicle, our solution to this problem. Vehicle is equipped with an expressive domain specific language for stating neural network specifications which can be compiled to both verifiers and ITPs. It overcomes previous issues with maintainability and scalability in similar ITP formalisations by using a standard ONNX file as the single canonical representation of the network. We demonstrate its utility by using it to connect the neural network verifier Marabou to Agda and then formally verifying that a car steered by a neural network never leaves the road, even in the face of an unpredictable cross wind and imperfect sensors. The network has over 20,000 nodes, and therefore this proof represents an improvement of 3 orders of magnitude over prior proofs about neural network enhanced systems in ITPs.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science, Computer Science - Machine Learning",
    "URLtext": "2202.05207",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2202.05207",
    "URL": "http://arxiv.org/abs/2202.05207",
    "_line": "MachineLearning.bib:160"
  },
  "schumi_exais_2022": {
    "id": "schumi_exais_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Schumi",
        "given": "Richard"
      },
      {
        "family": "Sun",
        "given": "Jun"
      }
    ],
    "title": "ExAIS: Executable AI Semantics",
    "container-title": "arXiv:2202.09868 \\[cs\\]",
    "container-title-short": "ExAIS",
    "title-short": "ExAIS",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "3",
          "3"
        ]
      ]
    },
    "abstract": "Neural networks can be regarded as a new programming paradigm, i.e., instead of building ever-more complex programs through (often informal) logical reasoning in the programmers' mind, complex 'AI' systems are built by optimising generic neural network models with big data. In this new paradigm, AI frameworks such as TensorFlow and PyTorch play a key role, which is as essential as the compiler for traditional programs. It is known that the lack of a proper semantics for programming languages (such as C), i.e., a correctness specification for compilers, has contributed to many problematic program behaviours and security issues. While it is in general hard to have a correctness specification for compilers due to the high complexity of programming languages and their rapid evolution, we have a unique opportunity to do it right this time for neural networks (which have a limited set of functions, and most of them have stable semantics). In this work, we report our effort on providing a correctness specification of neural network frameworks such as TensorFlow. We specify the semantics of almost all TensorFlow layers in the logical programming language Prolog. We demonstrate the usefulness of the semantics through two applications. One is a fuzzing engine for TensorFlow, which features a strong oracle and a systematic way of generating valid neural networks. The other is a model validation approach which enables consistent bug reporting for TensorFlow models.",
    "keywords": "Computer Science - Artificial Intelligence, Computer Science - Machine Learning",
    "URLtext": "2202.09868",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2202.09868",
    "URL": "http://arxiv.org/abs/2202.09868",
    "DOI": "10.1145/3510003.3510112",
    "_line": "MachineLearning.bib:175"
  },
  "roy_learning_2021": {
    "id": "roy_learning_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Roy",
        "given": "Subhajit"
      },
      {
        "family": "Hsu",
        "given": "Justin"
      },
      {
        "family": "Albarghouthi",
        "given": "Aws"
      }
    ],
    "title": "Learning Differentially Private Mechanisms",
    "container-title": "arXiv:2101.00961 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "14"
        ]
      ]
    },
    "abstract": "Differential privacy is a formal, mathematical definition of data privacy that has gained traction in academia, industry, and government. The task of correctly constructing differentially private algorithms is non-trivial, and mistakes have been made in foundational algorithms. Currently, there is no automated support for converting an existing, non-private program into a differentially private version. In this paper, we propose a technique for automatically learning an accurate and differentially private version of a given non-private program. We show how to solve this difficult program synthesis problem via a combination of techniques: carefully picking representative example inputs, reducing the problem to continuous optimization, and mapping the results back to symbolic expressions. We demonstrate that our approach is able to learn foundational algorithms from the differential privacy literature and significantly outperforms natural program synthesis baselines.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages, Computer Science - Machine Learning",
    "URLtext": "2101.00961",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2101.00961",
    "URL": "http://arxiv.org/abs/2101.00961",
    "_line": "Mathematics.bib:10"
  },
  "weisstein_mathworld_nodate": {
    "id": "weisstein_mathworld_nodate",
    "type": "webpage",
    "genre": "Text",
    "author": [
      {
        "family": "Weisstein",
        "given": "Eric W."
      }
    ],
    "title": "Mathworld Classroom",
    "accessed": {
      "date-parts": [
        [
          "2020",
          "1",
          "12"
        ]
      ]
    },
    "abstract": "Course List",
    "URL": "http://mathworld.wolfram.com/classroom/",
    "language": "en-US",
    "_line": "Mathematics.bib:24"
  },
  "vishwanathan_semantics_2021": {
    "id": "vishwanathan_semantics_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Vishwanathan",
        "given": "Harishankar"
      },
      {
        "family": "Shachnai",
        "given": "Matan"
      },
      {
        "family": "Narayana",
        "given": "Srinivas"
      },
      {
        "family": "Nagarakatte",
        "given": "Santosh"
      }
    ],
    "title": "Semantics, Verification, and Efficient Implementations for Tristate Numbers",
    "container-title": "arXiv:2105.05398 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "19"
        ]
      ]
    },
    "abstract": "Extended Berkeley Packet Filter(BPF)is an in-kernel, register-based virtual machine in the Linux operating system that allows non-superusers to execute code at specific points within the Linux kernel. To ensure that such user code is safe within the kernel, BPF relies on an in-kernel static analyzer that proves properties such as bounded memory access and the absence of illegal operations. This static analyzer uses an abstract domain, which it calls tnums (tristate numbers), to over-approximate the set of values that a variable may store. This abstract domain is implemented efficiently with bitwise and arithmetic operations. This paper formalizes the semantics and various properties of tnums and provides the first proofs of soundness and precision of arithmetic and logical operations with tnums. We describe a novel sound algorithm for multiplying two tnums that is more precise and efficient (runs 55&perc; faster on average) than the Linux kernel's tnum multiplication.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2105.05398",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.05398",
    "URL": "http://arxiv.org/abs/2105.05398",
    "_line": "Networking.bib:2"
  },
  "burlo_monitorability_2021": {
    "id": "burlo_monitorability_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Burlò",
        "given": "Christian Batrolo"
      },
      {
        "family": "Francalanza",
        "given": "Adrian"
      },
      {
        "family": "Scalas",
        "given": "Alceste"
      }
    ],
    "title": "On the Monitorability of Session Types, in Theory and Practice",
    "container-title": "arXiv:2105.06291 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "19"
        ]
      ]
    },
    "abstract": "In concurrent and distributed systems, software components are expected to communicate according to predetermined protocols and APIs - and if a component does not observe them, the system's reliability is compromised. Furthermore, isolating and fixing protocol/API errors can be very difficult. Many methods have been proposed to check the correctness of communicating systems, ranging from compile-time to run-time verification; among such methods, session types have been applied for both static type-checking, and run-time monitoring. This work takes a fresh look at the run-time verification of communicating systems using session types, in theory and in practice. On the theoretical side, we develop a novel formal model of session-monitored processes; with it, we formulate and prove new results on the monitorability of session types, connecting their run-time and static verification - in terms of soundness (i.e., whether monitors only flag ill-typed processes) and completeness (i.e., whether all ill-typed processes can be flagged by a monitor). On the practical side, we show that our monitoring theory is indeed realisable: building upon our formal model, we develop a Scala toolkit for the automatic generation of session monitors. Our executable monitors can be used to instrument black-box processes written in any programming language; we assess the viability of our approach with a series of benchmarks.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2105.06291",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.06291",
    "URL": "http://arxiv.org/abs/2105.06291",
    "_line": "Networking.bib:16"
  },
  "paulson_inductive_2021": {
    "id": "paulson_inductive_2021",
    "type": "book",
    "author": [
      {
        "family": "Paulson",
        "given": "Lawrence"
      }
    ],
    "title": "The Inductive Approach to Verifying Cryptographic Protocols",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "13"
        ]
      ]
    },
    "abstract": "Informal arguments that cryptographic protocols are secure can be made rigorous using inductive definitions. The approach is based on ordinary predicate calculus and copes with infinite-state systems. Proofs are generated using Isabelle/HOL. The human effort required to analyze a protocol can be as little as a week or two, yielding a proof script that takes a few minutes to run. Protocols are inductively defined as sets of traces. A trace is a list of communication events, perhaps comprising many interleaved protocol runs. Protocol descriptions incorporate attacks and accidental losses. The model spy knows some private keys and can forge messages using components decrypted from previous traffic. Three protocols are analyzed below: Otway-Rees (which uses shared-key encryption), Needham-Schroeder (which uses public-key encryption), and a recursive protocol by Bull and Otway (which is of variable length). One can prove that event &dollar;ev&dollar; always precedes event &dollar;ev'&dollar; or that property &dollar;P&dollar; holds provided &dollar;X&dollar; remains secret. Properties can be proved from the viewpoint of the various principals: say, if &dollar;A&dollar; receives a final message from &dollar;B&dollar; then the session key it conveys is good.",
    "_line": "Networking.bib:30"
  },
  "csoma_escape_2014": {
    "id": "csoma_escape_2014",
    "type": "paper-conference",
    "author": [
      {
        "family": "Csoma",
        "given": "Attila"
      },
      {
        "family": "Sonkoly",
        "given": "Balázs"
      },
      {
        "family": "Csikor",
        "given": "Levente"
      },
      {
        "family": "Németh",
        "given": "Felicián"
      },
      {
        "family": "Gulyas",
        "given": "Andràs"
      },
      {
        "family": "Tavernier",
        "given": "Wouter"
      },
      {
        "family": "Sahhaf",
        "given": "Sahel"
      }
    ],
    "title": "ESCAPE: extensible service chain prototyping environment using mininet, click, NETCONF and POX",
    "container-title": "Proceedings of the 2014 ACM conference on SIGCOMM",
    "container-title-short": "ESCAPE",
    "title-short": "ESCAPE",
    "event-title": "SIGCOMM'14: ACM SIGCOMM 2014 Conference",
    "issued": {
      "date-parts": [
        [
          "2014",
          "8",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-2836-4",
    "abstract": "Mininet is a great prototyping tool which combines existing SDN-related software components (e.g., Open vSwitch, OpenFlow controllers, network namespaces, cgroups) into a framework, which can automatically set up and conﬁgure customized OpenFlow testbeds scaling up to hundreds of nodes. Standing on the shoulders of Mininet, we implement a similar prototyping system called ESCAPE, which can be used to develop and test various components of the service chaining architecture. Our framework incorporates Click for implementing Virtual Network Functions (VNF), NETCONF for managing Click-based VNFs and POX for taking care of traﬃc steering. We also add our extensible Orchestrator module, which can accommodate mapping algorithms from abstract service descriptions to deployed and running service chains.",
    "URL": "https://dl.acm.org/doi/10.1145/2619239.2631448",
    "DOI": "10.1145/2619239.2631448",
    "publisher-place": "Chicago Illinois USA",
    "page": "125-126",
    "page-first": "125",
    "language": "en-US",
    "_line": "Networking.bib:38"
  },
  "erickson_beacon_2013": {
    "id": "erickson_beacon_2013",
    "type": "paper-conference",
    "author": [
      {
        "family": "Erickson",
        "given": "David"
      }
    ],
    "title": "The beacon openflow controller",
    "container-title": "Proceedings of the second ACM SIGCOMM workshop on Hot topics in software defined networking",
    "collection-title": "HotSDN '13",
    "issued": {
      "date-parts": [
        [
          "2013",
          "8",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-2178-5",
    "abstract": "Beacon is a Java-based open source OpenFlow controller created in 2010. It has been widely used for teaching, research, and as the basis of Floodlight. This paper describes the architectural decisions and implementation that achieves three of Beacon's goals: to improve developer productivity, to provide the runtime ability to start and stop existing and new applications, and to be high performance.",
    "keywords": "beacon, controller, java, openflow",
    "URL": "https://doi.org/10.1145/2491185.2491189",
    "DOI": "10.1145/2491185.2491189",
    "publisher-place": "New York, NY, USA",
    "page": "13-18",
    "page-first": "13",
    "_line": "Networking.bib:57"
  },
  "foster_languages_2013": {
    "id": "foster_languages_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Guha",
        "given": "A."
      },
      {
        "family": "Reitblatt",
        "given": "M."
      },
      {
        "family": "Story",
        "given": "A."
      },
      {
        "family": "Freedman",
        "given": "M. J."
      },
      {
        "family": "Katta",
        "given": "N. P."
      },
      {
        "family": "Monsanto",
        "given": "C."
      },
      {
        "family": "Reich",
        "given": "J."
      },
      {
        "family": "Rexford",
        "given": "J."
      },
      {
        "family": "Schlesinger",
        "given": "C."
      },
      {
        "family": "Walker",
        "given": "D."
      },
      {
        "family": "Harrison",
        "given": "R."
      }
    ],
    "title": "Languages for software-defined networks",
    "container-title": "IEEE Communications Magazine",
    "issued": {
      "date-parts": [
        [
          "2013",
          "2"
        ]
      ]
    },
    "issn": "1558-1896",
    "abstract": "Modern computer networks perform a bewildering array of tasks, from routing and traffic monitoring, to access control and server load balancing. However, managing these networks is unnecessarily complicated and error-prone, due to a heterogeneous mix of devices (e.g., routers, switches, firewalls, and middleboxes) with closed and proprietary configuration interfaces. Softwaredefined networks are poised to change this by offering a clean and open interface between networking devices and the software that controls them. In particular, many commercial switches support the OpenFlow protocol, and a number of campus, data center, and backbone networks have deployed the new technology. However, while SDNs make it possible to program the network, they does not make it easy. Today's OpenFlow controllers offer low-level APIs that mimic the underlying switch hardware. To reach SDNs full potential, we need to identify the right higher-level abstractions for creating (and composing) applications. In the Frenetic project, we are designing simple and intuitive abstractions for programming the three main stages of network management: monitoring network traffic, specifying and composing packet forwarding policies, and updating policies in a consistent way. Overall, these abstractions make it dramatically easier for programmers to write and reason about SDN applications.",
    "keywords": "Runtime, access control, application program interfaces, backbone networks, computer network management, computer networks, Control systems, data center, Frenetic project, high level languages, higher-level abstractions, intuitive abstractions, IP networks, Languages, low-level API, Monitoring, network management, network traffic monitoring, networking devices, OpenFlow controllers, OpenFlow protocol, packet forwarding policies, Ports (Computers), proprietary configuration interfaces, protocols, Repeaters, SDN, server load balancing, software radio, Software radio, software-defined networks, switch hardware, telecommunication traffic",
    "DOI": "10.1109/MCOM.2013.6461197",
    "page": "128-134",
    "page-first": "128",
    "volume": "51",
    "note": "Conference Name: IEEE Communications Magazine",
    "issue": "2",
    "_line": "Networking.bib:75"
  },
  "kaur_network_2014": {
    "id": "kaur_network_2014",
    "type": "book",
    "author": [
      {
        "family": "Kaur",
        "given": "Sukhveer"
      },
      {
        "family": "Singh",
        "given": "Japinder"
      },
      {
        "family": "Ghumman",
        "given": "Navtej"
      }
    ],
    "title": "Network Programmability Using POX Controller",
    "issued": {
      "date-parts": [
        [
          "2014",
          "8",
          "24"
        ]
      ]
    },
    "abstract": "POX is a Python based open source OpenFlow/Software Defined Networking (SDN) Controller. POX is used for faster development and prototyping of new network applications. POX controller comes pre installed with the mininet virtual machine. Using POX controller you can turn dumb openflow devices into hub, switch, load balancer, firewall devices. The POX controller allows easy way to run OpenFlow/SDN experiments. POX can be passed different parameters according to real or experimental topologies, thus allowing you to run experiments on real hardware, testbeds or in mininet emulator. In this paper, first section will contain introduction about POX, OpenFlow and SDN, then discussion about relationship between POX and Mininet. Final Sections will be regarding creating and verifying behavior of network applications in POX.",
    "DOI": "10.13140/RG.2.1.1950.6961",
    "_line": "Networking.bib:91"
  },
  "reich_modular_2013": {
    "id": "reich_modular_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Reich",
        "given": "Joshua"
      },
      {
        "family": "Monsanto",
        "given": "Christopher"
      },
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Rexford",
        "given": "Jennifer"
      },
      {
        "family": "Walker",
        "given": "David"
      }
    ],
    "title": "Modular SDN Programming with Pyretic",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "page": "8",
    "page-first": "8",
    "volume": "38",
    "issue": "5",
    "language": "en-US",
    "_line": "Networking.bib:100"
  },
  "foster_frenetic_2010": {
    "id": "foster_frenetic_2010",
    "type": "paper-conference",
    "author": [
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Freedman",
        "given": "Michael J."
      },
      {
        "family": "Harrison",
        "given": "Rob"
      },
      {
        "family": "Rexford",
        "given": "Jennifer"
      },
      {
        "family": "Meola",
        "given": "Matthew L."
      },
      {
        "family": "Walker",
        "given": "David"
      }
    ],
    "title": "Frenetic: a high-level language for OpenFlow networks",
    "container-title": "Proceedings of the Workshop on Programmable Routers for Extensible Services of Tomorrow",
    "container-title-short": "Frenetic",
    "collection-title": "PRESTO '10",
    "title-short": "Frenetic",
    "issued": {
      "date-parts": [
        [
          "2010",
          "11",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "12"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-0467-2",
    "abstract": "Most interfaces for programming network devices are defined at the low level of abstraction supported by the underlying hardware, which leads to complicated programs that are prone to errors. This paper proposes a high-level programming language for OpenFlow networks based on ideas originally developed in the functional programming community. Our language, called Frenetic, includes a rich pattern algebra for classifying packets, a \"program like you see every packet\" abstraction, and a run-time system that automatically generates the low-level packet-processing rules. We describe the design and implementation of Frenetic, and show how to use it to implement common management tasks.",
    "URL": "https://doi.org/10.1145/1921151.1921160",
    "DOI": "10.1145/1921151.1921160",
    "publisher-place": "New York, NY, USA",
    "page": "1-6",
    "page-first": "1",
    "_line": "Networking.bib:111"
  },
  "gude_nox_2008": {
    "id": "gude_nox_2008",
    "type": "article-journal",
    "author": [
      {
        "family": "Gude",
        "given": "Natasha"
      },
      {
        "family": "Koponen",
        "given": "Teemu"
      },
      {
        "family": "Pettit",
        "given": "Justin"
      },
      {
        "family": "Pfaff",
        "given": "Ben"
      },
      {
        "family": "Casado",
        "given": "Martín"
      },
      {
        "family": "McKeown",
        "given": "Nick"
      },
      {
        "family": "Shenker",
        "given": "Scott"
      }
    ],
    "title": "NOX: towards an operating system for networks",
    "container-title": "ACM SIGCOMM Computer Communication Review",
    "container-title-short": "NOX",
    "title-short": "NOX",
    "issued": {
      "date-parts": [
        [
          "2008",
          "7",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "12"
        ]
      ]
    },
    "issn": "0146-4833",
    "keywords": "security, architecture, management, network",
    "URL": "https://doi.org/10.1145/1384609.1384625",
    "DOI": "10.1145/1384609.1384625",
    "page": "105-110",
    "page-first": "105",
    "volume": "38",
    "issue": "3",
    "_line": "Networking.bib:129"
  },
  "canini_nice_2012": {
    "id": "canini_nice_2012",
    "type": "article-journal",
    "author": [
      {
        "family": "Canini",
        "given": "Marco"
      },
      {
        "family": "Venzano",
        "given": "Daniele"
      },
      {
        "family": "Peresˇıni",
        "given": "Peter"
      },
      {
        "family": "Kostic",
        "given": "Dejan"
      },
      {
        "family": "Rexford",
        "given": "Jennifer"
      }
    ],
    "title": "A NICE Way to Test OpenFlow Applications",
    "issued": {
      "date-parts": [
        [
          "2012"
        ]
      ]
    },
    "abstract": "The emergence of OpenFlow-capable switches enables exciting new network functionality, at the risk of programming errors that make communication less reliable. The centralized programming model, where a single controller program manages the network, seems to reduce the likelihood of bugs. However, the system is inherently distributed and asynchronous, with events happening at different switches and end hosts, and inevitable delays affecting communication with the controller. In this paper, we present efﬁcient, systematic techniques for testing unmodiﬁed controller programs. Our NICE tool applies model checking to explore the state space of the entire system—the controller, the switches, and the hosts. Scalability is the main challenge, given the diversity of data packets, the large system state, and the many possible event orderings. To address this, we propose a novel way to augment model checking with symbolic execution of event handlers (to identify representative packets that exercise code paths on the controller). We also present a simpliﬁed OpenFlow switch model (to reduce the state space), and effective strategies for generating event interleavings likely to uncover bugs. Our prototype tests Python applications on the popular NOX platform. In testing three real applications—a MAC-learning switch, in-network server load balancing, and energyefﬁcient trafﬁc engineering—we uncover eleven bugs.",
    "page": "14",
    "page-first": "14",
    "language": "en-US",
    "_line": "Networking.bib:147"
  },
  "hogan_elastic_2020": {
    "id": "hogan_elastic_2020",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hogan",
        "given": "Mary"
      },
      {
        "family": "Landau-Feibish",
        "given": "Shir"
      },
      {
        "family": "Tahmasbi Arashloo",
        "given": "Mina"
      },
      {
        "family": "Rexford",
        "given": "Jennifer"
      },
      {
        "family": "Walker",
        "given": "David"
      },
      {
        "family": "Harrison",
        "given": "Rob"
      }
    ],
    "title": "Elastic Switch Programming with P4All",
    "container-title": "Proceedings of the 19th ACM Workshop on Hot Topics in Networks",
    "collection-title": "HotNets '20",
    "issued": {
      "date-parts": [
        [
          "2020",
          "11",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "4"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8145-1",
    "abstract": "The P4 language enables a range of new network applications. However, it is still far from easy to implement and optimize P4 programs for PISA hardware. Programmers must engage in a tedious \"trial and error\" process wherein they write their program (guessing it will fit within the hardware) and then check by compiling it. If it fails, they repeat the process. In this paper, we argue that programmers should define elastic data structures that stretch automatically to make use of available switch resources. We present P4All, an extension of P4 that supports elastic switch programming. Elastic data structures also make P4All modules reusable across different applications and hardware targets, where resource needs and constraints may vary.Our design is oriented around use of symbolic primitives (integers that may take on a range of possible values at compile time), arrays, and loops. We show how to use these primitive mechanisms to build a range of reusable libraries such as hash tables, Bloom filters, sketches, and key-value stores. We also explain the important role that elasticity plays in modular programming, and we allow programmers to declare utility functions that control the relative share of data-plane resources apportioned to each module.",
    "URL": "https://doi.org/10.1145/3422604.3425933",
    "DOI": "10.1145/3422604.3425933",
    "publisher-place": "New York, NY, USA",
    "page": "168-174",
    "page-first": "168",
    "_line": "Networking.bib:157"
  },
  "skalka_proof-carrying_2019": {
    "id": "skalka_proof-carrying_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Skalka",
        "given": "Christian"
      },
      {
        "family": "Ring",
        "given": "John"
      },
      {
        "family": "Darias",
        "given": "David"
      },
      {
        "family": "Kwon",
        "given": "Minseok"
      },
      {
        "family": "Gupta",
        "given": "Sahil"
      },
      {
        "family": "Diller",
        "given": "Kyle"
      },
      {
        "family": "Smolka",
        "given": "Steffen"
      },
      {
        "family": "Foster",
        "given": "Nate"
      }
    ],
    "title": "Proof-Carrying Network Code",
    "container-title": "Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security",
    "collection-title": "CCS '19",
    "issued": {
      "date-parts": [
        [
          "2019",
          "11",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "4"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-6747-9",
    "abstract": "Computer networks often serve as the first line of defense against malicious attacks. Although there are a growing number of tools for defining and enforcing security policies in software-defined networks (SDNs), most assume a single point of control and are unable to handle the challenges that arise in networks with multiple administrative domains. For example, consumers may want want to allow their home IoT networks to be configured by device vendors, which raises security and privacy concerns. In this paper we propose a framework called Proof-Carrying Network Code (PCNC) for specifying and enforcing security in SDNs with interacting administrative domains. Like Proof-Carrying Authorization (PCA), PCNC provides methods for managing authorization domains, and like Proof-Carrying Code (PCC), PCNC provides methods for enforcing behavioral properties of network programs. We develop theoretical foundations for PCNC and evaluate it in simulated and real network settings, including a case study that considers security in IoT networks for home health monitoring.",
    "keywords": "formal verification, software-defined networks, netkat, nexus authorization logic, trust management",
    "URL": "https://doi.org/10.1145/3319535.3363214",
    "DOI": "10.1145/3319535.3363214",
    "publisher-place": "New York, NY, USA",
    "page": "1115-1129",
    "page-first": "1115",
    "_line": "Networking.bib:174"
  },
  "smolka_guarded_2019": {
    "id": "smolka_guarded_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Smolka",
        "given": "Steffen"
      },
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Hsu",
        "given": "Justin"
      },
      {
        "family": "Kappé",
        "given": "Tobias"
      },
      {
        "family": "Kozen",
        "given": "Dexter"
      },
      {
        "family": "Silva",
        "given": "Alexandra"
      }
    ],
    "title": "Guarded Kleene algebra with tests: verification of uninterpreted programs in nearly linear time",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Guarded Kleene algebra with tests",
    "title-short": "Guarded Kleene algebra with tests",
    "issued": {
      "date-parts": [
        [
          "2019",
          "12",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "4"
        ]
      ]
    },
    "abstract": "Guarded Kleene Algebra with Tests (GKAT) is a variation on Kleene Algebra with Tests (KAT) that arises by restricting the union (+) and iteration (\\*) operations from KAT to predicate-guarded versions. We develop the (co)algebraic theory of GKAT and show how it can be efficiently used to reason about imperative programs. In contrast to KAT, whose equational theory is PSPACE-complete, we show that the equational theory of GKAT is (almost) linear time. We also provide a full Kleene theorem and prove completeness for an analogue of Salomaa’s axiomatization of Kleene Algebra.",
    "keywords": "program equivalence, coalgebra, guarded automata, Kleene algebra with Tests, program schemes, uninterpreted programs",
    "URL": "https://doi.org/10.1145/3371129",
    "DOI": "10.1145/3371129",
    "page": "61:1-61:28",
    "page-first": "61",
    "volume": "4",
    "_line": "Networking.bib:192"
  },
  "smolka_scalable_2019": {
    "id": "smolka_scalable_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Smolka",
        "given": "Steffen"
      },
      {
        "family": "Kumar",
        "given": "Praveen"
      },
      {
        "family": "Kahn",
        "given": "David M."
      },
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Hsu",
        "given": "Justin"
      },
      {
        "family": "Kozen",
        "given": "Dexter"
      },
      {
        "family": "Silva",
        "given": "Alexandra"
      }
    ],
    "title": "Scalable verification of probabilistic networks",
    "container-title": "Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation",
    "collection-title": "PLDI 2019",
    "issued": {
      "date-parts": [
        [
          "2019",
          "6",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "4"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-6712-7",
    "abstract": "This paper presents McNetKAT, a scalable tool for verifying probabilistic network programs. McNetKAT is based on a new semantics for the guarded and history-free fragment of Probabilistic NetKAT in terms of finite-state, absorbing Markov chains. This view allows the semantics of all programs to be computed exactly, enabling construction of an automatic verification tool. Domain-specific optimizations and a parallelizing backend enable McNetKAT to analyze networks with thousands of nodes, automatically reasoning about general properties such as probabilistic program equivalence and refinement, as well as networking properties such as resilience to failures. We evaluate McNetKAT's scalability using real-world topologies, compare its performance against state-of-the-art tools, and develop an extended case study on a recently proposed data center network design.",
    "keywords": "Network verification, Probabilistic Programming",
    "URL": "https://doi.org/10.1145/3314221.3314639",
    "DOI": "10.1145/3314221.3314639",
    "publisher-place": "New York, NY, USA",
    "page": "190-203",
    "page-first": "190",
    "_line": "Networking.bib:210"
  },
  "smolka_cantor_2017": {
    "id": "smolka_cantor_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Smolka",
        "given": "Steffen"
      },
      {
        "family": "Kumar",
        "given": "Praveen"
      },
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Kozen",
        "given": "Dexter"
      },
      {
        "family": "Silva",
        "given": "Alexandra"
      }
    ],
    "title": "Cantor meets Scott: semantic foundations for probabilistic networks",
    "container-title": "Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages",
    "container-title-short": "Cantor meets Scott",
    "collection-title": "POPL 2017",
    "title-short": "Cantor meets Scott",
    "issued": {
      "date-parts": [
        [
          "2017",
          "1",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "4"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-4660-3",
    "abstract": "ProbNetKAT is a probabilistic extension of NetKAT with a denotational semantics based on Markov kernels. The language is expressive enough to generate continuous distributions, which raises the question of how to compute effectively in the language. This paper gives an new characterization of ProbNetKAT’s semantics using domain theory, which provides the foundation needed to build a practical implementation. We show how to use the semantics to approximate the behavior of arbitrary ProbNetKAT programs using distributions with finite support. We develop a prototype implementation and show how to use it to solve a variety of problems including characterizing the expected congestion induced by different routing schemes and reasoning probabilistically about reachability in a network.",
    "keywords": "D.3.1 \\[Programming Lan- guages\\]: Formal Definitions and TheoryźSemantics Kleene algebra with tests, Domain theory, NetKAT, Probabilistic semantics, Software-defined networking",
    "URL": "https://doi.org/10.1145/3009837.3009843",
    "DOI": "10.1145/3009837.3009843",
    "publisher-place": "New York, NY, USA",
    "page": "557-571",
    "page-first": "557",
    "_line": "Networking.bib:228"
  },
  "foster_coalgebraic_2015": {
    "id": "foster_coalgebraic_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Kozen",
        "given": "Dexter"
      },
      {
        "family": "Milano",
        "given": "Matthew"
      },
      {
        "family": "Silva",
        "given": "Alexandra"
      },
      {
        "family": "Thompson",
        "given": "Laure"
      }
    ],
    "title": "A Coalgebraic Decision Procedure for NetKAT",
    "container-title": "Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "collection-title": "POPL '15",
    "issued": {
      "date-parts": [
        [
          "2015",
          "1",
          "14"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "4"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-3300-9",
    "abstract": "NetKAT is a domain-specific language and logic for specifying and verifying network packet-processing functions. It consists of Kleene algebra with tests (KAT) augmented with primitives for testing and modifying packet headers and encoding network topologies. Previous work developed the design of the language and its standard semantics, proved the soundness and completeness of the logic, defined a PSPACE algorithm for deciding equivalence, and presented several practical applications. This paper develops the coalgebraic theory of NetKAT, including a specialized version of the Brzozowski derivative, and presents a new efficient algorithm for deciding the equational theory using bisimulation. The coalgebraic structure admits an efficient sparse representation that results in a significant reduction in the size of the state space. We discuss the details of our implementation and optimizations that exploit NetKAT's equational axioms and coalgebraic structure to yield significantly improved performance. We present results from experiments demonstrating that our tool is competitive with state-of-the-art tools on several benchmarks including all-pairs connectivity, loop-freedom, and translation validation.",
    "keywords": "netkat, coalgebra, automata, brzozowski derivatives, kleene algebra with tests, network verification",
    "URL": "https://doi.org/10.1145/2676726.2677011",
    "DOI": "10.1145/2676726.2677011",
    "publisher-place": "New York, NY, USA",
    "page": "343-355",
    "page-first": "343",
    "_line": "Networking.bib:247"
  },
  "vandenbroucke_plonk_2019": {
    "id": "vandenbroucke_plonk_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Vandenbroucke",
        "given": "Alexander"
      },
      {
        "family": "Schrijvers",
        "given": "Tom"
      }
    ],
    "title": "PloNK: functional probabilistic NetKAT",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "P&amp;&hash;x3bb;&amp;&hash;x3c9;NK",
    "title-short": "P&amp;&hash;x3bb;&amp;&hash;x3c9;NK",
    "issued": {
      "date-parts": [
        [
          "2019",
          "12",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "4"
        ]
      ]
    },
    "abstract": "This work presents PλωNK, a functional probabilistic network programming language that extends Probabilistic NetKAT (PNK). Like PNK, it enables probabilistic modelling of network behaviour, by providing probabilistic choice and infinite iteration (to simulate looping network packets). Yet, unlike PNK, it also offers abstraction and higher-order functions to make programming much more convenient. The formalisation of PλωNK is challenging for two reasons: Firstly, network programming induces multiple side effects (in particular, parallelism and probabilistic choice) which need to be carefully controlled in a functional setting. Our system uses an explicit syntax for thunks and sequencing which makes the interplay of these effects explicit. Secondly, measure theory, the standard domain for formalisations of (continuous) probablistic languages, does not admit higher-order functions. We address this by leveraging ω-Quasi Borel Spaces (ωQBSes), a recent advancement in the domain theory of probabilistic programming languages. We believe that our work is not only useful for bringing abstraction to PNK, but that—as part of our contribution—we have developed the meta-theory for a probabilistic language that combines advanced features like higher-order functions, iteration and parallelism, which may inform similar meta-theoretic efforts.",
    "keywords": "Probabilistic Programming, NetKAT, ?-QBS, Network Modelling, Quasi-Borel Spaces",
    "URL": "https://doi.org/10.1145/3371107",
    "DOI": "10.1145/3371107",
    "page": "39:1-39:27",
    "page-first": "39",
    "volume": "4",
    "_line": "Networking.bib:265"
  },
  "beckett_temporal_2016": {
    "id": "beckett_temporal_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Beckett",
        "given": "Ryan"
      },
      {
        "family": "Greenberg",
        "given": "Michael"
      },
      {
        "family": "Walker",
        "given": "David"
      }
    ],
    "title": "Temporal NetKAT",
    "container-title": "Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation",
    "collection-title": "PLDI '16",
    "issued": {
      "date-parts": [
        [
          "2016",
          "6",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "4"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-4261-2",
    "abstract": "Over the past 5-10 years, the rise of software-defined networking (SDN) has inspired a wide range of new systems, libraries, hypervisors and languages for programming, monitoring, and debugging network behavior. Oftentimes, these systems are disjoint—one language for programming and another for verification, and yet another for run-time monitoring and debugging. In this paper, we present a new, unified framework, called Temporal NetKAT, capable of facilitating all of these tasks at once. As its name suggests, Temporal NetKAT is the synthesis of two formal theories: past-time (finite trace) linear temporal logic and (network) Kleene Algebra with Tests. Temporal predicates allow programmers to write down concise properties of a packet’s path through the network and to make dynamic packet-forwarding, access control or debugging decisions on that basis. In addition to being useful for programming, the combined equational theory of LTL and NetKAT facilitates proofs of path-based correctness properties. Using new, general, proof techniques, we show that the equational semantics is sound with respect to the denotational semantics, and, for a class of programs we call network-wide programs, complete. We have also implemented a compiler for temporal NetKAT, evaluated its performance on a range of benchmarks, and studied the effectiveness of several optimizations.",
    "keywords": "NetKAT, Domain-specific languages, Kleene algebra with tests, Network programming languages, Temporal logic",
    "URL": "https://doi.org/10.1145/2908080.2908108",
    "DOI": "10.1145/2908080.2908108",
    "publisher-place": "New York, NY, USA",
    "page": "386-401",
    "page-first": "386",
    "_line": "Networking.bib:283"
  },
  "bertino_computing_2021": {
    "id": "bertino_computing_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bertino",
        "given": "Elisa"
      },
      {
        "family": "Bliss",
        "given": "Daniel"
      },
      {
        "family": "Lopresti",
        "given": "Daniel"
      },
      {
        "family": "Peterson",
        "given": "Larry"
      },
      {
        "family": "Schulzrinne",
        "given": "Henning"
      }
    ],
    "title": "Computing Research Challenges in Next Generation Wireless Networking",
    "container-title": "arXiv:2101.01279 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "4"
        ]
      ]
    },
    "abstract": "By all measures, wireless networking has seen explosive growth over the past decade. Fourth Generation Long Term Evolution (4G LTE) cellular technology has increased the bandwidth available for smartphones, in essence, delivering broadband speeds to mobile devices. The most recent 5G technology is further enhancing the transmission speeds and cell capacity, as well as, reducing latency through the use of different radio technologies and is expected to provide Internet connections that are an order of magnitude faster than 4G LTE. Technology continues to advance rapidly, however, and the next generation, 6G, is already being envisioned. 6G will make possible a wide range of powerful, new applications including holographic telepresence, telehealth, remote education, ubiquitous robotics and autonomous vehicles, smart cities and communities (IoT), and advanced manufacturing (Industry 4.0, sometimes referred to as the Fourth Industrial Revolution), to name but a few. The advances we will see begin at the hardware level and extend all the way to the top of the software \"stack.\" Artificial Intelligence (AI) will also start playing a greater role in the development and management of wireless networking infrastructure by becoming embedded in applications throughout all levels of the network. The resulting benefits to society will be enormous. At the same time these exciting new wireless capabilities are appearing rapidly on the horizon, a broad range of research challenges loom ahead. These stem from the ever-increasing complexity of the hardware and software systems, along with the need to provide infrastructure that is robust and secure while simultaneously protecting the privacy of users. Here we outline some of those challenges and provide recommendations for the research that needs to be done to address them.",
    "keywords": "Computer Science - Computers and Society, Computer Science - Networking and Internet Architecture",
    "URLtext": "2101.01279",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2101.01279",
    "URL": "http://arxiv.org/abs/2101.01279",
    "_line": "Networking.bib:301"
  },
  "mccauley_enabling_2019": {
    "id": "mccauley_enabling_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "McCauley",
        "given": "James"
      },
      {
        "family": "Harchol",
        "given": "Yotam"
      },
      {
        "family": "Panda",
        "given": "Aurojit"
      },
      {
        "family": "Raghavan",
        "given": "Barath"
      },
      {
        "family": "Shenker",
        "given": "Scott"
      }
    ],
    "title": "Enabling a permanent revolution in internet architecture",
    "container-title": "Proceedings of the ACM Special Interest Group on Data Communication",
    "collection-title": "SIGCOMM '19",
    "issued": {
      "date-parts": [
        [
          "2019",
          "8",
          "19"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "3"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-5956-6",
    "abstract": "Recent Internet research has been driven by two facts and their contradictory implications: the current Internet architecture is both inherently flawed (so we should explore radically different alternative designs) and deeply entrenched (so we should restrict ourselves to backwards-compatible and therefore incrementally deployable improvements). In this paper, we try to reconcile these two perspectives by proposing a backwards-compatible architectural framework called Trotsky in which one can incrementally deploy radically new designs. We show how this can lead to a permanent revolution in Internet architecture by (i) easing the deployment of new architectures and (ii) allowing multiple coexisting architectures to be used simultaneously by applications. By enabling both architectural evolution and architectural diversity, Trotsky would create a far more extensible Internet whose functionality is not defined by a single narrow waist, but by the union of many coexisting architectures. By being incrementally deployable, Trotsky is not just an interesting but unrealistic clean-slate design, but a step forward that is clearly within our reach.",
    "keywords": "internet architecture, internet evolution",
    "URL": "https://doi.org/10.1145/3341302.3342075",
    "DOI": "10.1145/3341302.3342075",
    "publisher-place": "New York, NY, USA",
    "page": "1-14",
    "page-first": "1",
    "_line": "Networking.bib:315"
  },
  "zave_compositional_2019": {
    "id": "zave_compositional_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Zave",
        "given": "Pamela"
      },
      {
        "family": "Rexford",
        "given": "Jennifer"
      }
    ],
    "title": "The compositional architecture of the internet",
    "container-title": "Communications of the ACM",
    "container-title-short": "Commun. ACM",
    "issued": {
      "date-parts": [
        [
          "2019",
          "2",
          "21"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "3"
        ]
      ]
    },
    "issn": "0001-0782, 1557-7317",
    "URL": "https://dl.acm.org/doi/10.1145/3226588",
    "DOI": "10.1145/3226588",
    "page": "78-87",
    "page-first": "78",
    "volume": "62",
    "issue": "3",
    "language": "en-US",
    "_line": "Networking.bib:333"
  },
  "kim_experience-driven_2021": {
    "id": "kim_experience-driven_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Kim",
        "given": "Hyojoon"
      },
      {
        "family": "Chen",
        "given": "Xiaoqi"
      },
      {
        "family": "Brassil",
        "given": "Jack"
      },
      {
        "family": "Rexford",
        "given": "Jennifer"
      }
    ],
    "title": "Experience-Driven Research on Programmable Networks",
    "container-title": "SIGCOMM Computer Communications Review",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1"
        ]
      ]
    },
    "abstract": "Many promising networking research ideas in programmable networks never see the light of day. Yet, deploying research prototypes in production networks can help validate research ideas, improve them with faster feedback, uncover new research questions, and also ease the subsequent transition to practice. In this paper, we show how researchers can run and validate their research ideas in their own backyards—on their production campus networks—and we have seen that such a demonstrator can expedite the deployment of a research idea in practice to solve real network operation problems. We present Camp4, a proof-of-concept that encompasses tools, an infrastructure design, strategies, and best practices—–both technical and non-technical–—that can help researchers run experiments against their programmable network idea in their own network. We use network tapping devices, packet brokers, and commodity programmable switches to enable running experiments against research ideas on a production campus network. We present several compelling data-plane applications as use cases that run on our campus and solve production network problems. By sharing our experiences, we hope to encourage similar efforts on other campuses.",
    "page": "7",
    "page-first": "7",
    "language": "en-US",
    "_line": "Networking.bib:350"
  },
  "zave_remaining_2020": {
    "id": "zave_remaining_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Zave",
        "given": "Pamela"
      },
      {
        "family": "Rexford",
        "given": "Jennifer"
      },
      {
        "family": "Sonchack",
        "given": "John"
      }
    ],
    "title": "The Remaining Improbable: Toward Verifiable Network Services",
    "container-title": "arXiv:2009.12861 \\[cs\\]",
    "container-title-short": "The Remaining Improbable",
    "title-short": "The Remaining Improbable",
    "issued": {
      "date-parts": [
        [
          "2020",
          "9",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "3"
        ]
      ]
    },
    "abstract": "The trustworthiness of modern networked services is too important to leave to chance. We need to design these services with specific properties in mind, and verify that the properties hold. In this paper, we argue that a compositional network architecture, based on a notion of layering where each layer is its own complete network customized for a specific purpose, is the only plausible approach to making network services verifiable. Realistic examples show how to use the architecture to reason about sophisticated network properties in a modular way. We also describe a prototype in which the basic structures of the architectural model are implemented in efficient P4 code for programmable data planes, then explain how this scaffolding fits into an integrated process of specification, code generation, implementation of additional network functions, and automated verification.",
    "keywords": "Computer Science - Networking and Internet Architecture",
    "URLtext": "2009.12861",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2009.12861",
    "URL": "http://arxiv.org/abs/2009.12861",
    "_line": "Networking.bib:361"
  },
  "basu_languages_2018": {
    "id": "basu_languages_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Basu",
        "given": "Shrutarshi"
      }
    ],
    "title": "Languages for Path-Based Network Programming",
    "issued": {
      "date-parts": [
        [
          "2018",
          "8",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "3"
        ]
      ]
    },
    "issn": "1048-9738",
    "abstract": "The notion of a path is an important abstraction for reasoning about and managing computer networks. By thinking of network configuration in terms of paths (instead of individual devices), network administrators can reap significant benefits in terms of usability, performance and correctness. Paths themselves can be specified and reasoned about using foundational computational tools such as regular expressions and Kleene Algebras. Furthermore, path-based abstractions can express key network behavior such as isolation requirements and bandwidth constraints, while supporting heterogeneity in terms of devices, physical substrates and administrative domains. This dissertation shows that it is possible to use specifications of network paths, extended with constraints on traffic classes, bandwidth, and the capabilities of network devices, to enable flexible management of modern networks. This is realized by developing path-oriented domain-specific languages to express network policies. These languages show that by starting with abstractions for network paths, it is possible to specify route and bandwidth requirements for classes of traffic, delegate policy management to trusted parties, and enable high-level management of heterogenous networks. Furthermore, by leveraging techniques and tools such as mixed-integer programming, SAT solvers and SDN controller frameworks, we have built practical compilers and runtimes for these languages. These compilers take high-level specifications of network policies and generate efficient configurations for a range of network devices including packet and optical switches, middlebox frameworks and end-hosts. We have tested these implementations by building and benchmarking a range of practical applications on real-world networks.",
    "URL": "https://ecommons.cornell.edu/handle/1813/59642",
    "DOI": "10.7298/X4057D4D",
    "note": "Accepted: 2018-10-23T13:34:32Z",
    "language": "en-US",
    "_line": "Networking.bib:376"
  },
  "smolka_coalgebraic_2019": {
    "id": "smolka_coalgebraic_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Smolka",
        "given": "Steffen Juilf"
      }
    ],
    "title": "A (Co)algebraic Approach to Programming and Verifying Computer Networks",
    "issued": {
      "date-parts": [
        [
          "2019",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "3"
        ]
      ]
    },
    "abstract": "As computer networks have grown into some of the most complex and critical computing systems today, the means of configuring them have not kept up: they remain manual, low-level, and ad-hoc. This makes network operations expensive and network outages due to misconfigurations commonplace. The thesis of this dissertation is that high-level programming languages and formal methods can make network configuration dramatically easier and more reliable. The dissertation consists of three parts. In the first part, we develop algorithms for compiling a network programming language with high-level abstractions to low-level network configurations, and introduce a symbolic data structure that makes compilation efficient in practice. In the second part, we develop foundations for a probabilistic network programming language using measure and domain theory, showing that continuity can be exploited to approximate (statistics of) packet distributions algorithmically. Based on this foundation and the theory of Markov chains, we then design a network verification tool that can reason about fault-tolerance and other probabilistic properties, scaling to data-center-size networks. In the third part, we introduce a general-purpose (co)algebraic framework for designing and reasoning about programming languages, and show that it permits an almost linear-time decision procedure for program equivalence. We hope that the framework will serve as a foundation for efficient verification tools, for networks and beyond, in the future.",
    "URL": "https://ecommons.cornell.edu/handle/1813/70019",
    "DOI": "10.7298/1dpd-c128",
    "note": "Accepted: 2020-06-23T18:00:06Z",
    "language": "en-US",
    "_line": "Networking.bib:391"
  },
  "reitblatt_formal_2017": {
    "id": "reitblatt_formal_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Reitblatt",
        "given": "Mark"
      }
    ],
    "title": "Formal Reasoning in Software-defined Networks",
    "issued": {
      "date-parts": [
        [
          "2017",
          "1",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "3"
        ]
      ]
    },
    "abstract": "This thesis presents an end-to-end approach for building computer networks that can be reasoned about and verified formally. In it, we present a high-level specification language for describing the desired forwarding behavior of networks based on regular expressions over network paths, as well as a tool that automatically verifies network forwarding policies; an approach to building formally verified compilers and runtimes for forwarding policies written in a network programming language that preserve the semantics of the source policy; and a technique for updating network configurations while preserving correctness.",
    "URL": "https://ecommons.cornell.edu/handle/1813/47888",
    "DOI": "10.7298/X45M63PH",
    "note": "Accepted: 2017-04-04T20:28:27Z",
    "language": "en-US",
    "_line": "Networking.bib:405"
  },
  "foster_probabilistic_2016": {
    "id": "foster_probabilistic_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Kozen",
        "given": "Dexter"
      },
      {
        "family": "Mamouras",
        "given": "Konstantinos"
      },
      {
        "family": "Reitblatt",
        "given": "Mark"
      },
      {
        "family": "Silva",
        "given": "Alexandra"
      }
    ],
    "editor": [
      {
        "family": "Thiemann",
        "given": "Peter"
      }
    ],
    "title": "Probabilistic NetKAT",
    "container-title": "Programming Languages and Systems",
    "collection-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    },
    "publisher": "Springer",
    "isbn": "978-3-662-49498-1",
    "abstract": "This paper presents a new language for network programming based on a probabilistic semantics. We extend the NetKATlanguage with new primitives for expressing probabilistic behaviors and enrich the semantics from one based on deterministic functions to one based on measurable functions on sets of packet histories. We establish fundamental properties of the semantics, prove that it is a conservative extension of the deterministic semantics, show that it satisfies a number of natural equations, and develop a notion of approximation. We present case studies that show how the language can be used to model a diverse collection of scenarios drawn from real-world networks.",
    "keywords": "Denotational Semantic, Head Packet, Markov Kernel, Network Calculus, Parallel Composition",
    "DOI": "10.1007/978-3-662-49498-1_12",
    "publisher-place": "Berlin, Heidelberg",
    "page": "282-309",
    "page-first": "282",
    "language": "en-US",
    "_line": "Networking.bib:419"
  },
  "anderson_netkat_2014": {
    "id": "anderson_netkat_2014",
    "type": "paper-conference",
    "author": [
      {
        "family": "Anderson",
        "given": "Carolyn Jane"
      },
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Guha",
        "given": "Arjun"
      },
      {
        "family": "Jeannin",
        "given": "Jean-Baptiste"
      },
      {
        "family": "Kozen",
        "given": "Dexter"
      },
      {
        "family": "Schlesinger",
        "given": "Cole"
      },
      {
        "family": "Walker",
        "given": "David"
      }
    ],
    "title": "NetKAT: semantic foundations for networks",
    "container-title": "Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "container-title-short": "NetKAT",
    "collection-title": "POPL '14",
    "title-short": "NetKAT",
    "issued": {
      "date-parts": [
        [
          "2014",
          "1",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "2"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-2544-8",
    "abstract": "Recent years have seen growing interest in high-level languages for programming networks. But the design of these languages has been largely ad hoc, driven more by the needs of applications and the capabilities of network hardware than by foundational principles. The lack of a semantic foundation has left language designers with little guidance in determining how to incorporate new features, and programmers without a means to reason precisely about their code. This paper presents NetKAT, a new network programming language that is based on a solid mathematical foundation and comes equipped with a sound and complete equational theory. We describe the design of NetKAT, including primitives for filtering, modifying, and transmitting packets; union and sequential composition operators; and a Kleene star operator that iterates programs. We show that NetKAT is an instance of a canonical and well-studied mathematical structure called a Kleene algebra with tests (KAT) and prove that its equational theory is sound and complete with respect to its denotational semantics. Finally, we present practical applications of the equational theory including syntactic techniques for checking reachability, proving non-interference properties that ensure isolation between programs, and establishing the correctness of compilation algorithms.",
    "keywords": "netkat, kleene algebra with tests, domain-specific languages, frenetic, network programming languages, software-defined networking",
    "URL": "https://doi.org/10.1145/2535838.2535862",
    "DOI": "10.1145/2535838.2535862",
    "publisher-place": "New York, NY, USA",
    "page": "113-126",
    "page-first": "113",
    "_line": "Networking.bib:437"
  },
  "soni_composing_2020": {
    "id": "soni_composing_2020",
    "type": "paper-conference",
    "author": [
      {
        "family": "Soni",
        "given": "Hardik"
      },
      {
        "family": "Rifai",
        "given": "Myriana"
      },
      {
        "family": "Kumar",
        "given": "Praveen"
      },
      {
        "family": "Doenges",
        "given": "Ryan"
      },
      {
        "family": "Foster",
        "given": "Nate"
      }
    ],
    "title": "Composing Dataplane Programs with μP4",
    "container-title": "Proceedings of the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication",
    "event-title": "SIGCOMM '20: Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication",
    "issued": {
      "date-parts": [
        [
          "2020",
          "7",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "2"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-7955-7",
    "abstract": "Dataplane languages like P4 enable flexible and efficient packetprocessing using domain-specific primitives such as programmable parsers and match-action tables. Unfortunately, P4 programs tend to be monolithic and tightly coupled to the hardware architecture, which makes it hard to write programs in a portable and modular way—e.g., by composing reusable libraries of standard protocols.",
    "URL": "https://dl.acm.org/doi/10.1145/3387514.3405872",
    "DOI": "10.1145/3387514.3405872",
    "publisher-place": "Virtual Event USA",
    "page": "329-343",
    "page-first": "329",
    "language": "en-US",
    "_line": "Networking.bib:456"
  },
  "hauser_survey_2021": {
    "id": "hauser_survey_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Hauser",
        "given": "Frederik"
      },
      {
        "family": "Häberle",
        "given": "Marco"
      },
      {
        "family": "Merling",
        "given": "Daniel"
      },
      {
        "family": "Lindner",
        "given": "Steffen"
      },
      {
        "family": "Gurevich",
        "given": "Vladimir"
      },
      {
        "family": "Zeiger",
        "given": "Florian"
      },
      {
        "family": "Frank",
        "given": "Reinhard"
      },
      {
        "family": "Menth",
        "given": "Michael"
      }
    ],
    "title": "A Survey on Data Plane Programming with P4: Fundamentals, Advances, and Applied Research",
    "container-title": "arXiv:2101.10632 \\[cs\\]",
    "container-title-short": "A Survey on Data Plane Programming with P4",
    "title-short": "A Survey on Data Plane Programming with P4",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1",
          "26"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "1"
        ]
      ]
    },
    "abstract": "With traditional networking, users can configure control plane protocols to match the specific network configuration, but without the ability to fundamentally change the underlying algorithms. With SDN, the users may provide their own control plane, that can control network devices through their data plane APIs. Programmable data planes allow users to define their own data plane algorithms for network devices including appropriate data plane APIs which may be leveraged by user-defined SDN control. Thus, programmable data planes and SDN offer great flexibility for network customization, be it for specialized, commercial appliances, e.g., in 5G or data center networks, or for rapid prototyping in industrial and academic research. Programming protocol-independent packet processors (P4) has emerged as the currently most widespread abstraction, programming language, and concept for data plane programming. It is developed and standardized by an open community and it is supported by various software and hardware platforms. In this paper, we survey the literature from 2015 to 2020 on data plane programming with P4. Our survey covers 497 references of which 367 are scientific publications. We organize our work into two parts. In the first part, we give an overview of data plane programming models, the programming language, architectures, compilers, targets, and data plane APIs. We also consider research efforts to advance P4 technology. In the second part, we analyze a large body of literature considering P4-based applied research. We categorize 241 research papers into different application domains, summarize their contributions, and extract prototypes, target platforms, and source code availability.",
    "keywords": "Computer Science - Networking and Internet Architecture",
    "URLtext": "2101.10632",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2101.10632",
    "URL": "http://arxiv.org/abs/2101.10632",
    "_line": "Networking.bib:474"
  },
  "casado_abstractions_2014": {
    "id": "casado_abstractions_2014",
    "type": "article-journal",
    "author": [
      {
        "family": "Casado",
        "given": "Martin"
      },
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Guha",
        "given": "Arjun"
      }
    ],
    "title": "Abstractions for software-defined networks",
    "container-title": "Communications of the ACM",
    "container-title-short": "Commun. ACM",
    "issued": {
      "date-parts": [
        [
          "2014",
          "9",
          "23"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "21"
        ]
      ]
    },
    "issn": "0001-0782",
    "abstract": "New abstractions are critical for achieving SDN goals.",
    "URL": "https://doi.org/10.1145/2661061.2661063",
    "DOI": "10.1145/2661061.2661063",
    "page": "86-95",
    "page-first": "86",
    "volume": "57",
    "issue": "10",
    "_line": "Networking.bib:489"
  },
  "p4_consortium_p4_2021": {
    "id": "p4_consortium_p4_2021",
    "type": "webpage",
    "author": [
      {
        "family": "Consortium",
        "given": "P4"
      }
    ],
    "title": "P4 Language and Related Specifications",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "13"
        ]
      ]
    },
    "URL": "https://p4.org/specs/",
    "_line": "Networking.bib:506"
  },
  "p4org_application_working_group_inband_2020": {
    "id": "p4org_application_working_group_inband_2020",
    "type": "webpage",
    "author": [
      {
        "family": "Group",
        "given": "P4.org Application Working"
      }
    ],
    "title": "Inband Network Telemetry (INT) Dataplane Specification.GitHub",
    "issued": {
      "date-parts": [
        [
          "2020",
          "1",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "21"
        ]
      ]
    },
    "abstract": "Inband Network Telemetry (\"INT\") is a framework designed to allow the collection and reporting of network state, by the data plane, without intervention or work by the control plane.",
    "URL": "https://github.com/p4lang/p4-applications/blob/master/docs/INT_v2_1.pdf",
    "language": "en-US",
    "_line": "Networking.bib:515"
  },
  "onf_sdn_2021": {
    "id": "onf_sdn_2021",
    "type": "webpage",
    "author": [
      {
        "family": "ONF"
      }
    ],
    "title": "SDN Technical Specifications.Open Networking Foundation",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "21"
        ]
      ]
    },
    "abstract": "These technical SDN specifications include standards of protocol, informational model, functionality and related framework",
    "URL": "https://opennetworking.org/software-defined-standards/specifications/",
    "language": "en-US",
    "_line": "Networking.bib:527"
  },
  "riecke_frenetic_2016": {
    "id": "riecke_frenetic_2016",
    "type": "webpage",
    "author": [
      {
        "family": "Riecke",
        "given": "Craig"
      }
    ],
    "title": "Frenetic Programmers Guide.GitHub",
    "issued": {
      "date-parts": [
        [
          "2016",
          "7",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "13"
        ]
      ]
    },
    "abstract": "Programmers Guide and other written materials. Contribute to frenetic-lang/manual development by creating an account on GitHub.",
    "URL": "https://github.com/frenetic-lang/manual",
    "language": "en-US",
    "_line": "Networking.bib:539"
  },
  "tsuzaki_reactive_2017": {
    "id": "tsuzaki_reactive_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Tsuzaki",
        "given": "Y."
      },
      {
        "family": "Okabe",
        "given": "Y."
      }
    ],
    "title": "Reactive configuration updating for Intent-Based Networking",
    "container-title": "2017 International Conference on Information Networking (ICOIN)",
    "event-title": "2017 International Conference on Information Networking (ICOIN)",
    "issued": {
      "date-parts": [
        [
          "2017",
          "1"
        ]
      ]
    },
    "abstract": "Since network configurations in the Internet are getting complicated, managing networks has become a heavy task. As an attempt to solve this problem, the concept of Intent-Based Networking (IBN) has been proposed. While intent of a network administrator in conventional networks is used to be expressed in concrete description of configurations stored on devices each, intent of an administrator in IBN is expressed abstractly and prescriptively as what he wants to do. The concept of Intent-Based Network Modeling (NEMO) language has been discussed in IETF IB-Nemo BoF, and a draft specification and implementation of it is developed in the NEMO project. NEMO, a transaction based Northbound API, currently supports description of simple proactive policy only such as change of network path at a specified time. We propose a mechanism that enables an administrator to update network configuration automatically in accordance with the change of external environment, based on reactive configurations which express the intent of the administrator. In this paper, we will describe automatic network management methods that have been studied, and then describe our proposed procedure to update configuration reactively in an extended NEMO language. Issues on implementation are also discussed.",
    "keywords": "application program interfaces, computer network management, automatic network management methods, Bandwidth, Concrete, Data models, Event Condition Action, IBN, IETF IB-Nemo BoF, intent-based network modeling, Intent-Based Networking, Internet, NEMO language, network administrator, network configurations, Network Management, Protocols, reactive configuration updating, Routing, Software defined networking, Terminology, transaction based northbound API",
    "DOI": "10.1109/ICOIN.2017.7899484",
    "page": "97-102",
    "page-first": "97",
    "_line": "Networking.bib:551"
  },
  "mitchell_what_2015": {
    "id": "mitchell_what_2015",
    "type": "webpage",
    "author": [
      {
        "family": "Mitchell",
        "given": "Sean"
      }
    ],
    "title": "What is OpenFlow and why should you care?",
    "issued": {
      "date-parts": [
        [
          "2015",
          "12",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "21"
        ]
      ]
    },
    "abstract": "Network switch manufacturers traditionally competed on the speed, features and software running and monitoring their products - change is in the air.",
    "URL": "https://itbrief.co.nz/story/what-openflow-and-why-should-you-care",
    "language": "en-US",
    "_line": "Networking.bib:564"
  },
  "covington_implementing_nodate": {
    "id": "covington_implementing_nodate",
    "type": "book",
    "author": [
      {
        "family": "Covington",
        "given": "G. Adam"
      },
      {
        "family": "Naous",
        "given": "Jad"
      },
      {
        "family": "Erickson",
        "given": "David"
      },
      {
        "family": "Mckeown",
        "given": "Nick"
      }
    ],
    "title": "Implementing an OpenFlow Switch on the NetFPGA platform",
    "abstract": "We describe the implementation of an OpenFlow Switch on the NetFPGA platform. OpenFlow is a way to deploy experimental or new protocols in networks that carry production traffic. An OpenFlow network consists of simple flow-based switches in the datapath, with a remote controller to manage several switches. In practice, OpenFlow is most often added as a feature to an existing Ethernet switch, IPv4 router or wireless access point. An OpenFlow-enabled device has an internal flow-table and a standardized interface to add and remove flow entries remotely. Our implementation of OpenFlow on the NetFPGA is one of several reference implementations we have implemented on different platforms. Our simple OpenFlow implementation is capable of running at line-rate and handling all the traffic that is going through the Stanford Electrical Engineering and Computer Science building. We compare our implementation’s complexity to a basic IPv4 router implementation and a basic Ethernet learning switch implementation. We describe the OpenFlow deployment into the Stanford campus and the Internet2 backbone.",
    "_line": "Networking.bib:587"
  },
  "noauthor_open_nodate-1": {
    "id": "noauthor_open_nodate-1",
    "type": "webpage",
    "title": "Open Compute Project.Open Compute Project",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "abstract": "Open Compute Project",
    "URL": "https://www.opencompute.org",
    "language": "en-US",
    "_line": "Networking.bib:601"
  },
  "feamster_road_2013": {
    "id": "feamster_road_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Feamster",
        "given": "Nick"
      },
      {
        "family": "Rexford",
        "given": "Jennifer"
      },
      {
        "family": "Zegura",
        "given": "Ellen"
      }
    ],
    "title": "The Road to SDN: An Intellectual History of Programmable Networks",
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "abstract": "Software Deﬁned Networking (SDN) is an exciting technology that enables innovation in how we design and manage networks. Although this technology seems to have appeared suddenly, SDN is part of a long history of efforts to make computer networks more programmable. In this paper, we trace the intellectual history of programmable networks, including active networks, early efforts to separate the control and data plane, and more recent work on OpenFlow and network operating systems. We highlight key concepts, as well as the technology pushes and application pulls that spurred each innovation. Along the way, we debunk common myths and misconceptions about the technologies and clarify the relationship between SDN and related technologies such as network virtualization.",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "Networking.bib:611"
  },
  "davie_why_2020": {
    "id": "davie_why_2020",
    "type": "webpage",
    "author": [
      {
        "family": "Davie",
        "given": "Bruce"
      }
    ],
    "title": "Why 5G Matters.Systems Approach",
    "issued": {
      "date-parts": [
        [
          "2020",
          "12",
          "14"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "20"
        ]
      ]
    },
    "abstract": "Over the last month I undertook a detailed review of a new book in the Systems Approach series, 5G Mobile Networks: A Systems Approach by Larry Peterson and Oguz Sunay. Talking to people outside the...",
    "URL": "http://www.systemsapproach.org/1/archives/12-2020",
    "language": "en-US",
    "_line": "Networking.bib:621"
  },
  "schnepf_automated_2017": {
    "id": "schnepf_automated_2017",
    "type": "paper-conference",
    "author": [
      {
        "family": "Schnepf",
        "given": "N."
      },
      {
        "family": "Badonnel",
        "given": "R."
      },
      {
        "family": "Lahmadi",
        "given": "A."
      },
      {
        "family": "Merz",
        "given": "S."
      }
    ],
    "title": "Automated verification of security chains in software-defined networks with synaptic",
    "container-title": "2017 IEEE Conference on Network Softwarization (NetSoft)",
    "event-title": "2017 IEEE Conference on Network Softwarization (NetSoft)",
    "issued": {
      "date-parts": [
        [
          "2017",
          "7"
        ]
      ]
    },
    "abstract": "Software-defined networks provide new facilities for deploying security mechanisms dynamically. In particular, it is possible to build and adjust security chains to protect the infrastructures, by combining different security functions, such as firewalls, intrusion detection systems and services for preventing data leakage. It is important to ensure that these security chains, in view of their complexity and dynamics, are consistent and do not include security violations. We propose in this paper an automated strategy for supporting the verification of security chains in software-defined networks. It relies on an architecture integrating formal verification methods for checking both the control and data planes of these chains, before their deployment. We describe algorithms for translating specifications of security chains into formal models that can then be verified by SMT1 solving or model checking. Our solution is prototyped as a package, named Synaptic, built as an extension of the Frenetic family of SDN programming languages. The performances of our approach are evaluated through extensive experimentations based on the CVC4, veriT, and nuXmv checkers.",
    "keywords": "Security, formal verification, model checking, Control systems, software-defined networks, Formal Verification, SMT, formal specification, automated verification, Cloud computing, Computer languages, computer network security, CVC4, data leakage prevention, firewalls, formal models, Frenetic family, infrastructure protection, intrusion detection systems, Middleboxes, Model checking, nuXmv checkers, SDN programming languages, security chain verification, security chains, security functions, Security Management, Smart devices, software defined networking, Software-Defined Networking, specification translation, Synaptic, veriT",
    "DOI": "10.1109/NETSOFT.2017.8004195",
    "page": "1-9",
    "page-first": "1",
    "_line": "Networking.bib:633"
  },
  "kim_kinetic_nodate": {
    "id": "kim_kinetic_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Kim",
        "given": "Hyojoon"
      },
      {
        "family": "Reich",
        "given": "Joshua"
      },
      {
        "family": "Gupta",
        "given": "Arpit"
      },
      {
        "family": "Shahbaz",
        "given": "Muhammad"
      },
      {
        "family": "Feamster",
        "given": "Nick"
      },
      {
        "family": "Clark",
        "given": "Russ"
      }
    ],
    "title": "Kinetic: Veriﬁable Dynamic Network Control",
    "abstract": "Network conditions are dynamic; unfortunately, current approaches to conﬁguring networks. Network operators need tools to express how a network’s data-plane behavior should respond to a wide range of events and changing conditions, ranging from unexpected failures to shifting trafﬁc patterns to planned maintenance. Yet, to update the network conﬁguration today, operators typically rely on a combination of manual intervention and ad hoc scripts. In this paper, we present Kinetic, a domain speciﬁc language and network control system that enables operators to control their networks dynamically in a concise, intuitive way. Kinetic also automatically veriﬁes the correctness of these control programs with respect to user-speciﬁed temporal properties. Our user study of Kinetic with several hundred network operators demonstrates that Kinetic is intuitive and usable, and our performance evaluation shows that realistic Kinetic programs scale well with the number of policies and the size of the network.",
    "page": "15",
    "page-first": "15",
    "language": "en-US",
    "_line": "Networking.bib:646"
  },
  "darvas_formal_2016": {
    "id": "darvas_formal_2016",
    "type": "paper-conference",
    "author": [
      {
        "family": "Darvas",
        "given": "Dániel"
      },
      {
        "family": "Majzik",
        "given": "István"
      },
      {
        "family": "Blanco Viñuela",
        "given": "Enrique"
      }
    ],
    "title": "Formal Verification of Safety PLC Based Control Software",
    "container-title": "Proceedings of the 12th International Conference on Integrated Formal Methods - Volume 9681",
    "collection-title": "IFM 2016",
    "issued": {
      "date-parts": [
        [
          "2016",
          "6",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "19"
        ]
      ]
    },
    "publisher": "Springer-Verlag",
    "isbn": "978-3-319-33692-3",
    "abstract": "Programmable Logic Controllers PLCs are widely used in the industry for various industrial automation tasks. Besides non-safety applications, the usage of PLCs became accepted in safety-critical installations, where the cost of failure is high. In these cases the used hardware is special so-called fail-safe or safety PLCs, but also the software needs special considerations. Formal verification is a method that can help to develop high-quality software for critical tasks. However, such method should be adapted to the special needs of the safety PLCs, that are often particular compared to the normal PLC development domain. In this paper we propose two complementary solutions for the formal verification of safety-critical PLC programs based on model checking and equivalence checking using formal specification. Furthermore, a case study is presented, demonstrating our approach.",
    "keywords": "Safety-critical systems, Model checking, Formal specification, PLC",
    "URL": "https://doi.org/10.1007/978-3-319-33693-0_32",
    "DOI": "10.1007/978-3-319-33693-0_32",
    "publisher-place": "Berlin, Heidelberg",
    "page": "508-522",
    "page-first": "508",
    "_line": "Networking.bib:655"
  },
  "ball_vericon_2014": {
    "id": "ball_vericon_2014",
    "type": "paper-conference",
    "author": [
      {
        "family": "Ball",
        "given": "Thomas"
      },
      {
        "family": "Bjørner",
        "given": "Nikolaj"
      },
      {
        "family": "Gember",
        "given": "Aaron"
      },
      {
        "family": "Itzhaky",
        "given": "Shachar"
      },
      {
        "family": "Karbyshev",
        "given": "Aleksandr"
      },
      {
        "family": "Sagiv",
        "given": "Mooly"
      },
      {
        "family": "Schapira",
        "given": "Michael"
      },
      {
        "family": "Valadarsky",
        "given": "Asaf"
      }
    ],
    "title": "VeriCon: towards verifying controller programs in software-defined networks",
    "container-title": "Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation",
    "container-title-short": "VeriCon",
    "title-short": "VeriCon",
    "event-title": "PLDI '14: ACM SIGPLAN Conference on Programming Language Design and Implementation",
    "issued": {
      "date-parts": [
        [
          "2014",
          "6",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "19"
        ]
      ]
    },
    "publisher": "ACM",
    "isbn": "978-1-4503-2784-8",
    "abstract": "Software-deﬁned networking (SDN) is a new paradigm for operating and managing computer networks. SDN enables logicallycentralized control over network devices through a “controller” software that operates independently from the network hardware, and can be viewed as the network operating system. Network operators can run both inhouse and third-party SDN programs (often called applications) on top of the controller, e.g., to specify routing and access control policies. SDN opens up the possibility of applying formal methods to prove the correctness of computer networks. Indeed, recently much effort has been invested in applying ﬁnite state model checking to check that SDN programs behave correctly. However, in general, scaling these methods to large networks is challenging and, moreover, they cannot guarantee the absence of errors. We present VeriCon, the ﬁrst system for verifying that an SDN program is correct on all admissible topologies and for all possible (inﬁnite) sequences of network events. VeriCon either conﬁrms the correctness of the controller program on all admissible network topologies or outputs a concrete counterexample. VeriCon uses ﬁrst-order logic to specify admissible network topologies and desired network-wide invariants, and then implements classical FloydHoare-Dijkstra deductive veriﬁcation using Z3. Our preliminary experience indicates that VeriCon is able to rapidly verify correctness, or identify bugs, for a large repertoire of simple core SDN programs. VeriCon is compositional, in the sense that it veriﬁes the correctness of execution of any single network event w.r.t. the speciﬁed invariant, and can thus scale to handle large programs. To relieve the burden of specifying inductive invariants from the programmer, VeriCon includes a separate procedure for inferring invariants, which is shown to be effective on simple controller programs. We view VeriCon as a ﬁrst step en route to practical mechanisms for verifying network-wide invariants of SDN programs.",
    "URL": "https://dl.acm.org/doi/10.1145/2594291.2594317",
    "DOI": "10.1145/2594291.2594317",
    "publisher-place": "Edinburgh United Kingdom",
    "page": "282-293",
    "page-first": "282",
    "language": "en-US",
    "_line": "Networking.bib:673"
  },
  "peterson_democratizing_2019": {
    "id": "peterson_democratizing_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Peterson",
        "given": "Larry"
      },
      {
        "family": "Anderson",
        "given": "Tom"
      },
      {
        "family": "Katti",
        "given": "Sachin"
      },
      {
        "family": "McKeown",
        "given": "Nick"
      },
      {
        "family": "Parulkar",
        "given": "Guru"
      },
      {
        "family": "Rexford",
        "given": "Jennifer"
      },
      {
        "family": "Satyanarayanan",
        "given": "Mahadev"
      },
      {
        "family": "Sunay",
        "given": "Oguz"
      },
      {
        "family": "Vahdat",
        "given": "Amin"
      }
    ],
    "title": "Democratizing the Network Edge",
    "container-title": "ACM SIGCOMM Computer Communication Review",
    "container-title-short": "SIGCOMM Comput. Commun. Rev.",
    "issued": {
      "date-parts": [
        [
          "2019",
          "5",
          "21"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "14"
        ]
      ]
    },
    "issn": "0146-4833",
    "abstract": "With datacenters established as part of the global computing infrastructure, industry is now in the midst of a transition towards the edge. Previous research initiatives laid the groundwork for this transition, but that is no guarantee the emerging edge will continue to be open to researchers. This paper argues that there is a tremendous opportunity to innovate at the edge, but having impact requires understanding the nature of the current industry momentum, and making a concerted effort to align with that momentum. We believe there are three keys to doing this: (1) focus on the intersection of the cloud and access networks, (2) contribute to the relevant open source projects, and (3) address the challenge of operationalizing the results. The paper puts forward a concrete proposal for all three, and discusses the opportunity to influence how the Internet evolves at the edge and enable new and transformative edge applications.",
    "URL": "https://dl.acm.org/doi/10.1145/3336937.3336942",
    "DOI": "10.1145/3336937.3336942",
    "page": "31-36",
    "page-first": "31",
    "volume": "49",
    "issue": "2",
    "language": "en-US",
    "_line": "Networking.bib:692"
  },
  "jain_b4_nodate": {
    "id": "jain_b4_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Jain",
        "given": "Sushant"
      },
      {
        "family": "Kumar",
        "given": "Alok"
      },
      {
        "family": "Mandal",
        "given": "Subhasree"
      },
      {
        "family": "Ong",
        "given": "Joon"
      },
      {
        "family": "Poutievski",
        "given": "Leon"
      },
      {
        "family": "Singh",
        "given": "Arjun"
      },
      {
        "family": "Venkata",
        "given": "Subbaiah"
      },
      {
        "family": "Wanderer",
        "given": "Jim"
      },
      {
        "family": "Zhou",
        "given": "Junlan"
      },
      {
        "family": "Zhu",
        "given": "Min"
      },
      {
        "family": "Zolla",
        "given": "Jonathan"
      },
      {
        "family": "Hölzle",
        "given": "Urs"
      },
      {
        "family": "Stuart",
        "given": "Stephen"
      },
      {
        "family": "Vahdat",
        "given": "Amin"
      }
    ],
    "title": "B4: Experience with a Globally-Deployed Software Deﬁned WAN",
    "abstract": "We present the design, implementation, and evaluation of B , a private WAN connecting Google’s data centers across the planet. B has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic trafc demand that seeks to maximize average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge. ese characteristics led to a So ware De ned Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B ’s centralized tra c engineering service drives links to near utilization, while splitting application ows among multiple paths to balance capacity against application priority/demands. We describe experience with three years of B production deployment, lessons learned, and areas for future work.",
    "page": "12",
    "page-first": "12",
    "language": "en-US",
    "_line": "Networking.bib:729"
  },
  "noauthor_open_nodate-2": {
    "id": "noauthor_open_nodate-2",
    "type": "webpage",
    "title": "Open vSwitch",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "14"
        ]
      ]
    },
    "URL": "https://www.openvswitch.org/",
    "_line": "Networking.bib:738"
  },
  "dumitrescu_bf4_2020": {
    "id": "dumitrescu_bf4_2020",
    "type": "paper-conference",
    "author": [
      {
        "family": "Dumitrescu",
        "given": "Dragos"
      },
      {
        "family": "Stoenescu",
        "given": "Radu"
      },
      {
        "family": "Negreanu",
        "given": "Lorina"
      },
      {
        "family": "Raiciu",
        "given": "Costin"
      }
    ],
    "title": "bf4: towards bug-free P4 programs",
    "container-title": "Proceedings of the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication",
    "container-title-short": "bf4",
    "collection-title": "SIGCOMM '20",
    "title-short": "bf4",
    "issued": {
      "date-parts": [
        [
          "2020",
          "7",
          "30"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "13"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-7955-7",
    "abstract": "Recent verification work has made advances in finding bugs in P4 programs before deployment, but it requires that the programmer specifies table rules that are possible at runtime\\[32, 24, 27\\]. This imposes a specification burden on the programmer, while at the same time failing to guarantee that bugs will not be inserted at runtime by faulty controllers. We present bf4, a novel verification approach for P4 programs that uses a mix of static verification, code changes and runtime checks to ensure that the deployed P4 program is bug free. To achieve this, bf4 uses static analysis to find all possible bugs in the P4 program; for each possible bug, bf4 attempts to find predicates that, when applied to table rules inserted by the controller, make that bug unreachable. If such predicates do not exist, bf4 can change the P4 code and re-run the procedure above. We applied bf4 to a wide range of P4 programs; for all these, bf4 is able to generate controller assertions and propose fixes that guarantee no controller-induced bug is reachable. At runtime, bf4 checks that the controller does not insert faulty rules; when it does, it throws an exception which helps troubleshoot the bug.",
    "keywords": "Network dataplane verification, programmable networks",
    "URL": "https://doi.org/10.1145/3387514.3405888",
    "DOI": "10.1145/3387514.3405888",
    "publisher-place": "New York, NY, USA",
    "page": "571-585",
    "page-first": "571",
    "_line": "Networking.bib:745"
  },
  "noauthor_provable_nodate": {
    "id": "noauthor_provable_nodate",
    "type": "webpage",
    "title": "Provable Security - Amazon Web Services (AWS).Amazon Web Services, Inc.",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "14"
        ]
      ]
    },
    "abstract": "Insights and research from AWS Security experts on security assurance, backed by mathematical proof.",
    "URL": "https://aws.amazon.com/security/provable-security/",
    "language": "en-US",
    "_line": "Networking.bib:781"
  },
  "magrino_efficient_2019": {
    "id": "magrino_efficient_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Magrino",
        "given": "Tom"
      },
      {
        "family": "Liu",
        "given": "Jed"
      },
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Gehrke",
        "given": "Johannes"
      },
      {
        "family": "Myers",
        "given": "Andrew C."
      }
    ],
    "title": "Efficient, Consistent Distributed Computation with Predictive Treaties",
    "container-title": "Proceedings of the Fourteenth EuroSys Conference 2019",
    "collection-title": "EuroSys '19",
    "issued": {
      "date-parts": [
        [
          "2019",
          "3",
          "25"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "13"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-6281-8",
    "abstract": "To achieve good performance, modern applications often partition their state across multiple geographically distributed nodes. While this approach reduces latency in the common case, it can be challenging for programmers to use correctly, especially in applications that require strong consistency. We introduce predictive treaties, a mechanism that can significantly reduce distributed coordination without losing strong consistency. The central insight behind our approach is that many computations can be expressed in terms of predicates over distributed state that can be partitioned and enforced locally. Predictive treaties improve on previous work by allowing the locally enforced predicates to depend on time. Intuitively, by predicting the evolution of system state, coordination can be significantly reduced compared to static approaches. We implemented predictive treaties in a distributed system that exposes them in an intuitive programming model. We evaluate performance on several benchmarks, including TPC-C, showing that predictive treaties can significantly increase performance by orders of magnitude and can even outperform customized algorithms.",
    "URL": "https://doi.org/10.1145/3302424.3303987",
    "DOI": "10.1145/3302424.3303987",
    "publisher-place": "New York, NY, USA",
    "page": "1-16",
    "page-first": "1",
    "_line": "Networking.bib:791"
  },
  "liu_secure_2018": {
    "id": "liu_secure_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Liu",
        "given": "Jed"
      },
      {
        "family": "Corbett-Davies",
        "given": "Joe"
      },
      {
        "family": "Ferraiuolo",
        "given": "Andrew"
      },
      {
        "family": "Ivanov",
        "given": "Alexander"
      },
      {
        "family": "Luo",
        "given": "Mulong"
      },
      {
        "family": "Suh",
        "given": "G. Edward"
      },
      {
        "family": "Myers",
        "given": "Andrew C."
      },
      {
        "family": "Campbell",
        "given": "Mark"
      }
    ],
    "title": "Secure Autonomous Cyber-Physical Systems Through Verifiable Information Flow Control",
    "container-title": "Proceedings of the 2018 Workshop on Cyber-Physical Systems Security and PrivaCy",
    "collection-title": "CPS-SPC '18",
    "issued": {
      "date-parts": [
        [
          "2018",
          "1",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "13"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-5992-4",
    "abstract": "Modern cyber-physical systems are complex networked computing systems that electronically control physical systems. Autonomous road vehicles are an important and increasingly ubiquitous instance. Unfortunately, their increasing complexity often leads to security vulnerabilities. Network connectivity exposes these vulnerable systems to remote software attacks that can result in real-world physical damage, including vehicle crashes and loss of control authority. We introduce an integrated architecture to provide provable security and safety assurance for cyber-physical systems by ensuring that safety-critical operations and control cannot be unintentionally affected by potentially malicious parts of the system. Fine-grained information flow control is used to design both hardware and software, determining how low-integrity information can affect high-integrity control decisions. This security assurance is used to improve end-to-end security across the entire cyber-physical system. We demonstrate this integrated approach by developing a mobile robotic testbed modeling a self-driving system and testing it with a malicious attack.",
    "URL": "https://doi.org/10.1145/3264888.3264889",
    "DOI": "10.1145/3264888.3264889",
    "publisher-place": "New York, NY, USA",
    "page": "48-59",
    "page-first": "48",
    "_line": "Networking.bib:808"
  },
  "oconnor_using_2019": {
    "id": "oconnor_using_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "O'Connor",
        "given": "B."
      },
      {
        "family": "Tseng",
        "given": "Y."
      },
      {
        "family": "Pudelko",
        "given": "M."
      },
      {
        "family": "Cascone",
        "given": "C."
      },
      {
        "family": "Endurthi",
        "given": "A."
      },
      {
        "family": "Wang",
        "given": "Y."
      },
      {
        "family": "Ghaffarkhah",
        "given": "A."
      },
      {
        "family": "Gopalpur",
        "given": "D."
      },
      {
        "family": "Everman",
        "given": "T."
      },
      {
        "family": "Madejski",
        "given": "T."
      },
      {
        "family": "Wanderer",
        "given": "J."
      },
      {
        "family": "Vahdat",
        "given": "A."
      }
    ],
    "title": "Using P4 on Fixed-Pipeline and Programmable Stratum Switches",
    "container-title": "2019 ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS)",
    "event-title": "2019 ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS)",
    "issued": {
      "date-parts": [
        [
          "2019",
          "9"
        ]
      ]
    },
    "abstract": "Stratum is an open source network operating system (NOS)that provides a common implementation of P4Runtime and OpenConfig interfaces for white box switches. This demonstration will show an SDN leaf-spine fabric of Stratum-enabled white box switches managed by the ONOS SDN controller. The switching chips (ASICs)and platforms will come from different vendors, but they will share a common P4-defined pipeline and set of OpenConfig models.",
    "keywords": "Software defined networking, Computer networks, gNMI, gNOI, Next generation networking, Open-Config, P4, P4Runtime, Stratum",
    "DOI": "10.1109/ANCS.2019.8901885",
    "page": "1-2",
    "page-first": "1",
    "_line": "Networking.bib:825"
  },
  "ruffy_gauntlet_nodate": {
    "id": "ruffy_gauntlet_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Ruffy",
        "given": "Fabian"
      },
      {
        "family": "Wang",
        "given": "Tao"
      },
      {
        "family": "Sivaraman",
        "given": "Anirudh"
      }
    ],
    "title": "Gauntlet: Finding Bugs in Compilers for Programmable Packet Processing",
    "abstract": "Programmable packet-processing devices such as programmable switches and network interface cards are becoming mainstream. These devices are conﬁgured in a domainspeciﬁc language such as P4, using a compiler to translate packet-processing programs into instructions for different targets. As networks with programmable devices become widespread, it is critical that these compilers be dependable. This paper considers the problem of ﬁnding bugs in compilers for packet processing in the context of P416. We introduce domain-speciﬁc techniques to induce both abnormal termination of the compiler (crash bugs) and miscompilation (semantic bugs). We apply these techniques to (1) the opensource P4 compiler (P4C) infrastructure, which serves as a common base for different P4 back ends; (2) the P4 back end for the P4 reference software switch; and (3) the P4 back end for the Barefoot Toﬁno switch.",
    "page": "17",
    "page-first": "17",
    "language": "en-US",
    "_line": "Networking.bib:838"
  },
  "noauthor_o-ran_nodate": {
    "id": "noauthor_o-ran_nodate",
    "type": "webpage",
    "title": "O-RAN ALLIANCE.O-RAN ALLIANCE",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "14"
        ]
      ]
    },
    "URL": "https://www.o-ran.org",
    "language": "en-US",
    "_line": "Networking.bib:847"
  },
  "jordan_ultimate_nodate": {
    "id": "jordan_ultimate_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Jordan",
        "given": "Eugina"
      }
    ],
    "title": "The Ultimate Guide to Open RAN: Open RAN Intelligent Controller (RIC) - Part 2: Implementations",
    "container-title-short": "The Ultimate Guide to Open RAN",
    "title-short": "The Ultimate Guide to Open RAN",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "14"
        ]
      ]
    },
    "abstract": "When designing next-gen RAN, there are three objectives that need be achieved simultaneously including operation efficiency, automation of configuration and",
    "URL": "https://www.thefastmode.com/expert-opinion/18274-the-ultimate-guide-to-open-ran-open-ran-intelligent-controller-ric-part-2-implementations",
    "language": "en-US",
    "_line": "Networking.bib:856"
  },
  "jordan_ultimate_nodate-1": {
    "id": "jordan_ultimate_nodate-1",
    "type": "webpage",
    "author": [
      {
        "family": "Jordan",
        "given": "Eugina"
      }
    ],
    "title": "The Ultimate Guide to Open RAN: Open RAN Intelligent Controller (RIC) - Part 1",
    "container-title-short": "The Ultimate Guide to Open RAN",
    "title-short": "The Ultimate Guide to Open RAN",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "14"
        ]
      ]
    },
    "abstract": "In 2G and 3G, the mobile architectures had controllers that were responsible for RAN orchestration and management. With 4G, overall network architecture",
    "URL": "https://www.thefastmode.com/expert-opinion/18213-the-ultimate-guide-to-open-ran-open-ran-intelligent-controller-ric-part-1",
    "language": "en-US",
    "_line": "Networking.bib:867"
  },
  "ahrendt_dione_2019": {
    "id": "ahrendt_dione_2019",
    "type": "chapter",
    "author": [
      {
        "family": "Hsieh",
        "given": "Chiao"
      },
      {
        "family": "Mitra",
        "given": "Sayan"
      }
    ],
    "editor": [
      {
        "family": "Ahrendt",
        "given": "Wolfgang"
      },
      {
        "family": "Tapia Tarifa",
        "given": "Silvia Lizeth"
      }
    ],
    "title": "Dione: A Protocol Verification System Built with Dafny for I/O Automata",
    "container-title": "Integrated Formal Methods",
    "container-title-short": "Dione",
    "title-short": "Dione",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "13"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-34967-7 978-3-030-34968-4",
    "abstract": "Input/Output Automata (IOA) is an expressive speciﬁcation framework with built-in properties for compositional reasoning. It has been shown to be eﬀective in specifying and analyzing distributed and networked systems. The available veriﬁcation engines for IOA are based on interactive theorem provers such as Isabelle, Larch, PVS, and Coq, and are expressive but require heavy human interaction. Motivated by the advances in SMT solvers, in this work we explore a diﬀerent expressivity-automation tradeoﬀ for IOA. We present Dione, the ﬁrst IOA analysis system built with Dafny and its SMT-powered toolchain and demonstrate its eﬀectiveness on four distributed applications. Our translator tool converts Python-esque Dione language speciﬁcation of IOA and their properties to parameterized Dafny modules. Dione automatically generates the relevant compatibility and composition lemmas for the IOA speciﬁcations, which can then be checked with Dafny on a per module-basis. We ensure that all resulting formulas are expressed mostly in fragments solvable by SMT solvers and hence enables Bounded Model Checking and k-induction-based invariant checking using Z3. We present successful applications of Dione in veriﬁcation of an asynchronous leader election algorithm, two self-stabilizing mutual exclusion algorithms, and CAN bus Arbitration. We automatically prove key invariants of all four protocols; for the last three this involves reasoning about arbitrary number of participants. These analyses are largely automatic with minimal manual inputs needed, and they demonstrate the eﬀectiveness of this approach in analyzing networked and distributed systems.",
    "URL": "http://link.springer.com/10.1007/978-3-030-34968-4_13",
    "DOI": "10.1007/978-3-030-34968-4_13",
    "publisher-place": "Cham",
    "page": "227-245",
    "page-first": "227",
    "volume": "11918",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "Networking.bib:878"
  },
  "hsieh_dione_2019": {
    "id": "hsieh_dione_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Hsieh",
        "given": "Chiao"
      },
      {
        "family": "Mitra",
        "given": "Sayan"
      }
    ],
    "editor": [
      {
        "family": "Ahrendt",
        "given": "Wolfgang"
      },
      {
        "family": "Tapia Tarifa",
        "given": "Silvia Lizeth"
      }
    ],
    "title": "Dione: A Protocol Verification System Built with Dafny for I/O Automata",
    "container-title": "Integrated Formal Methods",
    "container-title-short": "Dione",
    "collection-title": "Lecture Notes in Computer Science",
    "title-short": "Dione",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-34968-4",
    "abstract": "Input/Output Automata (IOA) is an expressive specification framework with built-in properties for compositional reasoning. It has been shown to be effective in specifying and analyzing distributed and networked systems. The available verification engines for IOA are based on interactive theorem provers such as Isabelle, Larch, PVS, and Coq, and are expressive but require heavy human interaction. Motivated by the advances in SMT solvers, in this work we explore a different expressivity-automation tradeoff for IOA. We present Dione, the first IOA analysis system built with Dafny and its SMT-powered toolchain and demonstrate its effectiveness on four distributed applications. Our translator tool converts Python-esque Dione language specification of IOA and their properties to parameterized Dafny modules. Dione automatically generates the relevant compatibility and composition lemmas for the IOA specifications,which can then be checked with Dafny on a per module-basis. We ensure that all resulting formulas are expressed mostly in fragments solvable by SMT solvers and hence enables Bounded Model Checking and k-induction-based invariant checking using Z3. We present successful applications of Dione in verification of an asynchronous leader election algorithm, two self-stabilizing mutual exclusion algorithms, and CAN bus Arbitration. We automatically prove key invariants of all four protocols; for the last three this involves reasoning about arbitrary number of participants. These analyses are largely automatic with minimal manual inputs needed, and they demonstrate the effectiveness of this approach in analyzing networked and distributed systems.",
    "DOI": "10.1007/978-3-030-34968-4_13",
    "publisher-place": "Cham",
    "page": "227-245",
    "page-first": "227",
    "language": "en-US",
    "_line": "Networking.bib:899"
  },
  "bjorner_formal_2015": {
    "id": "bjorner_formal_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Bjorner",
        "given": "Nikolaj"
      },
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Godfrey",
        "given": "Philip Brighten"
      },
      {
        "family": "Zave",
        "given": "Pamela"
      }
    ],
    "editor": [
      {
        "family": "Bjorner",
        "given": "Nikolaj"
      },
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Godfrey",
        "given": "Philip Brighten"
      },
      {
        "family": "Zave",
        "given": "Pamela"
      }
    ],
    "title": "Formal Foundations for Networking (Dagstuhl Seminar 15071)",
    "container-title": "Dagstuhl Reports",
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "12"
        ]
      ]
    },
    "issn": "2192-5283",
    "keywords": "logic, model checking, security, software-defined networking, verification, Formal methods, middleboxes, networking, program synthesis",
    "URL": "http://drops.dagstuhl.de/opus/volltexte/2015/5044",
    "DOI": "10.4230/DagRep.5.2.44",
    "page": "44-63",
    "page-first": "44",
    "volume": "5",
    "note": "Place: Dagstuhl, Germany\nPublisher: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik",
    "issue": "2",
    "_line": "Networking.bib:930"
  },
  "stoenescu_debugging_2018": {
    "id": "stoenescu_debugging_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Stoenescu",
        "given": "Radu"
      },
      {
        "family": "Dumitrescu",
        "given": "Dragos"
      },
      {
        "family": "Popovici",
        "given": "Matei"
      },
      {
        "family": "Negreanu",
        "given": "Lorina"
      },
      {
        "family": "Raiciu",
        "given": "Costin"
      }
    ],
    "title": "Debugging P4 programs with vera",
    "container-title": "Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication",
    "collection-title": "SIGCOMM '18",
    "issued": {
      "date-parts": [
        [
          "2018",
          "8",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "12"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-5567-4",
    "abstract": "We present Vera, a tool that verifies P4 programs using symbolic execution. Vera automatically uncovers a number of common bugs including parsing/deparsing errors, invalid memory accesses, loops and tunneling errors, among others. Vera can also be used to verify user-specified properties in a novel language we call NetCTL. To enable scalable, exhaustive verification of P4 program snapshots, Vera automatically generates all valid header layouts and uses a novel data-structure for match-action processing optimized for verification. These techniques allow Vera to scale very well: it only takes between 5s-15s to track the execution of a purely symbolic packet in the largest P4 program currently available (6KLOC) and can compute SEFL model updates in milliseconds. Vera can also explore multiple concrete dataplanes at once by allowing the programmer to insert symbolic table entries; the resulting verification highlights possible control plane errors. We have used Vera to analyze many P4 programs including the P4 tutorials, P4 programs in the research literature and the switch code from https://p4.org. Vera has found several bugs in each of them in seconds/minutes.",
    "URL": "https://doi.org/10.1145/3230543.3230548",
    "DOI": "10.1145/3230543.3230548",
    "publisher-place": "New York, NY, USA",
    "page": "518-532",
    "page-first": "518",
    "_line": "Networking.bib:949"
  },
  "doenges_petr4_2020": {
    "id": "doenges_petr4_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Doenges",
        "given": "Ryan"
      },
      {
        "family": "Arashloo",
        "given": "Mina Tahmasbi"
      },
      {
        "family": "Bautista",
        "given": "Santiago"
      },
      {
        "family": "Chang",
        "given": "Alexander"
      },
      {
        "family": "Ni",
        "given": "Newton"
      },
      {
        "family": "Parkinson",
        "given": "Samwise"
      },
      {
        "family": "Peterson",
        "given": "Rudy"
      },
      {
        "family": "Solko-Breslin",
        "given": "Alaia"
      },
      {
        "family": "Xu",
        "given": "Amanda"
      },
      {
        "family": "Foster",
        "given": "Nate"
      }
    ],
    "title": "Petr4: Formal Foundations for P4 Data Planes",
    "container-title": "arXiv:2011.05948 \\[cs\\]",
    "container-title-short": "Petr4",
    "title-short": "Petr4",
    "issued": {
      "date-parts": [
        [
          "2020",
          "11",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "1",
          "12"
        ]
      ]
    },
    "abstract": "P4 is a domain-specific language for programming and specifying packet-processing systems. It is based on an elegant design with high-level abstractions like parsers and match-action pipelines that can be compiled to efficient implementations in software or hardware. Unfortunately, like many industrial languages, P4 has developed without a formal foundation. The P4 Language Specification is a 160-page document with a mixture of informal prose, graphical diagrams, and pseudocode. The P4 reference implementation is a complex system, running to over 40KLoC of C++ code. Clearly neither of these artifacts is suitable for formal reasoning. This paper presents a new framework, called Petr4, that puts P4 on a solid foundation. Petr4 consists of a clean-slate definitional interpreter and a calculus that models the semantics of a core fragment of P4. Throughout the specification, some aspects of program behavior are left up to targets. Our interpreter is parameterized over a target interface which collects all the target-specific behavior in the specification in a single interface. The specification makes ad-hoc restrictions on the nesting of certain program constructs in order to simplify compilation and avoid the possibility of nonterminating programs. We captured the latter intention in our core calculus by stratifying its type system, rather than imposing unnatural syntactic restrictions, and we proved that all programs in this core calculus terminate. We have validated the interpreter against a suite of over 750 tests from the P4 reference implementation, exercising our target interface with tests for different targets. We established termination for the core calculus by induction on the stratified type system. While developing Petr4, we reported dozens of bugs in the language specification and the reference implementation, many of which have been fixed.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Networking and Internet Architecture",
    "URLtext": "2011.05948",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2011.05948",
    "URL": "http://arxiv.org/abs/2011.05948",
    "_line": "Networking.bib:966"
  },
  "campbell_avenir_nodate": {
    "id": "campbell_avenir_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Campbell",
        "given": "Eric Hayden"
      },
      {
        "family": "Ramamurthy",
        "given": "Vignesh"
      },
      {
        "family": "Hallahan",
        "given": "William T"
      },
      {
        "family": "Srikumar",
        "given": "Priya"
      },
      {
        "family": "Cascone",
        "given": "Carmelo"
      },
      {
        "family": "Liu",
        "given": "Jed"
      },
      {
        "family": "Hojjat",
        "given": "Hossein"
      },
      {
        "family": "Piskac",
        "given": "Ruzica"
      },
      {
        "family": "Soulé",
        "given": "Robert"
      },
      {
        "family": "Foster",
        "given": "Nate"
      }
    ],
    "title": "Avenir: Managing Data Plane Diversity with Control Plane Synthesis",
    "abstract": "The classical conception of software-deﬁned networking (SDN) is based on an attractive myth: a logically centralized controller manages a collection of homogeneous data planes. In reality, however, SDN control planes must deal with signiﬁcant diversity in hardware, drivers, interfaces, and protocols, all of which contribute to idiosyncratic differences in forwarding behavior that must be dealt with by hand.",
    "page": "21",
    "page-first": "21",
    "language": "en-US",
    "_line": "Networking.bib:999"
  },
  "shi_network_2021": {
    "id": "shi_network_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Shi",
        "given": "Lei"
      },
      {
        "family": "Li",
        "given": "Yahui"
      },
      {
        "family": "Loo",
        "given": "Boon Thau"
      },
      {
        "family": "Alur",
        "given": "Rajeev"
      }
    ],
    "title": "Network Traﬃc Classiﬁcation by Program Synthesis",
    "container-title": "TACAS21",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Writing classiﬁcation rules to identify interesting network traﬃc is a time-consuming and error-prone task. Learning-based classiﬁcation systems automatically extract such rules from positive and negative traﬃc examples. However, due to limitations in the representation of network traﬃc and the learning strategy, these systems lack both expressiveness to cover a range of applications and interpretability in fully describing the traﬃc’s structure at the session layer. This paper presents Sharingan system, which uses program synthesis techniques to generate network classiﬁcation programs at the session layer. Sharingan accepts raw network traces as inputs and reports potential patterns of the target traﬃc in NetQRE, a domain speciﬁc language designed for specifying session-layer quantitative properties. We develop a range of novel optimizations that reduce the synthesis time for large and complex tasks to a matter of minutes. Our experiments show that Sharingan is able to correctly identify patterns from a diverse set of network traces and generates explainable outputs, while achieving accuracy comparable to state-of-the-art learning-based systems.",
    "page": "19",
    "page-first": "19",
    "language": "en-US",
    "_line": "Networking.bib:1008"
  },
  "eichholz_dependently-typed_nodate": {
    "id": "eichholz_dependently-typed_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Eichholz",
        "given": "Matthias"
      }
    ],
    "title": "Dependently-Typed Data Plane Programming",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "7"
        ]
      ]
    },
    "URL": "https://ericthewry.github.io/pdfs/pi4.pdf",
    "_line": "Networking.bib:1019"
  },
  "eichholz_dependently-typed_2022": {
    "id": "eichholz_dependently-typed_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Eichholz",
        "given": "Matthias"
      },
      {
        "family": "Campbell",
        "given": "Eric Hayden"
      },
      {
        "family": "Krebs",
        "given": "Matthias"
      },
      {
        "family": "Foster",
        "given": "Nate"
      },
      {
        "family": "Mezini",
        "given": "Mira"
      }
    ],
    "title": "Dependently-typed data plane programming",
    "container-title": "Proceedings of the ACM on Programming Languages",
    "container-title-short": "Proc. ACM Program. Lang.",
    "issued": {
      "date-parts": [
        [
          "2022",
          "1",
          "11"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "21"
        ]
      ]
    },
    "abstract": "Programming languages like P4 enable specifying the behavior of network data planes in software. However, with increasingly powerful and complex applications running in the network, the risk of faults also increases. Hence, there is growing recognition of the need for methods and tools to statically verify the correctness of P4 code, especially as the language lacks basic safety guarantees. Type systems are a lightweight and compositional way to establish program properties, but there is a significant gap between the kinds of properties that can be proved using simple type systems (e.g., SafeP4) and those that can be obtained using full-blown verification tools (e.g., p4v). In this paper, we close this gap by developing Π4, a dependently-typed version of P4 based on decidable refinements. We motivate the design of Π4, prove the soundness of its type system, develop an SMT-based implementation, and present case studies that illustrate its applicability to a variety of data plane programs.",
    "keywords": "Dependent Types, Software-Defined Networking, P4",
    "URL": "https://doi.org/10.1145/3498701",
    "DOI": "10.1145/3498701",
    "page": "40:1-40:28",
    "page-first": "40",
    "volume": "6",
    "_line": "Networking.bib:1027"
  },
  "raschke_flashix_2021": {
    "id": "raschke_flashix_2021",
    "type": "chapter",
    "author": [
      {
        "family": "Bodenmüller",
        "given": "Stefan"
      },
      {
        "family": "Schellhorn",
        "given": "Gerhard"
      },
      {
        "family": "Bitterlich",
        "given": "Martin"
      },
      {
        "family": "Reif",
        "given": "Wolfgang"
      }
    ],
    "editor": [
      {
        "family": "Raschke",
        "given": "Alexander"
      },
      {
        "family": "Riccobene",
        "given": "Elvinia"
      },
      {
        "family": "Schewe",
        "given": "Klaus-Dieter"
      }
    ],
    "title": "Flashix: Modular Verification of a Concurrent and Crash-Safe Flash File System",
    "container-title": "Logic, Computation and Rigorous Methods",
    "container-title-short": "Flashix",
    "title-short": "Flashix",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "21"
        ]
      ]
    },
    "publisher": "Springer International Publishing",
    "isbn": "978-3-030-76019-9 978-3-030-76020-5",
    "abstract": "The Flashix project has developed the ﬁrst realistic veriﬁed ﬁle system for Flash memory. This paper gives an overview over the project and the theory used. Speciﬁcation is based on modular components and subcomponents, which may have concurrent implementations connected via reﬁnement. Functional correctness and crash-safety of each component is veriﬁed separately. We highlight some components that were recently added to improve eﬃciency, such as ﬁle caches and concurrent garbage collection. The project generates 18K of C code that runs under Linux. We evaluate how eﬃciency has improved and compare to UBIFS, the most recent ﬂash ﬁle system implementation available for the Linux kernel.",
    "URL": "https://link.springer.com/10.1007/978-3-030-76020-5_14",
    "DOI": "10.1007/978-3-030-76020-5_14",
    "publisher-place": "Cham",
    "page": "239-265",
    "page-first": "239",
    "volume": "12750",
    "note": "Series Title: Lecture Notes in Computer Science",
    "language": "en-US",
    "_line": "OperatingSystems.bib:2"
  },
  "li_incremental_2021": {
    "id": "li_incremental_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "Jialin"
      },
      {
        "family": "Miller",
        "given": "Samantha"
      },
      {
        "family": "Zhuo",
        "given": "Danyang"
      },
      {
        "family": "Chen",
        "given": "Ang"
      },
      {
        "family": "Howell",
        "given": "Jon"
      },
      {
        "family": "Anderson",
        "given": "Thomas"
      }
    ],
    "title": "An Incremental Path Towards a Safer OS Kernel",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "Linux has become the de-facto operating system of our age, but its vulnerabilities are a constant threat to service availability, user privacy, and data integrity. While one might scrap Linux and start over, the cost of that would be prohibitive due to Linux’s ubiquitous deployment. In this paper, we propose an alternative, incremental route to a safer Linux through proper modularization and gradual replacement module by module. We lay out the research challenges and potential solutions for this route, and discuss the open questions ahead. CCS Concepts • Software and its engineering → Software verification; • Computer systems organization → Reliability.",
    "page": "8",
    "page-first": "8",
    "language": "en-US",
    "_line": "OperatingSystems.bib:23"
  },
  "labrosse_implementing_1992": {
    "id": "labrosse_implementing_1992",
    "type": "article-journal",
    "author": [
      {
        "family": "Labrosse",
        "given": "Jean"
      }
    ],
    "title": "Implementing a Real-Time Kernel, 6/92",
    "issued": {
      "date-parts": [
        [
          "1992",
          "6"
        ]
      ]
    },
    "page": "6",
    "page-first": "6",
    "language": "en-US",
    "_line": "OperatingSystems.bib:33"
  },
  "labrosse_portable_1992": {
    "id": "labrosse_portable_1992",
    "type": "article-journal",
    "author": [
      {
        "family": "Labrosse",
        "given": "Jean"
      }
    ],
    "title": "A Portable Real-Time Kernel in C, 5/92",
    "container-title": "Embedded Systems Programming",
    "issued": {
      "date-parts": [
        [
          "1992",
          "5"
        ]
      ]
    },
    "page": "12",
    "page-first": "12",
    "language": "en-US",
    "_line": "OperatingSystems.bib:42"
  },
  "baccelli_riot_2018": {
    "id": "baccelli_riot_2018",
    "type": "article-journal",
    "author": [
      {
        "family": "Baccelli",
        "given": "Emmanuel"
      },
      {
        "family": "Gundogan",
        "given": "Cenk"
      },
      {
        "family": "Hahm",
        "given": "Oliver"
      },
      {
        "family": "Kietzmann",
        "given": "Peter"
      },
      {
        "family": "Lenders",
        "given": "Martine S."
      },
      {
        "family": "Petersen",
        "given": "Hauke"
      },
      {
        "family": "Schleiser",
        "given": "Kaspar"
      },
      {
        "family": "Schmidt",
        "given": "Thomas C."
      },
      {
        "family": "Wahlisch",
        "given": "Matthias"
      }
    ],
    "title": "RIOT: An Open Source Operating System for Low-End Embedded Devices in the IoT",
    "container-title": "IEEE Internet of Things Journal",
    "container-title-short": "RIOT",
    "title-short": "RIOT",
    "issued": {
      "date-parts": [
        [
          "2018",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "17"
        ]
      ]
    },
    "issn": "2327-4662, 2372-2541",
    "abstract": "As the Internet of Things (IoT) emerges, compact operating systems are required on low-end devices to ease development and portability of IoT applications. RIOT is a prominent free and open source operating system in this space. In this paper, we provide the ﬁrst comprehensive overview of RIOT. We cover the key components of interest to potential developers and users: the kernel, hardware abstraction, and software modularity, both conceptually and in practice for various example conﬁgurations. We explain operational aspects like system boot-up, timers, power management, and the use of networking. Finally, the relevant APIs as exposed by the operating system are discussed along with the larger ecosystem around RIOT, including development and open source community aspects.",
    "URL": "https://ieeexplore.ieee.org/document/8315125/",
    "DOI": "10.1109/JIOT.2018.2815038",
    "page": "4428-4440",
    "page-first": "4428",
    "volume": "5",
    "issue": "6",
    "language": "en-US",
    "_line": "OperatingSystems.bib:52"
  },
  "lehmann_storm_2021": {
    "id": "lehmann_storm_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Lehmann",
        "given": "Nico"
      },
      {
        "family": "Kunkel",
        "given": "Rose"
      },
      {
        "family": "Brown",
        "given": "Jordan"
      },
      {
        "family": "Yang",
        "given": "Jean"
      },
      {
        "family": "Vazou",
        "given": "Niki"
      },
      {
        "family": "Polikarpova",
        "given": "Nadia"
      },
      {
        "family": "Stefan",
        "given": "Deian"
      },
      {
        "family": "Jhala",
        "given": "Ranjit"
      }
    ],
    "title": "STORM: Reﬁnement Types for Secure Web Applications",
    "container-title": "Proceedings of Symposium on Operating Systems Design and Implementation (OSDI)",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7"
        ]
      ]
    },
    "abstract": "We present STORM, a web framework that allows developers to build MVC applications with compile-time enforcement of centrally speciﬁed data-dependent security policies. STORM ensures security using a Security Typed ORM that reﬁnes the (type) abstractions of each layer of the MVC API with logical assertions that describe the data produced and consumed by the underlying operation and the users allowed access to that data. To evaluate the security guarantees of STORM, we build a formally veriﬁed reference implementation using the Labeled IO (LIO) IFC framework. We present case studies and end-toend applications that show how STORM lets developers specify diverse policies while centralizing the trusted code to under 1&perc; of the application, and statically enforces security with modest type annotation overhead, and no run-time cost.",
    "page": "19",
    "page-first": "19",
    "language": "en-US",
    "_line": "Security.bib:2"
  },
  "algehed_dynamic_nodate": {
    "id": "algehed_dynamic_nodate",
    "type": "paper-conference",
    "author": [
      {
        "family": "Algehed",
        "given": "Maximilian"
      },
      {
        "family": "Bernardy",
        "given": "Jean-Philippe"
      },
      {
        "family": "Hritcu",
        "given": "Catalin"
      }
    ],
    "title": "Dynamic IFC Theorems for Free!",
    "event-title": "2021 IEEE 34th Computer Security Foundations Symposium (CSF)",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "11"
        ]
      ]
    },
    "publisher": "IEEE Computer Society",
    "isbn": "978-1-72817-607-9",
    "abstract": "We show that noninterference and transparency, \nthe key soundness theorems for dynamic IFC libraries, can be \nobtained “for free”, as direct consequences of the more general \nparametricity theorem of type abstraction. This allows us to give \nvery short soundness proofs for dynamic IFC libraries such as \nfaceted values and LIO. Our proofs stay short even when fully \nmechanized for Agda implementations of the libraries in terms of type abstraction.",
    "URL": "https://www.computer.org/csdl/proceedings-article/csf/2021/760700a001/1pmB5edaFk4",
    "DOI": "10.1109/CSF51468.2021.00005",
    "page": "1-14",
    "page-first": "1",
    "note": "ISSN: 2374-8303",
    "_line": "Security.bib:13"
  },
  "tsampas_capableptrs_2021": {
    "id": "tsampas_capableptrs_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Tsampas",
        "given": "Akram El-Korashy Stelios"
      },
      {
        "family": "Patrignani",
        "given": "Marco"
      },
      {
        "family": "Devriese",
        "given": "Dominique"
      },
      {
        "family": "Piessens",
        "given": "Deepak Garg Frank"
      }
    ],
    "title": "CapablePtrs: Securely Compiling Partial Programs Using the Pointers-as-Capabilities Principle",
    "event-title": "34th IEEE Computer Security Foundations Symposium",
    "issued": {
      "date-parts": [
        [
          "2021",
          "6",
          "21"
        ]
      ]
    },
    "abstract": "Capability machines such as CHERI provide mem- to implement ﬁne-grained memory protection, and has the ory capabilities that can be used by compilers to provide security beneﬁts for compiled code (e.g., memory safety). The existing C to CHERI compiler, for example, achieves memory safety by following a principle called “pointers as capabilities” (PAC ). Informally, PAC says that a compiler should represent a source potential to provide protection against many software bugs.",
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "Security.bib:34"
  },
  "busi_brief_2019": {
    "id": "busi_brief_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Busi",
        "given": "Matteo"
      },
      {
        "family": "Galletta",
        "given": "Letterio"
      }
    ],
    "title": "A Brief Tour of Formally Secure Compilation",
    "container-title": "Proceedings of the Third Italian Conference on Cyber Security",
    "event-title": "Third Italian Conference on Cyber Security",
    "issued": {
      "date-parts": [
        [
          "2019",
          "2",
          "13"
        ]
      ]
    },
    "publisher": "CEUR-WS.org",
    "abstract": "Modern programming languages provide helpful high-level abstractions and mechanisms (e.g. types, module, automatic memory management) that enforce good programming practices and are crucial when writing correct and secure code. However, the security guarantees provided by such abstractions are not preserved when a compiler translates a source program into object code. Formally secure compilation is an emerging research ﬁeld concerned with the design and the implementation of compilers that preserve source-level security properties at the object level. This paper presents a short guided tour of the relevant literature on secure compilation. Our goal is to help newcomers to grasp the basic concepts of this ﬁeld and, for this reason, we rephrase and present the most relevant results in the literature in a common setting.",
    "URL": "http://ceur-ws.org/Vol-2315/paper03.pdf",
    "publisher-place": "Pisa, Italy",
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "Security.bib:45"
  },
  "clarkson_hyperproperties_2008": {
    "id": "clarkson_hyperproperties_2008",
    "type": "paper-conference",
    "author": [
      {
        "family": "Clarkson",
        "given": "Michael R."
      },
      {
        "family": "Schneider",
        "given": "Fred B."
      }
    ],
    "title": "Hyperproperties",
    "container-title": "2008 21st IEEE Computer Security Foundations Symposium",
    "event-title": "2008 21st IEEE Computer Security Foundations Symposium",
    "issued": {
      "date-parts": [
        [
          "2008",
          "6"
        ]
      ]
    },
    "abstract": "Properties, which have long been used for reasoning about systems, are sets of traces. Hyperproperties, introduced here, are sets of properties. Hyperproperties can express security policies, such as secure information flow, that properties cannot. Safety and liveness are generalized to hyperproperties, and every hyperproperty is shown to be the intersection of a safety hyperproperty and a liveness hyperproperty. A verification technique for safety hyperproperties is given and is shown to generalize prior techniques for verifying secure information flow. Refinement is shown to be valid for safety hyperproperties. A topological characterization of hyperproperties is given.",
    "keywords": "Computer science, Computer security, Delay effects, Information security, liveness, safety, Safety, Security policies, Topology, Writing",
    "DOI": "10.1109/CSF.2008.7",
    "page": "51-65",
    "page-first": "51",
    "note": "ISSN: 2377-5459",
    "_line": "Security.bib:60"
  },
  "juglaret_towards_2015": {
    "id": "juglaret_towards_2015",
    "type": "article-journal",
    "author": [
      {
        "family": "Juglaret",
        "given": "Yannis"
      },
      {
        "family": "Hritcu",
        "given": "Catalin"
      },
      {
        "family": "Amorim",
        "given": "Arthur Azevedo",
        "dropping-particle": "de"
      },
      {
        "family": "Pierce",
        "given": "Benjamin C."
      },
      {
        "family": "Spector-Zabusky",
        "given": "Antal"
      },
      {
        "family": "Tolmach",
        "given": "Andrew"
      }
    ],
    "title": "Towards a Fully Abstract Compiler Using Micro-Policies: Secure Compilation for Mutually Distrustful Components",
    "container-title": "arXiv:1510.00697 \\[cs\\]",
    "container-title-short": "Towards a Fully Abstract Compiler Using Micro-Policies",
    "title-short": "Towards a Fully Abstract Compiler Using Micro-Policies",
    "issued": {
      "date-parts": [
        [
          "2015",
          "10",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "7"
        ]
      ]
    },
    "abstract": "Secure compilation prevents all low-level attacks on compiled code and allows for sound reasoning about security in the source language. In this work we propose a new attacker model for secure compilation that extends the well-known notion of full abstraction to ensure protection for mutually distrustful components. We devise a compiler chain (compiler, linker, and loader) and a novel security monitor that together defend against this strong attacker model. The monitor is implemented using a recently proposed, generic tag-based protection framework called micro-policies, which comes with hardware support for efficient caching and with a formal verification methodology. Our monitor protects the abstractions of a simple object-oriented language&mdash;class isolation, the method call discipline, and type safety&mdash;against arbitrary low-level attackers.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages",
    "URLtext": "1510.00697",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1510.00697",
    "URL": "http://arxiv.org/abs/1510.00697",
    "_line": "Security.bib:74"
  },
  "abate_journey_2019": {
    "id": "abate_journey_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Abate",
        "given": "Carmine"
      },
      {
        "family": "Blanco",
        "given": "Roberto"
      },
      {
        "family": "Garg",
        "given": "Deepak"
      },
      {
        "family": "Hritcu",
        "given": "Catalin"
      },
      {
        "family": "Patrignani",
        "given": "Marco"
      },
      {
        "family": "Thibault",
        "given": "Jérémy"
      }
    ],
    "title": "Journey Beyond Full Abstraction: Exploring Robust Property Preservation for Secure Compilation",
    "container-title": "2019 IEEE 32nd Computer Security Foundations Symposium (CSF)",
    "container-title-short": "Journey Beyond Full Abstraction",
    "title-short": "Journey Beyond Full Abstraction",
    "issued": {
      "date-parts": [
        [
          "2019",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "7"
        ]
      ]
    },
    "abstract": "(CROPPED TO FIT IN ARXIV'S SILLY LIMIT. SEE PDF FOR COMPLETE ABSTRACT.) We are the first to thoroughly explore a large space of formal secure compilation criteria based on robust property preservation, i.e., the preservation of properties satisfied against arbitrary adversarial contexts. We study robustly preserving various classes of trace properties such as safety, of hyperproperties such as noninterference, and of relational hyperproperties such as trace equivalence. This leads to many new secure compilation criteria, some of which are easier to practically achieve and prove than full abstraction, and some of which provide strictly stronger security guarantees. For each of the studied criteria we propose an equivalent \"property-free\" characterization that clarifies which proof techniques apply. For relational properties and hyperproperties, which relate the behaviors of multiple programs, our formal definitions of the property classes themselves are novel. We order our criteria by their relative strength and show several collapses and separation results. Finally, we adapt existing proof techniques to show that even the strongest of our secure compilation criteria, the robust preservation of all relational hyperproperties, is achievable for a simple translation from a statically typed to a dynamically typed language.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages",
    "URLtext": "1807.04603",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1807.04603",
    "URL": "http://arxiv.org/abs/1807.04603",
    "DOI": "10.1109/CSF.2019.00025",
    "page": "256-25615",
    "page-first": "256",
    "_line": "Security.bib:89"
  },
  "abate_when_2019": {
    "id": "abate_when_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Abate",
        "given": "Carmine"
      },
      {
        "family": "Amorim",
        "given": "Arthur Azevedo",
        "dropping-particle": "de"
      },
      {
        "family": "Blanco",
        "given": "Roberto"
      },
      {
        "family": "Evans",
        "given": "Ana Nora"
      },
      {
        "family": "Fachini",
        "given": "Guglielmo"
      },
      {
        "family": "Hritcu",
        "given": "Catalin"
      },
      {
        "family": "Laurent",
        "given": "Théo"
      },
      {
        "family": "Pierce",
        "given": "Benjamin C."
      },
      {
        "family": "Stronati",
        "given": "Marco"
      },
      {
        "family": "Thibault",
        "given": "Jérémy"
      },
      {
        "family": "Tolmach",
        "given": "Andrew"
      }
    ],
    "title": "When Good Components Go Bad: Formally Secure Compilation Despite Dynamic Compromise",
    "container-title": "arXiv:1802.00588 \\[cs\\]",
    "container-title-short": "When Good Components Go Bad",
    "title-short": "When Good Components Go Bad",
    "issued": {
      "date-parts": [
        [
          "2019",
          "11",
          "29"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "7"
        ]
      ]
    },
    "abstract": "We propose a new formal criterion for evaluating secure compilation schemes for unsafe languages, expressing end-to-end security guarantees for software components that may become compromised after encountering undefined behavior&mdash;for example, by accessing an array out of bounds. Our criterion is the first to model dynamic compromise in a system of mutually distrustful components with clearly specified privileges. It articulates how each component should be protected from all the others&mdash;in particular, from components that have encountered undefined behavior and become compromised. Each component receives secure compilation guarantees&mdash;in particular, its internal invariants are protected from compromised components&mdash;up to the point when this component itself becomes compromised, after which we assume an attacker can take complete control and use this component's privileges to attack other components. More precisely, a secure compilation chain must ensure that a dynamically compromised component cannot break the safety properties of the system at the target level any more than an arbitrary attacker-controlled component (with the same interface and privileges, but without undefined behaviors) already could at the source level. To illustrate the model, we construct a secure compilation chain for a small unsafe language with buffers, procedures, and components, targeting a simple abstract machine with built-in compartmentalization. We give a machine-checked proof in Coq that this compiler satisfies our secure compilation criterion. Finally, we show that the protection guarantees offered by the compartmentalized abstract machine can be achieved at the machine-code level using either software fault isolation or a tag-based reference monitor.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages",
    "URLtext": "1802.00588",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1802.00588",
    "URL": "http://arxiv.org/abs/1802.00588",
    "_line": "Security.bib:106"
  },
  "juglaret_beyond_2017": {
    "id": "juglaret_beyond_2017",
    "type": "article-journal",
    "author": [
      {
        "family": "Juglaret",
        "given": "Yannis"
      },
      {
        "family": "Hritcu",
        "given": "Catalin"
      },
      {
        "family": "Amorim",
        "given": "Arthur Azevedo",
        "dropping-particle": "de"
      },
      {
        "family": "Eng",
        "given": "Boris"
      },
      {
        "family": "Pierce",
        "given": "Benjamin C."
      }
    ],
    "title": "Beyond Good and Evil: Formalizing the Security Guarantees of Compartmentalizing Compilation",
    "container-title": "arXiv:1602.04503 \\[cs\\]",
    "container-title-short": "Beyond Good and Evil",
    "title-short": "Beyond Good and Evil",
    "issued": {
      "date-parts": [
        [
          "2017",
          "4",
          "15"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "6",
          "7"
        ]
      ]
    },
    "abstract": "Compartmentalization is good security-engineering practice. By breaking a large software system into mutually distrustful components that run with minimal privileges, restricting their interactions to conform to well-defined interfaces, we can limit the damage caused by low-level attacks such as control-flow hijacking. When used to defend against such attacks, compartmentalization is often implemented cooperatively by a compiler and a low-level compartmentalization mechanism. However, the formal guarantees provided by such compartmentalizing compilation have seen surprisingly little investigation. We propose a new security property, secure compartmentalizing compilation (SCC), that formally characterizes the guarantees provided by compartmentalizing compilation and clarifies its attacker model. We reconstruct our property by starting from the well-established notion of fully abstract compilation, then identifying and lifting three important limitations that make standard full abstraction unsuitable for compartmentalization. The connection to full abstraction allows us to prove SCC by adapting established proof techniques; we illustrate this with a compiler from a simple unsafe imperative language with procedures to a compartmentalized abstract machine.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages",
    "URLtext": "1602.04503",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1602.04503",
    "URL": "http://arxiv.org/abs/1602.04503",
    "_line": "Security.bib:121"
  },
  "oak_language_nodate": {
    "id": "oak_language_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Oak",
        "given": "Aditya"
      },
      {
        "family": "Ahmadian",
        "given": "Amir M"
      },
      {
        "family": "Balliu",
        "given": "Musard"
      },
      {
        "family": "Salvaneschi",
        "given": "Guido"
      }
    ],
    "title": "Language Support for Secure Software Development with Enclaves",
    "abstract": "Conﬁdential computing is a promising technology for securing code and data-in-use on untrusted host machines, e.g., the cloud. Many hardware vendors offer different implementations of Trusted Execution Environments (TEEs). A TEE is a hardware protected execution environment that allows performing conﬁdential computations over sensitive data on untrusted hosts. Despite the appeal of achieving strong security guarantees against low-level attackers, two challenges hinder the adoption of TEEs. First, developing software in high-level managed languages, e.g., Java or Scala, taking advantage of existing TEEs is complex and error-prone. Second, partitioning an application into components that run inside and outside a TEE may break application-level security policies, resulting in an insecure application when facing a realistic attacker.",
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "Security.bib:150"
  },
  "plotkin_lcf_1977": {
    "id": "plotkin_lcf_1977",
    "type": "article-journal",
    "author": [
      {
        "family": "Plotkin",
        "given": "G. D."
      }
    ],
    "title": "LCF considered as a programming language",
    "container-title": "Theoretical Computer Science",
    "container-title-short": "Theoretical Computer Science",
    "issued": {
      "date-parts": [
        [
          "1977",
          "12",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "28"
        ]
      ]
    },
    "issn": "0304-3975",
    "abstract": "The paper studies connections between denotational and operational semantics for a simple programming language based on LCF. It begins with the connection between the behaviour of a program and its denotation. It turns out that a program denotes ⊥ in any of several possible semantics if it does not terminate. From this it follows that if two terms have the same denotation in one of these semantics, they have the same behaviour in all contexts. The converse fails for all the semantics. If, however, the language is extended to allow certain parallel facilities behavioural equivalence does coincide with denotational equivalence in one of the semantics considered, which may therefore be called “fully abstract”. Next a connection is given which actually determines the semantics up to isomorphism from the behaviour alone. Conversely, by allowing further parallel facilities, every r.e. element of the fully abstract semantics becomes definable, thus characterising the programming language, up to interdefinability, from the set of r.e. elements of the domains of the semantics.",
    "URL": "https://www.sciencedirect.com/science/article/pii/0304397577900445",
    "DOI": "10.1016/0304-3975(77)90044-5",
    "page": "223-255",
    "page-first": "223",
    "volume": "5",
    "issue": "3",
    "language": "en-US",
    "_line": "Security.bib:159"
  },
  "sidhpurwala_security_2019": {
    "id": "sidhpurwala_security_2019",
    "type": "webpage",
    "author": [
      {
        "family": "Sidhpurwala",
        "given": "Huzaifa"
      }
    ],
    "title": "Security flaws caused by compiler optimizations",
    "issued": {
      "date-parts": [
        [
          "2019",
          "8",
          "21"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "28"
        ]
      ]
    },
    "abstract": "An optimizing compiler is one that tries to maximize some attribute(s) of an executable program at the expense of other attribute(s). Usually the goal is to improve performance or code size at the expense of compiler time and the possibility to debug the program at a later stage. Most modern compilers support some sort of optimization. Normally code optimized for performance is the usual preference. In cases where space is a constraint like embedded systems, developers also prefer code optimized for size.",
    "URL": "https://www.redhat.com/en/blog/security-flaws-caused-compiler-optimizations",
    "language": "en-US",
    "_line": "Security.bib:177"
  },
  "guarnieri_contract-aware_2020": {
    "id": "guarnieri_contract-aware_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Guarnieri",
        "given": "Marco"
      },
      {
        "family": "Patrignani",
        "given": "Marco"
      }
    ],
    "title": "Contract-Aware Secure Compilation",
    "container-title": "arXiv:2012.14205 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2020",
          "12",
          "28"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "27"
        ]
      ]
    },
    "abstract": "Microarchitectural attacks exploit the abstraction gap between the Instruction Set Architecture (ISA) and how instructions are actually executed by processors to compromise the confidentiality and integrity of a system. To secure systems against microarchitectural attacks, programmers need to reason about and program against these microarchitectural side-effects. However, we cannot &ndash; and should not &ndash; expect programmers to manually tailor programs for specific processors and their security guarantees. Instead, we could rely on compilers (and the secure compilation community), as they can play a prominent role in bridging this gap: compilers should target specific processors microarchitectural security guarantees and they should leverage these guarantees to produce secure code. To achieve this, we outline the idea of Contract-Aware Secure COmpilation (CASCO) where compilers are parametric with respect to a hardware/software security-contract, an abstraction capturing a processor's security guarantees. That is, compilers will automatically leverage the guarantees formalized in the contract to ensure that program-level security properties are preserved at microarchitectural level.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages",
    "URLtext": "2012.14205",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2012.14205",
    "URL": "http://arxiv.org/abs/2012.14205",
    "_line": "Security.bib:188"
  },
  "sison_verified_2020": {
    "id": "sison_verified_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Sison",
        "given": "Robert"
      },
      {
        "family": "Murray",
        "given": "Toby"
      }
    ],
    "title": "Verified Secure Compilation for Mixed-Sensitivity Concurrent Programs",
    "container-title": "arXiv:2010.14032 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2020",
          "10",
          "26"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "27"
        ]
      ]
    },
    "abstract": "Proving only over source code that programs do not leak sensitive data leaves a gap between reasoning and reality that can only be filled by accounting for the behaviour of the compiler. Furthermore, software does not always have the luxury of limiting itself to single-threaded computation with resources statically dedicated to each user to ensure the confidentiality of their data. This results in mixed-sensitivity concurrent programs, which might reuse memory shared between their threads to hold data of different sensitivity levels at different times; for such programs, a compiler must preserve the value-dependent coordination of such mixed-sensitivity reuse despite the impact of concurrency. Here we demonstrate, using Isabelle/HOL, that it is feasible to verify that a compiler preserves noninterference, the strictest kind of confidentiality property, for mixed-sensitivity concurrent programs. First, we present notions of refinement that preserve a concurrent value-dependent notion of noninterference that we have designed to support such programs. As proving noninterference-preserving refinement can be considerably more complex than the standard refinements typically used to verify semantics &ndash; preserving compilation, our notions include a decomposition principle that separates the semantics &ndash; from the security-preservation concerns. Second, we demonstrate that these refinement notions are applicable to verified secure compilation, by exercising them on a single-pass compiler for mixed-sensitivity concurrent programs that synchronise using mutex locks, from a generic imperative language to a generic RISC-style assembly language. Finally, we execute our compiler on a nontrivial mixed-sensitivity concurrent program modelling a real-world use case, thus preserving its source-level noninterference properties down to an assembly-level model automatically. (See paper for complete abstract.)",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages, Computer Science - Logic in Computer Science, D.2.4, D.1.3, D.3.4, F.3.1",
    "URLtext": "2010.14032",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2010.14032",
    "URL": "http://arxiv.org/abs/2010.14032",
    "_line": "Security.bib:202"
  },
  "namjoshi_witnessing_2019": {
    "id": "namjoshi_witnessing_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Namjoshi",
        "given": "Kedar S."
      },
      {
        "family": "Tabajara",
        "given": "Lucas M."
      }
    ],
    "title": "Witnessing Secure Compilation",
    "container-title": "arXiv:1911.05866 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2019",
          "11",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "27"
        ]
      ]
    },
    "abstract": "Compiler optimizations are designed to improve run-time performance while preserving input-output behavior. Correctness in this sense does not necessarily preserve security: it is known that standard optimizations may break or weaken security properties that hold of the source program. This work develops a translation validation method for secure compilation. Security (hyper-)properties are expressed using automata operating over a bundle of program traces. A flexible, automaton-based refinement scheme, generalizing existing refinement methods, guarantees that the associated security property is preserved by a program transformation. In practice, the refinement relations (\"security witnesses\") can be generated during compilation and validated independently with a refinement checker. This process is illustrated for common optimizations. Crucially, it is not necessary to verify the compiler implementation itself, which is infeasible in practice for production compilers.",
    "keywords": "Computer Science - Formal Languages and Automata Theory",
    "URLtext": "1911.05866",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1911.05866",
    "URL": "http://arxiv.org/abs/1911.05866",
    "_line": "Security.bib:216"
  },
  "tsampas_categorical_2020": {
    "id": "tsampas_categorical_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Tsampas",
        "given": "Stelios"
      },
      {
        "family": "Nuyts",
        "given": "Andreas"
      },
      {
        "family": "Devriese",
        "given": "Dominique"
      },
      {
        "family": "Piessens",
        "given": "Frank"
      }
    ],
    "title": "A categorical approach to secure compilation",
    "container-title": "arXiv:2004.03557 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2020",
          "4",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "27"
        ]
      ]
    },
    "abstract": "We introduce a novel approach to secure compilation based on maps of distributive laws. We demonstrate through four examples that the coherence criterion for maps of distributive laws can potentially be a viable alternative for compiler security instead of full abstraction, which is the preservation and reflection of contextual equivalence. To that end, we also make use of the well-behavedness properties of distributive laws to construct a categorical argument for the contextual connotations of bisimilarity.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "2004.03557",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2004.03557",
    "URL": "http://arxiv.org/abs/2004.03557",
    "_line": "Security.bib:230"
  },
  "abadi_protection_1998": {
    "id": "abadi_protection_1998",
    "type": "article-journal",
    "author": [
      {
        "family": "Abadi",
        "given": "Martin"
      }
    ],
    "title": "Protection in programming-language translations",
    "container-title": "DECSRC",
    "issued": {
      "date-parts": [
        [
          "1998"
        ]
      ]
    },
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "Security.bib:244"
  },
  "agten_secure_2012": {
    "id": "agten_secure_2012",
    "type": "paper-conference",
    "author": [
      {
        "family": "Agten",
        "given": "Pieter"
      },
      {
        "family": "Strackx",
        "given": "Raoul"
      },
      {
        "family": "Jacobs",
        "given": "Bart"
      },
      {
        "family": "Piessens",
        "given": "Frank"
      }
    ],
    "title": "Secure Compilation to Modern Processors",
    "container-title": "2012 IEEE 25th Computer Security Foundations Symposium",
    "event-title": "2012 IEEE 25th Computer Security Foundations Symposium (CSF)",
    "issued": {
      "date-parts": [
        [
          "2012",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-4673-1918-8 978-0-7695-4718-3",
    "abstract": "We present a secure (fully abstract) compilation scheme to compile an object-based high-level language to lowlevel machine code. Full abstraction is achieved by relying on a ﬁne-grained program counter-based memory access protection scheme, which is part of our low-level target language. We discuss why standard compilers fail to provide full abstraction and introduce enhancements needed to achieve this goal. We prove that our enhanced compilation scheme provides full abstraction from our high-level source language to our lowlevel target language. Lastly, we show by means of a prototype implementation that our low-level language with ﬁne-grained memory access control can be realized efﬁciently on modern commodity platforms.",
    "URL": "http://ieeexplore.ieee.org/document/6266159/",
    "DOI": "10.1109/CSF.2012.12",
    "publisher-place": "Cambridge, MA, USA",
    "page": "171-185",
    "page-first": "171",
    "language": "en-US",
    "_line": "Security.bib:254"
  },
  "patrignani_formal_2019": {
    "id": "patrignani_formal_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Patrignani",
        "given": "Marco"
      },
      {
        "family": "Ahmed",
        "given": "Amal"
      },
      {
        "family": "Clarke",
        "given": "Dave"
      }
    ],
    "title": "Formal Approaches to Secure Compilation: A Survey of Fully Abstract Compilation and Related Work",
    "container-title": "ACM Computing Surveys",
    "container-title-short": "Formal Approaches to Secure Compilation",
    "title-short": "Formal Approaches to Secure Compilation",
    "issued": {
      "date-parts": [
        [
          "2019",
          "2",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "26"
        ]
      ]
    },
    "issn": "0360-0300",
    "abstract": "Secure compilation is a discipline aimed at developing compilers that preserve the security properties of the source programs they take as input in the target programs they produce as output. This discipline is broad in scope, targeting languages with a variety of features (including objects, higher-order functions, dynamic memory allocation, call/cc, concurrency) and employing a range of different techniques to ensure that source-level security is preserved at the target level. This article provides a survey of the existing literature on formal approaches to secure compilation with a focus on those that prove fully abstract compilation, which has been the criterion adopted by much of the literature thus far. This article then describes the formal techniques employed to prove secure compilation in existing work, introducing relevant terminology, and discussing the merits and limitations of each work. Finally, this article discusses open challenges and possible directions for future work in secure compilation.",
    "keywords": "contextual equivalence, fully abstract compilation, program equivalence, Secure compilation, type preserving compilation",
    "URL": "https://doi.org/10.1145/3280984",
    "DOI": "10.1145/3280984",
    "page": "125:1-125:36",
    "page-first": "125",
    "volume": "51",
    "issue": "6",
    "_line": "Security.bib:272"
  },
  "yousefi-azar_mutual_2021": {
    "id": "yousefi-azar_mutual_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Yousefi-Azar",
        "given": "Mahmood"
      },
      {
        "family": "Varadharajan",
        "given": "Vijay"
      },
      {
        "family": "Hamey",
        "given": "Len"
      },
      {
        "family": "Chen",
        "given": "Shiping"
      }
    ],
    "title": "Mutual Information and Feature Importance Gradient Boosting: Automatic byte n‐gram feature reranking for Android malware detection",
    "container-title": "Software: Practice and Experience",
    "container-title-short": "Mutual Information and Feature Importance Gradient Boosting",
    "title-short": "Mutual Information and Feature Importance Gradient Boosting",
    "issued": {
      "date-parts": [
        [
          "2021",
          "4",
          "5"
        ]
      ]
    },
    "abstract": "The fast pace evolving of Android malware demands for highly efficient strategy. That is, for a range of malware types, a malware detection scheme needs to be resilient and with minimum computation performs efficient and precise. In this paper, we propose Mutual Information and Feature Importance Gradient Boosting (MIFIBoost) tool that uses byte n‐gram frequency. MIFIBoost consists of four steps in the model construction phase and two steps in the prediction phase. For training, first, n‐grams of both the classes.dex and AndroidManifest.xml binary files are obtained. Then, MIFIBoost uses Mutual Information (MI) to determine the top most informative items from the entire n‐gram vocabulary. In the third phase, MIFIBoost utilizes the Gradient Boosting algorithm to re‐rank these top n‐grams. For testing, MIFIBoost uses the learned vocabulary of byte n‐grams term‐frequency (tf) to feed into the classifier for prediction. Thus, MIFIBoost does not require reverse engineering. A key insight from this work is that filtering using XGBoost helps us to address the hard problem of detecting obfuscated malware better while having a negligible impact on nonobfuscated malware. We have conducted a wide range of experiments on four different datasets one of which is obfuscated, and MIFIBoost outperforms state‐of‐the‐art tools. MIFIBoost's f1‐score for Drebin, DexShare, and AMD datasets is 99.1&perc;, 98.87&perc;, and 99.62&perc;, respectively, a False Positive Rate of 0.41&perc; using AMD dataset. On average, the False Negative Rate of MIFIBoost is 2.1&perc; for the PRAGuard dataset in which seven different obfuscation techniques are implemented. In addition to fast run‐time performance and resiliency against obfuscated malware, the experiments show that MIFIBoost performs quite efficiently for five zero‐day families with 99.78&perc; AUC.",
    "DOI": "10.1002/spe.2971",
    "_line": "Security.bib:291"
  },
  "schoolderman_efficient_2021": {
    "id": "schoolderman_efficient_2021",
    "type": "report",
    "author": [
      {
        "family": "Schoolderman",
        "given": "Marc"
      },
      {
        "family": "Moerman",
        "given": "Jonathan"
      },
      {
        "family": "Smetsers",
        "given": "Sjaak"
      },
      {
        "family": "Eekelen",
        "given": "Marko van"
      }
    ],
    "title": "Efficient Verification of Optimized Code: Correct High-speed X25519",
    "container-title-short": "Efficient Verification of Optimized Code",
    "title-short": "Efficient Verification of Optimized Code",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "26"
        ]
      ]
    },
    "abstract": "Code that is highly optimized poses a problem for program-level verification: programmers can employ various clever tricks that are non-trivial to reason about. For cryptography on low-power devices, it is nonetheless crucial that implementations be functionally correct, secure, and efficient. These are usually crafted in hand-optimized machine code that eschew conventional control flow as much as possible.\n\n\n\nWe have formally verified such code: a library which implements elliptic curve cryptography on 8-bit AVR microcontrollers. The chosen implementation is the most efficient currently known for this microarchitecture. It consists of over 3000 lines of assembly instructions. Building on earlier work, we use the Why3 platform to model the code and prove verification conditions, using automated provers. We expect the approach to be re-usable and adaptable, and it allows for validation. Furthermore, an error in the original implementation was found and corrected, at the same time reducing its memory footprint. This shows that practical verification of cutting-edge code is not only possible, but can in fact add to its efficiency—and is clearly necessary.",
    "keywords": "formal verification, elliptic curve cryptosystem, implementation",
    "URL": "https://eprint.iacr.org/2021/415",
    "number": "415",
    "_line": "Security.bib:302"
  },
  "poettering_sok_2021": {
    "id": "poettering_sok_2021",
    "type": "report",
    "author": [
      {
        "family": "Poettering",
        "given": "Bertram"
      },
      {
        "family": "Rösler",
        "given": "Paul"
      },
      {
        "family": "Schwenk",
        "given": "Jörg"
      },
      {
        "family": "Stebila",
        "given": "Douglas"
      }
    ],
    "title": "SoK: Game-based Security Models for Group Key Exchange",
    "container-title-short": "SoK",
    "title-short": "SoK",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "21"
        ]
      ]
    },
    "abstract": "Group key exchange (GKE) protocols let a group of users jointly establish fresh and secure key material. Many flavors of GKE have been proposed, differentiated by, among others, whether group membership is static or dynamic, whether a single key or a continuous stream of keys is established, and whether security is provided in the presence of state corruptions (forward and post-compromise security). In all cases, an indispensable ingredient to the rigorous analysis of a candidate solution is a corresponding formal security model. We observe, however, that most GKE-related publications are more focused on building new constructions that have more functionality or are more efficient than prior proposals, while leaving the job of identifying and working out the details of adequate security models a subordinate task.\n\n\n\nIn this systematization of knowledge we bring the formal modeling of GKE security to the fore by revisiting the intuitive goals of GKE, critically evaluating how these goals are reflected (or not) in the established models, and how they would be best considered in new models. We classify and compare characteristics of a large selection of game-based GKE models that appear in the academic literature, including those proposed for GKE with post-compromise security. We observe a range of shortcomings in some of the studied models, such as dependencies on overly restrictive syntactical constrains, unrealistic adversarial capabilities, or simply incomplete definitions. Our systematization enables us to identify a coherent suite of desirable characteristics that we believe should be represented in all general purpose GKE models. To demonstrate the feasibility of covering all these desirable characteristics simultaneously in one concise definition, we conclude with proposing a new generic reference model for GKE.",
    "keywords": "cryptographic protocols, Group key exchange, key agreement, key establishment, multi-user protocol, security model, systematization of knowledge",
    "URL": "https://eprint.iacr.org/2021/305",
    "number": "305",
    "_line": "Security.bib:319"
  },
  "cauligi_sok_2021": {
    "id": "cauligi_sok_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Cauligi",
        "given": "Sunjay"
      },
      {
        "family": "Disselkoen",
        "given": "Craig"
      },
      {
        "family": "Moghimi",
        "given": "Daniel"
      },
      {
        "family": "Barthe",
        "given": "Gilles"
      },
      {
        "family": "Stefan",
        "given": "Deian"
      }
    ],
    "title": "SoK: Practical Foundations for Spectre Defenses",
    "container-title": "arXiv:2105.05801 \\[cs\\]",
    "container-title-short": "SoK",
    "title-short": "SoK",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "19"
        ]
      ]
    },
    "abstract": "Spectre vulnerabilities violate our fundamental assumptions about architectural abstractions, allowing attackers to steal sensitive data despite previously state-of-the-art countermeasures. To defend against Spectre, developers of verification tools and compiler-based mitigations are forced to reason about microarchitectural details such as speculative execution. In order to aid developers with these attacks in a principled way, the research community has sought formal foundations for speculative execution upon which to rebuild provable security guarantees. This paper systematizes the community's current knowledge about software verification and mitigation for Spectre. We study state-of-the-art software defenses, both with and without associated formal models, and use a cohesive framework to compare the security properties each defense provides. We explore a wide variety of tradeoffs in the complexity of formal frameworks, the performance of defense tools, and the resulting security guarantees. As a result of our analysis, we suggest practical choices for developers of analysis and mitigation tools, and we identify several open problems in this area to guide future work on grounded software defenses.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages",
    "URLtext": "2105.05801",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.05801",
    "URL": "http://arxiv.org/abs/2105.05801",
    "_line": "Security.bib:336"
  },
  "kruger_crysl_2019": {
    "id": "kruger_crysl_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Krüger",
        "given": "Stefan"
      },
      {
        "family": "Späth",
        "given": "Johannes"
      },
      {
        "family": "Ali",
        "given": "Karim"
      },
      {
        "family": "Bodden",
        "given": "Eric"
      },
      {
        "family": "Mezini",
        "given": "Mira"
      }
    ],
    "title": "CrySL: An Extensible Approach to Validating the Correct Usage of Cryptographic APIs",
    "container-title": "IEEE Transactions on Software Engineering",
    "container-title-short": "CrySL",
    "title-short": "CrySL",
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "issn": "1939-3520",
    "abstract": "Various studies have empirically shown that the majority of Java and Android applications misuse cryptographic libraries, causing devastating breaches of data security. It is crucial to detect such misuses early in the development process. To detect cryptography misuses, one must define secure uses first, a process mastered primarily by cryptography experts but not by developers. In this paper, we present CrySL, a specification language for bridging the cognitive gap between cryptography experts and developers. CrySL enables cryptography experts to specify the secure usage of the cryptographic libraries they provide. We have implemented a compiler that translates such CrySL specification into a context-sensitive and flow-sensitive demand-driven static analysis. The analysis then helps developers by automatically checking a given Java or Android app for compliance with the CrySL-encoded rules. We have designed an extensive CrySL rule set for the Java Cryptography Architecture (JCA), and empirically evaluated it by analyzing 10,000 current Android apps and all 204,788 current Java software artefacts on Maven Central. Our results show that misuse of cryptographic APIs is still widespread, with 95&perc; of apps and 63&perc; of Maven artefacts containing at least one misuse. Our easily extensible CrySL rule set covers more violations than previous special-purpose tools that contain hard-coded rules, while still offering a more precise analysis.",
    "keywords": "static analysis, Ciphers, cryptography, domain-specific language, Encryption, Java, Semantics, Static analysis, Tools",
    "DOI": "10.1109/TSE.2019.2948910",
    "page": "1-1",
    "page-first": "1",
    "note": "Conference Name: IEEE Transactions on Software Engineering",
    "_line": "Security.bib:351"
  },
  "bonifacio_dealing_2021": {
    "id": "bonifacio_dealing_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bonifacio",
        "given": "Rodrigo"
      },
      {
        "family": "Krüger",
        "given": "Stefan"
      },
      {
        "family": "Narasimhan",
        "given": "Krishna"
      },
      {
        "family": "Bodden",
        "given": "Eric"
      },
      {
        "family": "Mezini",
        "given": "Mira"
      }
    ],
    "title": "Dealing with Variability in API Misuse Specification",
    "container-title": "arXiv:2105.04950 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "19"
        ]
      ]
    },
    "abstract": "APIs are the primary mechanism for developers to gain access to externally defined services and tools. However, previous research has revealed API misuses that violate the contract of APIs to be prevalent. Such misuses can have harmful consequences, especially in the context of cryptographic libraries. Various API misuse detectors have been proposed to address this issue including CogniCrypt, one of the most versatile of such detectors and that uses a language CrySL to specify cryptographic API usage contracts. Nonetheless, existing approaches to detect API misuse had not been designed for systematic reuse, ignoring the fact that different versions of a library, different versions of a platform, and different recommendations or guidelines might introduce variability in the correct usage of an API. Yet, little is known about how such variability impacts the specification of the correct API usage. This paper investigates this question by analyzing the impact of various sources of variability on widely used Java cryptographic libraries including JCA, Bouncy Castle, and Google Tink. The results of our investigation show that sources of variability like new versions of the API and security standards significantly impact the specifications. We then use the insights gained from our investigation to motivate an extension to the CrySL language named MetaCrySL, which builds on meta programming concepts. We evaluate MetaCrySL by specifying usage rules for a family of Android versions and illustrate that MetaCrySL can model all forms of variability we identified and drastically reduce the size of a family of specifications for the correct usage of cryptographic APIs",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Software Engineering, 68N19, D.2.1, D.3.3",
    "URLtext": "2105.04950",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2105.04950",
    "URL": "http://arxiv.org/abs/2105.04950",
    "_line": "Security.bib:366"
  },
  "barthe_secure_2021": {
    "id": "barthe_secure_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Barthe",
        "given": "Gilles"
      },
      {
        "family": "Blazy",
        "given": "Sandrine"
      },
      {
        "family": "Hutin",
        "given": "Rémi"
      },
      {
        "family": "Pichardie",
        "given": "David"
      }
    ],
    "title": "Secure Compilation of Constant-Resource Programs",
    "container-title": "IEEE 34th Computer Security Foundations Symposium (CSF)",
    "issued": {
      "date-parts": [
        [
          "2021",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "17"
        ]
      ]
    },
    "abstract": "Observational non-interference (ONI) is a generic information-flow policy for side-channel leakage. Informally, a program is ONI-secure if observing program leakage during execution does not reveal any information about secrets. Formally, ONI is parametrized by a leakage function , and different instances of ONI can be recovered through different instantiations of. One popular instance of ONI is the cryptographic constant-time (CCT) policy, which is widely used in cryptographic libraries to protect against timing and cache attacks. Informally, a program is CCT-secure if it does not branch on secrets and does not perform secret-dependent memory accesses. Another instance of ONI is the constant-resource (CR) policy, a relaxation of the CCT policy which is used in Amazon's s2n implementation of TLS and in several other security applications. Informally, a program is CR-secure if its cost (modelled by a tick operator over an arbitrary semi-group) does not depend on secrets. In this paper, we consider the problem of preserving ONI by compilation. Prior work on the preservation of the CCT policy develops proof techniques for showing that main compiler optimisations preserve the CCT policy. However, these proof techniques critically rely on the fact that the semi-group used for modelling leakage satisfies the property:",
    "URL": "https://hal.archives-ouvertes.fr/hal-03221440",
    "publisher-place": "Dubrovnik, Croatia",
    "_line": "Security.bib:380"
  },
  "zou_intdroid_2021": {
    "id": "zou_intdroid_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Zou",
        "given": "Deqing"
      },
      {
        "family": "Wu",
        "given": "Yueming"
      },
      {
        "family": "Yang",
        "given": "Siru"
      },
      {
        "family": "Chauhan",
        "given": "Anki"
      },
      {
        "family": "Yang",
        "given": "Wei"
      },
      {
        "family": "Zhong",
        "given": "Jiangying"
      },
      {
        "family": "Dou",
        "given": "Shihan"
      },
      {
        "family": "Jin",
        "given": "Hai"
      }
    ],
    "title": "IntDroid: Android Malware Detection Based on API Intimacy Analysis",
    "container-title": "ACM Transactions on Software Engineering and Methodology",
    "container-title-short": "IntDroid",
    "title-short": "IntDroid",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "5",
          "13"
        ]
      ]
    },
    "issn": "1049-331X",
    "abstract": "Android, the most popular mobile operating system, has attracted millions of users around the world. Meanwhile, the number of new Android malware instances has grown exponentially in recent years. On the one hand, existing Android malware detection systems have shown that distilling the program semantics into a graph representation and detecting malicious programs by conducting graph matching are able to achieve high accuracy on detecting Android malware. However, these traditional graph-based approaches always perform expensive program analysis and suffer from low scalability on malware detection. On the other hand, because of the high scalability of social network analysis, it has been applied to complete large-scale malware detection. However, the social-network-analysis-based method only considers simple semantic information (i.e., centrality) for achieving market-wide mobile malware scanning, which may limit the detection effectiveness when benign apps show some similar behaviors as malware. In this article, we aim to combine the high accuracy of traditional graph-based method with the high scalability of social-network-analysis&ndash;based method for Android malware detection. Instead of using traditional heavyweight static analysis, we treat function call graphs of apps as complex social networks and apply social-network&ndash;based centrality analysis to unearth the central nodes within call graphs. After obtaining the central nodes, the average intimacies between sensitive API calls and central nodes are computed to represent the semantic features of the graphs. We implement our approach in a tool called IntDroid and evaluate it on a dataset of 3,988 benign samples and 4,265 malicious samples. Experimental results show that IntDroid is capable of detecting Android malware with an F-measure of 97.1&perc; while maintaining a True-positive Rate of 99.1&perc;. Although the scalability is not as fast as a social-network-analysis&ndash;based method (i.e., MalScan), compared to a traditional graph-based method, IntDroid is more than six times faster than MaMaDroid. Moreover, in a corpus of apps collected from GooglePlay market, IntDroid is able to identify 28 zero-day malware that can evade detection of existing tools, one of which has been downloaded and installed by more than ten million users. This app has also been flagged as malware by six anti-virus scanners in VirusTotal, one of which is Symantec Mobile Insight.",
    "keywords": "Android malware, API intimacy, centrality, social network",
    "URL": "https://doi.org/10.1145/3442588",
    "DOI": "10.1145/3442588",
    "page": "39:1-39:32",
    "page-first": "39",
    "volume": "30",
    "issue": "3",
    "_line": "Security.bib:401"
  },
  "palit_dynpta_nodate": {
    "id": "palit_dynpta_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Palit",
        "given": "Tapti"
      },
      {
        "family": "Moon",
        "given": "Jarin Firose"
      },
      {
        "family": "Monrose",
        "given": "Fabian"
      },
      {
        "family": "Polychronakis",
        "given": "Michalis"
      }
    ],
    "title": "DynPTA: Combining Static and Dynamic Analysis for Practical Selective Data Protection",
    "abstract": "As control ﬂow hijacking attacks become more challenging due to the deployment of various exploit mitigation technologies, the leakage of sensitive process data through the exploitation of memory disclosure vulnerabilities is becoming an increasingly important threat. To make matters worse, recently introduced transient execution attacks provide a new avenue for leaking conﬁdential process data. As a response, various approaches for selectively protecting subsets of critical in-memory data have been proposed, which though either require a signiﬁcant code refactoring effort, or do not scale for large applications. In this paper we present DynPTA, a selective data protection approach that combines static analysis with scoped dynamic data ﬂow tracking (DFT) to keep a subset of manually annotated sensitive data always encrypted in memory. DynPTA ameliorates the inherent overapproximation of pointer analysis—a signiﬁcant challenge that has prevented previous approaches from supporting large applications—by relying on lightweight label lookups to determine if potentially sensitive data is actually sensitive. Labeled objects are tracked only within the subset of value ﬂows that may carry potentially sensitive data, requiring only a fraction of the program’s code to be instrumented for DFT. We experimentally evaluated DynPTA with real-world applications and demonstrate that it can prevent memory disclosure (Heartbleed) and transient execution (Spectre) attacks from leaking the protected data, while incurring a modest runtime overhead of up to 19.2&perc; when protecting the private TLS key of Nginx with OpenSSL.",
    "page": "19",
    "page-first": "19",
    "language": "en-US",
    "_line": "Security.bib:420"
  },
  "degano_secure_2009": {
    "id": "degano_secure_2009",
    "type": "chapter",
    "author": [
      {
        "family": "Boudol",
        "given": "Gérard"
      }
    ],
    "editor": [
      {
        "family": "Degano",
        "given": "Pierpaolo"
      },
      {
        "family": "Guttman",
        "given": "Joshua"
      },
      {
        "family": "Martinelli",
        "given": "Fabio"
      }
    ],
    "title": "Secure Information Flow as a Safety Property",
    "container-title": "Formal Aspects in Security and Trust",
    "issued": {
      "date-parts": [
        [
          "2009"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "publisher": "Springer Berlin Heidelberg",
    "isbn": "978-3-642-01464-2 978-3-642-01465-9",
    "URL": "http://link.springer.com/10.1007/978-3-642-01465-9_2",
    "DOI": "10.1007/978-3-642-01465-9_2",
    "publisher-place": "Berlin, Heidelberg",
    "page": "20-34",
    "page-first": "20",
    "volume": "5491",
    "note": "Series Title: Lecture Notes in Computer Science",
    "_line": "Security.bib:429"
  },
  "bengtson_refinement_2011": {
    "id": "bengtson_refinement_2011",
    "type": "article-journal",
    "author": [
      {
        "family": "Bengtson",
        "given": "Jesper"
      },
      {
        "family": "Bhargavan",
        "given": "Karthikeyan"
      },
      {
        "family": "Fournet",
        "given": "Cédric"
      },
      {
        "family": "Gordon",
        "given": "Andrew D."
      },
      {
        "family": "Maffeis",
        "given": "Sergio"
      }
    ],
    "title": "Refinement types for secure implementations",
    "container-title": "ACM Transactions on Programming Languages and Systems",
    "container-title-short": "ACM Trans. Program. Lang. Syst.",
    "issued": {
      "date-parts": [
        [
          "2011",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "issn": "0164-0925, 1558-4593",
    "URL": "https://dl.acm.org/doi/10.1145/1890028.1890031",
    "DOI": "10.1145/1890028.1890031",
    "page": "1-45",
    "page-first": "1",
    "volume": "33",
    "issue": "2",
    "language": "en-US",
    "_line": "Security.bib:446"
  },
  "barthe_secure_2018": {
    "id": "barthe_secure_2018",
    "type": "paper-conference",
    "author": [
      {
        "family": "Barthe",
        "given": "Gilles"
      },
      {
        "family": "Gregoire",
        "given": "Benjamin"
      },
      {
        "family": "Laporte",
        "given": "Vincent"
      }
    ],
    "title": "Secure Compilation of Side-Channel Countermeasures: The Case of Cryptographic “Constant-Time”",
    "container-title": "2018 IEEE 31st Computer Security Foundations Symposium (CSF)",
    "container-title-short": "Secure Compilation of Side-Channel Countermeasures",
    "title-short": "Secure Compilation of Side-Channel Countermeasures",
    "event-title": "2018 IEEE 31st Computer Security Foundations Symposium (CSF)",
    "issued": {
      "date-parts": [
        [
          "2018",
          "7"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-5386-6680-7",
    "URL": "https://ieeexplore.ieee.org/document/8429315/",
    "DOI": "10.1109/CSF.2018.00031",
    "publisher-place": "Oxford",
    "page": "328-343",
    "page-first": "328",
    "_line": "Security.bib:463"
  },
  "alpern_defining_1985": {
    "id": "alpern_defining_1985",
    "type": "article-journal",
    "author": [
      {
        "family": "Alpern",
        "given": "Bowen"
      },
      {
        "family": "Schneider",
        "given": "Fred B."
      }
    ],
    "title": "Defining liveness",
    "container-title": "Information Processing Letters",
    "container-title-short": "Information Processing Letters",
    "issued": {
      "date-parts": [
        [
          "1985",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "issn": "00200190",
    "URL": "https://linkinghub.elsevier.com/retrieve/pii/0020019085900560",
    "DOI": "10.1016/0020-0190(85)90056-0",
    "page": "181-185",
    "page-first": "181",
    "volume": "21",
    "issue": "4",
    "language": "en-US",
    "_line": "Security.bib:480"
  },
  "abadi_secure_2002": {
    "id": "abadi_secure_2002",
    "type": "article-journal",
    "author": [
      {
        "family": "Abadi",
        "given": "Martı́n"
      },
      {
        "family": "Fournet",
        "given": "Cédric"
      },
      {
        "family": "Gonthier",
        "given": "Georges"
      }
    ],
    "title": "Secure Implementation of Channel Abstractions",
    "container-title": "Information and Computation",
    "container-title-short": "Information and Computation",
    "issued": {
      "date-parts": [
        [
          "2002",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "issn": "08905401",
    "URL": "https://linkinghub.elsevier.com/retrieve/pii/S0890540102930865",
    "DOI": "10.1006/inco.2002.3086",
    "page": "37-83",
    "page-first": "37",
    "volume": "174",
    "issue": "1",
    "language": "en-US",
    "_line": "Security.bib:496"
  },
  "bugliesi_secure_2007": {
    "id": "bugliesi_secure_2007",
    "type": "paper-conference",
    "author": [
      {
        "family": "Bugliesi",
        "given": "Michele"
      },
      {
        "family": "Giunti",
        "given": "Marco"
      }
    ],
    "title": "Secure implementations of typed channel abstractions",
    "container-title": "Proceedings of the 34th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages  - POPL '07",
    "event-title": "the 34th annual ACM SIGPLAN-SIGACT symposium",
    "issued": {
      "date-parts": [
        [
          "2007"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "publisher": "ACM Press",
    "isbn": "978-1-59593-575-5",
    "URL": "http://portal.acm.org/citation.cfm?doid=1190216.1190253",
    "DOI": "10.1145/1190216.1190253",
    "publisher-place": "Nice, France",
    "page": "251",
    "page-first": "251",
    "language": "en-US",
    "_line": "Security.bib:513"
  },
  "abadi_secrecy_1999": {
    "id": "abadi_secrecy_1999",
    "type": "article-journal",
    "author": [
      {
        "family": "Abadi",
        "given": "Martín"
      }
    ],
    "title": "Secrecy by typing in security protocols",
    "container-title": "Journal of the ACM",
    "container-title-short": "J. ACM",
    "issued": {
      "date-parts": [
        [
          "1999",
          "9"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "issn": "0004-5411, 1557-735X",
    "URL": "https://dl.acm.org/doi/10.1145/324133.324266",
    "DOI": "10.1145/324133.324266",
    "page": "749-786",
    "page-first": "749",
    "volume": "46",
    "issue": "5",
    "language": "en-US",
    "_line": "Security.bib:529"
  },
  "patrignani_robustly_2021": {
    "id": "patrignani_robustly_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Patrignani",
        "given": "Marco"
      },
      {
        "family": "Garg",
        "given": "Deepak"
      }
    ],
    "title": "Robustly Safe Compilation, an Efficient Form of Secure Compilation",
    "container-title": "ACM Transactions on Programming Languages and Systems",
    "container-title-short": "ACM Trans. Program. Lang. Syst.",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "issn": "0164-0925, 1558-4593",
    "abstract": "Security-preserving compilers generate compiled code that withstands target-level attacks such as alteration of control flow, data leaks, or memory corruption. Many existing security-preserving compilers are proven to be fully abstract, meaning that they reflect and preserve observational equivalence. Fully abstract compilation is strong and useful but, in certain cases, comes at the cost of requiring expensive runtime constructs in compiled code. These constructs may have no relevance for security, but are needed to accommodate differences between the source and target languages that fully abstract compilation necessarily needs.\n   \n   As an alternative to fully abstract compilation, this article explores a different criterion for secure compilation called robustly safe compilation or\n   RSC\n   . Briefly, this criterion means that the compiled code preserves relevant safety properties of the source program against all adversarial contexts interacting with the compiled program. We show that\n   RSC\n   can be proved more easily than fully abstract compilation and also often results in more efficient code. We also present two different proof techniques for establishing that a compiler attains\n   RSC\n   and, to illustrate them, develop three illustrative robustly safe compilers that rely on different target-level protection mechanisms. We then proceed to turn one of our compilers into a fully abstract one and through this example argue that proving\n   RSC\n   can be simpler than proving full abstraction.\n   \n   \n   To better explain and clarify notions, this article uses syntax highlighting in a way that colourblind and black-8-white readers can benefit from Reference \\[58\\]. For a better experience, please print or view this article in colour\n   .\n   1",
    "URL": "https://dl.acm.org/doi/10.1145/3436809",
    "DOI": "10.1145/3436809",
    "page": "1-41",
    "page-first": "1",
    "volume": "43",
    "issue": "1",
    "language": "en-US",
    "_line": "Security.bib:545"
  },
  "habib_learning_2021": {
    "id": "habib_learning_2021",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Habib",
        "given": "Andrew"
      }
    ],
    "title": "Learning to Find Bugs in Programs and their Documentation",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "publisher": "Technische Universität",
    "abstract": "Although software is pervasive, almost all programs suffer from bugs and errors. To detect software bugs, developers use various techniques such as static analysis, dynamic analysis, and model checking. However, none of these techniques is bulletproof.\n\nThis dissertation argues that learning from programs and their documentation provides an effective means to prevent and detect software bugs. The main observation that motivates our work is that software documentation is often under-utilized by traditional bug detection techniques. Leveraging the documentation together with the program itself, whether its source code or runtime behavior, enables us to build unconventional bug detectors that benefit from the richness of natural language documentation and the formal algorithm of a program. More concretely, we present techniques that utilize the documentation of a program and the program itself to: (i) Improve the documentation by inferring missing information and detecting inconsistencies, and (ii) Find bugs in the source code or runtime behavior of the program. A key insight we build on is that machine learning provides a powerful means to deal with the fuzziness and nuances of natural language in software documentation and that source code is repetitive enough to also allow statistical learning from it. Therefore, several of the techniques proposed in this dissertation employ a learning component whether from documentation, source code, runtime behavior, and their combinations.\n\nWe envision the impact of our work to be two-fold. First, we provide developers with novel bug detection techniques that complement traditional ones. Our approaches learn bug detectors end-to-end from data and hence, do not require complex analysis frameworks. Second, we hope that our work will open the door for more research on automatically utilizing natural language in software development. Future work should explore more ideas on how to extract richer information from natural language to automate software engineering tasks, and how to utilize the programs themselves to enhance the state-of-the-practice in software documentation.",
    "URL": "https://tuprints.ulb.tu-darmstadt.de/17377/",
    "DOI": "10.26083/tuprints-00017377",
    "publisher-place": "Darmstadt",
    "language": "en-US",
    "_line": "Security.bib:578"
  },
  "anderson_comparison_2020": {
    "id": "anderson_comparison_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Anderson",
        "given": "Sean"
      },
      {
        "family": "Hood",
        "given": "Jonathan"
      },
      {
        "family": "Jones",
        "given": "Erica"
      }
    ],
    "title": "A Comparison of Fuzzing Dynamic Analysis and Static Code Analysis",
    "issued": {
      "date-parts": [
        [
          "2020",
          "11",
          "16"
        ]
      ]
    },
    "page": "13",
    "page-first": "13",
    "language": "en-US",
    "_line": "Security.bib:598"
  },
  "dsilva_correctness-security_2015": {
    "id": "dsilva_correctness-security_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "D'Silva",
        "given": "V."
      },
      {
        "family": "Payer",
        "given": "M."
      },
      {
        "family": "Song",
        "given": "D."
      }
    ],
    "title": "The Correctness-Security Gap in Compiler Optimization",
    "container-title": "2015 IEEE Security and Privacy Workshops",
    "event-title": "2015 IEEE Security and Privacy Workshops",
    "issued": {
      "date-parts": [
        [
          "2015",
          "5"
        ]
      ]
    },
    "abstract": "There is a significant body of work devoted to testing, verifying, and certifying the correctness of optimizing compilers. The focus of such work is to determine if source code and optimized code have the same functional semantics. In this paper, we introduce the correctness-security gap, which arises when a compiler optimization preserves the functionality of but violates a security guarantee made by source code. We show with concrete code examples that several standard optimizations, which have been formally proved correct, in-habit this correctness-security gap. We analyze this gap and conclude that it arises due to techniques that model the state of the program but not the state of the underlying machine. We propose a broad research programme whose goal is to identify, understand, and mitigate the impact of security errors introduced by compiler optimizations. Our proposal includes research in testing, program analysis, theorem proving, and the development of new, accurate machine models for reasoning about the impact of compiler optimizations on security.",
    "keywords": "Optimization, Semantics, compiler optimization, correctness certification, correctness testing, correctness verification, correctness-security gap, Cryptography, formal correctness, functional semantics, machine model, optimising compilers, optimized code, optimizing compiler, Optimizing compilers, program analysis, program diagnostics, program state, program testing, reasoning, reasoning about programs, security, security error, security guarantee, security of data, source code, Standards, Syntactics, theorem proving",
    "DOI": "10.1109/SPW.2015.33",
    "page": "73-87",
    "page-first": "73",
    "_line": "Security.bib:607"
  },
  "van_schaik_sgaxe_nodate": {
    "id": "van_schaik_sgaxe_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Schaik",
        "given": "Stephan",
        "dropping-particle": "van"
      },
      {
        "family": "Kwong",
        "given": "Andrew"
      },
      {
        "family": "Genkin",
        "given": "Daniel"
      },
      {
        "family": "Yarom",
        "given": "Yuval"
      }
    ],
    "title": "SGAxe: How SGX Fails in Practice",
    "abstract": "Intel’s Software Guard Extensions (SGX) promises an isolated execution environment, protected from all software running on the machine. A signiﬁcant limitation of SGX is its lack of protection against side-channel attacks. In particular, recent works have shown that transient-execution attacks can leak arbitrary data from SGX, breaching SGX’s conﬁdentiality. However, less work has been done on the implications of such attacks on the SGX ecosystems.",
    "page": "14",
    "page-first": "14",
    "language": "en-US",
    "_line": "Security.bib:620"
  },
  "schwarz_improving_2021": {
    "id": "schwarz_improving_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Schwarz",
        "given": "Michael"
      },
      {
        "family": "Saan",
        "given": "Simmo"
      },
      {
        "family": "Seidl",
        "given": "Helmut"
      },
      {
        "family": "Apinis",
        "given": "Kalmer"
      },
      {
        "family": "Erhard",
        "given": "Julian"
      },
      {
        "family": "Vojdani",
        "given": "Vesal"
      }
    ],
    "title": "Improving Thread-Modular Abstract Interpretation",
    "container-title": "arXiv:2108.07613 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "abstract": "We give thread-modular non-relational value analyses as abstractions of a local trace semantics. The semantics as well as the analyses are formulated by means of global invariants and side-effecting constraint systems. We show that a generalization of the analysis provided by the static analyzer Goblint as well as a natural improvement of Antoine Min&bslash;'e's approach can be obtained as instances of this general scheme. We show that these two analyses are incomparable w.r.t. precision and provide a refinement which improves on both precision-wise. We also report on a preliminary experimental comparison of the given analyses on a meaningful suite of benchmarks.",
    "keywords": "Computer Science - Programming Languages",
    "URLtext": "2108.07613",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2108.07613",
    "URL": "http://arxiv.org/abs/2108.07613",
    "_line": "Security.bib:629"
  },
  "singh_multi-view_2021": {
    "id": "singh_multi-view_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Singh",
        "given": "Shirish"
      },
      {
        "family": "Chaturvedy",
        "given": "Kushagra"
      },
      {
        "family": "Mishra",
        "given": "Bharavi"
      }
    ],
    "title": "Multi-View Learning for Repackaged Malware Detection",
    "container-title": "The 16th International Conference on Availability, Reliability and Security",
    "collection-title": "ARES 2021",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-9051-4",
    "abstract": "Repackaging refers to the core process of unpacking a software package, then repackaging it after a probable modification to the decompiled code and/or to other resource files. In the case of repackaged malware, benign apps are injected with a malicious payload. Repackaged malware pose a serious threat to the Android ecosystem. Moreover, repackaged malware and benign apps share more than 80&perc; of their features, which makes detection a challenging problem. This paper presents a novel technique based on multi-view learning to address this challenge of detecting repackaged malware. Multi-View Learning is a technique where data from multiple distinct feature groups, referred to as views, are fused to improve the model’s generalization performance. In the context of Android, we define views as different components of the app, such as permissions, APIs, sensor usage, etc. We analyzed 15,297 repackaged app pairs and extracted seven different views to represent an app. We perform an ablation study to identify which view(s) contribute more to the classification. The model was trained end-to-end to jointly learn appropriate features and to perform the classification. We show that our approach achieves accuracy and an F1-score of 97.46&perc; and 0.98, respectively.",
    "keywords": "Malware detection, Mobile apps, Multi-view learning, Repackaged malware",
    "URL": "https://doi.org/10.1145/3465481.3470040",
    "DOI": "10.1145/3465481.3470040",
    "publisher-place": "New York, NY, USA",
    "page": "1-9",
    "page-first": "1",
    "_line": "Security.bib:643"
  },
  "yu_detecting_2021": {
    "id": "yu_detecting_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Yu",
        "given": "Kunpeng"
      },
      {
        "family": "Wang",
        "given": "Chenxu"
      },
      {
        "family": "Cai",
        "given": "Yan"
      },
      {
        "family": "Luo",
        "given": "Xiapu"
      },
      {
        "family": "Yang",
        "given": "Zijiang"
      }
    ],
    "title": "Detecting concurrency vulnerabilities based on partial orders of memory and thread events",
    "container-title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
    "collection-title": "ESEC/FSE 2021",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8562-6",
    "abstract": "Memory vulnerabilities are the main causes of software security problems. However, detecting vulnerabilities in multi-threaded programs is challenging because many vulnerabilities occur under specific executions, and it is hard to explore all possible executions of a multi-threaded program. Existing approaches are either computationally intensive or likely to miss some vulnerabilities due to the complex thread interleaving. This paper introduces a novel approach to detect concurrency memory vulnerabilities based on partial orders of events. A partial order on a set of events represents the definite execution orders of events. It allows constructing feasible traces exposing specific vulnerabilities by exchanging the execution orders of vulnerability-potential events. It also reduces the search space of possible executions and thus improves computational efficiency. We propose new algorithms to extract vulnerability-potential event pairs for three kinds of memory vulnerabilities. We also design a novel algorithm to compute a potential event pair's feasible set, which contains the relevant events required by a feasible trace. Our method extends existing approaches for data race detection by considering that two events are protected by the same lock. We implement a prototype of our approach and conduct experiments to evaluate its performance. Experimental results show that our tool exhibits superiority over state-of-the-art algorithms in both effectiveness and efficiency.",
    "keywords": "concurrency vulnerability, multi-threaded programs, partial orders",
    "URL": "https://doi.org/10.1145/3468264.3468572",
    "DOI": "10.1145/3468264.3468572",
    "publisher-place": "New York, NY, USA",
    "page": "280-291",
    "page-first": "280",
    "_line": "Security.bib:661"
  },
  "cai_sound_2021": {
    "id": "cai_sound_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Cai",
        "given": "Yan"
      },
      {
        "family": "Yun",
        "given": "Hao"
      },
      {
        "family": "Wang",
        "given": "Jinqiu"
      },
      {
        "family": "Qiao",
        "given": "Lei"
      },
      {
        "family": "Palsberg",
        "given": "Jens"
      }
    ],
    "title": "Sound and efficient concurrency bug prediction",
    "container-title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
    "collection-title": "ESEC/FSE 2021",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "26"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8562-6",
    "abstract": "Concurrency bugs are extremely difficult to detect. Recently, several dynamic techniques achieve sound analysis. M2 is even complete for two threads. It is designed to decide whether two events can occur consecutively. However, real-world concurrency bugs can involve more events and threads. Some can occur when the order of two or more events can be exchanged even if they occur not consecutively. We propose a new technique SeqCheck to soundly decide whether a sequence of events can occur in a specified order. The ordered sequence represents a potential concurrency bug. And several known forms of concurrency bugs can be easily encoded into event sequences where each represents a way that the bug can occur. To achieve it, SeqCheck explicitly analyzes branch events and includes a set of efficient algorithms. We show that SeqCheck is sound; and it is also complete on traces of two threads. We have implemented SeqCheck to detect three types of concurrency bugs and evaluated it on 51 Java benchmarks producing up to billions of events. Compared with M2 and other three recent sound race detectors, SeqCheck detected 333 races in &tilde;30 minutes; while others detected from 130 to 285 races in &tilde;6 to &tilde;12 hours. SeqCheck detected 20 deadlocks in &tilde;6 seconds. This is only one less than Dirk; but Dirk spent more than one hour. SeqCheck also detected 30 atomicity violations in &tilde;20 minutes. The evaluation shows SeqCheck can significantly outperform existing concurrency bug detectors.",
    "keywords": "atomicity violations, Concurrency bugs, data races, deadlocks",
    "URL": "https://doi.org/10.1145/3468264.3468549",
    "DOI": "10.1145/3468264.3468549",
    "publisher-place": "New York, NY, USA",
    "page": "255-267",
    "page-first": "255",
    "_line": "Security.bib:679"
  },
  "han_precise_2021": {
    "id": "han_precise_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Han",
        "given": "HyungSeok"
      },
      {
        "family": "Wesie",
        "given": "Andrew"
      },
      {
        "family": "Pak",
        "given": "Brian"
      }
    ],
    "title": "Precise and Scalable Detection of Use-after-Compacting-Garbage-Collection Bugs",
    "container-title": "Proceeding of 30th USENIX Security Symposium",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "11"
        ]
      ]
    },
    "abstract": "Compacting garbage collection (compact-gc) is a method that improves memory utilization and reduces memory fragmentation by rearranging live objects and updating their references using an address table. A critical use-after-free bug may exist if an object reference that is not registered in the address table is used after compact-gc, as the live object may be moved but the reference will not be updated after compact-gc. We refer to this as a use-after-compact-gc (use-after-cgc) bug. Prior tools have attempted to statically detect these bugs with targetspeciﬁc heuristics. However, due to their path-insensitive analysis and imprecise target-speciﬁc heuristics, they have high false-positives and false-negatives. In this paper, we present a precise and scalable static analyzer, named CGSan, for ﬁnding use-after-cgc bugs. CGSan detects use-after-cgc bug candidates by intra-procedural static symbolic taint analysis and checks their feasibility by underconstrained directed symbolic execution. To mitigate the incompleteness of intra-procedural analysis, we employ a typebased taint policy. For scalability, we propose using directed inter-procedural control-ﬂow graphs, which reduce search spaces by excluding paths irrelevant to checking feasibility, and directed scheduling, which prioritizes paths to quickly check feasibility. We evaluated CGSan on Google V8 and Mozilla SpiderMonkey, and we found 13 unique use-after-cgc bugs with only 2 false-positives while two prior tools missed 10 bugs and had 34 false-positives in total.",
    "page": "17",
    "page-first": "17",
    "language": "en-US",
    "_line": "Security.bib:697"
  },
  "disselkoen_finding_nodate": {
    "id": "disselkoen_finding_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Disselkoen",
        "given": "Craig"
      },
      {
        "family": "Cauligi",
        "given": "Sunjay"
      },
      {
        "family": "Tullsen",
        "given": "Dean"
      },
      {
        "family": "Stefan",
        "given": "Deian"
      }
    ],
    "title": "Finding and Eliminating Timing Side-Channels in Crypto Code with Pitchfork",
    "abstract": "We present Pitchfork, a symbolic analysis tool which veriﬁes that code is constant-time and does not leak secret values. Writing constant-time code is the de-facto defense against timing side-channel attacks, used today by many major cryptographic libraries. Unfortunately, writing constant-time code is notoriously difﬁcult. Even experts have repeatedly written buggy code and overlooked critical vulnerabilities in widely used cryptographic libraries. To address these issues, Pitchfork veriﬁes that code is constant-time. In particular, we used Pitchfork to verify that a large portion of Mozilla’s NSS cryptographic library is constant-time, while also ﬁnding several constant-time vulnerabilities, including a critical vulnerability which was assigned CVE-2019-11745.",
    "page": "8",
    "page-first": "8",
    "language": "en-US",
    "_line": "Security.bib:708"
  },
  "li_towards_2021": {
    "id": "li_towards_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "Zhen"
      },
      {
        "family": "Tang",
        "given": "Jing"
      },
      {
        "family": "Zou",
        "given": "Deqing"
      },
      {
        "family": "Chen",
        "given": "Qian"
      },
      {
        "family": "Xu",
        "given": "Shouhuai"
      },
      {
        "family": "Zhang",
        "given": "Chao"
      },
      {
        "family": "Li",
        "given": "Yichen"
      },
      {
        "family": "Jin",
        "given": "Hai"
      }
    ],
    "title": "Towards Making Deep Learning-based Vulnerability Detectors Robust",
    "container-title": "arXiv:2108.00669 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "8",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "8",
          "9"
        ]
      ]
    },
    "abstract": "Automatically detecting software vulnerabilities in source code is an important problem that has attracted much attention. In particular, deep learning-based vulnerability detectors, or DL-based detectors, are attractive because they do not need human experts to define features or patterns of vulnerabilities. However, such detectors' robustness is unclear. In this paper, we initiate the study in this aspect by demonstrating that DL-based detectors are not robust against simple code transformations, dubbed attacks in this paper, as these transformations may be leveraged for malicious purposes. As a first step towards making DL-based detectors robust against such attacks, we propose an innovative framework, dubbed ZigZag, which is centered at (i) decoupling feature learning and classifier learning and (ii) using a ZigZag-style strategy to iteratively refine them until they converge to robust features and robust classifiers. Experimental results show that the ZigZag framework can substantially improve the robustness of DL-based detectors.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Machine Learning",
    "URLtext": "2108.00669",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2108.00669",
    "URL": "http://arxiv.org/abs/2108.00669",
    "_line": "Security.bib:735"
  },
  "strydonck_proving_nodate": {
    "id": "strydonck_proving_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Strydonck",
        "given": "Thomas Van"
      },
      {
        "family": "Leuven",
        "given": "KU"
      },
      {
        "family": "Georges",
        "given": "Aına Linn"
      },
      {
        "family": "Gueneau",
        "given": "Armael"
      },
      {
        "family": "Trieu",
        "given": "Alix"
      }
    ],
    "title": "Proving full-system security properties under multiple attacker models on capability machines",
    "abstract": "Assembly-level protection mechanisms (virtual memory, trusted execution environments, virtualization) make it possible to guarantee security properties of a full system in the presence of arbitrary attacker provided code. However, they typically only support a single trust boundary: code is either trusted or untrusted, and protection cannot be nested. Capability machines provide protection mechanisms that are more ﬁnegrained and that do support arbitrary nesting of protection. We show in this paper how this enables the formal veriﬁcation of fullsystem security properties under multiple attacker models: different security objectives of the full system can be veriﬁed under a different choice of trust boundary (i.e. under a different attacker model). The veriﬁcation approach we propose is modular, and is robust: code outside the trust boundary for a given security objective can be arbitrary, unveriﬁed attacker-provided code. It is based on the use of universal contracts for untrusted adversarial code: sound, conservative contracts which can be combined with manual veriﬁcation of trusted components in a compositional program logic. Compositionality of the program logic also allows us to reuse common parts in the analyses for different attacker models. We instantiate the approach concretely by extending an existing capability machine model with support for memorymapped I/O and we obtain full system, machine-veriﬁed security properties about external effect traces while limiting the manual veriﬁcation effort to a small trusted computing base relevant for the speciﬁc property under study.",
    "page": "16",
    "page-first": "16",
    "language": "en-US",
    "_line": "Security.bib:749"
  },
  "jourdan_verasco_2016": {
    "id": "jourdan_verasco_2016",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Jourdan",
        "given": "Jacques-Henri"
      }
    ],
    "title": "Verasco: a Formally Verified C Static Analyzer",
    "issued": {
      "date-parts": [
        [
          "2016",
          "5",
          "26"
        ]
      ]
    },
    "publisher": "L’université Paris Diderot (Paris 7) Sorbonne Paris Cité",
    "number-of-pages": "240",
    "abstract": "In order to develop safer software for critical applications, some static analyzers aim at establishing, with mathematical certitude, the absence of some classes of bug in the input program. A possible limit to this approach is the possibility of a soundness bug in the static analyzer itself, which would nullify the guarantees it is supposed to deliver.",
    "URL": "https://jhjourdan.mketjh.fr/thesis_jhjourdan.pdf",
    "publisher-place": "Paris, France",
    "language": "fr-FR",
    "_line": "Security.bib:758"
  },
  "marty_lio_2020": {
    "id": "marty_lio_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Marty",
        "given": "Jean-Joseph"
      },
      {
        "family": "Franceschino",
        "given": "Lucas"
      },
      {
        "family": "Talpin",
        "given": "Jean-Pierre"
      },
      {
        "family": "Vazou",
        "given": "Niki"
      }
    ],
    "title": "LIO\\*: Low Level Information Flow Control in F\\*",
    "container-title": "arXiv:2004.12885 \\[cs\\]",
    "container-title-short": "LIO\\*",
    "title-short": "LIO\\*",
    "issued": {
      "date-parts": [
        [
          "2020",
          "4",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "29"
        ]
      ]
    },
    "abstract": "We present Labeled Input Output in F\\* (LIO\\*), a verified framework that enforces information flow control (IFC) policies developed in F\\* and automatically extracted to C. Inspired by LIO, we encapsulated IFC policies into effects, but using F\\* we derived efficient, low level, and provably correct code. Concretely, runtime checks are lifted to static proof obligations, the developed code is automatically extracted to C and proved non-interferent using metaprogramming. We benchmarked our framework on three clients and observed up to 54&perc; speedup when IFC runtime checks are proved statically. Our framework is designed to aid development of embedded devices where both enforcement of security policies and low level efficient code is critical.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages",
    "URLtext": "2004.12885",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2004.12885",
    "URL": "http://arxiv.org/abs/2004.12885",
    "_line": "Security.bib:772"
  },
  "lu_eagle_2021": {
    "id": "lu_eagle_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Lu",
        "given": "Jingbo"
      },
      {
        "family": "He",
        "given": "Dongjie"
      },
      {
        "family": "Xue",
        "given": "Jingling"
      }
    ],
    "title": "Eagle: CFL-Reachability-Based Precision-Preserving Acceleration of Object-Sensitive Pointer Analysis with Partial Context Sensitivity",
    "container-title": "ACM Transactions on Software Engineering and Methodology",
    "container-title-short": "Eagle",
    "title-short": "Eagle",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "23"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "27"
        ]
      ]
    },
    "issn": "1049-331X",
    "abstract": "Object sensitivity is widely used as a context abstraction for computing the points-to information context-sensitively for object-oriented programming languages such as Java. Due to the combinatorial explosion of contexts in large object-oriented programs, k-object-sensitive pointer analysis (under k-limiting), denoted k-obj, is often inefficient even when it is scalable for small values of k, where k ⩽ 2 holds typically. A recent popular approach for accelerating k-obj trades precision for efficiency by instructing k-obj to analyze only some methods in a program context-sensitively, determined heuristically by a pre-analysis. In this article, we investigate how to develop a fundamentally different approach, Eagle, for designing a pre-analysis that can make k-obj run significantly faster while maintaining its precision. The novelty of Eagle is to enable k-obj to analyze a method with partial context sensitivity (i.e., context-sensitively for only some of its selected variables/allocation sites) by solving a context-free-language (CFL) reachability problem based on a new CFL-reachability formulation of k-obj. By regularizing one CFL for specifying field accesses and using another CFL for specifying method calls, we have formulated Eagle as a fully context-sensitive taint analysis (without k-limiting) that is both effective (by selecting the variables/allocation sites to be analyzed by k-obj context-insensitively so as to reduce the number of context-sensitive facts inferred by k-obj in the program) and efficient (by running linearly in terms of the number of pointer assignment edges in the program). As Eagle represents the first precision-preserving pre-analysis, our evaluation focuses on demonstrating its significant performance benefits in accelerating k-obj for a set of popular Java benchmarks and applications, with call graph construction, may-fail-casting, and polymorphic call detection as three important client analyses.",
    "keywords": "CFL-reachability, object sensitivity, Pointer analysis",
    "URL": "https://doi.org/10.1145/3450492",
    "DOI": "10.1145/3450492",
    "page": "46:1-46:46",
    "page-first": "46",
    "volume": "30",
    "issue": "4",
    "_line": "Security.bib:787"
  },
  "murray_incremental_2021": {
    "id": "murray_incremental_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Murray",
        "given": "Toby"
      },
      {
        "family": "Yan",
        "given": "Pengbo"
      },
      {
        "family": "Ernst",
        "given": "Gidon"
      }
    ],
    "title": "Incremental Vulnerability Detection via Back-Propagating Symbolic Execution of Insecurity Separation Logic",
    "container-title": "arXiv:2107.05225 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "20"
        ]
      ]
    },
    "abstract": "We present the first compositional, incremental static analysis for detecting memory-safety and information leakage vulnerabilities in C-like programs. To do so, we develop the first under-approximate relational program logics, including Insecurity Separation Logic (InsecSL). We show how InsecSL can be automated via back-propagating symbolic execution (BPSE) to build a bottom-up, inter-procedural and incremental analysis for detecting vulnerabilities. We prove our approach sound in Isabelle/HOL and implement it in a proof-of-concept tool, Underflow, for analysing C programs, which we apply to various case studies.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages, Computer Science - Logic in Computer Science",
    "URLtext": "2107.05225",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2107.05225",
    "URL": "http://arxiv.org/abs/2107.05225",
    "_line": "Security.bib:806"
  },
  "fei_security_2021": {
    "id": "fei_security_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Fei",
        "given": "Shufan"
      },
      {
        "family": "Yan",
        "given": "Zheng"
      },
      {
        "family": "Ding",
        "given": "Wenxiu"
      },
      {
        "family": "Xie",
        "given": "Haomeng"
      }
    ],
    "title": "Security Vulnerabilities of SGX and Countermeasures: A Survey",
    "container-title": "ACM Computing Surveys",
    "container-title-short": "Security Vulnerabilities of SGX and Countermeasures",
    "title-short": "Security Vulnerabilities of SGX and Countermeasures",
    "issued": {
      "date-parts": [
        [
          "2021",
          "7",
          "13"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "7",
          "20"
        ]
      ]
    },
    "issn": "0360-0300",
    "abstract": "Trusted Execution Environments (TEEs) have been widely used in many security-critical applications. The popularity of TEEs derives from its high security and trustworthiness supported by secure hardware. Intel Software Guard Extensions (SGX) is one of the most representative TEEs that creates an isolated environment on an untrusted operating system, thus providing run-time protection for the execution of security-critical code and data. However, Intel SGX is far from the acme of perfection. It has become a target of various attacks due to its security vulnerabilities. Researchers and practitioners have paid attention to the security vulnerabilities of SGX and investigated optimization solutions in real applications. Unfortunately, existing literature lacks a thorough review of security vulnerabilities of SGX and their countermeasures. In this article, we fill this gap. Specifically, we propose two sets of criteria for estimating security risks of existing attacks and evaluating defense effects brought by attack countermeasures. Furthermore, we propose a taxonomy of SGX security vulnerabilities and shed light on corresponding attack vectors. After that, we review published attacks and existing countermeasures, as well as evaluate them by employing our proposed criteria. At last, on the strength of our survey, we propose some open challenges and future directions in the research of SGX security.",
    "keywords": "security, side-channel attacks, Trusted execution environment, trustworthiness",
    "URL": "https://doi.org/10.1145/3456631",
    "DOI": "10.1145/3456631",
    "page": "126:1-126:36",
    "page-first": "126",
    "volume": "54",
    "issue": "6",
    "_line": "Security.bib:820"
  },
  "zhang_checking_2021": {
    "id": "zhang_checking_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Zhang",
        "given": "Zhen"
      },
      {
        "family": "Feng",
        "given": "Yu"
      },
      {
        "family": "Ernst",
        "given": "Michael D"
      },
      {
        "family": "Porst",
        "given": "Sebastian"
      },
      {
        "family": "Dillig",
        "given": "Isil"
      }
    ],
    "title": "Checking Conformance of Applications against GUI Policies",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "abstract": "A good graphical user interface (GUI) is crucial for an application’s usability, so vendors and regulatory agencies increasingly place restrictions on how GUI elements should appear to and interact with users. Motivated by this concern, this paper presents a new technique (based on static analysis) for checking conformance between (Android) applications and GUI g expressed in a formal specification language. In particular, this paper (1) describes a specification language for formalizing GUI policies, (2) proposes a new program abstraction called an event-driven layout forest, and (3) describes a static analysis for constructing this abstraction and checking it against a GUI policy. We have implemented the proposed approach in a tool called Venus, and we evaluate it on 2361 Android applications and 17 policies. Our evaluation shows that Venus can uncover malicious applications that perform ad fraud and identify violations of GUI design guidelines and GDPR laws.",
    "page": "12",
    "page-first": "12",
    "language": "en-US",
    "_line": "Security.bib:839"
  },
  "straub_use_2020": {
    "id": "straub_use_2020",
    "type": "paper-conference",
    "author": [
      {
        "family": "Straub",
        "given": "Jeremy"
      }
    ],
    "title": "The Use of Runtime Verification for Identifying and Responding to Cybersecurity Threats Posed to State Actors During Cyberwarfare",
    "container-title": "2020 International Conference on Computational Science and Computational Intelligence (CSCI)",
    "event-title": "2020 International Conference on Computational Science and Computational Intelligence (CSCI)",
    "issued": {
      "date-parts": [
        [
          "2020",
          "12"
        ]
      ]
    },
    "abstract": "This paper considers the utility of the use of runtime verification techniques for detecting and responding to cybersecurity threats. To this end, it considers two questions: First, it evaluates the efficacy of runtime verification for identifying zero-day threats and threats that are not otherwise widely known based up system operations. Second, it considers the particular use of these techniques by state actors (i.e., nation states engaged in declared or undeclared cyberwarfare) who are likely to encounter a greater level of such vulnerability exploits than individuals or private businesses during the normal operations. Drawing on the analysis in the two foregoing areas, the paper concludes by identifying key areas of needed future work to support runtime verification's application in this area.",
    "keywords": "Runtime, Business, Computational intelligence, Cyber warfare, cyberattacks, cybersecurity, cyberwarfare, runtime verification, Scientific computing, Systems operation, threats",
    "DOI": "10.1109/CSCI51800.2020.00021",
    "page": "83-87",
    "page-first": "83",
    "_line": "Security.bib:849"
  },
  "hublet_databank_2021": {
    "id": "hublet_databank_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Hublet",
        "given": "François"
      }
    ],
    "title": "The Databank Model",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "4",
          "20"
        ]
      ]
    },
    "URL": "http://hdl.handle.net/20.500.11850/477329",
    "DOI": "10.3929/ETHZ-B-000477329",
    "page": "212 p.",
    "page-first": "212",
    "note": "Artwork Size: 212 p.\nMedium: application/pdf\nPublisher: ETH Zurich",
    "language": "en-US",
    "_line": "Security.bib:861"
  },
  "cecchetti_compositional_nodate": {
    "id": "cecchetti_compositional_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Cecchetti",
        "given": "Ethan"
      },
      {
        "family": "Yao",
        "given": "Siqiu"
      },
      {
        "family": "Ni",
        "given": "Haobin"
      },
      {
        "family": "Myers",
        "given": "Andrew C"
      }
    ],
    "title": "Compositional Security for Reentrant Applications (Technical Report)",
    "abstract": "The disastrous vulnerabilities in smart contracts sharply remind us of our ignorance: we do not know how to write code that is secure in composition with malicious code. Information ﬂow control has long been proposed as a way to achieve compositional security, offering strong guarantees even when combining software from different trust domains. Unfortunately, this appealing story breaks down in the presence of reentrancy attacks. We formalize a general deﬁnition of reentrancy and introduce a security condition that allows software modules like smart contracts to protect their key invariants while retaining the expressive power of safe forms of reentrancy. We present a security type system that provably enforces secure information ﬂow; in conjunction with run-time mechanisms, it enforces secure reentrancy even in the presence of unknown code; and it helps locate and correct recent high-proﬁle vulnerabilities.",
    "page": "56",
    "page-first": "56",
    "language": "en-US",
    "_line": "Security.bib:879"
  },
  "wang_bci-cfi_2021": {
    "id": "wang_bci-cfi_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Wang",
        "given": "Ye"
      },
      {
        "family": "Li",
        "given": "Qingbao"
      },
      {
        "family": "Chen",
        "given": "Zhifeng"
      },
      {
        "family": "Zhang",
        "given": "Ping"
      },
      {
        "family": "Zhang",
        "given": "Guimin"
      },
      {
        "family": "Shi",
        "given": "Zhihui"
      }
    ],
    "title": "BCI-CFI: A Context-Sensitive Control-Flow Integrity Method Based on Branch Correlation Integrity",
    "container-title": "Information and Software Technology",
    "container-title-short": "BCI-CFI",
    "title-short": "BCI-CFI",
    "issued": {
      "date-parts": [
        [
          "2021",
          "3",
          "19"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "24"
        ]
      ]
    },
    "issn": "0950-5849",
    "abstract": "Context\n: As part of the arms race, one emerging attack methodology has been control-hijacking attacks, e.g., return-oriented programming (ROP). Control-flow integrity (CFI) is a generic and effective defence against most control-hijacking attacks. However, existing CFI mechanisms have poor security as demonstrated by their equivalence class (EC) sizes, which are sets of targets that CFI policies cannot distinguish. Adversaries can choose an illegitimate control transfer within an EC that is included in the resulting CFG and incorrectly allowed by CFI protection policies.\nObjective\n: The paper introduces a context-sensitive control-flow integrity method, which aims to improve the security of CFI and prevent ROP attacks.\nMethod\n: The paper presents BCI-CFI, a context-sensitive CFI technique based on branch correlation integrity (BCI), which can effectively break down EC sizes and improve the security of CFI. BCI-CFI takes the branch correlation relationship (i.e., a new type of context for CFI) as contextual information to refine the CFI policy and identify the BCI pairs in the target program via static analysis. Furthermore, the paper introduces a state machine MCFI for BCI-CFI to conduct target validation for the indirect control-flow transfer (ICT) instructions in the target program at runtime.\nResults\n: Our results show that, (i) BCI-CFI prevented adversaries from manipulating the control data and launching ROP attacks, (ii) protected both forward and backward ICT in the target program, and improved the security and effectiveness of CFI, and (iii) BCI-CFI introduced a 19.67&perc; runtime overhead on average and a maximum runtime overhead of 31.2&perc;\nConclusion\n: BCI-CFI is a context-sensitive CFI technique aiming to prevent adversaries from manipulating the control data of the target program to launch ROP attacks. BCI-CFI can reduce EC sizes and improve the security of CFI while incurring a moderate runtime overhead on average.",
    "keywords": "BCI-CFI, Branch correlation integrity, Equivalence class, Indirect control-flow transfer, ROP",
    "URL": "https://www.sciencedirect.com/science/article/pii/S0950584921000550",
    "DOI": "10.1016/j.infsof.2021.106572",
    "page": "106572",
    "page-first": "106572",
    "language": "en-US",
    "_line": "Security.bib:888"
  },
  "wu_vulnerability_2021": {
    "id": "wu_vulnerability_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Wu",
        "given": "Y."
      },
      {
        "family": "Lu",
        "given": "J."
      },
      {
        "family": "Zhang",
        "given": "Y."
      },
      {
        "family": "Jin",
        "given": "S."
      }
    ],
    "title": "Vulnerability Detection in C/C++ Source Code With Graph Representation Learning",
    "container-title": "2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC)",
    "event-title": "2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC)",
    "issued": {
      "date-parts": [
        [
          "2021",
          "1"
        ]
      ]
    },
    "abstract": "An open challenge in software vulnerability detection is how to identify potential vulnerabilities of source code at a fine-grained level automatically. This paper proposes an approach to automate vulnerability detection in source code at the software function level based on graph representation learning without the efforts of security experts. The proposed approach firstly represents software functions as Simplified Code Property Graphs (SCPG), which can conserve syntactic and semantic information of source code while keeping itself small enough for computing. It then utilizes graph neural network and multi layer perceptrons to learn graph representations and extract features automatically, saving efforts of feature engineering. The comparison experiments demonstrate the effectiveness of the proposed approach.",
    "keywords": "Security, Semantics, Syntactics, Software, Conferences, Feature extraction, Graph Neural Network, Graph neural networks, Source Code, Vulnerability Detection",
    "DOI": "10.1109/CCWC51732.2021.9376145",
    "page": "1519-1524",
    "page-first": "1519",
    "_line": "Security.bib:915"
  },
  "busse_running_2020": {
    "id": "busse_running_2020",
    "type": "paper-conference",
    "author": [
      {
        "family": "Busse",
        "given": "Frank"
      },
      {
        "family": "Nowack",
        "given": "Martin"
      },
      {
        "family": "Cadar",
        "given": "Cristian"
      }
    ],
    "title": "Running symbolic execution forever",
    "container-title": "Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis",
    "collection-title": "ISSTA 2020",
    "issued": {
      "date-parts": [
        [
          "2020",
          "7",
          "18"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "9"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8008-9",
    "abstract": "When symbolic execution is used to analyse real-world applications, it often consumes all available memory in a relatively short amount of time, sometimes making it impossible to analyse an application for an extended period. In this paper, we present a technique that can record an ongoing symbolic execution analysis to disk and selectively restore paths of interest later, making it possible to run symbolic execution indefinitely. To be successful, our approach addresses several essential research challenges related to detecting divergences on re-execution, storing long-running executions efficiently, changing search heuristics during re-execution, and providing a global view of the stored execution. Our extensive evaluation of 93 Linux applications shows that our approach is practical, enabling these applications to run for days while continuing to explore new execution paths.",
    "keywords": "KLEE, memoization, symbolic execution",
    "URL": "https://doi.org/10.1145/3395363.3397360",
    "DOI": "10.1145/3395363.3397360",
    "publisher-place": "New York, NY, USA",
    "page": "63-74",
    "page-first": "63",
    "_line": "Security.bib:928"
  },
  "cadar_klee_2008": {
    "id": "cadar_klee_2008",
    "type": "paper-conference",
    "author": [
      {
        "family": "Cadar",
        "given": "Cristian"
      },
      {
        "family": "Dunbar",
        "given": "Daniel"
      },
      {
        "family": "Engler",
        "given": "Dawson"
      }
    ],
    "title": "KLEE: unassisted and automatic generation of high-coverage tests for complex systems programs",
    "container-title": "Proceedings of the 8th USENIX conference on Operating systems design and implementation",
    "container-title-short": "KLEE",
    "collection-title": "OSDI'08",
    "title-short": "KLEE",
    "issued": {
      "date-parts": [
        [
          "2008",
          "12",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "8"
        ]
      ]
    },
    "publisher": "USENIX Association",
    "abstract": "We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage &ndash; on average over 90&perc; per tool (median: over 94&perc;) &ndash; and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100&perc; coverage on 31 of them. We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.",
    "publisher-place": "USA",
    "page": "209-224",
    "page-first": "209",
    "_line": "Security.bib:945"
  },
  "islam_classification_2010": {
    "id": "islam_classification_2010",
    "type": "paper-conference",
    "author": [
      {
        "family": "Islam",
        "given": "R."
      },
      {
        "family": "Tian",
        "given": "R."
      },
      {
        "family": "Batten",
        "given": "L."
      },
      {
        "family": "Versteeg",
        "given": "S."
      }
    ],
    "title": "Classification of Malware Based on String and Function Feature Selection",
    "container-title": "2010 Second Cybercrime and Trustworthy Computing Workshop",
    "event-title": "2010 Second Cybercrime and Trustworthy Computing Workshop",
    "issued": {
      "date-parts": [
        [
          "2010",
          "7"
        ]
      ]
    },
    "abstract": "Anti-malware software producers are continually challenged to identify and counter new malware as it is released into the wild. A dramatic increase in malware production in recent years has rendered the conventional method of manually determining a signature for each new malware sample untenable. This paper presents a scalable, automated approach for detecting and classifying malware by using pattern recognition algorithms and statistical methods at various stages of the malware analysis life cycle. Our framework combines the static features of function length and printable string information extracted from malware samples into a single test which gives classification results better than those achieved by using either feature individually. In our testing we input feature information from close to 1400 unpacked malware samples to a number of different classification algorithms. Using k-fold cross validation on the malware, which includes Trojans and viruses, along with 151 clean files, we achieve an overall classification accuracy of over 98&perc;.",
    "keywords": "Software, Feature extraction, Accuracy, classification, Data mining, Databases, function feature selection, function length, invasive software, k-fold cross validation, Malware, malware analysis life cycle, malware classification, pattern recognition, pattern recognition algorithm, static feature, statistical analysis, statistical method, string, string feature selection, Support vector machine classification",
    "DOI": "10.1109/CTC.2010.11",
    "page": "9-17",
    "page-first": "9",
    "_line": "Security.bib:960"
  },
  "witten_data_nodate": {
    "id": "witten_data_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Witten",
        "given": "Ian"
      },
      {
        "family": "Frank",
        "given": "Eibe"
      },
      {
        "family": "Hall",
        "given": "Mark"
      },
      {
        "family": "Pal",
        "given": "Chris"
      }
    ],
    "title": "Data Mining: Practical Machine Learning Tools and Techniques",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "5"
        ]
      ]
    },
    "URL": "https://www.cs.waikato.ac.nz/ml/weka/book.html",
    "_line": "Security.bib:973"
  },
  "rumelhart_learning_1986": {
    "id": "rumelhart_learning_1986",
    "type": "article-journal",
    "author": [
      {
        "family": "Rumelhart",
        "given": "David E."
      },
      {
        "family": "Hinton",
        "given": "Geoffrey E."
      },
      {
        "family": "Williams",
        "given": "Ronald J."
      }
    ],
    "title": "Learning representations by back-propagating errors",
    "container-title": "Nature",
    "issued": {
      "date-parts": [
        [
          "1986",
          "10"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "5"
        ]
      ]
    },
    "issn": "1476-4687",
    "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.",
    "URL": "https://www.nature.com/articles/323533a0",
    "DOI": "10.1038/323533a0",
    "page": "533-536",
    "page-first": "533",
    "volume": "323",
    "note": "Number: 6088\nPublisher: Nature Publishing Group",
    "issue": "6088",
    "language": "en-US",
    "_line": "Security.bib:981"
  },
  "jeong_razzer_2019": {
    "id": "jeong_razzer_2019",
    "type": "paper-conference",
    "author": [
      {
        "family": "Jeong",
        "given": "Dae R."
      },
      {
        "family": "Kim",
        "given": "Kyungtae"
      },
      {
        "family": "Shivakumar",
        "given": "Basavesh"
      },
      {
        "family": "Lee",
        "given": "Byoungyoung"
      },
      {
        "family": "Shin",
        "given": "Insik"
      }
    ],
    "title": "Razzer: Finding Kernel Race Bugs through Fuzzing",
    "container-title": "2019 IEEE Symposium on Security and Privacy (SP)",
    "container-title-short": "Razzer",
    "title-short": "Razzer",
    "event-title": "2019 IEEE Symposium on Security and Privacy (SP)",
    "issued": {
      "date-parts": [
        [
          "2019",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "5"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-5386-6660-9",
    "abstract": "A data race in a kernel is an important class of bugs, critically impacting the reliability and security of the associated system. As a result of a race, the kernel may become unresponsive. Even worse, an attacker may launch a privilege escalation attack to acquire root privileges. In this paper, we propose RAZZER, a tool to find race bugs in kernels. The core of RAZZER is in guiding fuzz testing towards potential data race spots in the kernel. RAZZER employs two techniques to find races efficiently: a static analysis and a deterministic thread interleaving technique. Using a static analysis, RAZZER identifies over-approximated potential data race spots, guiding the fuzzer to search for data races in the kernel more efficiently. Using the deterministic thread interleaving technique implemented at the hypervisor, RAZZER tames the non-deterministic behavior of the kernel such that it can deterministically trigger a race. We implemented a prototype of RAZZER and ran the latest Linux kernel (from v4.16-rc3 to v4.18rc3) using RAZZER. As a result, RAZZER discovered 30 new races in the kernel, with 16 subsequently confirmed and accordingly patched by kernel developers after they were reported.",
    "URL": "https://ieeexplore.ieee.org/document/8835326/",
    "DOI": "10.1109/SP.2019.00017",
    "publisher-place": "San Francisco, CA, USA",
    "page": "754-768",
    "page-first": "754",
    "language": "en-US",
    "_line": "Security.bib:1001"
  },
  "chen_savior_2019": {
    "id": "chen_savior_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Chen",
        "given": "Yaohui"
      },
      {
        "family": "Li",
        "given": "Peng"
      },
      {
        "family": "Xu",
        "given": "Jun"
      },
      {
        "family": "Guo",
        "given": "Shengjian"
      },
      {
        "family": "Zhou",
        "given": "Rundong"
      },
      {
        "family": "Zhang",
        "given": "Yulong"
      },
      {
        "family": "Taowei"
      },
      {
        "family": "Lu",
        "given": "Long"
      }
    ],
    "title": "SAVIOR: Towards Bug-Driven Hybrid Testing",
    "container-title": "arXiv:1906.07327 \\[cs\\]",
    "container-title-short": "SAVIOR",
    "title-short": "SAVIOR",
    "issued": {
      "date-parts": [
        [
          "2019",
          "6",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "3",
          "5"
        ]
      ]
    },
    "abstract": "Hybrid testing combines fuzz testing and concolic execution. It leverages fuzz testing to test easy-to-reach code regions and uses concolic execution to explore code blocks guarded by complex branch conditions. However, its code coverage-centric design is inefficient in vulnerability detection. First, it blindly selects seeds for concolic execution and aims to explore new code continuously. However, as statistics show, a large portion of the explored code is often bug-free. Therefore, giving equal attention to every part of the code during hybrid testing is a non-optimal strategy. It slows down the detection of real vulnerabilities by over 43&perc;. Second, classic hybrid testing quickly moves on after reaching a chunk of code, rather than examining the hidden defects inside. It may frequently miss subtle vulnerabilities despite that it has already explored the vulnerable code paths. We propose SAVIOR, a new hybrid testing framework pioneering a bug-driven principle. Unlike the existing hybrid testing tools, SAVIOR prioritizes the concolic execution of the seeds that are likely to uncover more vulnerabilities. Moreover, SAVIOR verifies all vulnerable program locations along the executing program path. By modeling faulty situations using SMT constraints, SAVIOR reasons the feasibility of vulnerabilities and generates concrete test cases as proofs. Our evaluation shows that the bug-driven approach outperforms mainstream automated testing techniques, including state-of-the-art hybrid testing systems driven by code coverage. On average, SAVIOR detects vulnerabilities 43.4&perc; faster than DRILLER and 44.3&perc; faster than QSYM, leading to the discovery of 88 and 76 more uniquebugs,respectively.Accordingtotheevaluationon11 well fuzzed benchmark programs, within the first 24 hours, SAVIOR triggers 481 UBSAN violations, among which 243 are real bugs.",
    "keywords": "Computer Science - Software Engineering",
    "URLtext": "1906.07327",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1906.07327",
    "URL": "http://arxiv.org/abs/1906.07327",
    "_line": "Security.bib:1020"
  },
  "wang_data-driven_2021": {
    "id": "wang_data-driven_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Wang",
        "given": "Jingbo"
      },
      {
        "family": "Sung",
        "given": "Chungha"
      },
      {
        "family": "Raghothaman",
        "given": "Mukund"
      },
      {
        "family": "Wang",
        "given": "Chao"
      }
    ],
    "title": "Data-Driven Synthesis of Provably Sound Side Channel Analyses",
    "container-title": "arXiv:2102.06753 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "22"
        ]
      ]
    },
    "abstract": "We propose a data-driven method for synthesizing a static analyzer to detect side-channel information leaks in cryptographic software. Compared to the conventional way of manually crafting such a static analyzer, which can be labor intensive, error prone and suboptimal, our learning-based technique is not only automated but also provably sound. Our analyzer consists of a set of type-inference rules learned from the training data, i.e., example code snippets annotated with ground truth. Internally, we use syntax-guided synthesis (SyGuS) to generate new features and decision tree learning (DTL) to generate type-inference rules based on these features. We guarantee soundness by formally proving each learned rule via a technique called Datalog query containment checking. We have implemented our technique in the LLVM compiler and used it to detect power side channels in C programs. Our results show that, in addition to being automated and provably sound during synthesis, the learned analyzer also has the same empirical accuracy as two state-of-the-art, manually crafted analyzers while being 300X and 900X faster, respectively.",
    "keywords": "Computer Science - Programming Languages, Computer Science - Software Engineering",
    "URLtext": "2102.06753",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2102.06753",
    "URL": "http://arxiv.org/abs/2102.06753",
    "_line": "Security.bib:1051"
  },
  "morrisett_ipdl_2021": {
    "id": "morrisett_ipdl_2021",
    "type": "report",
    "author": [
      {
        "family": "Morrisett",
        "given": "Greg"
      },
      {
        "family": "Shi",
        "given": "Elaine"
      },
      {
        "family": "Sojakova",
        "given": "Kristina"
      },
      {
        "family": "Fan",
        "given": "Xiong"
      },
      {
        "family": "Gancher",
        "given": "Joshua"
      }
    ],
    "title": "IPDL: A Simple Framework for Formally Verifying Distributed Cryptographic Protocols",
    "container-title-short": "IPDL",
    "title-short": "IPDL",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "22"
        ]
      ]
    },
    "abstract": "Although there have been many successes in verifying proofs of non-interactive cryptographic primitives such as encryption and signatures, formal verification of interactive cryptographic protocols is still a nascent area. While in principle, it seems possible to extend general frameworks such as Easycrypt to encode proofs for more complex, interactive protocols, a big challenge is whether the human effort would be scalable enough for proof mechanization to eventually acquire mainstream usage among the cryptography community. We work towards closing this gap by introducing a simple framework, Interactive Probabilistic Dependency Logic (IPDL), for reasoning about a certain well-behaved subset of cryptographic protocols. A primary design goal of IPDL is for formal cryptographic proofs to resemble their on-paper counterparts. To this end, IPDL includes an equational logic to reason about approximate observational equivalence (i.e., computational indistinguishability) properties between protocols. IPDL adopts a channel-centric core logic, which decomposes the behavior of the protocol into the behaviors along each communication channel. IPDL supports straight-line programs with statically bounded loops. This design allows us to capture a broad class of protocols encountered in the cryptography literature, including multi-party, reactive, and/or inductively-defined protocols; meanwhile, the logic can track the runtime of the computational reduction in security proofs, thus ensuring computational soundness. We demonstrate the use of IPDL by a number of case studies, including a multi-use, secure message communication protocol, a multi-party coin toss with abort protocol, several oblivious transfer constructions, as well as the two-party GMW protocol for securely evaluating general circuits. We provide a mechanization of the IPDL proof system and our case studies in Coq, and our code is open sourced at https://github.com/ipdl/ipdl.",
    "URL": "http://eprint.iacr.org/2021/147",
    "number": "147",
    "_line": "Security.bib:1065"
  },
  "yadav_light-weighted_2020": {
    "id": "yadav_light-weighted_2020",
    "type": "article-journal",
    "author": [
      {
        "family": "Yadav",
        "given": "Ritu"
      }
    ],
    "title": "Light-Weighted CNN for Text Classification",
    "container-title": "arXiv:2004.07922 \\[cs, stat\\]",
    "issued": {
      "date-parts": [
        [
          "2020",
          "4",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "22"
        ]
      ]
    },
    "abstract": "For management, documents are categorized into a specific category, and to do these, most of the organizations use manual labor. In today's automation era, manual efforts on such a task are not justified, and to avoid this, we have so many software out there in the market. However, efficiency and minimal resource consumption is the focal point which is also creating a competition. The categorization of such documents into specified classes by machine provides excellent help. One of categorization technique is text classification using a Convolutional neural network(TextCNN). TextCNN uses multiple sizes of filters, as in the case of the inception layer introduced in Googlenet. The network provides good accuracy but causes high memory consumption due to a large number of trainable parameters. As a solution to this problem, we introduced a whole new architecture based on separable convolution. The idea of separable convolution already exists in the field of image classification but not yet introduces to text classification tasks. With the help of this architecture, we can achieve a drastic reduction in trainable parameters.",
    "keywords": "Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language",
    "URLtext": "2004.07922",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2004.07922",
    "URL": "http://arxiv.org/abs/2004.07922",
    "_line": "Security.bib:1077"
  },
  "kim_convolutional_2014": {
    "id": "kim_convolutional_2014",
    "type": "article-journal",
    "author": [
      {
        "family": "Kim",
        "given": "Yoon"
      }
    ],
    "title": "Convolutional Neural Networks for Sentence Classification",
    "container-title": "arXiv:1408.5882 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2014",
          "9",
          "2"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "19"
        ]
      ]
    },
    "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.",
    "keywords": "Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing",
    "URLtext": "1408.5882",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1408.5882",
    "URL": "http://arxiv.org/abs/1408.5882",
    "_line": "Security.bib:1091"
  },
  "lee_instruction2vec_2021": {
    "id": "lee_instruction2vec_2021",
    "type": "book",
    "author": [
      {
        "family": "Lee",
        "given": "Yongjun"
      }
    ],
    "title": "instruction2vec",
    "issued": {
      "date-parts": [
        [
          "2021",
          "2",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "19"
        ]
      ]
    },
    "abstract": "Efficient Preprocessor of Assembly Code to Detect Software Weakness with CNN",
    "URL": "https://github.com/firmcode/instruction2vec",
    "note": "original-date: 2019-09-17T16:14:33Z",
    "_line": "Security.bib:1105"
  },
  "lee_instruction2vec_2019": {
    "id": "lee_instruction2vec_2019",
    "type": "article-journal",
    "author": [
      {
        "family": "Lee",
        "given": "Yongjun"
      },
      {
        "family": "Kwon",
        "given": "Hyun"
      },
      {
        "family": "Choi",
        "given": "Sang-Hoon"
      },
      {
        "family": "Lim",
        "given": "Seung-Ho"
      },
      {
        "family": "Baek",
        "given": "Sung Hoon"
      },
      {
        "family": "Park",
        "given": "Ki-Woong"
      }
    ],
    "title": "Instruction2vec: Efficient Preprocessor of Assembly Code to Detect Software Weakness with CNN",
    "container-title": "Applied Sciences",
    "container-title-short": "Instruction2vec",
    "title-short": "Instruction2vec",
    "issued": {
      "date-parts": [
        [
          "2019",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "19"
        ]
      ]
    },
    "abstract": "Potential software weakness, which can lead to exploitable security vulnerabilities, continues to pose a risk to computer systems. According to Common Vulnerability and Exposures, 14,714 vulnerabilities were reported in 2017, more than twice the number reported in 2016. Automated vulnerability detection was recommended to efficiently detect vulnerabilities. Among detection techniques, static binary analysis detects software weakness based on existing patterns. In addition, it is based on existing patterns or rules, making it difficult to add and patch new rules whenever an unknown vulnerability is encountered. To overcome this limitation, we propose a new method&amp;mdash;Instruction2vec&amp;mdash;an improved static binary analysis technique using machine. Our framework consists of two steps: (1) it models assembly code efficiently using Instruction2vec, based on Word2vec; and (2) it learns the features of software weakness code using the feature extraction of Text-CNN without creating patterns or rules and detects new software weakness. We compared the preprocessing performance of three frameworks&amp;mdash;Instruction2vec, Word2vec, and Binary2img&amp;mdash;to assess the efficiency of Instruction2vec. We used the Juliet Test Suite, particularly the part related to Common Weakness Enumeration(CWE)-121, for training and Securely Taking On New Executable Software of Uncertain Provenance (STONESOUP) for testing. Experimental results show that the proposed scheme can detect software vulnerabilities with an accuracy of 91&perc; of the assembly code.",
    "keywords": "_Word2vec_\\/, binary analysis, convolutional neural network, software weakness",
    "URL": "https://www.mdpi.com/2076-3417/9/19/4086",
    "DOI": "10.3390/app9194086",
    "page": "4086",
    "page-first": "4086",
    "volume": "9",
    "note": "Number: 19\nPublisher: Multidisciplinary Digital Publishing Institute",
    "issue": "19",
    "language": "en-US",
    "_line": "Security.bib:1116"
  },
  "staats_scanner_2021": {
    "id": "staats_scanner_2021",
    "type": "no-type",
    "author": [
      {
        "family": "Staats",
        "given": "Wayne"
      }
    ],
    "title": "Scanner Project V2",
    "issued": {
      "date-parts": [
        [
          "2021"
        ]
      ]
    },
    "publisher": "Colsa",
    "abstract": "Proposes an architecture for a scanner for vulnerabilites for programs in either source or binary format.",
    "_line": "Security.bib:1137"
  },
  "mikolov_distributed_2013": {
    "id": "mikolov_distributed_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Mikolov",
        "given": "Tomas"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Chen",
        "given": "Kai"
      },
      {
        "family": "Corrado",
        "given": "Greg"
      },
      {
        "family": "Dean",
        "given": "Jeffrey"
      }
    ],
    "title": "Distributed Representations of Words and Phrases and their Compositionality",
    "container-title": "arXiv:1310.4546 \\[cs, stat\\]",
    "issued": {
      "date-parts": [
        [
          "2013",
          "10",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "19"
        ]
      ]
    },
    "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "keywords": "Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language",
    "URLtext": "1310.4546",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1310.4546",
    "URL": "http://arxiv.org/abs/1310.4546",
    "_line": "Security.bib:1145"
  },
  "mikolov_linguistic_2013": {
    "id": "mikolov_linguistic_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Mikolov",
        "given": "Tomas"
      },
      {
        "family": "Yih",
        "given": "Scott Wen-tau"
      },
      {
        "family": "Zweig",
        "given": "Geoffrey"
      }
    ],
    "title": "Linguistic Regularities in Continuous Space Word Representations",
    "issued": {
      "date-parts": [
        [
          "2013",
          "5",
          "27"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "19"
        ]
      ]
    },
    "abstract": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a \\[…\\]",
    "URL": "https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/",
    "language": "en-US",
    "_line": "Security.bib:1159"
  },
  "mikolov_efficient_2013": {
    "id": "mikolov_efficient_2013",
    "type": "article-journal",
    "author": [
      {
        "family": "Mikolov",
        "given": "Tomas"
      },
      {
        "family": "Chen",
        "given": "Kai"
      },
      {
        "family": "Corrado",
        "given": "Greg"
      },
      {
        "family": "Dean",
        "given": "Jeffrey"
      }
    ],
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "container-title": "arXiv:1301.3781 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2013",
          "9",
          "6"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "19"
        ]
      ]
    },
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
    "keywords": "Computer Science - Computation and Language",
    "URLtext": "1301.3781",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "1301.3781",
    "URL": "http://arxiv.org/abs/1301.3781",
    "_line": "Security.bib:1170"
  },
  "korczynski_comparison_2019": {
    "id": "korczynski_comparison_2019",
    "type": "webpage",
    "author": [
      {
        "family": "Korczynski",
        "given": "David"
      }
    ],
    "title": "Comparison of the LLVM IR generated by three binary-to-llvm translators",
    "issued": {
      "date-parts": [
        [
          "2019",
          "9",
          "17"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "2",
          "17"
        ]
      ]
    },
    "URL": "https://adalogics.com/blog/binary-to-llvm-comparison",
    "_line": "Security.bib:1184"
  },
  "nguyen_regvd_2021": {
    "id": "nguyen_regvd_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Nguyen",
        "given": "Van-Anh"
      },
      {
        "family": "Nguyen",
        "given": "Dai Quoc"
      },
      {
        "family": "Nguyen",
        "given": "Van"
      },
      {
        "family": "Le",
        "given": "Trung"
      },
      {
        "family": "Tran",
        "given": "Quan Hung"
      },
      {
        "family": "Phung",
        "given": "Dinh"
      }
    ],
    "title": "ReGVD: Revisiting Graph Neural Networks for Vulnerability Detection",
    "container-title": "arXiv:2110.07317 \\[cs\\]",
    "container-title-short": "ReGVD",
    "title-short": "ReGVD",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "14"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "26"
        ]
      ]
    },
    "abstract": "Identifying vulnerabilities in the source code is essential to protect the software systems from cyber security attacks. It, however, is also a challenging step that requires specialized expertise in security and code representation. Inspired by the successful applications of pre-trained programming language (PL) models such as CodeBERT and graph neural networks (GNNs), we propose ReGVD, a general and novel graph neural network-based model for vulnerability detection. In particular, ReGVD views a given source code as a flat sequence of tokens and then examines two effective methods of utilizing unique tokens and indexes respectively to construct a single graph as an input, wherein node features are initialized only by the embedding layer of a pre-trained PL model. Next, ReGVD leverages a practical advantage of residual connection among GNN layers and explores a beneficial mixture of graph-level sum and max poolings to return a graph embedding for the given source code. Experimental results demonstrate that ReGVD outperforms the existing state-of-the-art models and obtain the highest accuracy on the real-world benchmark dataset from CodeXGLUE for vulnerability detection.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Machine Learning",
    "URLtext": "2110.07317",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2110.07317",
    "URL": "http://arxiv.org/abs/2110.07317",
    "_line": "Security.bib:1210"
  },
  "georges_cerise_nodate": {
    "id": "georges_cerise_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Georges",
        "given": "Aïna Linn"
      },
      {
        "family": "Guéneau",
        "given": "Armaël"
      },
      {
        "family": "Strydonck",
        "given": "Thomas Van"
      },
      {
        "family": "Timany",
        "given": "Amin"
      },
      {
        "family": "Trieu",
        "given": "Alix"
      },
      {
        "family": "Devriese",
        "given": "Dominique"
      },
      {
        "family": "Birkedal",
        "given": "Lars"
      }
    ],
    "title": "Cerise: Program Verification on a Capability Machine in the Presence of Untrusted Code",
    "page": "55",
    "page-first": "55",
    "volume": "1",
    "issue": "1",
    "language": "en-US",
    "_line": "Security.bib:1225"
  },
  "el-korashy_secureptrs_2021": {
    "id": "el-korashy_secureptrs_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "El-Korashy",
        "given": "Akram"
      },
      {
        "family": "Blanco",
        "given": "Roberto"
      },
      {
        "family": "Thibault",
        "given": "Jérémy"
      },
      {
        "family": "Durier",
        "given": "Adrien"
      },
      {
        "family": "Garg",
        "given": "Deepak"
      },
      {
        "family": "Hritcu",
        "given": "Catalin"
      }
    ],
    "title": "SecurePtrs: Proving Secure Compilation with Data-Flow Back-Translation and Turn-Taking Simulation",
    "container-title": "arXiv:2110.01439 \\[cs\\]",
    "container-title-short": "SecurePtrs",
    "title-short": "SecurePtrs",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "12"
        ]
      ]
    },
    "abstract": "Proving secure compilation of partial programs typically requires back-translating a target attack against the compiled program to an attack against the source program. To prove this back-translation step, one can syntactically translate the target attacker to a source one &ndash; i.e., syntax-directed back-translation &ndash; or show that the interaction traces of the target attacker can also be produced by source attackers &ndash; i.e., trace-directed back-translation. Syntax-directed back-translation is not suitable when the target attacker uses unstructured control flow that the source language cannot directly represent. Trace-directed back-translation works with such syntactic dissimilarity because only the external interactions of the target attacker have to be mimicked in the source, not its internal control flow. Revealing only external interactions is, however, inconvenient when sharing memory via unforgeable pointers, since information about stashed pointers to shared memory gets lost. This made prior proofs complex, since the generated attacker had to stash all reachable pointers. In this work, we introduce more informative data-flow traces, which allow us to combine the best of syntax-directed and trace-directed back-translation. Our data-flow back-translation is simple, handles both syntactic dissimilarity and memory sharing well, and we have proved it correct in Coq. We, moreover, develop a novel turn-taking simulation relation and use it to prove a recomposition lemma, which is key to reusing compiler correctness in such secure compilation proofs. We are the first to mechanize such a recomposition lemma in a proof assistant in the presence of memory sharing. We put these two key innovations to use in a secure compilation proof for a code generation compiler pass between a safe source language with pointers and components, and a target language with unstructured control flow.",
    "keywords": "Computer Science - Cryptography and Security, Computer Science - Programming Languages",
    "URLtext": "2110.01439",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2110.01439",
    "URL": "http://arxiv.org/abs/2110.01439",
    "_line": "Security.bib:1235"
  },
  "messadi_precursor_2021": {
    "id": "messadi_precursor_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Messadi",
        "given": "Ines"
      },
      {
        "family": "Neumann",
        "given": "Shivananda"
      },
      {
        "family": "Weichbrodt",
        "given": "Nico"
      },
      {
        "family": "Almstedt",
        "given": "Lennart"
      },
      {
        "family": "Mahhouk",
        "given": "Mohammad"
      },
      {
        "family": "Kapitza",
        "given": "Rüdiger"
      }
    ],
    "title": "Precursor: a fast, client-centric and trusted key-value store using RDMA and Intel SGX",
    "container-title": "Proceedings of the 22nd International Middleware Conference",
    "container-title-short": "Precursor",
    "collection-title": "Middleware '21",
    "title-short": "Precursor",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "22"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "10",
          "12"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8534-3",
    "abstract": "As offered by the Intel Software Guard Extensions (SGX), trusted execution enables confidentiality and integrity for off-site deployed services. Thereby, securing key-value stores has received particular attention, as they are a building block for many complex applications to speed-up request processing. Initially, the developers' main design challenge has been to address the performance barriers of SGX. Besides, we identified the integration of a SGX-secured key-value store with recent network technologies, especially RDMA, as an essential emerging requirement. RDMA allows fast direct access to remote memory at high bandwidth. As SGX-protected memory cannot be directly accessed over the network, a fast exchange between the main and trusted memory must be enabled. More importantly, SGX-protected services can be expected to be CPU-bound as a result of the vast number of cryptographic operations required to transfer and store data securely. In this paper, we present Precursor, a new key-value store design that utilizes trusted execution to offer confidentiality and integrity while relying on RDMA for low latency and high bandwidth communication. Precursor offloads cryptographic operations to the client-side to prevent a server-side CPU bottleneck and reduces data movement in and out of the trusted execution environment. Our evaluation shows that Precursor achieves up to 6&ndash;8.5 times higher throughput when compared against similar SGX-secured key-value store approaches.",
    "keywords": "Intel SGX, key-value stores, RDMA",
    "URL": "https://doi.org/10.1145/3464298.3476129",
    "DOI": "10.1145/3464298.3476129",
    "publisher-place": "New York, NY, USA",
    "page": "1-13",
    "page-first": "1",
    "_line": "Security.bib:1250"
  },
  "recto_secure_nodate": {
    "id": "recto_secure_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Recto",
        "given": "Rolph"
      },
      {
        "family": "Algehed",
        "given": "Maximilian"
      },
      {
        "family": "Myers",
        "given": "Andrew C"
      }
    ],
    "title": "Secure Information Flow for Concurrent Programs with Expressive Synchronization",
    "abstract": "Practical enforcement of secure information ﬂow in concurrent programs remains notoriously difﬁcult. An underexplored reason is that concurrent programs rely on a wide variety of synchronization primitives, which can leak information. However, the existing literature on security for concurrent programs has focused on a small set of such primitives.",
    "page": "28",
    "page-first": "28",
    "language": "en-US",
    "_line": "Security.bib:1268"
  },
  "cauligi_foundations_nodate": {
    "id": "cauligi_foundations_nodate",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "Cauligi",
        "given": "Sunjay R"
      }
    ],
    "title": "Foundations for Speculative Side Channels",
    "publisher": "UC San Diego",
    "number-of-pages": "190",
    "URL": "https://escholarship.org/uc/item/64n9f44x",
    "language": "en-US",
    "_line": "Security.bib:1277"
  },
  "zhang_statically_nodate": {
    "id": "zhang_statically_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Zhang",
        "given": "Hang"
      },
      {
        "family": "Chen",
        "given": "Weiteng"
      },
      {
        "family": "Hao",
        "given": "Yu"
      },
      {
        "family": "Li",
        "given": "Guoren"
      },
      {
        "family": "Zhai",
        "given": "Yizhuo"
      },
      {
        "family": "Zou",
        "given": "Xiaochen"
      },
      {
        "family": "Qian",
        "given": "Zhiyun"
      }
    ],
    "title": "Statically Discovering High-Order Taint Style Vulnerabilities in OS Kernels",
    "abstract": "Static analysis is known to yield numerous false alarms when used in bug finding, especially for complex vulnerabilities in large code bases like the Linux kernel. One important class of such complex vulnerabilities is what we call “high-order taint style vulnerability”, where the taint flow from the user input to the vulnerable site crosses the boundary of a single entry function invocation (i.e., syscall). Due to the large scope and high precision requirement, few have attempted to solve the problem.",
    "page": "14",
    "page-first": "14",
    "language": "en-US",
    "_line": "Security.bib:1288"
  },
  "li_towards_2021-1": {
    "id": "li_towards_2021-1",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "Peixuan"
      },
      {
        "family": "Zhang",
        "given": "Danfeng"
      }
    ],
    "title": "Towards a General-Purpose Dynamic Information Flow Policy",
    "container-title": "arXiv:2109.08096 \\[cs\\]",
    "issued": {
      "date-parts": [
        [
          "2021",
          "9",
          "16"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "24"
        ]
      ]
    },
    "abstract": "Noninterference offers a rigorous end-to-end guarantee for secure propagation of information. However, real-world systems almost always involve security requirements that change during program execution, making noninterference inapplicable. Prior works alleviate the limitation to some extent, but even for a veteran in information flow security, understanding the subtleties in the syntax and semantics of each policy is challenging, largely due to very different policy specification languages, and more fundamentally, semantic requirements of each policy. We take a top-down approach and present a novel information flow policy, called Dynamic Release, which allows information flow restrictions to downgrade and upgrade in arbitrary ways. Dynamic Release is formalized on a novel framework that, for the first time, allows us to compare and contrast various dynamic policies in the literature. We show that Dynamic Release generalizes declassification, erasure, delegation and revocation. Moreover, it is the only dynamic policy that is both applicable and correct on a benchmark of tests with dynamic policy.",
    "keywords": "Computer Science - Cryptography and Security",
    "URLtext": "2109.08096",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2109.08096",
    "URL": "http://arxiv.org/abs/2109.08096",
    "_line": "Security.bib:1297"
  },
  "ishimwe_dynaplex_nodate": {
    "id": "ishimwe_dynaplex_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Ishimwe",
        "given": "Didier"
      },
      {
        "family": "Nguyen",
        "given": "Kimhao"
      },
      {
        "family": "Nguyen",
        "given": "Thanhvu"
      }
    ],
    "title": "Dynaplex: Analyzing Program Complexity using Dynamically Inferred Recurrence Relations",
    "page": "23",
    "page-first": "23",
    "volume": "5",
    "language": "en-US",
    "_line": "Security.bib:1311"
  },
  "vu_reconciling_2021": {
    "id": "vu_reconciling_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Vu",
        "given": "Son Tuan"
      },
      {
        "family": "Cohen",
        "given": "Albert"
      },
      {
        "family": "Grandmaison",
        "given": "Arnaud De"
      },
      {
        "family": "Guillon",
        "given": "Christophe"
      },
      {
        "family": "Heydemann",
        "given": "Karine"
      }
    ],
    "title": "Reconciling Optimization with Secure Compilation",
    "issued": {
      "date-parts": [
        [
          "2021",
          "10"
        ]
      ]
    },
    "page": "30",
    "page-first": "30",
    "volume": "5",
    "language": "en-US",
    "_line": "Security.bib:1320"
  },
  "neumann_risks_nodate": {
    "id": "neumann_risks_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Neumann",
        "given": "Peter G."
      }
    ],
    "title": "The RISKS Digest",
    "container-title": "The RISKS Digest",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "24"
        ]
      ]
    },
    "abstract": "The web page of the RISKS digest moderated by Peter G. Neumman of SRI",
    "URL": "http://catless.ncl.ac.uk/risks/",
    "language": "en-US",
    "_line": "Security.bib:1330"
  },
  "li_potential_2015": {
    "id": "li_potential_2015",
    "type": "paper-conference",
    "author": [
      {
        "family": "Li",
        "given": "Li"
      },
      {
        "family": "Allix",
        "given": "Kevin"
      },
      {
        "family": "Li",
        "given": "Daoyuan"
      },
      {
        "family": "Bartel",
        "given": "Alexandre"
      },
      {
        "family": "Bissyande",
        "given": "Tegawende F."
      },
      {
        "family": "Klein",
        "given": "Jacques"
      }
    ],
    "title": "Potential Component Leaks in Android Apps: An Investigation into a New Feature Set for Malware Detection",
    "container-title": "2015 IEEE International Conference on Software Quality, Reliability and Security",
    "container-title-short": "Potential Component Leaks in Android Apps",
    "title-short": "Potential Component Leaks in Android Apps",
    "event-title": "2015 IEEE International Conference on Software Quality, Reliability and Security (QRS)",
    "issued": {
      "date-parts": [
        [
          "2015",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "20"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-4673-7989-2",
    "abstract": "We discuss the capability of a new feature set for malware detection based on potential component leaks (PCLs). PCLs are deﬁned as sensitive data-ﬂows that involve Android inter-component communications. We show that PCLs are common in Android apps and that malicious applications indeed manipulate signiﬁcantly more PCLs than benign apps. Then, we evaluate a machine learning-based approach relying on PCLs. Experimental validations show high performance for identifying malware, demonstrating that PCLs can be used for discriminating malicious apps from benign apps.",
    "URL": "http://ieeexplore.ieee.org/document/7272932/",
    "DOI": "10.1109/QRS.2015.36",
    "publisher-place": "Vancouver, BC, Canada",
    "page": "195-200",
    "page-first": "195",
    "language": "en-US",
    "_line": "Security.bib:1341"
  },
  "van_schaik_cacheout_2021": {
    "id": "van_schaik_cacheout_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Schaik",
        "given": "Stephan",
        "dropping-particle": "van"
      },
      {
        "family": "Minkin",
        "given": "Marina"
      },
      {
        "family": "Kwong",
        "given": "Andrew"
      },
      {
        "family": "Genkin",
        "given": "Daniel"
      },
      {
        "family": "Yarom",
        "given": "Yuval"
      }
    ],
    "title": "CacheOut: Leaking Data on Intel CPUs via Cache Evictions",
    "container-title": "2021 IEEE Symposium on Security and Privacy (SP)",
    "container-title-short": "CacheOut",
    "title-short": "CacheOut",
    "event-title": "2021 IEEE Symposium on Security and Privacy (SP)",
    "issued": {
      "date-parts": [
        [
          "2021",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "9",
          "20"
        ]
      ]
    },
    "publisher": "IEEE",
    "isbn": "978-1-72818-934-5",
    "abstract": "Recent transient-execution attacks, such as RIDL, Fallout, and ZombieLoad, demonstrated that attackers can leak information while it transits through microarchitectural buffers. Named Microarchitectural Data Sampling (MDS) by Intel, these attacks are likened to “drinking from the ﬁrehose”, as the attacker has little control over what data is observed and from what origin. Unable to prevent the buffers from leaking, Intel issued countermeasures via microcode updates that overwrite the buffers when the CPU changes security domains.",
    "URL": "https://ieeexplore.ieee.org/document/9519461/",
    "DOI": "10.1109/SP40001.2021.00064",
    "publisher-place": "San Francisco, CA, USA",
    "page": "339-354",
    "page-first": "339",
    "language": "en-US",
    "_line": "Security.bib:1360"
  },
  "alam_tailoring_nodate": {
    "id": "alam_tailoring_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Alam",
        "given": "Imran"
      }
    ],
    "title": "Tailoring Taint Analysis for Database Applications in the K Framework",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "7"
        ]
      ]
    },
    "URL": "https://www.scitepress.org/Papers/2021/106186/106186.pdf",
    "_line": "Security.bib:1379"
  },
  "brown_semi-automatic_nodate": {
    "id": "brown_semi-automatic_nodate",
    "type": "webpage",
    "author": [
      {
        "family": "Brown",
        "given": "Christopher"
      }
    ],
    "title": "Semi-Automatic Ladderisation: Improving Code Security through Rewriting and Dependent Types",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "7"
        ]
      ]
    },
    "URL": "https://research-repository.st-andrews.ac.uk/bitstream/handle/10023/24384/Brown_2021_Semi_automatic_ladderisation_PEPM2022_AAM.pdf?sequence=1&isAllowed=y",
    "_line": "Security.bib:1387"
  },
  "genkin_lend_nodate": {
    "id": "genkin_lend_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Genkin",
        "given": "Daniel"
      },
      {
        "family": "Nissan",
        "given": "Noam"
      },
      {
        "family": "Schuster",
        "given": "Roei"
      },
      {
        "family": "Tromer",
        "given": "Eran"
      }
    ],
    "title": "Lend Me Your Ear: Passive Remote Physical Side Channels on PCs",
    "abstract": "We show that built-in sensors in commodity PCs, such as microphones, inadvertently capture electromagnetic sidechannel leakage from ongoing computation. Moreover, this information is often conveyed by supposedly-benign channels such as audio recordings and common Voice-over-IP applications, even after lossy compression. Thus, we show, it is possible to conduct physical sidechannel attacks on computation by remote and purely passive analysis of commonly-shared channels. These attacks require neither physical proximity (which could be mitigated by distance and shielding), nor the ability to run code on the target or conﬁgure its hardware. Consequently, we argue, physical side channels on PCs can no longer be excluded from remoteattack threat models. We analyze the computation-dependent leakage captured by internal microphones, and empirically demonstrate its efﬁcacy for attacks. In one scenario, an attacker steals the secret ECDSA signing keys of the counterparty in a voice call. In another, the attacker detects what web page their counterparty is loading. In the third scenario, a player in the Counter-Strike online multiplayer game can detect a hidden opponent waiting in ambush, by analyzing how the 3D rendering done by the opponent’s computer induces faint but detectable signals into the opponent’s audio feed.",
    "page": "18",
    "page-first": "18",
    "language": "en-US",
    "_line": "Security.bib:1395"
  },
  "li_towards_nodate": {
    "id": "li_towards_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Li",
        "given": "Peixuan"
      }
    ],
    "title": "TOWARDS PRACTICAL INFORMATION FLOW ANALYSIS",
    "page": "226",
    "page-first": "226",
    "language": "en-US",
    "_line": "Security.bib:1404"
  },
  "bacelar_almeida_formal_2021": {
    "id": "bacelar_almeida_formal_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Bacelar Almeida",
        "given": "José Carlos"
      },
      {
        "family": "Barbosa",
        "given": "Manuel"
      },
      {
        "family": "Barthe",
        "given": "Gilles"
      },
      {
        "family": "Pacheco",
        "given": "Hugo"
      },
      {
        "family": "Pereira",
        "given": "Vitor"
      },
      {
        "family": "Portela",
        "given": "Bernardo"
      }
    ],
    "title": "A Formal Treatment of the Role of Verified Compilers in Secure Computation",
    "container-title": "Journal of Logical and Algebraic Methods in Programming",
    "container-title-short": "Journal of Logical and Algebraic Methods in Programming",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "19"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "26"
        ]
      ]
    },
    "issn": "2352-2208",
    "abstract": "Secure multiparty computation (SMC) allows for complex computations over encrypted data. Privacy concerns for cloud applications makes this a highly desired technology and recent performance improvements show that it is practical. To make SMC accessible to non-experts and empower its use in varied applications, many domain-specific compilers are being proposed. We review the role of these compilers and provide a formal treatment of the core steps that they perform to bridge the abstraction gap between high-level ideal specifications and efficient SMC protocols. Our abstract framework bridges this secure compilation problem across two dimensions: 1) language-based source- to target-level semantic and efficiency gaps, and 2) cryptographic ideal- to real-world security gaps. We link the former to the setting of certified compilation, paving the way to leverage long-run efforts such as CompCert in future SMC compilers. Security is framed in the standard cryptographic sense. Our results are supported by a machine-checked formalisation carried out in EasyCrypt.",
    "keywords": "formal verification, certified compilation, computer-aided cryptography, EasyCrypt, secure compilation, Secure multiparty computation",
    "URL": "https://www.sciencedirect.com/science/article/pii/S2352220821000997",
    "DOI": "10.1016/j.jlamp.2021.100736",
    "page": "100736",
    "page-first": "100736",
    "language": "en-US",
    "_line": "Security.bib:1412"
  },
  "debnath_re-engineering_2021": {
    "id": "debnath_re-engineering_2021",
    "type": "paper-conference",
    "author": [
      {
        "family": "Debnath",
        "given": "Joyanta"
      },
      {
        "family": "Chau",
        "given": "Sze Yiu"
      },
      {
        "family": "Chowdhury",
        "given": "Omar"
      }
    ],
    "title": "On Re-engineering the X.509 PKI with Executable Specification for Better Implementation Guarantees",
    "container-title": "Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
    "collection-title": "CCS '21",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "26"
        ]
      ]
    },
    "publisher": "Association for Computing Machinery",
    "isbn": "978-1-4503-8454-4",
    "abstract": "The X.509 Public-Key Infrastructure (PKI) standard is widely used as a scalable and flexible authentication mechanism. Flaws in X.509 implementations can make relying applications susceptible to impersonation attacks or interoperability issues. In practice, many libraries implementing X.509 have been shown to suffer from flaws that are due to noncompliance with the standard. Developing a compliant implementation is especially hindered by the design complexity, ambiguities, or under-specifications in the standard written in natural languages. In this paper, we set out to alleviate this unsatisfactory state of affairs by re-engineering and formalizing a widely used fragment of the X.509 standard specification, and then using it to develop a high-assurance implementation. Our X.509 specification re-engineering effort is guided by the principle of decoupling the syntactic requirements from the semantic requirements. For formalizing the syntactic requirements of X.509 standard, we observe that a restricted fragment of attribute grammar is sufficient. In contrast, for precisely capturing the semantic requirements imposed on the most-widely used X.509 features, we use quantifier-free first-order logic (QFFOL). Interestingly, using QFFOL results in an executable specification that can be efficiently enforced by an SMT solver. We use these and other insights to develop a high-assurance X.509 implementation named CERES. A comparison of CERES with 3 mainstream libraries (i.e., mbedTLS, OpenSSL, and GnuTLS) based on 2 million real certificate chains and 2 million synthetic certificate chains shows that CERES rightfully rejects malformed and invalid certificates.",
    "keywords": "authentication, differential testing, network security, PKI, SMT solver, SSL/TLS protocol, X.509 certificate",
    "URL": "https://doi.org/10.1145/3460120.3484793",
    "DOI": "10.1145/3460120.3484793",
    "publisher-place": "New York, NY, USA",
    "page": "1388-1404",
    "page-first": "1388",
    "_line": "Security.bib:1429"
  },
  "noauthor_convergence_nodate": {
    "id": "noauthor_convergence_nodate",
    "type": "webpage",
    "title": "The convergence of source code and binary vulnerability discovery - A case study &bar; EURECOM",
    "accessed": {
      "date-parts": [
        [
          "2021",
          "11",
          "26"
        ]
      ]
    },
    "URL": "https://www.eurecom.fr/publication/6732",
    "_line": "Security.bib:1447"
  },
  "ding_velvet_2021": {
    "id": "ding_velvet_2021",
    "type": "article-journal",
    "author": [
      {
        "family": "Ding",
        "given": "Yangruibo"
      },
      {
        "family": "Suneja",
        "given": "Sahil"
      },
      {
        "family": "Zheng",
        "given": "Yunhui"
      },
      {
        "family": "Laredo",
        "given": "Jim"
      },
      {
        "family": "Morari",
        "given": "Alessandro"
      },
      {
        "family": "Kaiser",
        "given": "Gail"
      },
      {
        "family": "Ray",
        "given": "Baishakhi"
      }
    ],
    "title": "VELVET: a noVel Ensemble Learning approach to automatically locate VulnErable sTatements",
    "container-title": "arXiv:2112.10893 \\[cs\\]",
    "container-title-short": "VELVET",
    "title-short": "VELVET",
    "issued": {
      "date-parts": [
        [
          "2021",
          "12",
          "20"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          "12",
          "27"
        ]
      ]
    },
    "abstract": "Automatically locating vulnerable statements in source code is crucial to assure software security and alleviate developers' debugging efforts. This becomes even more important in today's software ecosystem, where vulnerable code can flow easily and unwittingly within and across software repositories like GitHub. Across such millions of lines of code, traditional static and dynamic approaches struggle to scale. Although existing machine-learning-based approaches look promising in such a setting, most work detects vulnerable code at a higher granularity &ndash; at the method or file level. Thus, developers still need to inspect a significant amount of code to locate the vulnerable statement(s) that need to be fixed. This paper presents VELVET, a novel ensemble learning approach to locate vulnerable statements. Our model combines graph-based and sequence-based neural networks to successfully capture the local and global context of a program graph and effectively understand code semantics and vulnerable patterns. To study VELVET's effectiveness, we use an off-the-shelf synthetic dataset and a recently published real-world dataset. In the static analysis setting, where vulnerable functions are not detected in advance, VELVET achieves 4.5x better performance than the baseline static analyzers on the real-world data. For the isolated vulnerability localization task, where we assume the vulnerability of a function is known while the specific vulnerable statement is unknown, we compare VELVET with several neural networks that also attend to local and global context of code. VELVET achieves 99.6&perc; and 43.6&perc; top-1 accuracy over synthetic data and real-world data, respectively, outperforming the baseline deep-learning models by 5.3-29.0&perc;.",
    "keywords": "Computer Science - Machine Learning, Computer Science - Software Engineering",
    "URLtext": "2112.10893",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2112.10893",
    "URL": "http://arxiv.org/abs/2112.10893",
    "_line": "Security.bib:1454"
  },
  "hiet_security_2021": {
    "id": "hiet_security_2021",
    "type": "thesis",
    "genre": "Habilitation à diriger des recherches",
    "author": [
      {
        "family": "Hiet",
        "given": "Guillaume"
      }
    ],
    "title": "Security at the Hardware/Software Interface",
    "issued": {
      "date-parts": [
        [
          "2021",
          "12"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "14"
        ]
      ]
    },
    "publisher": "Université de Rennes 1",
    "keywords": "formal methods, computer security, détection d'intrusion, intrusion detection, intrusion response, méthodes formelles, réponse aux intrusions, sécurité informatique",
    "URL": "https://hal.archives-ouvertes.fr/tel-03511334",
    "_line": "Security.bib:1469"
  },
  "huang_taming_nodate": {
    "id": "huang_taming_nodate",
    "type": "article-journal",
    "author": [
      {
        "family": "Huang",
        "given": "Kaiming"
      },
      {
        "family": "Huang",
        "given": "Yongzhe"
      },
      {
        "family": "Payer",
        "given": "Mathias"
      },
      {
        "family": "Qian",
        "given": "Zhiyun"
      },
      {
        "family": "Sampson",
        "given": "Jack"
      },
      {
        "family": "Tan",
        "given": "Gang"
      },
      {
        "family": "Jaeger",
        "given": "Trent"
      }
    ],
    "title": "The Taming of the Stack: Isolating Stack Data from Memory Errors",
    "container-title": "Network and Distributed Systems Security (NDSS) Symposium 2022",
    "abstract": "Despite vast research on defenses to protect stack objects from the exploitation of memory errors, much stack data remains at risk. Historically, stack defenses focus on the protection of code pointers, such as return addresses, but emerging techniques to exploit memory errors motivate the need for practical solutions to protect stack data objects as well. However, recent approaches provide an incomplete view of security by not accounting for memory errors comprehensively and by limiting the set of objects that can be protected unnecessarily. In this paper, we present the DATAGUARD system that identiﬁes which stack objects are safe statically from spatial, type, and temporal memory errors to protect those objects efﬁciently. DATAGUARD improves security through a more comprehensive and accurate safety analysis that proves a larger number of stack objects are safe from memory errors, while ensuring that no unsafe stack objects are mistakenly classiﬁed as safe. DATAGUARD’s analysis of server programs and the SPEC CPU2006 benchmark suite shows that DATAGUARD improves security by: (1) ensuring that no memory safety violations are possible for any stack objects classiﬁed as safe, removing 6.3&perc; of the stack objects previously classiﬁed safe by the Safe Stack method, and (2) blocking exploit of all 118 stack vulnerabilities in the CGC Binaries. DATAGUARD extends the scope of stack protection by validating as safe over 70&perc; of the stack objects classiﬁed as unsafe by the Safe Stack method, leading to an average of 91.45&perc; of all stack objects that can only be referenced safely. By identifying more functions with only safe stack objects, DATAGUARD reduces the overhead of using Clang’s Safe Stack defense for protection of the SPEC CPU2006 benchmarks from 11.3&perc; to 4.3&perc;. Thus, DATAGUARD shows that a comprehensive and accurate analysis can both increase the scope of stack data protection and reduce overheads.",
    "page": "17",
    "page-first": "17",
    "language": "en-US",
    "_line": "Security.bib:1481"
  },
  "leger_exploring_2021": {
    "id": "leger_exploring_2021",
    "type": "report",
    "author": [
      {
        "family": "Leger",
        "given": "Michelle"
      },
      {
        "family": "Darling",
        "given": "Michael"
      },
      {
        "family": "Jones",
        "given": "Stephen"
      },
      {
        "family": "Matzen",
        "given": "Laura"
      },
      {
        "family": "Stracuzzi",
        "given": "David"
      },
      {
        "family": "Wilson",
        "given": "Andrew"
      },
      {
        "family": "Bueno",
        "given": "Denis"
      },
      {
        "family": "Christentsen",
        "given": "Matthew"
      },
      {
        "family": "Ginaldi",
        "given": "Melissa"
      },
      {
        "family": "Hannasch",
        "given": "David"
      },
      {
        "family": "Heidbrink",
        "given": "Scott"
      },
      {
        "family": "Howell",
        "given": "Breannan"
      },
      {
        "family": "Leger",
        "given": "Chris"
      },
      {
        "family": "Reedy",
        "given": "Geoffrey"
      },
      {
        "family": "Rogers",
        "given": "Alisa"
      },
      {
        "family": "Williams",
        "given": "Jack"
      }
    ],
    "title": "Exploring Explicit Uncertainty for Binary Analysis (EUBA).",
    "issued": {
      "date-parts": [
        [
          "2021",
          "11",
          "1"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "1",
          "27"
        ]
      ]
    },
    "URL": "https://www.osti.gov/servlets/purl/1832314/",
    "DOI": "10.2172/1832314",
    "page": "SAND2021-14600, 1832314, 701941",
    "page-first": "SAND2021--14600, 1832314, 701941",
    "number": "{SAND}2021-14600, 1832314, 701941",
    "language": "en-US",
    "_line": "Security.bib:1491"
  },
  "cowley_job_2014": {
    "id": "cowley_job_2014",
    "type": "article-journal",
    "author": [
      {
        "family": "Cowley",
        "given": "Jennifer"
      }
    ],
    "title": "Job Analysis Results for Malicious-Code Reverse Engineers: A Case Study",
    "issued": {
      "date-parts": [
        [
          "2014",
          "5"
        ]
      ]
    },
    "page": "114",
    "page-first": "114",
    "language": "en-US",
    "_line": "Security.bib:1504"
  },
  "mazuera-rozo_taxonomy_2022": {
    "id": "mazuera-rozo_taxonomy_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Mazuera-Rozo",
        "given": "Alejandro"
      },
      {
        "family": "Escobar-Velásquez",
        "given": "Camilo"
      },
      {
        "family": "Espitia-Acero",
        "given": "Juan"
      },
      {
        "family": "Vega-Guzmán",
        "given": "David"
      },
      {
        "family": "Trubiani",
        "given": "Catia"
      },
      {
        "family": "Linares-Vásquez",
        "given": "Mario"
      },
      {
        "family": "Bavota",
        "given": "Gabriele"
      }
    ],
    "title": "Taxonomy of security weaknesses in Java and Kotlin Android apps",
    "container-title": "Journal of Systems and Software",
    "container-title-short": "Journal of Systems and Software",
    "issued": {
      "date-parts": [
        [
          "2022",
          "1",
          "31"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "8"
        ]
      ]
    },
    "issn": "0164-1212",
    "abstract": "Android is nowadays the most popular operating system in the world, not only in the realm of mobile devices, but also when considering desktop and laptop computers. Such a popularity makes it an attractive target for security attacks, also due to the sensitive information often manipulated by mobile apps. The latter are going through a transition in which the Android ecosystem is moving from the usage of Java as the official language for developing apps, to the adoption of Kotlin as the first choice supported by Google. While previous studies have partially studied security weaknesses affecting Java Android apps, there is no comprehensive empirical investigation studying software security weaknesses affecting Android apps considering (and comparing) the two main languages used for their development, namely Java and Kotlin. We present an empirical study in which we: (i) manually analyze 681 commits including security weaknesses fixed by developers in Java and Kotlin apps, with the goal of defining a taxonomy highlighting the types of software security weaknesses affecting Java and Kotlin Android apps; (ii) survey 43 Android developers to validate and complement our taxonomy. Based on our findings, we propose a list of future actions that could be performed by researchers and practitioners to improve the security of Android apps.",
    "keywords": "Security, Android",
    "URL": "https://www.sciencedirect.com/science/article/pii/S0164121222000103",
    "DOI": "10.1016/j.jss.2022.111233",
    "page": "111233",
    "page-first": "111233",
    "language": "en-US",
    "_line": "Security.bib:1513"
  },
  "garcia_side-channel_nodate": {
    "id": "garcia_side-channel_nodate",
    "type": "thesis",
    "genre": "phdthesis",
    "author": [
      {
        "family": "GARCÍA",
        "given": "CESAR PEREIDA"
      }
    ],
    "title": "Side-Channel Analysis and Cryptography Engineering - Getting OpenSSL Closer to Constant-Time",
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "8"
        ]
      ]
    },
    "URL": "https://trepo.tuni.fi/bitstream/handle/10024/137100/978-952-03-2289-2.pdf?sequence=2",
    "_line": "Security.bib:1530"
  },
  "potteiger_moving_2022": {
    "id": "potteiger_moving_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Potteiger",
        "given": "Bradley"
      },
      {
        "family": "Dubey",
        "given": "Abhishek"
      },
      {
        "family": "Cai",
        "given": "Feiyang"
      },
      {
        "family": "Koutsoukos",
        "given": "Xenofon"
      },
      {
        "family": "Zhang",
        "given": "Zhenkai"
      }
    ],
    "title": "Moving target defense for the security and resilience of mixed time and event triggered cyber-physical systems",
    "container-title": "Journal of Systems Architecture",
    "container-title-short": "Journal of Systems Architecture",
    "issued": {
      "date-parts": [
        [
          "2022",
          "2",
          "5"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "11"
        ]
      ]
    },
    "issn": "1383-7621",
    "abstract": "Memory corruption attacks such as code injection, code reuse, and non-control data attacks have become widely popular for compromising safety-critical Cyber-Physical Systems (CPS). Moving target defense (MTD) techniques such as instruction set randomization (ISR), address space randomization (ASR), and data space randomization (DSR) can be used to protect systems against such attacks. CPS often use time-triggered architectures to guarantee predictable and reliable operation. MTD techniques can cause time delays with unpredictable behavior. To protect CPS against memory corruption attacks, MTD techniques can be implemented in a mixed time and event-triggered architecture that provides capabilities for maintaining safety and availability during an attack. This paper presents a mixed time and event-triggered MTD security approach based on the ARINC 653 architecture that provides predictable and reliable operation during normal operation and rapid detection and reconfiguration upon detection of attacks. We leverage a hardware-in-the-loop testbed and an advanced emergency braking system (AEBS) case study to show the effectiveness of our approach.",
    "keywords": "Cyber-physical systems, Event triggered, Moving target defense, Time triggered",
    "URL": "https://www.sciencedirect.com/science/article/pii/S1383762122000212",
    "DOI": "10.1016/j.sysarc.2022.102420",
    "page": "102420",
    "page-first": "102420",
    "language": "en-US",
    "_line": "Security.bib:1539"
  },
  "janson_sponge-based_2022": {
    "id": "janson_sponge-based_2022",
    "type": "report",
    "author": [
      {
        "family": "Janson",
        "given": "Christian"
      },
      {
        "family": "Struck",
        "given": "Patrick"
      }
    ],
    "title": "Sponge-based Authenticated Encryption: Security against Quantum Attackers",
    "container-title-short": "Sponge-based Authenticated Encryption",
    "title-short": "Sponge-based Authenticated Encryption",
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "13"
        ]
      ]
    },
    "abstract": "In this work, we study the security of sponge-based authenticated encryption schemes against quantum attackers. In particular, we analyse the sponge-based authenticated encryption scheme SLAE as put forward by Degabriele et al. (ASIACRYPT'19). We show that the scheme achieves security in the post-quantum (QS1) setting in the quantum random oracle model by using the one-way to hiding lemma. Furthermore, we analyse the scheme in a fully-quantum (QS2) setting. There we provide a set of attacks showing that SLAE does not achieve ciphertext indistinguishability and hence overall does not provide the desired level of security.",
    "keywords": "secret-key cryptography",
    "URL": "http://eprint.iacr.org/2022/139",
    "number": "139",
    "_line": "Security.bib:1556"
  },
  "bertoni_sponge_nodate": {
    "id": "bertoni_sponge_nodate",
    "type": "report",
    "author": [
      {
        "family": "Bertoni",
        "given": "Guido"
      },
      {
        "family": "Daemen",
        "given": "Joan"
      },
      {
        "family": "Peeters",
        "given": "Micha¨el"
      }
    ],
    "title": "Sponge Functions",
    "accessed": {
      "date-parts": [
        [
          "2022",
          "2",
          "13"
        ]
      ]
    },
    "publisher": "STMicroelectronics, Radbound University",
    "abstract": "A good cryptographic hash function should behave like a random oracle: it should\nnot have weaknesses that a random oracle does not have. Due to the existence of inner collisions,\niterated hash functions can never satisfy this ideal. We propose a construction with a finite\nstate called a sponge and show that a random sponge can only be distinguished from a random\noracle due to inner collisions. We evaluate the strength of random sponges by computing the\nprobability of success for a number of attacks as a function of their workload and show that\nthese results shed a new light on the classical Merkle-Damg˚ard construction. We propose to\nuse random sponges of given parameters as a reference for specifying security claims for hash\nfunctions, but also MAC functions and some types of stream ciphers. The main goal of sponge\nfunctions is for designers to be able to formulate a compact security claim.",
    "URL": "https://keccak.team/files/SpongeFunctions.pdf",
    "_line": "Security.bib:1569"
  },
  "zou_buddy_2022": {
    "id": "zou_buddy_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Zou",
        "given": "Changwei"
      },
      {
        "family": "Wang",
        "given": "Xudong"
      },
      {
        "family": "Gao",
        "given": "Yaoqing"
      },
      {
        "family": "Xue",
        "given": "Jingling"
      }
    ],
    "title": "Buddy Stacks: Protecting Return Addresses with Efficient Thread-Local Storage and Runtime Re-Randomization",
    "container-title": "ACM Transactions on Software Engineering and Methodology",
    "container-title-short": "Buddy Stacks",
    "title-short": "Buddy Stacks",
    "issued": {
      "date-parts": [
        [
          "2022",
          "3",
          "4"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "3",
          "8"
        ]
      ]
    },
    "issn": "1049-331X",
    "abstract": "Shadow stacks play an important role in protecting return addresses to mitigate ROP attacks. Parallel shadow stacks, which shadow the call stack of each thread at the same constant offset for all threads, are known not to support multi-threading well. On the other hand, compact shadow stacks must maintain a separate shadow stack pointer in thread-local storage (TLS), which can be implemented in terms of a register or the per-thread Thread-Control-Block (TCB), suffering from poor compatibility in the former or high performance overhead in the latter. In addition, shadow stacks are vulnerable to information disclosure attacks. In this paper, we propose to mitigate ROP attacks for single- and multi-threaded server programs running on general-purpose computing systems by using a novel stack layout, called a buddy stack (referred to as Bustk), that is highly performant, compatible with existing code, and provides meaningful security. These goals are met due to three novel design aspects in Bustk. First, Bustk places a parallel shadow stack just below a thread’s call stack (as each other’s buddies allocated together), avoiding the need to maintain a separate shadow stack pointer and making it now well-suited for multi-threading. Second, Bustk uses an efficient stack-based thread-local storage mechanism, denoted STK-TLS, to store thread-specific metadata in two TLS sections just below the shadow stack in dual redundancy (as each other’s buddies), so that both can be accessed and updated in a lightweight manner from the call stack pointer rsp alone. Finally, Bustk re-randomizes continuously (on the order of milliseconds) the return addresses on the shadow stack by using a new microsecond-level runtime re-randomization technique, denoted STK-MSR. This mechanism aims to obsolete leaked information, making it extremely unlikely for the attacker to hijack return addresses, particularly against a server program that sits often tens of milliseconds away from the attacker. Our evaluation using web servers, Nginx and Apache Httpd, shows that Bustk works well in terms of performance, compatibility, and security provided, with its parallel shadow stacks incurring acceptable memory overhead for real-world applications and its STK-TLS mechanism costing only two pages per thread. In particular, Bustk can protect the Nginx and Apache servers with an adaptive 1-ms re-randomization policy (without observable overheads when IO is intensive, with about 17,000 requests per second). In addition, we have also evaluated Bustk using other non-server applications, Firefox, Python, LLVM, JDK and SPEC CPU2006, to demonstrate further the same degree of performance and compatibility provided, but the protection provided for, say, browsers, is weaker (since network-access delays can no longer be assumed).",
    "keywords": "buddy stack, CFI, ROP, runtime re-randomization, Shadow stack",
    "URL": "https://doi.org/10.1145/3494516",
    "DOI": "10.1145/3494516",
    "page": "35e:1-35e:37",
    "page-first": "35",
    "volume": "31",
    "issue": "2",
    "_line": "Security.bib:1588"
  },
  "bernhard_xtag_2022": {
    "id": "bernhard_xtag_2022",
    "type": "article-journal",
    "author": [
      {
        "family": "Bernhard",
        "given": "Lukas"
      },
      {
        "family": "Rodler",
        "given": "Michael"
      },
      {
        "family": "Holz",
        "given": "Thorsten"
      },
      {
        "family": "Davi",
        "given": "Lucas"
      }
    ],
    "title": "xTag: Mitigating Use-After-Free Vulnerabilities via Software-Based Pointer Tagging on Intel x86-64",
    "container-title": "arXiv:2203.04117 \\[cs\\]",
    "container-title-short": "xTag",
    "title-short": "xTag",
    "issued": {
      "date-parts": [
        [
          "2022",
          "3",
          "8"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2022",
          "3",
          "14"
        ]
      ]
    },
    "abstract": "Memory safety in complex applications implemented in unsafe programming languages such as C/C++ is still an unresolved problem in practice. Many different types of defenses have been proposed in the past to mitigate this problem. The most promising next step is a tighter integration of the hardware and software level: modern mitigation techniques are either accelerated using hardware extensions or implemented in the hardware by extensions of the ISA. In particular, memory tagging, as proposed by ARM or SPARC, promises to solve many issues for practical memory safety. Unfortunately, Intel x86-64, which represents the most important ISA for both the desktop and server domain, lacks support for hardware-accelerated memory tagging, so memory tagging is not considered practical for this platform. In this paper, we present the design and implementation of an efficient, software-only pointer tagging scheme for Intel x86-64 based on a novel metadata embedding scheme. The basic idea is to alias multiple virtual pages to one physical page so that we can efficiently embed tag bits into a pointer. Furthermore, we introduce several optimizations that significantly reduce the performance impact of this approach to memory tagging. Based on this scheme, we propose a novel use-after-free mitigation scheme, called xTag, that offers better performance and strong security properties compared to state-of-the-art methods. We also show how double-free vulnerabilities can be mitigated. Our approach is highly compatible, allowing pointers to be passed back and forth between instrumented and non-instrumented code without losing metadata, and it is even compatible with inline assembly. We conclude that building exploit mitigation mechanisms on top of our memory tagging scheme is feasible on Intel x86-64, as demonstrated by the effective prevention of use-after-free bugs in the Firefox web browser.",
    "keywords": "Computer Science - Cryptography and Security",
    "URLtext": "2203.04117",
    "URLpretext": "[arXiv:]{.etype}",
    "eprint-type": "arxiv",
    "eprint": "2203.04117",
    "URL": "http://arxiv.org/abs/2203.04117",
    "_line": "Security.bib:1607"
  }
}