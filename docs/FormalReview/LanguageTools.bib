
@article{yang_c2aadl_reverse_2021,
	title = {C2AADL\_Reverse: A model-driven reverse engineering approach to development and verification of safety-critical software},
	volume = {118},
	issn = {1383-7621},
	url = {https://www.sciencedirect.com/science/article/pii/S1383762121001454},
	doi = {10.1016/j.sysarc.2021.102202},
	shorttitle = {C2AADL\_Reverse},
	abstract = {The safety-critical system communities have been struggling to manage and maintain their legacy softwaresystems because upgrading such systems has been a complex challenge. To overcome or reduce this problem, reverse engineering has been increasingly used in safety-critical systems. This paper proposes C2AADL\_Reverse, a model-driven reverse engineering approach for safety-critical software development and verification. C2AADL\_Reverse takes multi-task C source code as input, and generates {AADL} (Architecture Analysis and Design Language) model of the legacy software systems. Compared with the existing works, this paper considers more reversed construction including {AADL} component structure, behavior, and multi-threaded run-time information. Moreover, two types of activities are proposed to ensure the correctness of C2AADL\_Reverse. First, it is necessary to validate the reverse engineering process. Second, the generated {AADL} models should conform to desired critical properties. We propose the verification of the reverse-engineered {AADL} model by using {UPPAAL} to establish component-level properties and the Assume Guarantee {REasoning} Environment ({AGREE}) to perform compositional verification of the architecture. This combination of verification tools allows us to iteratively explore design and verification of detailed behavioral models, and to scale formal analysis to large models. In addition, the prototype tool and the evaluation of C2AADL\_Reverse using a real-world aerospace case study are presented.},
	pages = {102202},
	journaltitle = {Journal of Systems Architecture},
	shortjournal = {Journal of Systems Architecture},
	author = {Yang, Zhibin and Qiu, Zhikai and Zhou, Yong and Huang, Zhiqiu and Bodeveix, Jean-Paul and Filali, Mamoun},
	urldate = {2021-06-14},
	date = {2021-09-01},
	langid = {english},
	keywords = {{AADL}, Compositional verification, Model-driven development, Model-driven reverse engineering, Safety-critical systems},
	file = {ScienceDirect Snapshot:/home/fordrl/Zotero/storage/W3WXFRFH/S1383762121001454.html:text/html},
}

@article{wolf_gobra_2021,
	title = {Gobra: Modular Specification and Verification of Go Programs (extended version)},
	url = {http://arxiv.org/abs/2105.13840},
	shorttitle = {Gobra},
	abstract = {Go is an increasingly-popular systems programming language targeting, especially, concurrent and distributed systems. Go differentiates itself from other imperative languages by offering structural subtyping and lightweight concurrency through goroutines with message-passing communication. This combination of features poses interesting challenges for static verification, most prominently the combination of a mutable heap and advanced concurrency primitives. We present Gobra, a modular, deductive program verifier for Go that proves memory safety, crash safety, data-race freedom, and user-provided specifications. Gobra is based on separation logic and supports a large subset of Go. Its implementation translates an annotated Go program into the Viper intermediate verification language and uses an existing {SMT}-based verification backend to compute and discharge proof obligations.},
	journaltitle = {{arXiv}:2105.13840 [cs]},
	author = {Wolf, Felix A. and Arquint, Linard and Clochard, Martin and Oortwijn, Wytse and Pereira, João C. and Müller, Peter},
	urldate = {2021-06-07},
	date = {2021-05-28},
	eprinttype = {arxiv},
	eprint = {2105.13840},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/KBY7HYCN/2105.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/GT59VTYM/Wolf et al. - 2021 - Gobra Modular Specification and Verification of G.pdf:application/pdf},
}

@article{noauthor_perceus_2020,
	title = {Perceus: Garbage Free Reference Counting with {ReuseMicrosoft} Technical Report, {MSR}-{TR}-2020-42, Jan 11, 2021, v3.},
	abstract = {We introduce Perceus, an algorithm for precise reference counting with reuse and specialization. Starting from a functional core language with explicit control-flow, Perceus emits precise reference counting instructions such that programs are garbage free, where only live references are retained. This enables further optimizations, like reuse analysis that allows for guaranteed in-place updates at runtime. This in turn enables a novel programming paradigm that we call functional but in-place ({FBIP}). Much like tail-call optimization enables writing loops with regular function calls, reuse analysis enables writing in-place mutating algorithms in a purely functional way. We give a novel formalization of reference counting in a linear resource calculus, and prove that Perceus is sound and garbage free. We show evidence that Perceus, as implemented in Koka, has good performance and is competitive with other state-of-the-art memory collectors.},
	pages = {41},
	date = {2020},
	langid = {english},
	file = {2020 - Perceus Garbage Free Reference Counting with Reus.pdf:/home/fordrl/Zotero/storage/T4DWHG3T/2020 - Perceus Garbage Free Reference Counting with Reus.pdf:application/pdf},
}

@article{zuo_chianina_2021,
	title = {Chianina: An Evolving Graph System for Flow- and Context-Sensitive Analyses of Million Lines of C Code},
	abstract = {Sophisticated static analysis techniques often have complicated implementations, much of which provides logic for tuning and scaling rather than basic analysis functionalities. This tight coupling of basic algorithms with special treatments for scalability makes an analysis implementation hard to (1) make correct, (2) understand/work with, and (3) reuse for other clients. This paper presents Chianina, a graph system we developed for fully context- and flow-sensitive analysis of large C programs. Chianina overcomes these challenges by allowing the developer to provide only the basic algorithm of an analysis and pushing the tuning/scaling work to the underlying system. Key to the success of Chianina is (1) an evolving graph formulation of flow sensitivity and (2) the leverage of out-of-core, disk support to deal with memory blowup resulting from context sensitivity. We implemented three context- and flow-sensitive analyses on top of Chianina and scaled them to large C programs like Linux (17M {LoC}) on a single commodity {PC}. {CCS} Concepts: • Computer systems organization → Special purpose systems; Reliability; • Theory of computation → Program analysis.},
	pages = {16},
	author = {Zuo, Zhiqiang and Zhang, Yiyu and Pan, Qiuhong and Lu, Shenming and Li, Yue and Wang, Linzhang and Li, Xuandong and Xu, Guoqing Harry},
	date = {2021},
	langid = {english},
	file = {Zuo et al. - 2021 - Chianina An Evolving Graph System for Flow- and C.pdf:/home/fordrl/Zotero/storage/79556CDT/Zuo et al. - 2021 - Chianina An Evolving Graph System for Flow- and C.pdf:application/pdf},
}

@article{parr_adaptive_nodate,
	title = {Adaptive {LL}(*) Parsing: The Power of Dynamic Analysis},
	abstract = {Despite the advances made by modern parsing strategies such as {PEG}, {LL}(*), {GLR}, and {GLL}, parsing is not a solved problem. Existing approaches suffer from a number of weaknesses, including difﬁculties supporting side-effecting embedded actions, slow and/or unpredictable performance, and counterintuitive matching strategies. This paper introduces the {ALL}(*) parsing strategy that combines the simplicity, efﬁciency, and predictability of conventional top-down {LL}(k) parsers with the power of a {GLR}-like mechanism to make parsing decisions. The critical innovation is to move grammar analysis to parsetime, which lets {ALL}(*) handle any non-left-recursive contextfree grammar. {ALL}(*) is O(n4) in theory but consistently performs linearly on grammars used in practice, outperforming general strategies such as {GLL} and {GLR} by orders of magnitude. {ANTLR} 4 generates {ALL}(*) parsers and supports direct left-recursion through grammar rewriting. Widespread {ANTLR} 4 use (5000 downloads/month in 2013) provides evidence that {ALL}(*) is effective for a wide variety of applications.},
	pages = {19},
	author = {Parr, Terence and Harwell, Sam and Fisher, Kathleen},
	langid = {english},
	file = {Parr et al. - Adaptive LL() Parsing The Power of Dynamic Analy.pdf:/home/fordrl/Zotero/storage/GPZFYGPK/Parr et al. - Adaptive LL() Parsing The Power of Dynamic Analy.pdf:application/pdf},
}

@online{noauthor_writing_2021,
	title = {Writing Compiler Front-Ends for {LLVM} with Lua using Mewa},
	url = {https://www.codeproject.com/Articles/5301384/Writing-Compiler-Front-Ends-for-LLVM-with-Lua-usin},
	abstract = {In this article we see how a very primitive compiler is written in Lua using Mewa and how to compile and run a simple demo program in the shell.},
	titleaddon = {{CodeProject}},
	urldate = {2021-06-01},
	date = {2021-05-26},
	langid = {american},
	file = {Snapshot:/home/fordrl/Zotero/storage/QXEU4NLI/Writing-Compiler-Front-Ends-for-LLVM-with-Lua-usin.html:text/html},
}

@article{sammler_refinedc_2021,
	title = {{RefinedC}: Automating the Foundational Verification of C Code with Refined Ownership Types},
	abstract = {Given the central role that C continues to play in systems software, and the difficulty of writing safe and correct C code, it remains a grand challenge to develop effective formal methods for verifying C programs. In this paper, we propose a new approach to this problem: a type system we call {RefinedC}, which combines ownership types (for modular reasoning about shared state and concurrency) with refinement types (for encoding precise invariants on C data types and Hoare-style specifications for C functions).},
	pages = {17},
	author = {Sammler, Michael and Lepigre, Rodolphe and Krebbers, Robbert},
	date = {2021},
	langid = {english},
	file = {Sammler et al. - 2021 - RefinedC Automating the Foundational Verification.pdf:/home/fordrl/Zotero/storage/BR5C2FN5/Sammler et al. - 2021 - RefinedC Automating the Foundational Verification.pdf:application/pdf},
}

@article{huihui_optimizing_nodate,
	title = {Optimizing demand-driven null dereference verification via merging branches},
	volume = {n/a},
	issn = {1468-0394},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12707},
	doi = {https://doi.org/10.1111/exsy.12707},
	abstract = {Null dereference is a common type of runtime failure in Java programs, and it is necessary to verify whether a dereference in the program is safe. However, previous works often have redundant path exploration and high false positive rate. In this paper, we propose a merged null dereference verification ({MNDV}) approach. {MNDV} employs a backward, path-sensitive inter-procedural analysis technique to verify a given dereference as safe or potentially unsafe. It uses a branch merging strategy to remove redundant paths, and a method call's relevance to the null references is checked to determine whether it is necessary to explore the internal codes of the method. We have evaluated the approach in some standard benchmark programs. Compared with some existing approaches, our approach reduces false alarm rate and effectively reduce time and memory consumption.},
	pages = {e12707},
	issue = {n/a},
	journaltitle = {Expert Systems},
	author = {Huihui, Cheng and Hongwei, Zeng},
	urldate = {2021-06-01},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12707},
	keywords = {backward analysis, data-flow analysis, null dereference, static analysis},
	file = {Snapshot:/home/fordrl/Zotero/storage/8SVSG743/exsy.html:text/html},
}

@inproceedings{vu_secure_2020,
	location = {New York, {NY}, {USA}},
	title = {Secure delivery of program properties through optimizing compilation},
	isbn = {978-1-4503-7120-9},
	url = {https://doi.org/10.1145/3377555.3377897},
	doi = {10.1145/3377555.3377897},
	series = {{CC} 2020},
	abstract = {Annotations and assertions capturing static program properties are ubiquitous, from robust software engineering to safety-critical or secure code. These may be functional or non-functional properties of control and data flow, memory usage, I/O and real time. We propose an approach to encode, translate, and preserve the semantics of both functional and non-functional properties along the optimizing compilation of C to machine code. The approach involves (1) capturing and translating source-level properties through lowering passes and intermediate representations, such that data and control flow optimizations will preserve their consistency with the transformed program, and (2) carrying properties and their translation as debug information down to machine code. Our experiments using {LLVM} validate the soundness, expressiveness and efficiency of the approach, considering a reference suite of functional properties as well as established security properties and applications hardened against side-channel attacks.},
	pages = {14--26},
	booktitle = {Proceedings of the 29th International Conference on Compiler Construction},
	publisher = {Association for Computing Machinery},
	author = {Vu, Son Tuan and Heydemann, Karine and de Grandmaison, Arnaud and Cohen, Albert},
	urldate = {2021-05-27},
	date = {2020-02-22},
	keywords = {Annotation, Compiler, {LLVM}, Optimization, Security},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/TUZWCL5P/Vu et al. - 2020 - Secure delivery of program properties through opti.pdf:application/pdf},
}

@article{blouin_interacto_2021,
	title = {Interacto: A Modern User Interaction Processing Model},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9440800/},
	doi = {10.1109/TSE.2021.3083321},
	shorttitle = {Interacto},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Blouin, Arnaud and Jezequel, Jean-Marc},
	urldate = {2021-05-26},
	date = {2021},
	langid = {english},
	file = {Blouin and Jezequel - 2021 - Interacto A Modern User Interaction Processing Mo.pdf:/home/fordrl/Zotero/storage/55IDSV2B/Blouin and Jezequel - 2021 - Interacto A Modern User Interaction Processing Mo.pdf:application/pdf},
}

@article{giallorenzo_multiparty_2021,
	title = {Multiparty Languages: The Choreographic and Multitier Cases},
	abstract = {Choreographic languages aim to express multiparty communication protocols, by providing primitives that make interaction manifest. Multitier languages enable programming computation that spans across several tiers of a distributed system, by supporting primitives that allow computation to change the location of execution. Rooted into different theoretical underpinnings—respectively process calculi and lambda calculus—the two paradigms have been investigated independently by different research communities with little or no contact. As a result, the link between the two paradigms has remained hidden for long.},
	pages = {28},
	author = {Giallorenzo, Saverio and Montesi, Fabrizio and Peressotti, Marco and Richter, David and Salvaneschi, Guido and Weisenburger, Pascal},
	date = {2021},
	langid = {english},
	file = {Giallorenzo et al. - 2021 - Multiparty Languages The Choreographic and Multit.pdf:/home/fordrl/Zotero/storage/HVVGF7FY/Giallorenzo et al. - 2021 - Multiparty Languages The Choreographic and Multit.pdf:application/pdf},
}

@article{egolf_verbatim_nodate,
	title = {Verbatim: A Veriﬁed Lexer Generator},
	abstract = {Lexers and parsers are often used as front ends to connect input from the outside world with the internals of a larger software system. These front ends are natural targets for attackers who wish to compromise the larger system. A formally veriﬁed tool that performs mechanized lexical analysis would render attacks on these front ends less effective.},
	pages = {9},
	author = {Egolf, Derek and Lasser, Sam and Fisher, Kathleen},
	langid = {english},
	file = {Egolf et al. - Verbatim A Veriﬁed Lexer Generator.pdf:/home/fordrl/Zotero/storage/2VCICDH6/Egolf et al. - Verbatim A Veriﬁed Lexer Generator.pdf:application/pdf},
}

@article{ernst_deductive_nodate,
	title = {Deductive Verification via the Debug Adapter Protocol},
	pages = {8},
	author = {Ernst, Gidon and Blau, Johannes and Murray, Toby},
	langid = {english},
	file = {Ernst et al. - Deductive Verification via the Debug Adapter Proto.pdf:/home/fordrl/Zotero/storage/JSFRG6V2/Ernst et al. - Deductive Verification via the Debug Adapter Proto.pdf:application/pdf},
}

@article{kj_specication_nodate,
	title = {The Speciﬁcation Language Server Protocol: A Proposal for Standardised {LSP} Extensions},
	pages = {16},
	journaltitle = {F-{IDE}2021},
	author = {Kj, Jonas and Madsen, Frederik Palludan and Battle, Nick},
	langid = {english},
	file = {Kj et al. - The Speciﬁcation Language Server Protocol A Propo.pdf:/home/fordrl/Zotero/storage/KEXG2E7D/Kj et al. - The Speciﬁcation Language Server Protocol A Propo.pdf:application/pdf},
}

@article{shi_path-sensitive_2021,
	title = {Path-Sensitive Sparse Analysis {withoutPath} Conditions},
	abstract = {Sparse program analysis is fast as it propagates data �ow facts via data dependence, skipping unnecessary control �ows. However, when path-sensitively checking millions of lines of code, it is still prohibitively expensive because a huge number of path conditions have to be computed and solved via an {SMT} solver. This paper presents Fusion, a fused approach to inter-procedurally path-sensitive sparse analysis. In Fusion, the {SMT} solver does not work as a standalone tool on path conditions but directly on the program together with the sparse analysis. Such a fused design allows us to determine the path feasibility without explicitly computing path conditions, not only saving the cost of computing path conditions but also providing an opportunity to enhance the {SMT} solving algorithm. To the best of our knowledge, Fusion, for the �rst time, enables whole program bug detection on millions of lines of code in a common personal computer, with the precision of inter-procedural path-sensitivity. Compared to two state-of-the-art tools, Fusion is 10⇥ faster but consumes only 10\% of memory on average. Fusion has detected over a hundred bugs in mature open-source software, some of which have even been assigned {CVE} identi�ers due to their security impact.},
	pages = {14},
	author = {Shi, Qingkai and Yao, Peisen and Wu, Rongxin and Zhang, Charles},
	date = {2021},
	langid = {english},
	file = {Shi et al. - 2021 - Path-Sensitive Sparse Analysis withoutPath Conditi.pdf:/home/fordrl/Zotero/storage/TNUQF8KD/Shi et al. - 2021 - Path-Sensitive Sparse Analysis withoutPath Conditi.pdf:application/pdf},
}

@article{cai_canary_2021,
	title = {Canary: Practical Static Detection of  Inter-thread Value-Flow Bugs},
	abstract = {Concurrent programs are still prone to bugs arising from the subtle interleavings of threads. Traditional static analysis for concurrent programs, such as data-�ow analysis and symbolic execution, has to explicitly explore redundant control states, leading to prohibitive computational complexity. This paper presents a value �ow analysis framework for concurrent programs called C����� that is practical to statically �nd diversi�ed inter-thread value-�ow bugs. Our work is the �rst to convert the concurrency bug detection to a source-sink reachability problem, e�ectively reducing redundant thread interleavings. Speci�cally, we propose a scalable thread-modular algorithm to capture data and interference dependence in a value-�ow graph. The relevant edges of value �ows are annotated with execution constraints as guards to describe the conditions of value �ows. C����� then traverses the graph to detect concurrency defects via tracking the source-sink properties and solving the aggregated guards of value �ows with an {SMT} solver to decide the realizability of interleaving executions. Experiments show that C����� is precise, scalable and practical, detecting over eighteen previously unknown concurrency bugs in large, widely-used software systems with low false positives.},
	pages = {15},
	author = {Cai, Yuandao and Yao, Peisen and Zhang, Charles},
	date = {2021},
	langid = {english},
	file = {Cai et al. - 2021 - Canary Practical Static Detection of  Inter-threa.pdf:/home/fordrl/Zotero/storage/NGRBWWMR/Cai et al. - 2021 - Canary Practical Static Detection of  Inter-threa.pdf:application/pdf},
}

@article{dietz_understanding_nodate,
	title = {Understanding Integer Overﬂow in C/C++},
	abstract = {Integer overﬂow bugs in C and C++ programs are difﬁcult to track down and may lead to fatal errors or exploitable vulnerabilities. Although a number of tools for ﬁnding these bugs exist, the situation is complicated because not all overﬂows are bugs. Better tools need to be constructed—but a thorough understanding of the issues behind these errors does not yet exist. We developed {IOC}, a dynamic checking tool for integer overﬂows, and used it to conduct the ﬁrst detailed empirical study of the prevalence and patterns of occurrence of integer overﬂows in C and C++ code. Our results show that intentional uses of wraparound behaviors are more common than is widely believed; for example, there are over 200 distinct locations in the {SPEC} {CINT}2000 benchmarks where overﬂow occurs. Although many overﬂows are intentional, a large number of accidental overﬂows also occur. Orthogonal to programmers’ intent, overﬂows are found in both welldeﬁned and undeﬁned ﬂavors. Applications executing undeﬁned operations can be, and have been, broken by improvements in compiler optimizations. Looking beyond {SPEC}, we found and reported undeﬁned integer overﬂows in {SQLite}, {PostgreSQL}, {SafeInt}, {GNU} {MPC} and {GMP}, Firefox, {GCC}, {LLVM}, Python, {BIND}, and {OpenSSL}; many of these have since been ﬁxed. Our results show that integer overﬂow issues in C and C++ are subtle and complex, that they are common even in mature, widely used programs, and that they are widely misunderstood by developers.},
	pages = {11},
	author = {Dietz, Will and Li, Peng and Regehr, John and Adve, Vikram},
	langid = {english},
	file = {Dietz et al. - Understanding Integer Overﬂow in CC++.pdf:/home/fordrl/Zotero/storage/Q4X47I2F/Dietz et al. - Understanding Integer Overﬂow in CC++.pdf:application/pdf},
}

@article{sun_taming_2021,
	title = {Taming Reflection: An Essential Step Toward Whole-program Analysis of Android Apps},
	volume = {30},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3440033},
	doi = {10.1145/3440033},
	shorttitle = {Taming Reflection},
	abstract = {Android developers heavily use reflection in their apps for legitimate reasons. However, reflection is also significantly used for hiding malicious actions. Unfortunately, current state-of-the-art static analysis tools for Android are challenged by the presence of reflective calls, which they usually ignore. Thus, the results of their security analysis, e.g., for private data leaks, are incomplete, given the measures taken by malware writers to elude static detection. We propose a new instrumentation-based approach to address this issue in a non-invasive way. Specifically, we introduce to the community a prototype tool called {DroidRA}, which reduces the resolution of reflective calls to a composite constant propagation problem and then leverages the {COAL} solver to infer the values of reflection targets. After that, it automatically instruments the app to replace reflective calls with their corresponding Java calls in a traditional paradigm. Our approach augments an app so that it can be more effectively statically analyzable, including by such static analyzers that are not reflection-aware. We evaluate {DroidRA} on benchmark apps as well as on real-world apps, and we demonstrate that it can indeed infer the target values of reflective calls and subsequently allow state-of-the-art tools to provide more sound and complete analysis results.},
	pages = {32:1--32:36},
	number = {3},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Sun, Xiaoyu and Li, Li and Bissyandé, Tegawendé F. and Klein, Jacques and Octeau, Damien and Grundy, John},
	urldate = {2021-05-04},
	date = {2021-04-23},
	keywords = {static analysis, Android, {DroidRA}, reflection},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/C5R74AKP/Sun et al. - 2021 - Taming Reflection An Essential Step Toward Whole-.pdf:application/pdf},
}

@article{wang_find_2021,
	title = {Find Bugs in Static Bug Finders},
	url = {http://arxiv.org/abs/2109.02245},
	abstract = {Static bug finders have been widely-adopted by developers to find bugs in real world software projects. They leverage predefined heuristic static analysis rules to scan source code or binary code of a software project, and report violations to these rules as warnings to be verified. However, the advantages of static bug finders are overshadowed by such issues as uncovered obvious bugs, false positives, etc. To improve these tools, many techniques have been proposed to filter out false positives reported or design new static analysis rules. Nevertheless, the under-performance of bug finders can also be caused by the incorrectness of current rules contained in the static bug finders, which is not explored yet. In this work, we propose a differential testing approach to detect bugs in the rules of four widely-used static bug finders, i.e., {SonarQube}, {PMD}, {SpotBugs}, and {ErrorProne}, and conduct a qualitative study about the bugs found. To retrieve paired rules across static bug finders for differential testing, we design a heuristic-based rule mapping method which combines the similarity in rules description and the overlap in warning information reported by the tools. The experiment on 2,728 open source projects reveals 46 bugs in the static bug finders, among which 24 are fixed or confirmed and the left are awaiting confirmation. We also summarize 13 bug patterns in the static analysis rules based on their context and root causes, which can serve as the checklist for designing and implementing other rules and or in other tools. This study indicates that the commonly-used static bug finders are not as reliable as they might have been envisaged. It not only demonstrates the effectiveness of our approach, but also highlights the need to continue improving the reliability of the static bug finders.},
	journaltitle = {{arXiv}:2109.02245 [cs]},
	author = {Wang, Junjie and Huang, Yuchao and Wang, Song and Wang, Qing},
	urldate = {2021-09-13},
	date = {2021-09-06},
	eprinttype = {arxiv},
	eprint = {2109.02245},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/URVYN2JQ/2109.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/6GLCZVCF/Wang et al. - 2021 - Find Bugs in Static Bug Finders.pdf:application/pdf},
}

@inproceedings{willis_design_2021,
	location = {Virtual Republic of Korea},
	title = {Design patterns for parser combinators (functional pearl)},
	isbn = {978-1-4503-8615-9},
	url = {https://dl.acm.org/doi/10.1145/3471874.3472984},
	doi = {10.1145/3471874.3472984},
	abstract = {Parser combinators are a popular and elegant approach for parsing in functional languages. The design and implementation of such libraries are well discussed, but having a welldesigned library is only one-half of the story. In this paper we explore several reusable approaches to writing parsers in combinator style, focusing on easy to apply patterns to keep parsing code simple, separated, and maintainable.},
	eventtitle = {{ICFP} '21: 26th {ACM} {SIGPLAN} International Conference on Functional Programming},
	pages = {71--84},
	booktitle = {Proceedings of the 14th {ACM} {SIGPLAN} International Symposium on Haskell},
	publisher = {{ACM}},
	author = {Willis, Jamie and Wu, Nicolas},
	urldate = {2021-08-26},
	date = {2021-08-18},
	langid = {english},
	file = {Willis and Wu - 2021 - Design patterns for parser combinators (functional.pdf:/home/fordrl/Zotero/storage/F3C88HRA/Willis and Wu - 2021 - Design patterns for parser combinators (functional.pdf:application/pdf},
}

@inproceedings{vintila_mesh_2021,
	location = {New York, {NY}, {USA}},
	title = {{MESH}: A Memory-Efficient Safe Heap for C/C++},
	isbn = {978-1-4503-9051-4},
	url = {https://doi.org/10.1145/3465481.3465760},
	doi = {10.1145/3465481.3465760},
	series = {{ARES} 2021},
	shorttitle = {{MESH}},
	abstract = {While memory corruption bugs stemming from the use of unsafe programming languages are an old and well-researched problem, the resulting vulnerabilities still dominate real-world exploitation today. Various mitigations have been proposed to alleviate the problem, mainly in the form of language dialects, static program analysis, and code or binary instrumentation. Solutions like {AdressSanitizer} ({ASan}) and Softbound/{CETS} have proven that the latter approach is very promising, being able to achieve memory safety without requiring manual source code adaptions, albeit suffering substantial performance and memory overheads. While performance overhead can be seen as a flexible constraint, extensive memory overheads can be prohibitive for the use of such solutions in memory-constrained environments. To address this problem, we propose {MESH}, a highly memory-efficient safe heap for C/C++. With its constant, very small memory overhead (configurable up to 2 {MB} on x86-64) and constant complexity for pointer access checking, {MESH} offers efficient, byte-precise spatial and temporal memory safety for memory-constrained scenarios. Without jeopardizing the security of safe heap objects, {MESH} is fully compatible with existing code and uninstrumented libraries, making it practical to use in heterogeneous environments. We show the feasibility of our approach with a full {LLVM}-based prototype supporting both major architectures, i.e., x86-64 and {ARM}64, in a Linux runtime environment. Our prototype evaluation shows that, compared to {ASan} and Softbound/{CETS}, {MESH} can achieve huge memory savings while preserving similar execution performance.},
	pages = {1--10},
	booktitle = {The 16th International Conference on Availability, Reliability and Security},
	publisher = {Association for Computing Machinery},
	author = {Vintila, Emanuel Q. and Zieris, Philipp and Horsch, Julian},
	urldate = {2021-08-26},
	date = {2021-08-17},
	keywords = {memory safety, buffer overflows, dangling pointers, pointer tagging, unsafe programming languages, use-after-free},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/UYUYMVJI/Vintila et al. - 2021 - MESH A Memory-Efficient Safe Heap for CC++.pdf:application/pdf},
}

@article{devkota_cfgconf_2021,
	title = {{CFGConf}: Supporting high level requirements for visualizing Control Flow Graphs},
	url = {http://arxiv.org/abs/2108.03047},
	shorttitle = {{CFGConf}},
	abstract = {Control Flow Graphs ({CFGs}) are directed graphs that represent all possible walks a program can take during its execution. {CFGs} are used to analyze computer programs for purposes such as compilation, performance, and security. They are commonly drawn using hierarchical layouts. However, the general nature of such layouts may not capture {CFG}-specific structures, making it more difficult to match the drawing to the domain. Domain-specific drawings often require the help of a graph drawing expert, despite the computing expertise of the target audience. To alleviate these issues, we conduct a survey of drawing conventions and needs for {CFGs}. We then, through an iterative design process, design a flexible set of representations based on these findings and develop {CFGConf}, a {JSON} specification for specifying and drawing these higher-level drawing requirements, thereby allowing users to generate and integrate their own {CFG}-aware graph drawings. The {CFGConf} language enables the creation of domain-aware graph drawings of {CFGs} by increasing the notational efficiency of specifying the requirements while also retaining the expressiveness found in commonly used systems such as dot/graphviz. We evaluate {CFGConf} in terms of notational efficiency, expressiveness, and accessibility through user study and illustrative examples.},
	journaltitle = {{arXiv}:2108.03047 [cs]},
	author = {Devkota, Sabin and Legendre, Matthew and Kunen, Adam and Aschwanden, Pascal and Isaacs, Katherine E.},
	urldate = {2021-08-16},
	date = {2021-08-06},
	eprinttype = {arxiv},
	eprint = {2108.03047},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/ISDYBJJU/2108.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/I5UE8WUT/Devkota et al. - 2021 - CFGConf Supporting high level requirements for vi.pdf:application/pdf},
}

@article{costea_hippodrome_2021,
	title = {{HIPPODROME}: Data Race Repair using Static Analysis Summaries},
	url = {http://arxiv.org/abs/2108.02490},
	shorttitle = {{HIPPODROME}},
	abstract = {Implementing bug-free concurrent programs is a challenging task in modern software development. State-of-the-art static analyses find hundreds of concurrency bugs in production code, scaling to large codebases. Yet, fixing these bugs in constantly changing codebases represents a daunting effort for programmers, particularly because a fix in the concurrent code can introduce other bugs in a subtle way. In this work, we show how to harness compositional static analysis for concurrency bug detection, to enable a new Automated Program Repair ({APR}) technique for data races in large concurrent Java codebases. The key innovation of our work is an algorithm that translates procedure summaries inferred by the analysis tool for the purpose of bug reporting, into small local patches that fix concurrency bugs (without introducing new ones). This synergy makes it possible to extend the virtues of compositional static concurrency analysis to {APR}, making our approach effective (it can detect and fix many more bugs than existing tools for data race repair), scalable (it takes seconds to analyse and suggest fixes for sizeable codebases), and usable (generally, it does not require annotations from the users and can perform continuous automated repair). Our study conducted on popular open-source projects has confirmed that our tool automatically produces concurrency fixes similar to those proposed by the developers in the past.},
	journaltitle = {{arXiv}:2108.02490 [cs]},
	author = {Costea, Andreea and Tiwari, Abhishek and Chianasta, Sigmund and R, Kishore and Roychoudhury, Abhik and Sergey, Ilya},
	urldate = {2021-08-11},
	date = {2021-08-06},
	eprinttype = {arxiv},
	eprint = {2108.02490},
	keywords = {Computer Science - Programming Languages, Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/Z6FVMB7A/2108.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/66Z94WZT/Costea et al. - 2021 - HIPPODROME Data Race Repair using Static Analysis.pdf:application/pdf},
}

@article{zuo_systemizing_2021,
	title = {Systemizing Interprocedural Static Analysis of Large-scale Systems Code with Graspan},
	volume = {38},
	issn = {0734-2071},
	url = {https://doi.org/10.1145/3466820},
	doi = {10.1145/3466820},
	abstract = {There is more than a decade-long history of using static analysis to find bugs in systems such as Linux. Most of the existing static analyses developed for these systems are simple checkers that find bugs based on pattern matching. Despite the presence of many sophisticated interprocedural analyses, few of them have been employed to improve checkers for systems code due to their complex implementations and poor scalability. In this article, we revisit the scalability problem of interprocedural static analysis from a “Big Data” perspective. That is, we turn sophisticated code analysis into Big Data analytics and leverage novel data processing techniques to solve this traditional programming language problem. We propose Graspan, a disk-based parallel graph system that uses an edge-pair centric computation model to compute dynamic transitive closures on very large program graphs. We develop two backends for Graspan, namely, Graspan-C running on {CPUs} and Graspan-G on {GPUs}, and present their designs in the article. Graspan-C can analyze large-scale systems code on any commodity {PC}, while, if {GPUs} are available, Graspan-G can be readily used to achieve orders of magnitude speedup by harnessing a {GPU}’s massive parallelism. We have implemented fully context-sensitive pointer/alias and dataflow analyses on Graspan. An evaluation of these analyses on large codebases written in multiple languages such as Linux and Apache Hadoop demonstrates that their Graspan implementations are language-independent, scale to millions of lines of code, and are much simpler than their original implementations. Moreover, we show that these analyses can be used to uncover many real-world bugs in large-scale systems code.},
	pages = {4:1--4:39},
	number = {1},
	journaltitle = {{ACM} Transactions on Computer Systems},
	shortjournal = {{ACM} Trans. Comput. Syst.},
	author = {Zuo, Zhiqiang and Wang, Kai and Hussain, Aftab and Sani, Ardalan Amiri and Zhang, Yiyu and Lu, Shenming and Dou, Wensheng and Wang, Linzhang and Li, Xuandong and Wang, Chenxi and Xu, Guoqing Harry},
	urldate = {2021-08-04},
	date = {2021-07-29},
	keywords = {static analysis, disk-based systems, graph processing},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/E5L38LQC/Zuo et al. - 2021 - Systemizing Interprocedural Static Analysis of Lar.pdf:application/pdf},
}

@article{swierstra_data_2008,
	title = {Data types à la carte},
	volume = {18},
	issn = {0956-7968, 1469-7653},
	url = {http://www.journals.cambridge.org/abstract_S0956796808006758},
	doi = {10.1017/S0956796808006758},
	abstract = {This paper describes a technique for assembling both data types and functions from isolated individual components. We also explore how the same technology can be used to combine free monads and, as a result, structure Haskell’s monolithic {IO} monad.},
	number = {4},
	journaltitle = {Journal of Functional Programming},
	shortjournal = {J. Funct. Prog.},
	author = {Swierstra, Wouter},
	urldate = {2021-07-30},
	date = {2008-07},
	langid = {english},
	file = {Swierstra - 2008 - Data types à la carte.pdf:/home/fordrl/Zotero/storage/R8YM7YW3/Swierstra - 2008 - Data types à la carte.pdf:application/pdf},
}

@inproceedings{panchenko_bolt_2019,
	location = {Washington, {DC}, {USA}},
	title = {{BOLT}: a practical binary optimizer for data centers and beyond},
	isbn = {978-1-72811-436-1},
	series = {{CGO} 2019},
	shorttitle = {{BOLT}},
	abstract = {Abstract — Performance optimization for large-scale applications has recently become more important as computation continues to move towards data centers. Data-center applications are generally very large and complex, which makes code layout an important optimization to improve their performance. This has motivated recent investigation of practical techniques to improve code layout at both compile time and link time. Although post-link optimizers had some success in the past, no recent work has explored their benefits in the context of modern data-center applications. In this paper, we present {BOLT}, an open-source post-link optimizer built on top of the {LLVM} framework. Utilizing sample-based profiling, {BOLT} boosts the performance of real-world applications even for highly optimized binaries built with both feedback-driven optimizations ({FDO}) and link-time optimizations ({LTO}). We demonstrate that post-link performance improvements are complementary to conventional compiler optimizations, even when the latter are done at a whole-program level and in the presence of profile information. We evaluated {BOLT} on both Facebook data-center workloads and open-source compilers. For data-center applications, {BOLT} achieves up to 7.0\% performance speedups on top of profile-guided function reordering and {LTO}. For the {GCC} and Clang compilers, our evaluation shows that {BOLT} speeds up their binaries by up to 20.4\% on top of {FDO} and {LTO}, and up to 52.1\% if the binaries are built without {FDO} and {LTO}.},
	pages = {2--14},
	booktitle = {Proceedings of the 2019 {IEEE}/{ACM} International Symposium on Code Generation and Optimization},
	publisher = {{IEEE} Press},
	author = {Panchenko, Maksim and Auler, Rafael and Nell, Bill and Ottoni, Guilherme},
	urldate = {2021-07-20},
	date = {2019-02-16},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/JVE4X89I/Panchenko et al. - 2019 - BOLT a practical binary optimizer for data center.pdf:application/pdf},
}

@article{six_certified_2020,
	title = {Certified and efficient instruction scheduling: application to interlocked {VLIW} processors},
	volume = {4},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3428197},
	doi = {10.1145/3428197},
	shorttitle = {Certified and efficient instruction scheduling},
	abstract = {{CYRIL} {SIX}, Kalray S.A., France and Univ. Grenoble Alpes, {CNRS}, Grenoble {INP}, Verimag, France {SYLVAIN} {BOULMÉ}, Univ. Grenoble Alpes, {CNRS}, Grenoble {INP}, Verimag, France {DAVID} {MONNIAUX}, Univ. Grenoble Alpes, {CNRS}, Grenoble {INP}, Verimag, France {CompCert} is a moderately optimizing C compiler with a formal, machine-checked, proof of correctness: after successful compilation, the assembly code has a behavior faithful to the source code. Previously, it only supported target instruction sets with sequential semantics, and did not attempt reordering instructions for optimization. We present here a {CompCert} backend for a {VLIW} core (i.e. with explicit parallelism at the instruction level), the first {CompCert} backend providing scalable and efficient instruction scheduling. Furthermore, its highly modular implementation can be easily adapted to other {VLIW} or non-{VLIW} pipelined processors. {CCS} Concepts: • Software and its engineering → Formal software verification; Retargetable compilers; • Theory of computation → Scheduling algorithms; • General and reference → Performance; • Computer systems organization → Superscalar architectures; Very long instruction word.},
	pages = {1--29},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Six, Cyril and Boulmé, Sylvain and Monniaux, David},
	urldate = {2021-06-30},
	date = {2020-11-13},
	langid = {english},
	file = {Six et al. - 2020 - Certified and efficient instruction scheduling ap.pdf:/home/fordrl/Zotero/storage/IQCHP2TD/Six et al. - 2020 - Certified and efficient instruction scheduling ap.pdf:application/pdf},
}

@inproceedings{lasser_costar_2021,
	location = {New York, {NY}, {USA}},
	title = {{CoStar}: a verified {ALL}(*) parser},
	isbn = {978-1-4503-8391-2},
	url = {https://doi.org/10.1145/3453483.3454053},
	doi = {10.1145/3453483.3454053},
	series = {{PLDI} 2021},
	shorttitle = {{CoStar}},
	abstract = {Parsers are security-critical components of many software systems, and verified parsing therefore has a key role to play in secure software design. However, existing verified parsers for context-free grammars are limited in their expressiveness, termination properties, or performance characteristics. They are only compatible with a restricted class of grammars, they are not guaranteed to terminate on all inputs, or they are not designed to be performant on grammars for real-world programming languages and data formats. In this work, we present {CoStar}, a verified parser that addresses these limitations. The parser is implemented with the Coq Proof Assistant and is based on the {ALL}(*) parsing algorithm. {CoStar} is sound and complete for all non-left-recursive grammars; it produces a correct parse tree for its input whenever such a tree exists, and it correctly detects ambiguous inputs. {CoStar} also provides strong termination guarantees; it terminates without error on all inputs when applied to a non-left-recursive grammar. Finally, {CoStar} achieves linear-time performance on a range of unambiguous grammars for commonly used languages and data formats.},
	pages = {420--434},
	booktitle = {Proceedings of the 42nd {ACM} {SIGPLAN} International Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Lasser, Sam and Casinghino, Chris and Fisher, Kathleen and Roux, Cody},
	urldate = {2021-06-25},
	date = {2021-06-19},
	keywords = {interactive theorem proving, parsing},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/9VDQZZKD/Lasser et al. - 2021 - CoStar a verified ALL() parser.pdf:application/pdf},
}

@inproceedings{liu_when_2021,
	location = {New York, {NY}, {USA}},
	title = {When threads meet events: efficient and precise static race detection with origins},
	isbn = {978-1-4503-8391-2},
	url = {https://doi.org/10.1145/3453483.3454073},
	doi = {10.1145/3453483.3454073},
	series = {{PLDI} 2021},
	shorttitle = {When threads meet events},
	abstract = {Data races are among the worst bugs in software in that they exhibit non-deterministic symptoms and are notoriously difficult to detect. The problem is exacerbated by interactions between threads and events in real-world applications. We present a novel static analysis technique, O2, to detect data races in large complex multithreaded and event-driven software. O2 is powered by “origins”, an abstraction that unifies threads and events by treating them as entry points of code paths attributed with data pointers. Origins in most cases are inferred automatically, but can also be specified by developers. More importantly, origins provide an efficient way to precisely reason about shared memory and pointer aliases. Together with several important design choices for race detection, we have implemented O2 for both C/C++ and Java/Android applications and applied it to a wide range of open-source software. O2 has found new races in every single real-world code base we evaluated with, including Linux kernel, Redis, {OVS}, Memcached, Hadoop, Tomcat, {ZooKeeper} and Firefox Android. Moreover, O2 scales to millions of lines of code in a few minutes, on average 70x faster (up to 568x) compared to an existing static analysis tool from our prior work, and reduces false positives by 77\%. We also compared O2 with the state-of-the-art static race detection tool, {RacerD}, showing highly promising results. At the time of writing, O2 has revealed more than 40 unique previously unknown races that have been confirmed or fixed by developers.},
	pages = {725--739},
	booktitle = {Proceedings of the 42nd {ACM} {SIGPLAN} International Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Liu, Bozhen and Liu, Peiming and Li, Yanze and Tsai, Chia-Che and Da Silva, Dilma and Huang, Jeff},
	urldate = {2021-06-25},
	date = {2021-06-19},
	keywords = {Data Race Detection, Origins, Pointer Analysis, Static Analysis},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/EXKCN64H/Liu et al. - 2021 - When threads meet events efficient and precise st.pdf:application/pdf},
}

@article{athaiya_data_2021,
	title = {Data Flow Analysis of Asynchronous Systems using Infinite Abstract Domains},
	volume = {12648},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7984527/},
	doi = {10.1007/978-3-030-72019-3_2},
	abstract = {Asynchronous message-passing systems are employed frequently to implement distributed mechanisms, protocols, and processes. This paper addresses the problem of precise data flow analysis for such systems. To obtain good precision, data flow analysis needs to somehow skip execution paths that read more messages than the number of messages sent so far in the path, as such paths are infeasible at run time. Existing data flow analysis techniques do elide a subset of such infeasible paths, but have the restriction that they admit only finite abstract analysis domains. In this paper we propose a generalization of these approaches to admit infinite abstract analysis domains, as such domains are commonly used in practice to obtain high precision. We have implemented our approach, and have analyzed its performance on a set of 14 benchmarks. On these benchmarks our tool obtains significantly higher precision compared to a baseline approach that does not elide any infeasible paths and to another baseline that elides infeasible paths but admits only finite abstract domains.},
	pages = {30--58},
	journaltitle = {Programming Languages and Systems},
	shortjournal = {Programming Languages and Systems},
	author = {Athaiya, Snigdha and Komondoor, Raghavan and Kumar, K. Narayan},
	urldate = {2021-04-15},
	date = {2021-03-23},
	pmid = {null},
	pmcid = {PMC7984527},
	file = {PubMed Central Full Text PDF:/home/fordrl/Zotero/storage/2IKT9XGV/Athaiya et al. - 2021 - Data Flow Analysis of Asynchronous Systems using I.pdf:application/pdf},
}

@article{barbar_flow-sensitive_2020,
	title = {Flow-Sensitive Type-Based Heap Cloning},
	abstract = {By respecting program control-ﬂow, ﬂow-sensitive pointer analysis promises more precise results than its ﬂow-insensitive counterpart. However, existing heap abstractions for C and C++ ﬂow-sensitive pointer analyses model the heap by creating a single abstract heap object for each memory allocation. Two runtime heap objects which originate from the same allocation site are imprecisely modelled using one abstract object, which makes them share the same imprecise points-to sets and thus reduces the beneﬁt of analysing heap objects ﬂow-sensitively. On the other hand, equipping ﬂow-sensitive analysis with context-sensitivity, whereby an abstract heap object would be created (cloned) per calling context, can yield a more precise heap model, but at the cost of uncontrollable analysis overhead when analysing larger programs.},
	pages = {26},
	journaltitle = {34th European Conference on Object-Oriented Programming},
	shortjournal = {{ECOOP}},
	author = {Barbar, Mohamad and Sui, Yulei and Chen, Shiping},
	date = {2020},
	langid = {english},
	file = {Barbar et al. - 2020 - Flow-Sensitive Type-Based Heap Cloning.pdf:/home/fordrl/Zotero/storage/3FATB8R7/Barbar et al. - 2020 - Flow-Sensitive Type-Based Heap Cloning.pdf:application/pdf},
}

@article{sui_loop-oriented_2018,
	title = {Loop-Oriented Pointer Analysis for Automatic {SIMD} Vectorization},
	volume = {17},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/3168364},
	doi = {10.1145/3168364},
	abstract = {Compiler-based vectorization represents a promising solution to automatically generate code that makes efficient use of modern {CPUs} with {SIMD} extensions. Two main auto-vectorization techniques, superword-level parallelism vectorization ({SLP}) and loop-level vectorization ({LLV}), require precise dependence analysis on arrays and structs to vectorize isomorphic scalar instructions (in the case of {SLP}) and reduce dynamic dependence checks at runtime (in the case of {LLV}). The alias analyses used in modern vectorizing compilers are either intra-procedural (without tracking inter-procedural data-flows) or inter-procedural (by using field-sensitive models, which are too imprecise in handling arrays and structs). This article proposes an inter-procedural Loop-oriented Pointer Analysis for C, called Lpa, for analyzing arrays and structs to support aggressive {SLP} and {LLV} optimizations effectively. Unlike field-insensitive solutions that pre-allocate objects for each memory allocation site, our approach uses a lazy memory model to generate access-based location sets based on how structs and arrays are accessed. Lpa can precisely analyze arrays and nested aggregate structures to enable {SIMD} optimizations for large programs. By separating the location set generation as an independent concern from the rest of the pointer analysis, Lpa is designed so that existing points-to resolution algorithms (e.g., flow-insensitive and flow-sensitive pointer analysis) can be reused easily. We have implemented Lpa fully in the {LLVM} compiler infrastructure (version 3.8.0). We evaluate Lpa by considering {SLP} and {LLV}, the two classic vectorization techniques, on a set of 20 C and Fortran {CPU}2000/2006 benchmarks. For {SLP}, Lpa outperforms {LLVM}’s {BasicAA} and {ScevAA} by discovering 139 and 273 more vectorizable basic blocks, respectively, resulting in the best speedup of 2.95\% for 173.applu. For {LLV}, {LLVM} introduces totally 551 and 652 static bound checks under {BasicAA} and {ScevAA}, respectively. In contrast, Lpa has reduced these static checks to 220, with an average of 15.7 checks per benchmark, resulting in the best speedup of 7.23\% for 177.mesa.},
	pages = {56:1--56:31},
	number = {2},
	journaltitle = {{ACM} Transactions on Embedded Computing Systems},
	shortjournal = {{ACM} Trans. Embed. Comput. Syst.},
	author = {Sui, Yulei and Fan, Xiaokang and Zhou, Hao and Xue, Jingling},
	urldate = {2021-03-18},
	date = {2018-01-30},
	keywords = {Pointer analysis, compiler optimisation, loop-oriented, {SIMD}},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/CEXB5SM8/Sui et al. - 2018 - Loop-Oriented Pointer Analysis for Automatic SIMD .pdf:application/pdf},
}

@article{sui_flow2vec_2020,
	title = {Flow2Vec: value-flow-based precise code embedding},
	volume = {4},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3428301},
	doi = {10.1145/3428301},
	shorttitle = {Flow2Vec},
	pages = {1--27},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Sui, Yulei and Cheng, Xiao and Zhang, Guanqin and Wang, Haoyu},
	urldate = {2021-03-18},
	date = {2020-11-13},
	langid = {english},
	file = {Sui et al. - 2020 - Flow2Vec value-flow-based precise code embedding.pdf:/home/fordrl/Zotero/storage/K8KF3YMR/Sui et al. - 2020 - Flow2Vec value-flow-based precise code embedding.pdf:application/pdf},
}

@book{moller_static_nodate,
	title = {Static Program Analysis},
	pagetotal = {176},
	author = {Møller, Anders and Schwartzbach, Michael I},
	langid = {english},
	file = {Møller and Schwartzbach - Department of Computer Science Aarhus University, .pdf:/home/fordrl/Zotero/storage/H3KCEI22/Møller and Schwartzbach - Department of Computer Science Aarhus University, .pdf:application/pdf},
}

@inproceedings{barbar_object_2021,
	location = {Seoul, Korea (South)},
	title = {Object Versioning for Flow-Sensitive Pointer Analysis},
	isbn = {978-1-72818-613-9},
	url = {https://ieeexplore.ieee.org/document/9370334/},
	doi = {10.1109/CGO51591.2021.9370334},
	abstract = {Flow-sensitive points-to analysis provides better precision than its ﬂow-insensitive counterpart. Traditionally performed on the control-ﬂow graph, it incurs heavy analysis overhead. For performance, staged ﬂow-sensitive analysis ({SFS}) is conducted on a pre-computed def-use (value-ﬂow) graph where points-to sets of variables are propagated across def-use chains sparsely rather than across control-ﬂow in the control-ﬂow graph. {SFS} makes the propagation of different objects’ pointsto sets sparse (multiple-object sparsity), however, it suffers from redundant propagation between instructions of the same object’s points-to sets (single-object sparsity). The points-to set of an object is often duplicated, resulting in redundant propagation and storage, especially in real-world heap-intensive programs.},
	eventtitle = {2021 {IEEE}/{ACM} International Symposium on Code Generation and Optimization ({CGO})},
	pages = {222--235},
	booktitle = {2021 {IEEE}/{ACM} International Symposium on Code Generation and Optimization ({CGO})},
	publisher = {{IEEE}},
	author = {Barbar, Mohamad and Sui, Yulei and Chen, Shiping},
	urldate = {2021-03-18},
	date = {2021-02-27},
	langid = {english},
	file = {Barbar et al. - 2021 - Object Versioning for Flow-Sensitive Pointer Analy.pdf:/home/fordrl/Zotero/storage/F9FRBVXU/Barbar et al. - 2021 - Object Versioning for Flow-Sensitive Pointer Analy.pdf:application/pdf},
}

@article{sui_value-flow-based_2020,
	title = {Value-Flow-Based Demand-Driven Pointer Analysis for C and C++},
	volume = {46},
	issn = {1939-3520},
	doi = {10.1109/TSE.2018.2869336},
	abstract = {We present Supa, a value-flow-based demand-driven flow- and context-sensitive pointer analysis with strong updates for C and C++ programs. Supa enables computing points-to information via value-flow refinement, in environments with small time and memory budgets. We formulate Supa by solving a graph-reachability problem on an inter-procedural value-flow graph representing a program's def-use chains, which are pre-computed efficiently but over-approximately. To answer a client query (a request for a variable's points-to set), Supa reasons about the flow of values along the pre-computed def-use chains sparsely (rather than across all program points), by performing only the work necessary for the query (rather than analyzing the whole program). In particular, strong updates are performed to filter out spurious def-use chains through value-flow refinement as long as the total budget is not exhausted. We have implemented Supa on top of {LLVM} (4.0.0) together with a comprehensive micro-benchmark suite after a years-long effort (consisting of around 400 test cases, including hand-written ones and the ones extracted from real programs). We have evaluated Supa by choosing uninitialized pointer detection and C++ virtual table resolution as two major clients, using 24 real-world programs including 18 open-source C programs and 6 large {CPU}2000/2006 C++ benchmarks. For uninitialized pointer client, Supa achieves improved precision as the analysis budget increases, with its flow-sensitive (context-insensitive) analysis reaching 97.4 percent of that achieved by whole-program Sparse Flow-Sensitive analysis ({SFS}) by consuming about 0.18 seconds and 65 {KB} of memory per query, on average (with a budget of at most 10,000 value-flow edges per query). With context-sensitivity also considered, Supa becomes more precise for some programs but also incurs more analysis times. To further demonstrate the effectiveness of Supa, we have also evaluated Supa in resolving C++ virtual tables by querying the function pointers at every virtual callsite. Compared to analysis without strong updates for heap objects, Supa's demand-driven context-sensitive strong update analysis reduces 7.35 percent spurious virtual table targets with only 0.4 secs per query, on average.},
	pages = {812--835},
	number = {8},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Sui, Y. and Xue, J.},
	date = {2020-08},
	note = {Conference Name: {IEEE} Transactions on Software Engineering},
	keywords = {Resource management, optimising compilers, program diagnostics, flow sensitivity, C program, C++ languages, C++ program, context-sensitive pointer analysis, data flow analysis, data structures, efficiency 7.35 percent, efficiency 97.4 percent, flow graphs, Instruction sets, inter-procedural value-flow graph, memory size 65.0 {KByte}, object-oriented programming, Open source software, pointer analysis, pointer client, pre-computed def-use chains, reachability analysis, Reachability analysis, Registers, Sensitivity, sensitivity analysis, sparse flow-sensitive analysis, Strong updates, Supa demand-driven context-sensitive strong update analysis, temperature 2006.0 C, time 0.4 s, value flow, value-flow refinement, value-flow-based demand-driven flow, value-flow-based demand-driven pointer analysis},
	file = {IEEE Xplore Abstract Record:/home/fordrl/Zotero/storage/FMCA3WCF/8457263.html:text/html;Full Text:/home/fordrl/Zotero/storage/D2U9JRUF/Sui and Xue - 2020 - Value-Flow-Based Demand-Driven Pointer Analysis fo.pdf:application/pdf},
}

@inproceedings{xu_vfix_2019,
	location = {Montreal, Quebec, Canada},
	title = {{VFix}: value-flow-guided precise program repair for null pointer dereferences},
	url = {https://doi.org/10.1109/ICSE.2019.00063},
	doi = {10.1109/ICSE.2019.00063},
	series = {{ICSE} '19},
	shorttitle = {{VFix}},
	abstract = {Automated Program Repair ({APR}) faces a key challenge in efficiently generating correct patches from a potentially infinite solution space. Existing approaches, which attempt to reason about the entire solution space, can be ineffective (by often producing no plausible patches at all) and imprecise (by often producing plausible but incorrect patches). We present {VFix}, a new value-flow-guided {APR} approach, to fix null pointer exception ({NPE}) bugs by considering a substantially reduced solution space in order to greatly increase the number of correct patches generated. By reasoning about the data and control dependences in the program, {VFix} can identify bug-relevant repair statements more accurately and generate more correct repairs than before. {VFix} outperforms a set of 8 state-of-the-art {APR} tools in fixing the {NPE} bugs in Defects4j in terms of both precision (by correctly fixing 3 times as many bugs as the most precise one and 50\% more than all the bugs correctly fixed by these 8 tools altogether) and efficiency (by producing a correct patch in minutes instead of hours).},
	pages = {512--523},
	booktitle = {Proceedings of the 41st International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Xu, Xuezheng and Sui, Yulei and Yan, Hua and Xue, Jingling},
	urldate = {2021-03-17},
	date = {2019-05-25},
	keywords = {null dereference, static analysis, program repair},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/U6M3HSZL/Xu et al. - 2019 - VFix value-flow-guided precise program repair for.pdf:application/pdf},
}

@inproceedings{ye_accelerating_2014,
	location = {New York, {NY}, {USA}},
	title = {Accelerating Dynamic Detection of Uses of Undefined Values with Static Value-Flow Analysis},
	isbn = {978-1-4503-2670-4},
	url = {https://doi.org/10.1145/2581122.2544154},
	doi = {10.1145/2581122.2544154},
	series = {{CGO} '14},
	abstract = {Uninitialized variables can cause system crashes when used and security vulnerabilities when exploited. With source rather than binary instrumentation, dynamic analysis tools such as {MSan} can detect uninitialized memory uses at significantly reduced overhead but are still costly. In this paper, we introduce a static value-flow analysis, called Usher, to guide and accelerate the dynamic analysis performed by such tools. Usher reasons about the definedness of values using a value-flow graph ({VFG}) that captures def-use chains for both top-level and address-taken variables interprocedurally and removes unnecessary instrumentation by solving a graph reachability problem. Usher works well with any pointer analysis (done a priori) and facilitates advanced instrumentation-reducing optimizations (with two demonstrated here). Implemented in {LLVM} and evaluated using all the 15 {SPEC}2000 C programs, Usher can reduce the slowdown of {MSan} from 212\% -- 302\% to 123\% -- 140\% for a number of configurations tested.},
	pages = {154--164},
	booktitle = {Proceedings of Annual {IEEE}/{ACM} International Symposium on Code Generation and Optimization},
	publisher = {Association for Computing Machinery},
	author = {Ye, Ding and Sui, Yulei and Xue, Jingling},
	urldate = {2021-03-17},
	date = {2014-02-15},
	keywords = {static and dynamic analysis, Undefined values},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/UR6BBXN5/Ye et al. - 2014 - Accelerating Dynamic Detection of Uses of Undefine.pdf:application/pdf},
}

@inproceedings{sui_sparse_2016,
	location = {New York, {NY}, {USA}},
	title = {Sparse flow-sensitive pointer analysis for multithreaded programs},
	isbn = {978-1-4503-3778-6},
	url = {https://doi.org/10.1145/2854038.2854043},
	doi = {10.1145/2854038.2854043},
	series = {{CGO} '16},
	abstract = {For C programs, flow-sensitivity is important to enable pointer analysis to achieve highly usable precision. Despite significant recent advances in scaling flow-sensitive pointer analysis sparsely for sequential C programs, relatively little progress has been made for multithreaded C programs. In this paper, we present {FSAM}, a new Flow-Sensitive pointer Analysis that achieves its scalability for large Multithreaded C programs by performing sparse analysis on top of a series of thread interference analysis phases. We evaluate {FSAM} with 10 multithreaded C programs (with more than 100K lines of code for the largest) from Phoenix-2.0, Parsec-3.0 and open-source applications. For two programs, raytrace and x264, the traditional data-flow-based flow-sensitive pointer analysis is un- scalable (under two hours) but our analysis spends just under 5 minutes on raytrace and 9 minutes on x264. For the rest, our analysis is 12x faster and uses 28x less memory.},
	pages = {160--170},
	booktitle = {Proceedings of the 2016 International Symposium on Code Generation and Optimization},
	publisher = {Association for Computing Machinery},
	author = {Sui, Yulei and Di, Peng and Xue, Jingling},
	urldate = {2021-03-17},
	date = {2016-02-29},
	keywords = {Pointer Analysis, Flow-Sensitivity, Sparse Analysis},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/8CUS77UT/Sui et al. - 2016 - Sparse flow-sensitive pointer analysis for multith.pdf:application/pdf},
}

@inproceedings{sui_-demand_2016,
	location = {New York, {NY}, {USA}},
	title = {On-demand strong update analysis via value-flow refinement},
	isbn = {978-1-4503-4218-6},
	url = {https://doi.org/10.1145/2950290.2950296},
	doi = {10.1145/2950290.2950296},
	series = {{FSE} 2016},
	abstract = {We present a new Strong {UPdate} Analysis for C programs, called Supa, that enables computing points-to information on-demand via value-flow refinement, in environments with small time and memory budgets such as {IDEs}. We formulate Supa by solving a graph-reachability problem on a value- flow graph representation of the program, so that strong updates are performed where needed, as long as the total analysis budget is not exhausted. Supa facilitates efficiency and precision tradeoffs by allowing different pointer analyses to be applied in a hybrid multi-stage analysis framework. We have implemented Supa in {LLVM} with its artifact available at [1]. We evaluate Supa by choosing uninitialized pointer detection as a major client on 12 open-source C programs. As the analysis budget increases, Supa achieves improved precision, with its single-stage flow-sensitive analysis reaching 97\% of that achieved by whole-program flow- sensitive analysis by consuming about 0.19 seconds and 36KB of memory per query, on average (with a budget of at most 10000 value-flow edges per query).},
	pages = {460--473},
	booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} International Symposium on Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Sui, Yulei and Xue, Jingling},
	urldate = {2021-03-17},
	date = {2016-11-01},
	keywords = {flow sensitivity, pointer analysis, value flow, strong updates},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/9DNHFR2T/Sui and Xue - 2016 - On-demand strong update analysis via value-flow re.pdf:application/pdf},
}

@inproceedings{di_accelerating_2016,
	location = {New York, {NY}, {USA}},
	title = {Accelerating Dynamic Data Race Detection Using Static Thread Interference Analysis},
	isbn = {978-1-4503-4196-7},
	url = {https://doi.org/10.1145/2883404.2883405},
	doi = {10.1145/2883404.2883405},
	series = {{PMAM}'16},
	abstract = {Precise dynamic race detectors report an error if and only if more than one thread concurrently exhibits conflict on a memory access. They insert instrumentations at compile-time to perform runtime checks on all memory accesses to ensure that all races are captured and no spurious warnings are generated. However, a dynamic race check for a particular memory access statement is guaranteed to be redundant if the statement can be statically identified as thread interference-free. Despite significant recent advances in dynamic detection techniques, the redundant check remains a critical factor that leads to prohibitive overhead of dynamic race detection for multithreaded programs. In this paper, we present a new framework that eliminates redundant race check and boosts the dynamic race detection by performing static optimizations on top of a series of thread interference analysis phases. Our framework is implemented on top of {LLVM} 3.5.0 and evaluated with an industry dynamic race detector {TSAN} which is available as a part of {LLVM} tool chain. 11 benchmarks from {SPLASH}2 are used to evaluate the effectiveness of our approach in accelerating {TSAN} by eliminating redundant interference-free checks. The experimental result demonstrates our new approach achieves from 1.4x to 4.0x (2.4x on average) speedup over original {TSAN} under 4 threads setting, and achieves from 1.3x to 4.6x (2.6x on average) speedup under 16 threads setting.},
	pages = {30--39},
	booktitle = {Proceedings of the 7th International Workshop on Programming Models and Applications for Multicores and Manycores},
	publisher = {Association for Computing Machinery},
	author = {Di, Peng and Sui, Yulei},
	urldate = {2021-03-17},
	date = {2016-03-12},
	keywords = {static analysis, concurrent program, data race, multithreaded},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/NN29GCIS/Di and Sui - 2016 - Accelerating Dynamic Data Race Detection Using Sta.pdf:application/pdf},
}

@inproceedings{pereira_wave_2009,
	location = {{USA}},
	title = {Wave Propagation and Deep Propagation for Pointer Analysis},
	isbn = {978-0-7695-3576-0},
	url = {https://doi.org/10.1109/CGO.2009.9},
	doi = {10.1109/CGO.2009.9},
	series = {{CGO} '09},
	abstract = {This paper describes two new algorithms for solving inclusion based points-to analysis. The first algorithm, the Wave Propagation Method, is a modified version of an early technique presented by Pearce et al; however, it greatly improves on the running time of its predecessor. The second algorithm, the Deep Propagation Method, is a more light-weighted analysis, that requires less memory. We have compared these algorithms with three state-of-the-art techniques by Hardekopf-Lin, Heintze-Tardieu and Pearce-Kelly-Hankin. Our experiments show that Deep Propagation has the best average execution time across a suite of 17 well-known benchmarks, the lowest memory requirements in absolute numbers, and the fastest absolute times for benchmarks under 100,000 lines of code. The memory-hungry Wave Propagation has the fastest absolute running times in a memory rich execution environment, matching the speed of the best known points-to analysis algorithms in large benchmarks.},
	pages = {126--135},
	booktitle = {Proceedings of the 7th annual {IEEE}/{ACM} International Symposium on Code Generation and Optimization},
	publisher = {{IEEE} Computer Society},
	author = {Pereira, Fernando Magno Quintao and Berlin, Daniel},
	urldate = {2021-03-15},
	date = {2009-03-22},
	keywords = {Pointer analysis, Context insensitive, Cycle detection, Inclusion based},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/4QDZ5LCP/Pereira and Berlin - 2009 - Wave Propagation and Deep Propagation for Pointer .pdf:application/pdf},
}

@thesis{andersen_program_1994,
	title = {Program Analysis and Specialization for the C Programming Language},
	abstract = {Software engineers are faced with a dilemma. They want to write general and wellstructured programs that are flexible and easy to maintain. On the other hand, generality has a price: efficiency. A specialized program solving a particular problem is often significantly faster than a general program. However, the development of specialized software is time-consuming, and is likely to exceed the production of today’s programmers. New techniques are required to solve this so-called software crisis. Partial evaluation is a program specialization technique that reconciles the benefits of generality with efficiency. This thesis presents an automatic partial evaluator for the Ansi C programming language. The content of this thesis is analysis and transformation of C programs. We develop several analyses that support the transformation of a program into its generating extension. A generating extension is a program that produces specialized programs when executed on parts of the input. The thesis contains the following main results.},
	pagetotal = {311},
	institution = {University of Copenhagen},
	type = {phdthesis},
	author = {Andersen, Lars Ole},
	date = {1994},
	file = {Citeseer - Snapshot:/home/fordrl/Zotero/storage/EDYAYKCN/download.html:text/html;Citeseer - Full Text PDF:/home/fordrl/Zotero/storage/A9EIV93G/Andersen - 1994 - Program Analysis and Specialization for the C Prog.pdf:application/pdf},
}

@inproceedings{liew_floating-point_2017,
	location = {Urbana, {IL}},
	title = {Floating-point symbolic execution: A case study in N-version programming},
	isbn = {978-1-5386-2684-9},
	url = {http://ieeexplore.ieee.org/document/8115670/},
	doi = {10.1109/ASE.2017.8115670},
	shorttitle = {Floating-point symbolic execution},
	abstract = {Symbolic execution is a well-known program analysis technique for testing software, which makes intensive use of constraint solvers. Recent support for ﬂoating-point constraint solving has made it feasible to support ﬂoating-point reasoning in symbolic execution tools. In this paper, we present the experience of two research teams that independently added ﬂoating-point support to {KLEE}, a popular symbolic execution engine. Since the two teams independently developed their extensions, this created the rare opportunity to conduct a rigorous comparison between the two implementations, essentially a modern case study on Nversion programming. As part of our comparison, we report on the different design and implementation decisions taken by each team, and show their impact on a rigorously assembled and tested set of benchmarks, itself a contribution of the paper.},
	eventtitle = {2017 32nd {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {601--612},
	booktitle = {2017 32nd {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	publisher = {{IEEE}},
	author = {Liew, Daniel and Schemmel, Daniel and Cadar, Cristian and Donaldson, Alastair F. and Zahl, Rafael and Wehrle, Klaus},
	urldate = {2021-03-10},
	date = {2017-10},
	langid = {english},
	file = {Liew et al. - 2017 - Floating-point symbolic execution A case study in.pdf:/home/fordrl/Zotero/storage/F9KLX7L7/Liew et al. - 2017 - Floating-point symbolic execution A case study in.pdf:application/pdf},
}

@thesis{korencik_decompiling_2019,
	title = {Decompiling Binaries into {LLVM} {IR} Using {McSema} and Dyninst},
	url = {https://is.muni.cz/th/pxe1j/thesis.pdf},
	abstract = {This thesis provides alternative implementation to the proprietary component of {McSema} which uses open-source Dyninst disassembler. With
this new implementation {McSema} can be used without proprietary software. The performance of the open source version is demonstrated on
a set of programs and the results are compared with already existing
components.},
	pagetotal = {78},
	institution = {Masaryk University},
	type = {Diploma},
	author = {Korencik, Lukáš and University, Masaryk},
	date = {2019},
	langid = {english},
	file = {Korencik and University - Decompiling Binaries into LLVM IR Using McSema and.pdf:/home/fordrl/Zotero/storage/QA9UTFFP/Korencik and University - Decompiling Binaries into LLVM IR Using McSema and.pdf:application/pdf},
}

@article{sui_detecting_2014,
	title = {Detecting Memory Leaks Statically with Full-Sparse Value-Flow Analysis},
	volume = {40},
	issn = {0098-5589, 1939-3520},
	url = {http://ieeexplore.ieee.org/document/6720116/},
	doi = {10.1109/TSE.2014.2302311},
	abstract = {We introduce a static detector, {SABER}, for detecting memory leaks in C programs. Leveraging recent advances on sparse pointer analysis, {SABER} is the ﬁrst to use a full-sparse value-ﬂow analysis for detecting memory leaks statically. {SABER} tracks the ﬂow of values from allocation to free sites using a sparse value-ﬂow graph ({SVFG}) that captures def-use chains and value ﬂows via assignments for all memory locations represented by both top-level and address-taken pointers. By exploiting ﬁeld-, ﬂow- and contextsensitivity during different phases of the analysis, {SABER} detects memory leaks in a program by solving a graph reachability problem on its {SVFG}. {SABER}, which is fully implemented in Open64, is effective at detecting 254 leaks in the 15 {SPEC}2000 C programs and seven applications, while keeping the false positive rate at 18.3 percent. {SABER} compares favorably with several static leak detectors in terms of accuracy (leaks and false alarms reported) and scalability ({LOC} analyzed per second). In particular, compared with {FASTCHECK} (which analyzes allocated objects ﬂowing only into top-level pointers) using the 15 {SPEC}2000 C programs, {SABER} detects 44.1 percent more leaks at a slightly higher false positive rate but is only a few times slower.},
	pages = {107--122},
	number = {2},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Sui, Yulei and Ye, Ding and Xue, Jingling},
	urldate = {2021-03-04},
	date = {2014-02},
	langid = {english},
	file = {Sui et al. - 2014 - Detecting Memory Leaks Statically with Full-Sparse.pdf:/home/fordrl/Zotero/storage/QN7A62RQ/Sui et al. - 2014 - Detecting Memory Leaks Statically with Full-Sparse.pdf:application/pdf},
}

@inproceedings{sui_svf_2016,
	location = {New York, {NY}, {USA}},
	title = {{SVF}: interprocedural static value-flow analysis in {LLVM}},
	isbn = {978-1-4503-4241-4},
	url = {https://doi.org/10.1145/2892208.2892235},
	doi = {10.1145/2892208.2892235},
	series = {{CC} 2016},
	shorttitle = {{SVF}},
	abstract = {This paper presents {SVF}, a tool that enables scalable and precise interprocedural Static Value-Flow analysis for C programs by leveraging recent advances in sparse analysis. {SVF}, which is fully implemented in {LLVM}, allows value-flow construction and pointer analysis to be performed in an iterative manner, thereby providing increasingly improved precision for both. {SVF} accepts points- to information generated by any pointer analysis (e.g., Andersen’s analysis) and constructs an interprocedural memory {SSA} form, in which the def-use chains of both top-level and address-taken variables are captured. Such value-flows can be subsequently exploited to support various forms of program analysis or enable more precise pointer analysis (e.g., flow-sensitive analysis) to be performed sparsely. By dividing a pointer analysis into three loosely coupled components: Graph, Rules and Solver, {SVF} provides an extensible interface for users to write their own solutions easily. {SVF} is publicly available at http://unsw-corg.github.io/{SVF}.},
	pages = {265--266},
	booktitle = {Proceedings of the 25th International Conference on Compiler Construction},
	publisher = {Association for Computing Machinery},
	author = {Sui, Yulei and Xue, Jingling},
	urldate = {2021-03-04},
	date = {2016-03-17},
	keywords = {Pointer Analysis, {SVF}, Value-Flow},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/LDBQIAS8/Sui and Xue - 2016 - SVF interprocedural static value-flow analysis in.pdf:application/pdf},
}

@article{curry_bi-abduction_2020,
	title = {Bi-Abduction for Shapes with Ordered Data},
	url = {http://arxiv.org/abs/2006.10439},
	abstract = {Shape analysis is of great importance for the verification of the correctness and memory-safety of heap-manipulating programs, yet such analyses have been shown to be highly difficult problems. The integration of separation logic into shape analyses has improved the effectiveness of the techniques, but the most significant advancement in this area is bi-abductive inference. Enabled by separation logic, bi-abduction - a combination of abductive inference and frame inference - is the key enabler for compositional reasoning, helping to scale up verification significantly. Indeed, the success of bi-abduction has led to the development of Infer, the tool used daily to verify Facebook's codebase of millions of lines of code. However, this success currently stays largely within the shape domain. To extend this impact towards the combination of shape and arithmetic domains, in this work, we present a novel one-stage bi-abductive procedure for a combination of data structures and ordering values. The procedure is designed in the spirit of the Unfold-and-Match paradigm where the inference is utilized to derive any mismatched portion. We have also implemented a prototype solver, based on the Cyclist library, and demonstrate its capabilities over a range of benchmarks from the {SL}-{COMP} competition. The experimental results show that our proposal shows promise for the specification inference in an automated verification of heap-manipulating programs.},
	journaltitle = {{arXiv}:2006.10439 [cs]},
	author = {Curry, Christopher and Le, Quang Loc},
	urldate = {2021-02-26},
	date = {2020-06-18},
	eprinttype = {arxiv},
	eprint = {2006.10439},
	keywords = {Computer Science - Logic in Computer Science},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/RFU8VKXA/2006.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/M5D4M8FE/Curry and Le - 2020 - Bi-Abduction for Shapes with Ordered Data.pdf:application/pdf},
}

@incollection{hutchison_bi-abduction_2013,
	location = {Cham},
	title = {Bi-Abduction with Pure Properties for Specification Inference},
	volume = {8301},
	isbn = {978-3-319-03541-3 978-3-319-03542-0},
	url = {http://link.springer.com/10.1007/978-3-319-03542-0_8},
	abstract = {Separation logic is a state-of-the-art logic for dealing with the heap. Using its frame rule, initial works have strived towards automated modular veriﬁcation for heap-manipulating programs against user-supplied speciﬁcations. Since manually writing speciﬁcations is a tedious and error-prone engineering process, the so-called bi-abduction (a combination of the frame rule and abductive inference) is proposed to automatically infer pre/post speciﬁcations on data structure shapes. However, it has omitted the inference of pure properties of data structures such as their size, sum, height, content and min/max, which are needed to express a higher level of program correctness. In this paper, we propose a novel approach, called pure bi-abduction, for inferring pure information for pre/post speciﬁcations, using the result from a prior shape analysis step. Additionally, we design a predicate extension mechanism to systematically extend shape predicates with pure properties. We have implemented our inference mechanism and evaluated its utility on a benchmark of programs. We show that pure properties are prerequisite to allow the correctness of about 20\% of analyzed procedures to be captured and veriﬁed.},
	pages = {107--123},
	booktitle = {Programming Languages and Systems},
	publisher = {Springer International Publishing},
	author = {Trinh, Minh-Thai and Le, Quang Loc and David, Cristina and Chin, Wei-Ngan},
	editor = {Shan, Chung-chieh},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2021-02-26},
	date = {2013},
	langid = {english},
	doi = {10.1007/978-3-319-03542-0_8},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Trinh et al. - 2013 - Bi-Abduction with Pure Properties for Specificatio.pdf:/home/fordrl/Zotero/storage/3QNM3JDG/Trinh et al. - 2013 - Bi-Abduction with Pure Properties for Specificatio.pdf:application/pdf},
}

@article{calcagno_compositional_2011,
	title = {Compositional Shape Analysis by Means of Bi-Abduction},
	volume = {58},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/2049697.2049700},
	doi = {10.1145/2049697.2049700},
	abstract = {The accurate and efficient treatment of mutable data structures is one of the outstanding problem areas in automatic program verification and analysis. Shape analysis is a form of program analysis that attempts to infer descriptions of the data structures in a program, and to prove that these structures are not misused or corrupted. It is one of the more challenging and expensive forms of program analysis, due to the complexity of aliasing and the need to look arbitrarily deeply into the program heap. This article describes a method of boosting shape analyses by defining a compositional method, where each procedure is analyzed independently of its callers. The analysis algorithm uses a restricted fragment of separation logic, and assigns a collection of Hoare triples to each procedure; the triples provide an over-approximation of data structure usage. Our method brings the usual benefits of compositionality---increased potential to scale, ability to deal with incomplete programs, graceful way to deal with imprecision---to shape analysis, for the first time. The analysis rests on a generalized form of abduction (inference of explanatory hypotheses), which we call bi-abduction. Bi-abduction displays abduction as a kind of inverse to the frame problem: it jointly infers anti-frames (missing portions of state) and frames (portions of state not touched by an operation), and is the basis of a new analysis algorithm. We have implemented our analysis and we report case studies on smaller programs to evaluate the quality of discovered specifications, and larger code bases (e.g., sendmail, an imap server, a Linux distribution) to illustrate the level of automation and scalability that we obtain from our compositional method. This article makes number of specific technical contributions on proof procedures and analysis algorithms, but in a sense its more important contribution is holistic: the explanation and demonstration of how a massive increase in automation is possible using abductive inference.},
	pages = {26:1--26:66},
	number = {6},
	journaltitle = {Journal of the {ACM}},
	shortjournal = {J. {ACM}},
	author = {Calcagno, Cristiano and Distefano, Dino and O’Hearn, Peter W. and Yang, Hongseok},
	urldate = {2021-02-26},
	date = {2011-12-01},
	keywords = {static analysis, separation logic, Abstract interpretation, compositionality, program proving},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/6AWAN7TL/Calcagno et al. - 2011 - Compositional Shape Analysis by Means of Bi-Abduct.pdf:application/pdf},
}

@article{brotherston_biabduction_2016,
	title = {Biabduction (and Related Problems) in Array Separation Logic},
	url = {http://arxiv.org/abs/1607.01993},
	abstract = {We investigate array separation logic ({ASL}), a variant of symbolic-heap separation logic in which the data structures are either pointers or arrays, i.e., contiguous blocks of allocated memory. This logic provides a language for compositional memory safety proofs of imperative array programs. We focus on the biabduction problem for this logic, which has been established as the key to automatic specification inference at the industrial scale. We present an {NP} decision procedure for biabduction in {ASL} that produces solutions of reasonable quality, and we also show that the problem of finding a consistent solution is {NP}-hard. Along the way, we study satisfiability and entailment in our logic, giving decision procedures and complexity bounds for both problems. We show satisfiability to be {NP}-complete, and entailment to be decidable with high complexity. The somewhat surprising fact that biabduction is much simpler than entailment is explained by the fact that, as we show, the element of choice over biabduction solutions enables us to dramatically reduce the search space.},
	journaltitle = {{arXiv}:1607.01993 [cs, math]},
	author = {Brotherston, James and Gorogiannis, Nikos and Kanovich, Max},
	urldate = {2021-02-26},
	date = {2016-11-18},
	eprinttype = {arxiv},
	eprint = {1607.01993},
	keywords = {Computer Science - Programming Languages, Computer Science - Logic in Computer Science, D.2.4, F.3.1, Mathematics - Logic, Computer Science - Data Structures and Algorithms, F.2},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/Z5KWAZ5L/1607.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/IFCFMXHL/Brotherston et al. - 2016 - Biabduction (and Related Problems) in Array Separa.pdf:application/pdf},
}

@article{liu_kubo_nodate,
	title = {{KUBO}: Precise and Scalable Detection of User-triggerable Undeﬁned Behavior Bugs in {OS} Kernel},
	abstract = {Undeﬁned Behavior bugs ({UB}) often refer to a wide range of programming errors that mainly reside in software implemented in relatively low-level programming languages e.g., C/C++. {OS} kernels are particularly plagued by {UB} due to their close interactions with the hardware. A triggered {UB} can often lead to exploitation from unprivileged userspace programs and cause critical security and reliability issues inside the {OS}. The previous works on detecting {UB} in kernels had to sacriﬁce precision for scalability, and in turn, suffered from extremely high false positives which severely impaired their usability.},
	pages = {15},
	author = {Liu, Changming and Chen, Yaohui and Lu, Long},
	langid = {english},
	file = {Liu et al. - KUBO Precise and Scalable Detection of User-trigg.pdf:/home/fordrl/Zotero/storage/LYHABDR5/Liu et al. - KUBO Precise and Scalable Detection of User-trigg.pdf:application/pdf},
}

@article{lattner_mlir_2020,
	title = {{MLIR}: A Compiler Infrastructure for the End of Moore's Law},
	url = {http://arxiv.org/abs/2002.11054},
	shorttitle = {{MLIR}},
	abstract = {This work presents {MLIR}, a novel approach to building reusable and extensible compiler infrastructure. {MLIR} aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together. {MLIR} facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and also across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of {MLIR} as a research artifact, built for extension and evolution, and identifying the challenges and opportunities posed by this novel design point in design, semantics, optimization specification, system, and engineering. (2) evaluation of {MLIR} as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for {MLIR}, its original design principles, structures and semantics.},
	journaltitle = {{arXiv}:2002.11054 [cs]},
	author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
	urldate = {2021-02-25},
	date = {2020-02-29},
	eprinttype = {arxiv},
	eprint = {2002.11054},
	keywords = {Computer Science - Programming Languages, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/4REY6XJW/2002.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/NGRD56QA/Lattner et al. - 2020 - MLIR A Compiler Infrastructure for the End of Moo.pdf:application/pdf},
}

@article{arusoaie_proof-carrying_2021,
	title = {Proof-Carrying Parameters in Certified Symbolic Execution: The Case Study of Antiunification},
	url = {http://arxiv.org/abs/2110.11700},
	shorttitle = {Proof-Carrying Parameters in Certified Symbolic Execution},
	abstract = {Unification and antiunification are essential algorithms used by symbolic execution engines and verification tools. Complex frameworks for defining programming languages, such as K, aim to generate various tools (e.g., interpreters, symbolic execution engines, deductive verifiers, etc.) using only the formal definition of a language. K is the best effort implementation of Matching Logic, a logical framework for defining languages. When used at an industrial scale, a tool like the K framework is constantly updated, and in the same time it is required to be trustworthy. Ensuring the correctness of such a framework is practically impossible. A solution is to generate proof objects as correctness certificates that can be checked by an external trusted checker. In K, symbolic execution makes intensive use of unification and antiunification algorithms to handle conjunctions and disjunctions of term patterns. Conjunctions and disjunctions of formulae have to be automatically normalised and the generation of proof objects needs to take such normalisations into account. The executions of these algorithms can be seen as parameters of the symbolic execution steps and they have to provide proof objects that are used then to generate the proof object for the program execution step. We show in this paper that Plotkin's antiunification can be used to normalise disjunctions and to generate the corresponding proof objects. We provide a prototype implementation of our proof object generation technique and a checker for certifying the generated objects.},
	journaltitle = {{arXiv}:2110.11700 [cs]},
	author = {Arusoaie, Andrei and Lucanu, Dorel},
	urldate = {2021-11-01},
	date = {2021-10-22},
	eprinttype = {arxiv},
	eprint = {2110.11700},
	keywords = {Computer Science - Logic in Computer Science},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/BA8L9RCE/2110.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/7XZFZVTI/Arusoaie and Lucanu - 2021 - Proof-Carrying Parameters in Certified Symbolic Ex.pdf:application/pdf},
}

@inproceedings{bae_rudra_2021,
	location = {New York, {NY}, {USA}},
	title = {Rudra: Finding Memory Safety Bugs in Rust at the Ecosystem Scale},
	isbn = {978-1-4503-8709-5},
	url = {https://doi.org/10.1145/3477132.3483570},
	doi = {10.1145/3477132.3483570},
	series = {{SOSP} '21},
	shorttitle = {Rudra},
	abstract = {Rust is a promising system programming language that guarantees memory safety at compile time. To support diverse requirements for system software such as accessing low-level hardware, Rust allows programmers to perform operations that are not protected by the Rust compiler with the unsafe keyword. However, Rust's safety guarantee relies on the soundness of all unsafe code in the program as well as the standard and external libraries, making it hard to reason about their correctness. In other words, a single bug in any unsafe code breaks the whole program's safety guarantee. In this paper, we introduce {RUDRA}, a program that analyzes and reports potential memory safety bugs in unsafe Rust. Since a bug in unsafe code threatens the foundation of Rust's safety guarantee, our primary focus is to scale our analysis to all the packages hosted in the Rust package registry. {RUDRA} can scan the entire registry (43k packages) in 6.5 hours and identified 264 previously unknown memory safety bugs---leading to 76 {CVEs} and 112 {RustSec} advisories being filed, which represent 51.6\% of memory safety bugs reported to {RustSec} since 2016. The new bugs {RUDRA} found are non-trivial, subtle, and often made by Rust experts: two in the Rust standard library, one in the official futures library, and one in the Rust compiler. {RUDRA} is open-source, and part of its algorithm is integrated into the official Rust linter.},
	pages = {84--99},
	booktitle = {Proceedings of the {ACM} {SIGOPS} 28th Symposium on Operating Systems Principles},
	publisher = {Association for Computing Machinery},
	author = {Bae, Yechan and Kim, Youngsuk and Askar, Ammar and Lim, Jungwon and Kim, Taesoo},
	urldate = {2021-11-01},
	date = {2021-10-26},
	keywords = {Rust, Memory-safety, Program analysis},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/9JS4DP53/Bae et al. - 2021 - Rudra Finding Memory Safety Bugs in Rust at the E.pdf:application/pdf},
}

@article{emre_translating_2021,
	title = {Translating C to safer Rust},
	volume = {5},
	url = {https://doi.org/10.1145/3485498},
	doi = {10.1145/3485498},
	abstract = {Rust is a relatively new programming language that targets efficient and safe systems-level applications. It includes a sophisticated type system that allows for provable memory- and thread-safety, and is explicitly designed to take the place of unsafe languages such as C and C++ in the coding ecosystem. There is a large existing C and C++ codebase (many of which have been affected by bugs and security vulnerabilities due to unsafety) that would benefit from being rewritten in Rust to remove an entire class of potential bugs. However, porting these applications to Rust manually is a daunting task. In this paper we investigate the problem of automatically translating C programs into safer Rust programs--that is, Rust programs that improve on the safety guarantees of the original C programs. We conduct an in-depth study into the underlying causes of unsafety in translated programs and the relative impact of fixing each cause. We also describe a novel technique for automatically removing a particular cause of unsafety and evaluate its effectiveness and impact. This paper presents the first empirical study of unsafety in translated Rust programs (as opposed to programs originally written in Rust) and also the first technique for automatically removing causes of unsafety in translated Rust programs.},
	pages = {121:1--121:29},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Emre, Mehmet and Schroeder, Ryan and Dewey, Kyle and Hardekopf, Ben},
	urldate = {2021-10-26},
	date = {2021-10-15},
	keywords = {C, Rust, Automatic Translation, Empirical Study, Memory-Safety},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/CCWJQKMW/Emre et al. - 2021 - Translating C to safer Rust.pdf:application/pdf},
}

@article{shen_toward_2021,
	title = {Toward {SMT}-Based Refinement Types in Agda},
	url = {http://arxiv.org/abs/2110.05771},
	abstract = {Dependent types offer great versatility and power, but developing proofs with them can be tedious and requires considerable human guidance. We propose to integrate Satisfiability Modulo Theories ({SMT})-based refinement types into the dependently-typed language Agda in an effort to ease some of the burden of programming with dependent types and combine the strengths of the two approaches to mechanized theorem proving.},
	journaltitle = {{arXiv}:2110.05771 [cs]},
	author = {Shen, Gan and Kuper, Lindsey},
	urldate = {2021-10-18},
	date = {2021-10-12},
	eprinttype = {arxiv},
	eprint = {2110.05771},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/YF8XDA2M/2110.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/P9653PKN/Shen and Kuper - 2021 - Toward SMT-Based Refinement Types in Agda.pdf:application/pdf},
}

@article{redmond_toward_2021,
	title = {Toward Hole-Driven Development with Liquid Haskell},
	url = {http://arxiv.org/abs/2110.04461},
	abstract = {Liquid Haskell is an extension to the Haskell programming language that adds support for refinement types: data types augmented with {SMT}-decidable logical predicates that refine the set of values that can inhabit a type. Furthermore, Liquid Haskell's support for refinement reflection enables the use of Haskell for general-purpose mechanized theorem proving. A growing list of large-scale mechanized proof developments in Liquid Haskell take advantage of this capability. Adding theorem-proving capabilities to a "legacy" language like Haskell lets programmers directly verify properties of real-world Haskell programs (taking advantage of the existing highly tuned compiler, run-time system, and libraries), just by writing Haskell. However, more established proof assistants like Agda and Coq offer far better support for interactive proof development and insight into the proof state (for instance, what subgoals still need to be proved to finish a partially-complete proof). In contrast, Liquid Haskell provides only coarse-grained feedback to the user -- either it reports a type error, or not -- unfortunately hindering its usability as a theorem prover. In this paper, we propose improving the usability of Liquid Haskell by extending it with support for Agda-style typed holes and interactive editing commands that take advantage of them. In Agda, typed holes allow programmers to indicate unfinished parts of a proof, and incrementally complete the proof in a dialogue with the compiler. While {GHC} Haskell already has its own Agda-inspired support for typed holes, we posit that typed holes would be especially powerful and useful if combined with Liquid Haskell's refinement types and {SMT} automation. We discuss how typed holes might work in Liquid Haskell, and we consider possible implementation approaches and next steps.},
	journaltitle = {{arXiv}:2110.04461 [cs]},
	author = {Redmond, Patrick and Shen, Gan and Kuper, Lindsey},
	urldate = {2021-10-18},
	date = {2021-10-09},
	eprinttype = {arxiv},
	eprint = {2110.04461},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/F68849CY/2110.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/3EWQQJQA/Redmond et al. - 2021 - Toward Hole-Driven Development with Liquid Haskell.pdf:application/pdf},
}

@article{griesemer_featherweight_2020,
	title = {Featherweight go},
	volume = {4},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3428217},
	doi = {10.1145/3428217},
	abstract = {{ROBERT} {GRIESEMER}, Google, {USA} {RAYMOND} {HU}, University of Hertfordshire, {UK} {WEN} {KOKKE}, University of Edinburgh, {UK} {JULIEN} {LANGE}, Royal Holloway, University of London, {UK} {IAN} {LANCE} {TAYLOR}, Google, {USA} {BERNARDO} {TONINHO}, {NOVA}-{LINCS}, {FCT}-{NOVA}, Universidade Nova de Lisboa, Portugal {PHILIP} {WADLER}, University of Edinburgh, {UK} {NOBUKO} {YOSHIDA}, Imperial College London, {UK} We describe a design for generics in Go inspired by previous work on Featherweight Java by Igarashi, Pierce, and Wadler. Whereas subtyping in Java is nominal, in Go it is structural, and whereas generics in Java are defined via erasure, in Go we use monomorphisation. Although monomorphisation is widely used, we are one of the first to formalise it. Our design also supports a solution to The Expression Problem. {CCS} Concepts: • Theory of computation → Program semantics; Type structures; • Software and its engineering → Polymorphism.},
	pages = {1--29},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Griesemer, Robert and Hu, Raymond and Kokke, Wen and Lange, Julien and Taylor, Ian Lance and Toninho, Bernardo and Wadler, Philip and Yoshida, Nobuko},
	urldate = {2021-10-18},
	date = {2020-11-13},
	langid = {english},
	file = {Griesemer et al. - 2020 - Featherweight go.pdf:/home/fordrl/Zotero/storage/99GZLZF3/Griesemer et al. - 2020 - Featherweight go.pdf:application/pdf},
}

@inproceedings{yodaiken_how_2021,
	location = {Virtual Event Germany},
	title = {How {ISO} C became unusable for operating systems development},
	isbn = {978-1-4503-8707-1},
	url = {https://dl.acm.org/doi/10.1145/3477113.3487274},
	doi = {10.1145/3477113.3487274},
	abstract = {The C programming language was developed in the 1970s as a fairly unconventional systems and operating systems development tool, but has, through the course of the {ISO} Standards process, added many attributes of more conventional programming languages and become less suitable for operating systems development. Operating system programming continues to be done in non-{ISO} dialects of C. The differences provide a glimpse of operating system requirements for programming languages.},
	eventtitle = {{SOSP} '21: {ACM} {SIGOPS} 28th Symposium on Operating Systems Principles},
	pages = {84--90},
	booktitle = {Proceedings of the 11th Workshop on Programming Languages and Operating Systems},
	publisher = {{ACM}},
	author = {Yodaiken, Victor},
	urldate = {2021-10-18},
	date = {2021-10-25},
	langid = {english},
	file = {Yodaiken - 2021 - How ISO C became unusable for operating systems de.pdf:/home/fordrl/Zotero/storage/GVNNELI7/Yodaiken - 2021 - How ISO C became unusable for operating systems de.pdf:application/pdf},
}

@article{barbar_compacting_nodate,
	title = {Compacting Points-To Sets through Object Clustering},
	volume = {5},
	pages = {27},
	author = {Barbar, Mohamad and Sui, Yulei},
	langid = {english},
	file = {Barbar and Sui - Compacting Points-To Sets through Object Clusterin.pdf:/home/fordrl/Zotero/storage/L6ASI57B/Barbar and Sui - Compacting Points-To Sets through Object Clusterin.pdf:application/pdf},
}

@book{alomari_clone_2021,
	title = {Clone Detection through {srcClone}: A Program Slicing Based Approach},
	shorttitle = {Clone Detection through {srcClone}},
	abstract = {Software clone detection is an often used approach to understand and maintain software systems. One category of clones that is challenging to detect but very useful is semantic clones, which are similar in semantics but differ in syntax significantly. Semantic clone detectors have trouble scaling to larger systems and sometimes struggle with recall and precision. To address this, we developed a slice-based scalable approach that detects both syntactic and semantic code clones, {srcClone}. {srcClone} ascertains code segment similarity by assessing the similarity of their corresponding program slices. We employ a lightweight, publicly-available, scalable program slicer within our clone detection approach. Using dependency analysis to detect and assess cloned components, we discover insights about code components that can be affected by a clone pair or set. These elements are critical in impact analysis. It can also be used by program analysts to run on non-compilable and incomplete source code, which serves comprehension and maintenance tasks very well. We first evaluate {srcClone} by comparing it to six state-of-the-art tools and two additional semantic clone detectors in performance, recall, and precision. We use the {BigCloneBench} real clones benchmark to facilitate comparison. We use an additional 16 established benchmark scenarios to perform a qualitative comparison between {srcClone} and 44 clone detection approaches in their capabilities to detect these scenarios. To further measure scalability, we evaluate {srcClone} on 191 versions of Linux kernel, containing approximately 87 {MLOC}. In our evaluations, we illustrate our approach is both relatively scalable and accurate. While its precision is slightly less than some other techniques, it makes up for it in higher recall including semantic clones unable to be found by any existing techniques. We contend our approach is an important advancement in software cloning that it is both demonstrably scalable and can detect more types of clones than existing work, thus providing developers greater information into their software.},
	author = {Alomari, Hakam and Stephan, Matthew},
	date = {2021-09-28},
}

@article{gong_snowboard_nodate,
	title = {Snowboard: Finding Kernel Concurrency Bugs through Systematic Inter-thread Communication Analysis},
	abstract = {Kernel concurrency bugs are challenging to find because they depend on very specific thread interleavings and test inputs. While separately exploring kernel thread interleavings or test inputs has been closely examined, jointly exploring interleavings and test inputs has received little attention, in part due to the resulting vast search space. Using precious, limited testing resources to explore this search space and execute just the right concurrent tests in the proper order is critical.},
	pages = {18},
	author = {Gong, Sishuai and Altınbüken, Deniz and Fonseca, Pedro and Maniatis, Petros},
	langid = {english},
	file = {Gong et al. - Snowboard Finding Kernel Concurrency Bugs through.pdf:/home/fordrl/Zotero/storage/BYNNUULF/Gong et al. - Snowboard Finding Kernel Concurrency Bugs through.pdf:application/pdf},
}

@article{ma_detecting_nodate,
	title = {Detecting Memory-Related Bugs by Tracking Heap Memory Management of C++ Smart Pointers},
	abstract = {The smart pointer mechanism, which is improved in the continuous versions of the C++ standards over the last decade, is designed to prevent memory-leak bugs by automatically deallocating the managed memory blocks. However, not all kinds of memory errors can be immunized by adopting this mechanism. For example, dereferencing a null smart pointer will lead to a software failure. Due to the lack of specialized support for smart pointers, the off-the-shelf C++ static analyzers cannot effectively reveal these bugs.},
	pages = {12},
	author = {Ma, Xutong and Yan, Jiwei and Wang, Wei and Yan, Jun and Zhang, Jian and Qiu, Zongyan},
	langid = {english},
	file = {Ma et al. - Detecting Memory-Related Bugs by Tracking Heap Mem.pdf:/home/fordrl/Zotero/storage/46T7ZNC9/Ma et al. - Detecting Memory-Related Bugs by Tracking Heap Mem.pdf:application/pdf},
}

@article{ocallahan_engineering_2017,
	title = {Engineering Record And Replay For Deployability: Extended Technical Report},
	url = {http://arxiv.org/abs/1705.05937},
	shorttitle = {Engineering Record And Replay For Deployability},
	abstract = {The ability to record and replay program executions with low overhead enables many applications, such as reverse-execution debugging, debugging of hard-to-reproduce test failures, and "black box" forensic analysis of failures in deployed systems. Existing record-and-replay approaches limit deployability by recording an entire virtual machine (heavyweight), modifying the {OS} kernel (adding deployment and maintenance costs), requiring pervasive code instrumentation (imposing significant performance and complexity overhead), or modifying compilers and runtime systems (limiting generality). We investigated whether it is possible to build a practical record-and-replay system avoiding all these issues. The answer turns out to be yes - if the {CPU} and operating system meet certain non-obvious constraints. Fortunately modern Intel {CPUs}, Linux kernels and user-space frameworks do meet these constraints, although this has only become true recently. With some novel optimizations, our system 'rr' records and replays real-world low-parallelism workloads with low overhead, with an entirely user-space implementation, using stock hardware, compilers, runtimes and operating systems. "rr" forms the basis of an open-source reverse-execution debugger seeing significant use in practice. We present the design and implementation of 'rr', describe its performance on a variety of workloads, and identify constraints on hardware and operating system design required to support our approach.},
	journaltitle = {{arXiv}:1705.05937 [cs]},
	author = {O'Callahan, Robert and Jones, Chris and Froyd, Nathan and Huey, Kyle and Noll, Albert and Partush, Nimrod},
	urldate = {2021-10-06},
	date = {2017-05-16},
	eprinttype = {arxiv},
	eprint = {1705.05937},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/55397LSH/1705.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/24QJJBKH/O'Callahan et al. - 2017 - Engineering Record And Replay For Deployability E.pdf:application/pdf},
}

@article{wickerson_pearl_2020,
	title = {Pearl: Diagrams for Composing Compilers},
	abstract = {T-diagrams (or ‘tombstone diagrams’) are widely used in teaching for explaining how compilers and interpreters can be composed together to build and execute software. In this Pearl, we revisit these diagrams, and show how they can be redesigned for better readability. We demonstrate how they can be applied to explain compiler concepts including bootstrapping and cross-compilation. We provide a formal semantics for our redesigned diagrams, based on binary trees. Finally, we suggest how our diagrams could be used to analyse the performance of a compilation system.},
	pages = {13},
	author = {Wickerson, John and Brunet, Paul},
	date = {2020},
	langid = {english},
	file = {Wickerson and Brunet - 2020 - Pearl Diagrams for Composing Compilers.pdf:/home/fordrl/Zotero/storage/4NI959BR/Wickerson and Brunet - 2020 - Pearl Diagrams for Composing Compilers.pdf:application/pdf},
}

@article{honore_much_nodate,
	title = {Much {ADO} about Failures: A Fault-Aware Model for Compositional Verification of Strongly Consistent Distributed Systems},
	volume = {5},
	abstract = {Despite recent advances, guaranteeing the correctness of large-scale distributed applications without compromising performance remains a challenging problem. Network and node failures are inevitable and, for some applications, careful control over how they are handled is essential. Unfortunately, existing approaches either completely hide these failures behind an atomic state machine replication ({SMR}) interface, or expose all of the network-level details, sacrificing atomicity. We propose a novel, compositional, atomic distributed object ({ADO}) model for strongly consistent distributed systems that combines the best of both options. The object-oriented {API} abstracts over protocol-specific details and decouples high-level correctness reasoning from implementation choices. At the same time, it intentionally exposes an abstract view of certain key distributed failure cases, thus allowing for more fine-grained control over them than {SMR}-like models. We demonstrate that proving properties even of composite distributed systems can be straightforward with our Coq verification framework, Advert, thanks to the {ADO} model. We also show that a variety of common protocols including multi-Paxos and Chain Replication refine the {ADO} semantics, which allows one to freely choose among them for an application’s implementation without modifying {ADO}-level correctness proofs. {CCS} Concepts: • Software and its engineering → Software verification; Distributed programming languages; • Theory of computation → Distributed computing models; Object oriented constructs.},
	pages = {42},
	author = {Honoré, Wolf and Kim, Jieung and Shin, Ji-Yong and Shao, Zhong},
	langid = {english},
	file = {Honoré et al. - Much ADO about Failures A Fault-Aware Model for C.pdf:/home/fordrl/Zotero/storage/VFK3UWQT/Honoré et al. - Much ADO about Failures A Fault-Aware Model for C.pdf:application/pdf},
}

@article{yao_efficient_2021,
	title = {Efficient Path-Sensitive Data-Dependence Analysis},
	url = {http://arxiv.org/abs/2109.07923},
	abstract = {This paper presents a scalable path- and context-sensitive data-dependence analysis. The key is to address the aliasing-path-explosion problem via a sparse, demand-driven, and fused approach that piggybacks the computation of pointer information with the resolution of data dependence. Specifically, our approach decomposes the computational efforts of disjunctive reasoning into 1) a context- and semi-path-sensitive analysis that concisely summarizes data dependence as the symbolic and storeless value-flow graphs, and 2) a demand-driven phase that resolves transitive data dependence over the graphs. We have applied the approach to two clients, namely thin slicing and value flow analysis. Using a suite of 16 programs ranging from 13 {KLoC} to 8 {MLoC}, we compare our techniques against a diverse group of state-of-the-art analyses, illustrating significant precision and scalability advantages of our approach.},
	journaltitle = {{arXiv}:2109.07923 [cs]},
	author = {Yao, Peisen and Zhou, Jinguo and Xiao, Xiao and Shi, Qingkai and Wu, Rongxin and Zhang, Charles},
	urldate = {2021-09-24},
	date = {2021-09-16},
	eprinttype = {arxiv},
	eprint = {2109.07923},
	keywords = {Computer Science - Programming Languages, Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/VDLVUYSH/2109.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/688G2GN5/Yao et al. - 2021 - Efficient Path-Sensitive Data-Dependence Analysis.pdf:application/pdf},
}

@incollection{hutchison_practical_2010,
	location = {Berlin, Heidelberg},
	title = {Practical Extensions to the {IFDS} Algorithm},
	volume = {6011},
	isbn = {978-3-642-11969-9 978-3-642-11970-5},
	url = {http://link.springer.com/10.1007/978-3-642-11970-5_8},
	abstract = {This paper presents four extensions to the Interprocedural Finite Distributive Subset ({IFDS}) algorithm that make it applicable to a wider class of analysis problems. {IFDS} is a dynamic programming algorithm that implements context-sensitive ﬂow-sensitive interprocedural dataﬂow analysis. The ﬁrst extension constructs the nodes of the supergraph on demand as the analysis requires them, eliminating the need to build a full supergraph before the analysis. The second extension provides the procedure-return ﬂow function with additional information about the program state before the procedure was called. The third extension improves the precision with which φ instructions are modelled when analyzing a program in {SSA} form. The fourth extension speeds up the algorithm on domains in which some of the dataﬂow facts subsume each other. These extensions are often necessary when applying the {IFDS} algorithm to non-separable (i.e. non-bit-vector) problems. We have found them necessary for alias set analysis and multi-object typestate analysis. In this paper, we illustrate and evaluate the extensions on a simpler problem, a variation of variable type analysis.},
	pages = {124--144},
	booktitle = {Compiler Construction},
	publisher = {Springer Berlin Heidelberg},
	author = {Naeem, Nomair A. and Lhoták, Ondřej and Rodriguez, Jonathan},
	editor = {Gupta, Rajiv},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2021-09-23},
	date = {2010},
	langid = {english},
	doi = {10.1007/978-3-642-11970-5_8},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Naeem et al. - 2010 - Practical Extensions to the IFDS Algorithm.pdf:/home/fordrl/Zotero/storage/BEC49AUS/Naeem et al. - 2010 - Practical Extensions to the IFDS Algorithm.pdf:application/pdf},
}

@inproceedings{kruger_cognicrypt_2017,
	location = {Urbana, {IL}},
	title = {{CogniCrypt}: Supporting developers in using cryptography},
	isbn = {978-1-5386-2684-9},
	url = {http://ieeexplore.ieee.org/document/8115707/},
	doi = {10.1109/ASE.2017.8115707},
	shorttitle = {{CogniCrypt}},
	abstract = {Previous research suggests that developers often struggle using low-level cryptographic {APIs} and, as a result, produce insecure code. When asked, developers desire, among other things, more tool support to help them use such {APIs}. In this paper, we present {CogniCrypt}, a tool that supports developers with the use of cryptographic {APIs}. {CogniCrypt} assists the developer in two ways. First, for a number of common cryptographic tasks, {CogniCrypt} generates code that implements the respective task in a secure manner. Currently, {CogniCrypt} supports tasks such as data encryption, communication over secure channels, and long-term archiving. Second, {CogniCrypt} continuously runs static analyses in the background to ensure a secure integration of the generated code into the developer’s workspace. This video demo showcases the main features of {CogniCrypt}: youtube.com/watch?v={JUq}5mRHfAWY.},
	eventtitle = {2017 32nd {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {931--936},
	booktitle = {2017 32nd {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	publisher = {{IEEE}},
	author = {Kruger, Stefan and Nadi, Sarah and Reif, Michael and Ali, Karim and Mezini, Mira and Bodden, Eric and Gopfert, Florian and Gunther, Felix and Weinert, Christian and Demmler, Daniel and Kamath, Ram},
	urldate = {2021-09-23},
	date = {2017-10},
	langid = {english},
	file = {Kruger et al. - 2017 - CogniCrypt Supporting developers in using cryptog.pdf:/home/fordrl/Zotero/storage/EGYAZ8KQ/Kruger et al. - 2017 - CogniCrypt Supporting developers in using cryptog.pdf:application/pdf},
}

@inproceedings{bodden_secret_2018,
	location = {New York, {NY}, {USA}},
	title = {The secret sauce in efficient and precise static analysis: the beauty of distributive, summary-based static analyses (and how to master them)},
	isbn = {978-1-4503-5939-9},
	url = {https://doi.org/10.1145/3236454.3236500},
	doi = {10.1145/3236454.3236500},
	series = {{ISSTA} '18},
	shorttitle = {The secret sauce in efficient and precise static analysis},
	abstract = {In this paper I report on experiences gained from more than five years of extensively designing static code analysis tools- in particular such ones with a focus on security- to scale to real-world projects within an industrial context. Within this time frame, my team and I were able to design static-analysis algorithms that yield both largely improved precision and performance compared to previous approaches. I will give a number of insights regarding important design decisions that made this possible. In particular, I argue that summary-based static-analysis techniques for distributive problems, such as {IFDS}, {IDE} and {WPDS} have been unduly under-appreciated. As my experience shows, those techniques can tremendously benefit both precision and performance, if one uses them in a well-informed way, using carefully designed abstract domains. As one example, I will explain how in previous work on {BOOMERANG} we were able to decompose pointer analysis, a static analysis problem that is actually not distributive, into sub-problems that are distributive. This yields an implementation that is both highly precise and efficient. This breakthrough, along with the use of a demand-driven program-analysis design, has recently allowed us to implement practical static analysis tools such as the crypto-misuse checker {CogniCrypt}, which can analyze the entire Maven-Central repository with more than 200.000 binaries in under five days, although its analysis is flow-sensitive, field-sensitive, and fully context-sensitive.},
	pages = {85--93},
	booktitle = {Companion Proceedings for the {ISSTA}/{ECOOP} 2018 Workshops},
	publisher = {Association for Computing Machinery},
	author = {Bodden, Eric},
	urldate = {2021-09-22},
	date = {2018-07-16},
	keywords = {static analysis, abstract domains, performance, precision, summarization},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/9W88UAZP/Bodden - 2018 - The secret sauce in efficient and precise static a.pdf:application/pdf},
}

@inproceedings{bodden_inter-procedural_2012,
	location = {Beijing, China},
	title = {Inter-procedural data-flow analysis with {IFDS}/{IDE} and Soot},
	isbn = {978-1-4503-1490-9},
	url = {http://dl.acm.org/citation.cfm?doid=2259051.2259052},
	doi = {10.1145/2259051.2259052},
	abstract = {The {IFDS} and {IDE} frameworks by Reps, Horwitz and Sagiv are two general frameworks for the inter-procedural analysis of data-ﬂow problems with distributive ﬂow functions over ﬁnite domains. Many data-ﬂow problems do have distributive ﬂow functions and are thus expressible as {IFDS} or {IDE} problems, reaching from basic analyses like truly-live variables to complex analyses for problems from the current literature such as typestate and secure information-ﬂow.},
	eventtitle = {the {ACM} {SIGPLAN} International Workshop},
	pages = {3--8},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} International Workshop on State of the Art in Java Program analysis - {SOAP} '12},
	publisher = {{ACM} Press},
	author = {Bodden, Eric},
	urldate = {2021-09-23},
	date = {2012},
	langid = {english},
	file = {Bodden - 2012 - Inter-procedural data-flow analysis with IFDSIDE .pdf:/home/fordrl/Zotero/storage/NBGBFSZ2/Bodden - 2012 - Inter-procedural data-flow analysis with IFDSIDE .pdf:application/pdf},
}

@inproceedings{li_fast_2020,
	location = {New York, {NY}, {USA}},
	title = {Fast graph simplification for interleaved Dyck-reachability},
	isbn = {978-1-4503-7613-6},
	url = {https://doi.org/10.1145/3385412.3386021},
	doi = {10.1145/3385412.3386021},
	series = {{PLDI} 2020},
	abstract = {Many program-analysis problems can be formulated as graph-reachability problems. Interleaved Dyck language reachability. Interleaved Dyck language reachability ({InterDyck}-reachability) is a fundamental framework to express a wide variety of program-analysis problems over edge-labeled graphs. The {InterDyck} language represents an intersection of multiple matched-parenthesis languages (i.e., Dyck languages). In practice, program analyses typically leverage one Dyck language to achieve context-sensitivity, and other Dyck languages to model data dependences, such as field-sensitivity and pointer references/dereferences. In the ideal case, an {InterDyck}-reachability framework should model multiple Dyck languages simultaneously. Unfortunately, precise {InterDyck}-reachability is undecidable. Any practical solution must over-approximate the exact answer. In the literature, a lot of work has been proposed to over-approximate the {InterDyck}-reachability formulation. This paper offers a new perspective on improving both the precision and the scalability of {InterDyck}-reachability: we aim to simplify the underlying input graph G. Our key insight is based on the observation that if an edge is not contributing to any {InterDyck}-path, we can safely eliminate it from G. Our technique is orthogonal to the {InterDyck}-reachability formulation, and can serve as a pre-processing step with any over-approximating approaches for {InterDyck}-reachability. We have applied our graph simplification algorithm to pre-processing the graphs from a recent {InterDyck}-reachability-based taint analysis for Android. Our evaluation on three popular {InterDyck}-reachability algorithms yields promising results. In particular, our graph-simplification method improves both the scalability and precision of all three {InterDyck}-reachability algorithms, sometimes dramatically.},
	pages = {780--793},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Li, Yuanbo and Zhang, Qirun and Reps, Thomas},
	urldate = {2021-09-22},
	date = {2020-06-11},
	keywords = {Static Analysis, {CFL}-Reachability},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/Y3WWB65N/Li et al. - 2020 - Fast graph simplification for interleaved Dyck-rea.pdf:application/pdf},
}

@thesis{spath_synchronized_2019,
	location = {Paderborn},
	title = {Synchronized Pushdown Systems for Pointer and Data-Flow Analysis},
	pagetotal = {152},
	institution = {Paderborn University},
	type = {phdthesis},
	author = {Spath, Johannes},
	date = {2019-03-15},
	langid = {english},
	file = {Spath - Synchronized Pushdown Systems for Pointer and Data.pdf:/home/fordrl/Zotero/storage/QV8IF424/Spath - Synchronized Pushdown Systems for Pointer and Data.pdf:application/pdf},
}

@article{spath_context-_2019,
	title = {Context-, flow-, and field-sensitive data-flow analysis using synchronized Pushdown systems},
	volume = {3},
	url = {https://doi.org/10.1145/3290361},
	doi = {10.1145/3290361},
	abstract = {Precise static analyses are context-, field- and flow-sensitive. Context- and field-sensitivity are both expressible as context-free language ({CFL}) reachability problems. Solving both {CFL} problems along the same data-flow path is undecidable, which is why most flow-sensitive data-flow analyses over-approximate field-sensitivity through k-limited access-path, or through access graphs. Unfortunately, as our experience and this paper show, both representations do not scale very well when used to analyze programs with recursive data structures. Any single {CFL}-reachability problem is efficiently solvable, by means of a pushdown system. This work thus introduces the concept of synchronized pushdown systems ({SPDS}). {SPDS} encode both procedure calls/returns and field stores/loads as separate but “synchronized” {CFL} reachability problems. An {SPDS} solves both individual problems precisely, and approximation occurs only in corner cases that are apparently rare in practice: at statements where both problems are satisfied but not along the same data-flow path. {SPDS} are also efficient: formal complexity analysis shows that {SPDS} shift the complexity from {\textbar}F{\textbar}3k under k-limiting to {\textbar}S{\textbar}{\textbar}F{\textbar}2, where F is the set of fields and S the set of statements involved in a data-flow. Our evaluation using {DaCapo} shows this shift to pay off in practice: {SPDS} are almost as efficient as k-limiting with k=1 although their precision equals k=∞. For a typestate analysis {SPDS} accelerate the analysis up to 83× for data-flows of objects that involve many field accesses but span rather few methods. We conclude that {SPDS} can provide high precision and further improve scalability, in particularly when used in analyses that expose rather local data flows.},
	pages = {48:1--48:29},
	issue = {{POPL}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Späth, Johannes and Ali, Karim and Bodden, Eric},
	urldate = {2021-09-22},
	date = {2019-01-02},
	keywords = {static analysis, aliasing, access paths, data-flow, pushdown system},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/D5B7IM22/Späth et al. - 2019 - Context-, flow-, and field-sensitive data-flow ana.pdf:application/pdf},
}

@article{spath_ideal_2017,
	title = {{IDEal}: efficient and precise alias-aware dataflow analysis},
	volume = {1},
	url = {https://doi.org/10.1145/3133923},
	doi = {10.1145/3133923},
	shorttitle = {{IDE}$^{\textrm{\textit{al}}}$},
	abstract = {Program analyses frequently track objects throughout a program, which requires reasoning about aliases. Most dataflow analysis frameworks, however, delegate the task of handling aliases to the analysis clients, which causes a number of problems. For instance, custom-made extensions for alias analysis are complex and cannot easily be reused. On the other hand, due to the complex interfaces involved, off-the-shelf alias analyses are hard to integrate precisely into clients. Lastly, for precision many clients require strong updates, and alias abstractions supporting strong updates are often relatively inefficient. In this paper, we present {IDEal}, an alias-aware extension to the framework for Interprocedural Distributive Environment ({IDE}) problems. {IDEal} relieves static-analysis authors completely of the burden of handling aliases by automatically resolving alias queries on-demand, both efficiently and precisely. {IDEal} supports a highly precise analysis using strong updates by resorting to an on-demand, flow-sensitive, and context-sensitive all-alias analysis. Yet, it achieves previously unseen efficiency by propagating aliases individually, creating highly reusable per-pointer summaries. We empirically evaluate {IDEal} by comparing {TSf}, a state-of-the-art typestate analysis, to {TSal}, an {IDEal}-based typestate analysis. Our experiments show that the individual propagation of aliases within {IDEal} enables {TSal} to propagate 10.4x fewer dataflow facts and analyze 10.3x fewer methods when compared to {TSf}. On the {DaCapo} benchmark suite, {TSal} is able to efficiently compute precise results.},
	pages = {99:1--99:27},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Späth, Johannes and Ali, Karim and Bodden, Eric},
	urldate = {2021-09-22},
	date = {2017-10-12},
	keywords = {static analysis, aliasing, dataflow},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/6VE55SGZ/Späth et al. - 2017 - IDEal efficient and precise ali.pdf:application/pdf},
}

@article{spath_boomerang_2016,
	title = {Boomerang: Demand-Driven Flow- and Context-Sensitive Pointer Analysis for Java},
	abstract = {Many current program analyses require highly precise pointer information about small, targeted parts of a given program. This motivates the need for demand-driven pointer analyses that compute information only where required. Pointer analyses generally compute points-to sets of program variables or answer boolean alias queries. However, many client analyses require richer pointer information. For example, taint and typestate analyses often need to know the set of all aliases of a given variable under a certain calling context. With most current pointer analyses, clients must compute such information through repeated points-to or alias queries, increasing complexity and computation time for them. This paper presents Boomerang, a demand-driven, ﬂow-, ﬁeld-, and context-sensitive pointer analysis for Java programs. Boomerang computes rich results that include both the possible allocation sites of a given pointer (points-to information) and all pointers that can point to those allocation sites (alias information). For increased precision and scalability, clients can query Boomerang with respect to particular calling contexts of interest.},
	pages = {26},
	author = {Späth, Johannes and Do, Lisa Nguyen Quang and Ali, Karim and Bodden, Eric},
	date = {2016},
	langid = {english},
	file = {Späth et al. - 2016 - Boomerang Demand-Driven Flow- and Context-Sensiti.pdf:/home/fordrl/Zotero/storage/WFDXVPNN/Späth et al. - 2016 - Boomerang Demand-Driven Flow- and Context-Sensiti.pdf:application/pdf},
}

@inproceedings{schubert_know_2019,
	location = {New York, {NY}, {USA}},
	title = {Know your analysis: how instrumentation aids understanding static analysis},
	isbn = {978-1-4503-6720-2},
	url = {https://doi.org/10.1145/3315568.3329965},
	doi = {10.1145/3315568.3329965},
	series = {{SOAP} 2019},
	shorttitle = {Know your analysis},
	abstract = {The development of a high-quality data-flow analysis---one that is precise and scalable---is a challenging task. A concrete client analysis not only requires data-flow but, in addition, type-hierarchy, points-to, and call-graph information, all of which need to be obtained by wisely chosen and correctly parameterized algorithms. Therefore, many static analysis frameworks have been developed that provide analysis writers with generic data-flow solvers as well as those additional pieces of information. Such frameworks ease the development of an analysis by requiring only a description of the data-flow problem to be solved and a set of framework parameters. Yet, analysis writers often struggle when an analysis does not behave as expected on real-world code. It is usually not apparent what causes a failure due to the complex interplay of the several algorithms and the client analysis code within such frameworks. In this work, we present some of the insights we gained by instrumenting the {LLVM}-based static analysis framework {PhASAR} for C/C++ code and show the broad area of applications at which flexible instrumentation supports analysis and framework developers. We present five cases in which instrumentation gave us valuable insights to debug and improve both, the concrete analyses and the underlying {PhASAR} framework.},
	pages = {8--13},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} International Workshop on State Of the Art in Program Analysis},
	publisher = {Association for Computing Machinery},
	author = {Schubert, Philipp Dominik and Leer, Richard and Hermann, Ben and Bodden, Eric},
	urldate = {2021-09-22},
	date = {2019-06-22},
	keywords = {Static analysis, C/C++, framework, instrumentation},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/YJBU6GZ8/Schubert et al. - 2019 - Know your analysis how instrumentation aids under.pdf:application/pdf},
}

@article{li_complexity_2021,
	title = {On the complexity of bidirected interleaved Dyck-reachability},
	volume = {5},
	url = {https://doi.org/10.1145/3434340},
	doi = {10.1145/3434340},
	abstract = {Many program analyses need to reason about pairs of matching actions, such as call/return, lock/unlock, or set-field/get-field. The family of Dyck languages \{Dk\}, where Dk has k kinds of parenthesis pairs, can be used to model matching actions as balanced parentheses. Consequently, many program-analysis problems can be formulated as Dyck-reachability problems on edge-labeled digraphs. Interleaved Dyck-reachability ({InterDyck}-reachability), denoted by Dk ⊙ Dk-reachability, is a natural extension of Dyck-reachability that allows one to formulate program-analysis problems that involve multiple kinds of matching-action pairs. Unfortunately, the general {InterDyck}-reachability problem is undecidable. In this paper, we study variants of {InterDyck}-reachability on bidirected graphs, where for each edge ⟨ p, q ⟩ labeled by an open parenthesis ”(a”, there is an edge ⟨ q, p ⟩ labeled by the corresponding close parenthesis ”)a”, and vice versa. Language-reachability on a bidirected graph has proven to be useful both (1) in its own right, as a way to formalize many program-analysis problems, such as pointer analysis, and (2) as a relaxation method that uses a fast algorithm to over-approximate language-reachability on a directed graph. However, unlike its directed counterpart, the complexity of bidirected {InterDyck}-reachability still remains open. We establish the first decidable variant (i.e., D1 ⊙ D1-reachability) of bidirected {InterDyck}-reachability. In D1 ⊙ D1-reachability, each of the two Dyck languages is restricted to have only a single kind of parenthesis pair. In particular, we show that the bidirected D1 ⊙ D1 problem is in {PTIME}. We also show that when one extends each Dyck language to involve k different kinds of parentheses (i.e., Dk ⊙ Dk-reachability with k ≥ 2), the problem is {NP}-hard (and therefore much harder). We have implemented the polynomial-time algorithm for bidirected D1 ⊙ D1-reachability. Dk ⊙ Dk-reachability provides a new over-approximation method for bidirected Dk ⊙ Dk-reachability in the sense that Dk⊙ Dk-reachability can first be relaxed to bidirected D1 ⊙ D1-reachability, and then the resulting bidirected D1 ⊙ D1-reachability problem is solved precisely. We compare this D1 ⊙ D1-reachability-based approach against another known over-approximating Dk ⊙ Dk-reachability algorithm. Surprisingly, we found that the over-approximation approach based on bidirected D1 ⊙ D1-reachability computes more precise solutions, even though the D1⊙ D1 formalism is inherently less expressive than the Dk⊙ Dk formalism.},
	pages = {59:1--59:28},
	issue = {{POPL}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Li, Yuanbo and Zhang, Qirun and Reps, Thomas},
	urldate = {2021-09-22},
	date = {2021-01-04},
	keywords = {Static Analysis, Complexity, Formal Language Graph Reachability, Interleaved-Dyck Language},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/IQ8V7UMW/Li et al. - 2021 - On the complexity of bidirected interleaved Dyck-r.pdf:application/pdf},
}

@online{reps_precise_nodate,
	title = {Precise Interprocedural Dataflow Analysis with Applications to Constant Propagation},
	url = {https://reader.elsevier.com/reader/sd/pii/0304397596000722?token=15CB6A19CA3A743944564BDD3617A93CE015B932950E8BD0E9FBBD0AEC87E0476ACCE1E93B67F2D1AE1DCE570B05F8DA&originRegion=us-east-1&originCreation=20210920234905},
	shorttitle = {{PII}},
	author = {Reps, Thomas and Horwitz, Susan and Sagiv, Mooly},
	urldate = {2021-09-20},
	langid = {english},
	doi = {10.1016/0304-3975(96)00072-2},
	note = {{ISSN}: 0304-3975},
	file = {Snapshot:/home/fordrl/Zotero/storage/QDTS7AKX/0304397596000722.html:text/html;Submitted Version:/home/fordrl/Zotero/storage/JHJB4QFT/PII 0304-3975(96)00072-2  Elsevier Enhanced Read.pdf:application/pdf},
}

@inproceedings{reps_precise_1995,
	location = {New York, {NY}, {USA}},
	title = {Precise interprocedural dataflow analysis via graph reachability},
	isbn = {978-0-89791-692-9},
	url = {https://doi.org/10.1145/199448.199462},
	doi = {10.1145/199448.199462},
	series = {{POPL} '95},
	abstract = {The paper shows how a large class of interprocedural dataflow-analysis problems can be solved precisely in polynomial time by transforming them into a special kind of graph-reachability problem. The only restrictions are that the set of dataflow facts must be a finite set, and that the dataflow functions must distribute over the confluence operator (either union or intersection). This class of probable problems includes—but is not limited to—the classical separable problems (also known as “gen/kill” or “bit-vector” problems)—e.g., reaching definitions, available expressions, and live variables. In addition, the class of problems that our techniques handle includes many non-separable problems, including truly-live variables, copy constant propagation, and possibly-uninitialized variables. Results are reported from a preliminary experimental study of C programs (for the problem of finding possibly-uninitialized variables).},
	pages = {49--61},
	booktitle = {Proceedings of the 22nd {ACM} {SIGPLAN}-{SIGACT} symposium on Principles of programming languages},
	publisher = {Association for Computing Machinery},
	author = {Reps, Thomas and Horwitz, Susan and Sagiv, Mooly},
	urldate = {2021-09-20},
	date = {1995-01-25},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/EKGLD89C/Reps et al. - 1995 - Precise interprocedural dataflow analysis via grap.pdf:application/pdf},
}

@article{schubert_modeling_nodate,
	title = {Modeling the Effects of Global Variables in Data-Flow Analysis for C/C++},
	abstract = {Global variables make software systems hard to maintain and debug, and break local reasoning. They also impose a non-trivial challenge to static analysis which needs to model its effects to obtain sound analysis results. However, global variable initialization, codes of corresponding constructors and destructors as well as dynamic library code executed during load and unload not only affect control ﬂows but data ﬂows, too. The {PhASAR} static data-ﬂow analysis framework does not handle these special cases and also does not provide any functionalities to model the effects of globals. Analysis writers are forced to model the desired effects in an ad-hoc manner increasing an analysis’ complexity and imposing an additional repetitive task. In this paper, we present the challenges of modeling globals, elaborate on the impact they have on analysis information, and present a suitable model to capture their effects, allowing for an easier development of global-aware static data-ﬂow analyses. We present an implementation of our model within the {PhASAR} framework and show its usefulness for an {IDE}-based linearconstant propagation that crucially requires correct modeling of globals for correctness.},
	pages = {6},
	author = {Schubert, Philipp Dominik and Sattler, Florian and Schiebel, Fabian and Hermann, Ben and Bodden, Eric},
	langid = {english},
	file = {Schubert et al. - Modeling the Effects of Global Variables in Data-F.pdf:/home/fordrl/Zotero/storage/EYE7V3AE/Schubert et al. - Modeling the Effects of Global Variables in Data-F.pdf:application/pdf},
}

@incollection{vojnar_phasar_2019,
	location = {Cham},
	title = {{PhASAR}: An Inter-procedural Static Analysis Framework for C/C++},
	volume = {11428},
	isbn = {978-3-030-17464-4 978-3-030-17465-1},
	url = {http://link.springer.com/10.1007/978-3-030-17465-1_22},
	shorttitle = {{PhASAR}},
	abstract = {Static program analysis is used to automatically determine program properties, or to detect bugs or security vulnerabilities in programs. It can be used as a stand-alone tool or to aid compiler optimization as an intermediary step. Developing precise, inter-procedural static analyses, however, is a challenging task, due to the algorithmic complexity, implementation eﬀort, and the threat of state explosion which leads to unsatisfactory performance. Software written in C and C++ is notoriously hard to analyze because of the deliberately unsafe type system, unrestricted use of pointers, and (for C++) virtual dispatch. In this work, we describe the design and implementation of the {LLVM}-based static analysis framework {PhASAR} for C/C++ code. {PhASAR} allows data-ﬂow problems to be solved in a fully automated manner. It provides class hierarchy, call-graph, points-to, and data-ﬂow information, hence requiring analysis developers only to specify a deﬁnition of the data-ﬂow problem. {PhASAR} thus hides the complexity of static analysis behind a high-level {API}, making static program analysis more accessible and easy to use. {PhASAR} is available as an open-source project. We evaluate {PhASAR}’s scalability during whole-program analysis. Analyzing 12 real-world programs using a taint analysis written in {PhASAR}, we found {PhASAR}’s abstractions and their implementations to provide a whole-program analysis that scales well to real-world programs. Furthermore, we peek into the details of analysis runs, discuss our experience in developing static analyses for C/C++, and present possible future improvements. Data or code related to this paper is available at: [34].},
	pages = {393--410},
	booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
	publisher = {Springer International Publishing},
	author = {Schubert, Philipp Dominik and Hermann, Ben and Bodden, Eric},
	editor = {Vojnar, Tomáš and Zhang, Lijun},
	urldate = {2021-09-20},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-030-17465-1_22},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Schubert et al. - 2019 - PhASAR An Inter-procedural Static Analysis Framew.pdf:/home/fordrl/Zotero/storage/B8SQYN8K/Schubert et al. - 2019 - PhASAR An Inter-procedural Static Analysis Framew.pdf:application/pdf},
}

@article{schubert_into_nodate,
	title = {Into the Woods: Experiences from Building a Dataﬂow Analysis Framework for C/C++},
	abstract = {While traditional static analysis—albeit its complexity—is a topic that is well understood, we especially struggle when it comes to implementing its concepts. Designing and modeling software that implements static analysis is a challenging task. However, developing usable static analysis implementations and providing toolboxes to researchers and practitioners is key to advance the overall progress in this ﬁeld. This paper reports on the lessons learned from developing the {PhASAR} and Soot static data-ﬂow analysis frameworks. We present some of the key mistakes of our ﬁrst implementations of {PhASAR} and their corrections. From those corrections we distill guidelines that will be helpful to static analysis developers and their users. In our work, we identiﬁed modularity as the key guiding principle supported—directly or indirectly—by virtually all other guidelines.},
	pages = {6},
	author = {Schubert, Philipp Dominik and Leer, Richard and Hermann, Ben and Bodden, Eric},
	langid = {english},
	file = {Schubert et al. - Into the Woods Experiences from Building a Dataﬂo.pdf:/home/fordrl/Zotero/storage/RLS933QS/Schubert et al. - Into the Woods Experiences from Building a Dataﬂo.pdf:application/pdf},
}

@article{barbar_hash_nodate,
	title = {Hash Consed Points-To Sets},
	abstract = {Points-to analysis is a fundamental static analysis, on which many other analyses and optimisations are built. The goal of points-to analysis is to statically approximate the set of abstract objects that a pointer can point to at runtime. Due to the nature of static analysis, points-to analysis introduces much redundancy which can result in duplicate points-to sets and duplicate set union operations, particularly when analysing large programs precisely. To improve performance, there has been extensive eﬀort in mitigating duplication at the algorithmic level through, for example, cycle elimination and variable substitution.},
	pages = {24},
	author = {Barbar, Mohamad and Sui, Yulei},
	langid = {english},
	file = {Barbar and Sui - Hash Consed Points-To Sets.pdf:/home/fordrl/Zotero/storage/PAKG4E6Y/Barbar and Sui - Hash Consed Points-To Sets.pdf:application/pdf},
}

@article{lahav_whats_nodate,
	title = {What's Decidable about Causally Consistent Shared Memory?},
	volume = {0},
	abstract = {{CCS} Concepts: • Software and its engineering → Software verification; Concurrent programming languages; • Theory of computation → Concurrency; Logic and verification; Program verification; • Information systems → Distributed database transactions.},
	pages = {54},
	number = {0},
	author = {Lahav, Ori and Boker, Udi},
	langid = {english},
	file = {Lahav and Boker - What's Decidable about Causally Consistent Shared .pdf:/home/fordrl/Zotero/storage/2ULVLVI6/Lahav and Boker - What's Decidable about Causally Consistent Shared .pdf:application/pdf},
}

@article{mine_static_nodate,
	title = {Static Type and Value Analysis by Abstract Interpretation of Python Programs with Native C Libraries},
	pages = {271},
	author = {Miné, Antoine and Mastroeni, Isabella and Møller, Anders and Chailloux, Emmanuel and Logozzo, Francesco and Müller, Peter and Schmitt, Alan and Ouadjaout, Abdelraouf and di Verona, Università},
	langid = {french},
	file = {Miné et al. - Static Type and Value Analysis by Abstract Interpr.pdf:/home/fordrl/Zotero/storage/ASRJGEQV/Miné et al. - Static Type and Value Analysis by Abstract Interpr.pdf:application/pdf},
}

@article{lin_translation_nodate,
	title = {A Translation Validation Algorithm for {LLVM} Register Allocators},
	abstract = {Register allocation is a crucial and complex phase of any modern production compiler. In this work, we present a translation validation algorithm that veriﬁes each single instance of register allocation. Our algorithm is external to the compiler and independent of the register allocation algorithm. We support all major register allocation optimizations such as live range splitting, register coalescing, and rematerialization. We developed a prototype of this approach for the production-quality register allocation phase of {LLVM}. We evaluated this prototype for compiling the source code of {SPECint} 2006 benchmark, and we were able to verify the register allocation of over 88\% of supported functions in the benchmark, using all 4 register allocators available in {LLVM} 5.0.2.},
	pages = {11},
	author = {Lin, Zhengyao and Kasampalis, Theodoros and Adve, Vikram},
	langid = {english},
	file = {Lin et al. - A Translation Validation Algorithm for LLVM Regist.pdf:/home/fordrl/Zotero/storage/EHXF654T/Lin et al. - A Translation Validation Algorithm for LLVM Regist.pdf:application/pdf},
}

@inproceedings{wang_reassembleable_2015,
	title = {Reassembleable Disassembling},
	isbn = {978-1-939133-11-3},
	url = {https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/wang-shuai},
	eventtitle = {24th {USENIX} Security Symposium ({USENIX} Security 15)},
	pages = {627--642},
	author = {Wang, Shuai and Wang, Pei and Wu, Dinghao},
	urldate = {2022-01-27},
	date = {2015},
	langid = {english},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/LID5HT52/Wang et al. - 2015 - Reassembleable Disassembling.pdf:application/pdf;Snapshot:/home/fordrl/Zotero/storage/24FWJTVK/wang-shuai.html:text/html},
}

@article{eklind_compositional_2015,
	title = {Compositional Decompilation using {LLVM} {IR}},
	abstract = {Decompilation or reverse compilation is the process of translating low-level machine-readable code into high-level human-readable code. The problem is nontrivial due to the amount of information lost during compilation, but it can be divided into several smaller problems which may be solved independently. This report explores the feasibility of composing a decompilation pipeline from independent components, and the potential of exposing those components to the end-user. The components of the decompilation pipeline are conceptually grouped into three modules. Firstly, the front-end translates a source language (e.g. x86 assembly) into {LLVM} {IR}; a platform-independent low-level intermediate representation. Secondly, the middle-end structures the {LLVM} {IR} by identifying high-level control ﬂow primitives (e.g. pre-test loops, 2-way conditionals). Lastly, the back-end translates the structured {LLVM} {IR} into a high-level target programming language (e.g. Go). The control ﬂow analysis stage of the middle-end uses subgraph isomorphism search algorithms to locate control ﬂow primitives in {CFGs}, both of which are described using Graphviz {DOT} ﬁles.},
	pages = {100},
	author = {Eklind, Robin},
	date = {2015-04-21},
	langid = {english},
	file = {Eklind - Compositional Decompilation using LLVM IR.pdf:/home/fordrl/Zotero/storage/5NB3AQEZ/Eklind - Compositional Decompilation using LLVM IR.pdf:application/pdf},
}

@thesis{moll_decompilation_nodate,
	title = {Decompilation of {LLVM} {IR}},
	url = {https://compilers.cs.uni-saarland.de/publications/theses/moll_bsc.pdf},
	pagetotal = {69},
	institution = {Saarland University},
	type = {Bachelor's Thesis},
	author = {Moll, Simon},
	urldate = {2022-01-27},
	file = {moll_bsc.pdf:/home/fordrl/Zotero/storage/6M8GS4W7/moll_bsc.pdf:application/pdf},
}

@article{schulte_gtirb_2020,
	title = {{GTIRB}: Intermediate Representation for Binaries},
	url = {http://arxiv.org/abs/1907.02859},
	shorttitle = {{GTIRB}},
	abstract = {{GTIRB} is an intermediate representation for binary analysis and rewriting tools including disassemblers, lifters, analyzers, rewriters, and pretty-printers. {GTIRB} is designed to enable communication between tools in a format that provides the basic information necessary for analysis and rewriting while making no further assumptions about domain (e.g., malware vs. cleanware, or {PE} vs. {ELF}) or semantic interpretation (functional vs. operational semantics). This design supports the goals of (1) encouraging tool modularization and re-use allowing researchers and developers to focus on a single aspect of binary analysis and rewriting without committing to any single tool chain and (2) facilitating communication and comparison between tools.},
	journaltitle = {{arXiv}:1907.02859 [cs]},
	author = {Schulte, Eric and Dorn, Jonathan and Flores-Montoya, Antonio and Ballman, Aaron and Johnson, Tom},
	urldate = {2022-01-27},
	date = {2020-04-02},
	eprinttype = {arxiv},
	eprint = {1907.02859},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/home/fordrl/Zotero/storage/M4DZDQ67/Schulte et al. - 2020 - GTIRB Intermediate Representation for Binaries.pdf:application/pdf;arXiv.org Snapshot:/home/fordrl/Zotero/storage/CDLSLUUJ/1907.html:text/html},
}

@inproceedings{wang_uroboros_2016,
	location = {Suita},
	title = {{UROBOROS}: Instrumenting Stripped Binaries with Static Reassembling},
	isbn = {978-1-5090-1855-0},
	url = {http://ieeexplore.ieee.org/document/7476646/},
	doi = {10.1109/SANER.2016.106},
	shorttitle = {{UROBOROS}},
	abstract = {Software instrumentation techniques are widely used in program analysis tasks such as program proﬁling, vulnerability discovering, and security-oriented transforming. In this paper, we present an instrumentation tool called {UROBOROS}, which supports static instrumentation on stripped binaries. Due to the lack of relocation and debug information, reverse engineering of stripped binaries is challenging. Compared with the previous work, {UROBOROS} can provide complete, easy-touse, transparent, and efﬁcient static instrumentation on stripped binaries. {UROBOROS} supports complete instrumentation by statically recovering the relocatable program (including both code and data sections) and the control ﬂow structures from binary code. {UROBOROS} provides a rich {API} to access and manipulate different levels of the program structure. The instrumentation facilities of {UROBOROS} are easy-to-use, users with no binary rewriting and patching skills can directly manipulate stripped binaries to perform smooth program transformations. Distinguished from most instrumentation tools that need to patch the instrumentation code as new sections, {UROBOROS} can directly inline the instrumentation code into the disassembled program, which provides transparent instrumentation on stripped binaries. For efﬁciency, in the rewritten output of existing tools, frequent control transfers between the attached and original sections can incur a considerable performance penalty. However, the output from {UROBOROS} incurs no extra cost because the original and instrumentation code are connected by “fall-through” transfers. We perform comparative evaluations between {UROBOROS} and the state-of-the-art binary instrumentation tools, including {DynInst} and Pin. To demonstrate the versatility of {UROBOROS}, we also implement two real-world reengineering tasks which could be challenging for other instrumentation tools to accomplish. Our experimental results show that {UROBOROS} outperforms the existing binary instrumentation tools with better performance, lower labor cost, and a broader scope of applications.},
	eventtitle = {2016 {IEEE} 23rd International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	pages = {236--247},
	booktitle = {2016 {IEEE} 23rd International Conference on Software Analysis, Evolution, and Reengineering ({SANER})},
	publisher = {{IEEE}},
	author = {Wang, Shuai and Wang, Pei and Wu, Dinghao},
	urldate = {2022-01-27},
	date = {2016-03},
	langid = {english},
	file = {Wang et al. - 2016 - UROBOROS Instrumenting Stripped Binaries with Sta.pdf:/home/fordrl/Zotero/storage/CDYWNKWC/Wang et al. - 2016 - UROBOROS Instrumenting Stripped Binaries with Sta.pdf:application/pdf},
}

@article{kiaei_rewrite_2020,
	title = {Rewrite to Reinforce: Rewriting the Binary to Apply Countermeasures against Fault Injection},
	url = {http://arxiv.org/abs/2011.14067},
	shorttitle = {Rewrite to Reinforce},
	abstract = {Fault injection attacks can cause errors in software for malicious purposes. Oftentimes, vulnerable points of a program are detected after its development. It is therefore critical for the user of the program to be able to apply last-minute security assurance to the executable file without having access to the source code. In this work, we explore two methodologies based on binary rewriting that aid in injecting countermeasures in the binary file. The first approach injects countermeasures by reassembling the disassembly whereas the second approach leverages a full translation to a high-level {IR} and lowering that back to the target architecture.},
	journaltitle = {{arXiv}:2011.14067 [cs]},
	author = {Kiaei, Pantea and Breunesse, Cees-Bart and Ahmadi, Mohsen and Schaumont, Patrick and van Woudenberg, Jasper},
	urldate = {2022-01-27},
	date = {2020-11-27},
	eprinttype = {arxiv},
	eprint = {2011.14067},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/home/fordrl/Zotero/storage/ICNQH55M/Kiaei et al. - 2020 - Rewrite to Reinforce Rewriting the Binary to Appl.pdf:application/pdf;arXiv.org Snapshot:/home/fordrl/Zotero/storage/48SC3QFD/2011.html:text/html},
}

@article{jain_bird_2021,
	title = {{BiRD}: Race Detection in Software Binaries under Relaxed Memory Models},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3498538},
	doi = {10.1145/3498538},
	shorttitle = {{BiRD}},
	abstract = {Instruction reordering and interleavings in program execution under relaxed memory semantics result in non-intuitive behaviors, making it difficult to provide assurances about program correctness. Studies have shown that up to 90\% of the concurrency bugs reported by state-of-the-art static analyzers are false alarms. As a result, filtering false alarms and detecting real concurrency bugs is a challenging problem. Unsurprisingly, this problem has attracted the interest of the research community over the past few decades. Nonetheless, many of the existing techniques rely on analyzing source code, rarely consider the effects introduced by compilers, and assume a sequentially consistent memory model. In a practical setting, however, developers often do not have access to the source code, and even commodity architectures such as x86 and {ARM} are not sequentially consistent. In this work, we present Bird, a prototype tool, to dynamically detect harmful data races in x86 binaries under relaxed memory models, {TSO} and {PSO}. Bird employs source-{DPOR} to explore all distinct feasible interleavings for a multithreaded application. Our evaluation of Bird on 42 publicly available benchmarks and its comparison with the state-of-the-art tools indicate Bird’s potential in effectively detecting data races in software binaries.},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Jain, Ridhi and Purandare, Rahul and Sharma, Subodh},
	urldate = {2022-02-08},
	date = {2021-11-08},
	note = {Just Accepted},
	keywords = {Dynamic race detection, Relaxed memory models, Software binaries, {TSO} and {PSO}},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/HUJHKPT9/Jain et al. - 2021 - BiRD Race Detection in Software Binaries under Re.pdf:application/pdf},
}

@article{suchy_carat_2022,
	title = {{CARAT} {CAKE}: Replacing Paging via Compiler/Kernel Cooperation},
	abstract = {Virtual memory, specifically paging, is undergoing significant innovation due to being challenged by new demands from modern workloads. Recent work has demonstrated an alternative softwareonly design that can result in simplified hardware requirements, even supporting purely physical addressing. While we have made the case for this Compiler- And Runtime-based Address Translation ({CARAT}) concept, its evaluation was based on a user-level prototype. We now report on incorporating {CARAT} into a kernel, forming Compiler- And Runtime-based Address Translation for {CollAborative} Kernel Environments ({CARAT} {CAKE}). In our implementation, a Linux-compatible x64 process abstraction can be based either on {CARAT} {CAKE}, or on a sophisticated paging implementation. Implementing {CARAT} {CAKE} involves kernel changes and compiler optimizations/transformations that must work on all code in the system, including kernel code. We evaluate {CARAT} {CAKE} in comparison with paging and find that {CARAT} {CAKE} is able to achieve the functionality of paging (protection, mapping, and movement properties) with minimal overhead. In turn, {CARAT} {CAKE} allows significant new benefits for systems including energy savings, larger L1 caches, and arbitrary granularity memory management.},
	pages = {18},
	author = {Suchy, Brian and Ghosh, Souradip and Kersnar, Drew and Chai, Siyuan and Huang, Zhen and Nelson, Aaron and Cuevas, Michael and Bernat, Alex and Chaudhary, Gaurav and Hardavellas, Nikos and Campanoni, Simone and Dinda, Peter},
	date = {2022},
	langid = {english},
	file = {Suchy et al. - 2022 - CARAT CAKE Replacing Paging via CompilerKernel C.pdf:/home/fordrl/Zotero/storage/3UIMD3HF/Suchy et al. - 2022 - CARAT CAKE Replacing Paging via CompilerKernel C.pdf:application/pdf},
}

@article{vasilache_composable_2022,
	title = {Composable and Modular Code Generation in {MLIR}: A Structured and Retargetable Approach to Tensor Compiler Construction},
	url = {http://arxiv.org/abs/2202.03293},
	shorttitle = {Composable and Modular Code Generation in {MLIR}},
	abstract = {Despite significant investment in software infrastructure, machine learning systems, runtimes and compilers do not compose properly. We propose a new design aiming at providing unprecedented degrees of modularity, composability and genericity. This paper discusses a structured approach to the construction of domain-specific code generators for tensor compilers, with the stated goal of improving the productivity of both compiler engineers and end-users. The approach leverages the natural structure of tensor algebra. It has been the main driver for the design of progressive lowering paths in {\textbackslash}{MLIR}. The proposed abstractions and transformations span data structures and control flow with both functional ({SSA} form) and imperative (side-effecting) semantics. We discuss the implications of this infrastructure on compiler construction and present preliminary experimental results.},
	journaltitle = {{arXiv}:2202.03293 [cs]},
	author = {Vasilache, Nicolas and Zinenko, Oleksandr and Bik, Aart J. C. and Ravishankar, Mahesh and Raoux, Thomas and Belyaev, Alexander and Springer, Matthias and Gysi, Tobias and Caballero, Diego and Herhut, Stephan and Laurenzo, Stella and Cohen, Albert},
	urldate = {2022-02-13},
	date = {2022-02-07},
	eprinttype = {arxiv},
	eprint = {2202.03293},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/home/fordrl/Zotero/storage/AV49EPVA/Vasilache et al. - 2022 - Composable and Modular Code Generation in MLIR A .pdf:application/pdf;arXiv.org Snapshot:/home/fordrl/Zotero/storage/5X9PTDBZ/2202.html:text/html},
}

@article{zhang_heterogen_2022,
	title = {{HeteroGen}: Transpiling C to Heterogeneous {HLS} Code with Automated Test Generation and Program Repair},
	abstract = {Despite the trend of incorporating heterogeneity and specialization in hardware, the development of heterogeneous applications is limited to a handful of engineers with deep hardware expertise. We propose {HeteroGen} that takes C/C++ code as input and automatically generates an {HLS} version with test behavior preservation and better performance. Key to the success of {HeteroGen} is adapting the idea of search-based program repair to the heterogeneous computing domain, while addressing two technical challenges. First, the turn-around time of {HLS} compilation and simulation is much longer than the usual C/C++ compilation and execution time; therefore, {HeteroGen} applies pattern-oriented program edits guided by common fix patterns and their dependences. Second, behavior and performance checking requires testing, but test cases are often unavailable. Thus, {HeteroGen} auto-generates test inputs suitable for checking C to {HLS}-C conversion errors, while providing high branch coverage for the original C code.},
	pages = {13},
	author = {Zhang, Qian and Wang, Jiyuan and Xu, Guoqing Harry and Kim, Miryung},
	date = {2022},
	langid = {english},
	file = {Zhang et al. - 2022 - HeteroGen Transpiling C to Heterogeneous HLS Code.pdf:/home/fordrl/Zotero/storage/GD6QHCHW/Zhang et al. - 2022 - HeteroGen Transpiling C to Heterogeneous HLS Code.pdf:application/pdf},
}

@thesis{christensen_programming_2021,
	location = {Santa Barbara, {CA}},
	title = {Programming Language Techniques for Improving {ISA} and {HDL} Design},
	url = {https://escholarship.org/content/qt7sz5r3vd/qt7sz5r3vd.pdf},
	pagetotal = {243},
	institution = {{UC} Santa Barbara},
	type = {phdthesis},
	author = {Christensen, Michael Alexandre},
	urldate = {2022-02-13},
	date = {2021},
	file = {qt7sz5r3vd.pdf:/home/fordrl/Zotero/storage/CVBSQTIG/qt7sz5r3vd.pdf:application/pdf},
}

@thesis{bartell_optimizing_2021,
	location = {Urbana, Illinois},
	title = {Optimizing Whole Programs for Code Size},
	pagetotal = {145},
	institution = {University of Illinois Urbana–Champaign},
	type = {phdthesis},
	author = {Bartell, Sean},
	date = {2021},
	langid = {english},
	file = {Bartell - Optimizing Whole Programs for Code Size.pdf:/home/fordrl/Zotero/storage/D3J33SSV/Bartell - Optimizing Whole Programs for Code Size.pdf:application/pdf},
}

@article{chen_tree_2022,
	title = {Tree Traversal Synthesis Using Domain-{SpecificSymbolic} Compilation},
	abstract = {Efficient computation on tree data structures is important in compilers, numeric computations, and web browser layout engines. Efficiency is achieved by statically scheduling the computation into a small number of tree traversals and by performing the traversals in parallel when possible. Manual design of such traversals leads to bugs, as observed in web browsers. Automatic schedulers avoid these bugs but they currently cannot explore a space of legal traversals, which prevents exploring the trade-offs between parallelism and minimizing the number of traversals. We describe Hecate, a synthesizer of tree traversals that can produce both serial and parallel traversals. A key feature is that the synthesizer is extensible by the programmer who can define a template for new kinds of traversals. Hecate is constructed as a solver-aided domain-specific language, meaning that the synthesizer is generated automatically by translating the tree traversal {DSL} to an {SMT} solver that synthesizes the traversals. We improve on the general-purpose solver-aided architecture with a schedulingspecific symbolic evaluation that maintains the engineering advantages solver-aided design but generates efficient {ILP} encoding that is much more efficient to solve than {SMT} constraints. On the set of Grafter problems, Hecate synthesizes traversals that trade off traversal fusion to exploit parallelism. Additionally, Hecate allows defining a tree data structure with an arbitrary number of children. Together, parallelism and data structure improvements accelerate the computation 2× on a tree rendering problem. Finally, Hecate’s domain-specific symbolic compilation accelerates synthesis 3× compared to the general-purpose compilation to an {SMT} solver; when scheduling a {CSS} engine traversal, this {ILP}-based synthesis executes orders of magnitude faster.},
	pages = {13},
	author = {Chen, Yanju and Liu, Junrui and Feng, Yu and Bodik, Rastislav},
	date = {2022},
	langid = {english},
	file = {Chen et al. - 2022 - Tree Traversal Synthesis Using Domain-SpecificSymb.pdf:/home/fordrl/Zotero/storage/6I3KEJLE/Chen et al. - 2022 - Tree Traversal Synthesis Using Domain-SpecificSymb.pdf:application/pdf},
}

@article{riganelli_proactive_2022,
	title = {Proactive Libraries: Enforcing Correct Behaviors in Android Apps},
	url = {http://arxiv.org/abs/2202.11999},
	doi = {10.1145/3510454.3516837},
	shorttitle = {Proactive Libraries},
	abstract = {The Android framework provides a rich set of {APIs} that can be exploited by developers to build their apps. However, the rapid evolution of these {APIs} jointly with the specific characteristics of the lifecycle of the Android components challenge developers, who may release apps that use {APIs} incorrectly. In this demo, we present Proactive Libraries, a tool that can be used to decorate regular libraries with the capability of proactively detecting and healing {API} misuses at runtime. Proactive Libraries blend libraries with multiple proactive modules that collect data, check the compliance of {API} usages with correctness policies, and heal executions as soon as the possible violation of a policy is detected. The results of our evaluation with 27 possible {API} misuses show the effectiveness of Proactive Libraries in correcting {API} misuses with negligible runtime overhead.},
	journaltitle = {{arXiv}:2202.11999 [cs]},
	author = {Riganelli, Oliviero and Fagadau, Ionut Daniel and Micucci, Daniela and Mariani, Leonardo},
	urldate = {2022-03-03},
	date = {2022-02-24},
	eprinttype = {arxiv},
	eprint = {2202.11999},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/home/fordrl/Zotero/storage/2FZDQIGV/Riganelli et al. - 2022 - Proactive Libraries Enforcing Correct Behaviors i.pdf:application/pdf;arXiv.org Snapshot:/home/fordrl/Zotero/storage/TSAVB7EA/2202.html:text/html},
}

@inproceedings{theodoridis_finding_2022,
	location = {Lausanne Switzerland},
	title = {Finding missed optimizations through the lens of dead code elimination},
	isbn = {978-1-4503-9205-1},
	url = {https://dl.acm.org/doi/10.1145/3503222.3507764},
	doi = {10.1145/3503222.3507764},
	abstract = {Compilers are foundational software development tools and incorporate increasingly sophisticated optimizations. Due to their complexity, it is difficult to systematically identify opportunities for improving them. Indeed, the automatic discovery of missed optimizations has been an important and significant challenge. The few existing approaches either cannot accurately pinpoint missed optimizations or target only specific analyses. This paper tackles this challenge by introducing a novel, effective approach that — in a simple and general manner — automatically identifies a wide range of missed optimizations. Our core insight is to leverage dead code elimination ({DCE}) to both analyze how well compilers optimize code and identify missed optimizations: (1) insert “optimization markers” in the basic blocks of a given program, (2) compute the program’s live/dead basic blocks using the “optimization markers”, and (3) identify missed optimizations from how well compilers eliminate dead blocks. We essentially exploit that, since {DCE} heavily depends on the rest of the optimization pipeline, through the lens of {DCE}, one can systematically quantify how well compilers optimize code. We conduct an extensive analysis of {GCC} and {LLVM} using our approach, which (1) provides quantitative and qualitative insights regarding their optimization capabilities, and (2) uncovers a diverse set of missed optimizations. Our results also lead to 84 bug reports for {GCC} and {LLVM}, of which 62 have already been confirmed or fixed, demonstrating our work’s strong practical utility. We expect that the simplicity and generality of our approach will make it widely applicable for understanding compiler performance and finding missed optimizations. This work opens and initiates this promising direction.},
	eventtitle = {{ASPLOS} '22: 27th {ACM} International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {697--709},
	booktitle = {Proceedings of the 27th {ACM} International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {{ACM}},
	author = {Theodoridis, Theodoros and Rigger, Manuel and Su, Zhendong},
	urldate = {2022-03-08},
	date = {2022-02-28},
	langid = {english},
	file = {Theodoridis et al. - 2022 - Finding missed optimizations through the lens of d.pdf:/home/fordrl/Zotero/storage/JME86L6H/Theodoridis et al. - 2022 - Finding missed optimizations through the lens of d.pdf:application/pdf},
}