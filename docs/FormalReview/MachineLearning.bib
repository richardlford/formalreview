
@article{peng_how_2021,
	title = {How could Neural Networks understand Programs?},
	url = {http://arxiv.org/abs/2105.04297},
	abstract = {Semantic understanding of programs is a fundamental problem for programming language processing ({PLP}). Recent works that learn representations of code based on pre-training techniques in {NLP} have pushed the frontiers in this direction. However, the semantics of {PL} and {NL} have essential differences. These being ignored, we believe it is difficult to build a model to better understand programs, by either directly applying off-the-shelf {NLP} pre-training techniques to the source code, or adding features to the model by the heuristic. In fact, the semantics of a program can be rigorously defined by formal semantics in {PL} theory. For example, the operational semantics, describes the meaning of a valid program as updating the environment (i.e., the memory address-value function) through fundamental operations, such as memory I/O and conditional branching. Inspired by this, we propose a novel program semantics learning paradigm, that the model should learn from information composed of (1) the representations which align well with the fundamental operations in operational semantics, and (2) the information of environment transition, which is indispensable for program understanding. To validate our proposal, we present a hierarchical Transformer-based pre-training model called {OSCAR} to better facilitate the understanding of programs. {OSCAR} learns from intermediate representation ({IR}) and an encoded representation derived from static analysis, which are used for representing the fundamental operations and approximating the environment transitions respectively. {OSCAR} empirically shows the outstanding capability of program semantics understanding on many practical software engineering tasks.},
	journaltitle = {{arXiv}:2105.04297 [cs]},
	author = {Peng, Dinglan and Zheng, Shuxin and Li, Yatao and Ke, Guolin and He, Di and Liu, Tie-Yan},
	urldate = {2021-05-19},
	date = {2021-05-10},
	eprinttype = {arxiv},
	eprint = {2105.04297},
	keywords = {Computer Science - Programming Languages, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/QAE3V2CM/2105.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/FPRAUSZM/Peng et al. - 2021 - How could Neural Networks understand Programs.pdf:application/pdf},
}

@article{urban_review_2021,
	title = {A Review of Formal Methods applied to Machine Learning},
	url = {http://arxiv.org/abs/2104.02466},
	abstract = {We review state-of-the-art formal methods applied to the emerging field of the verification of machine learning systems. Formal methods can provide rigorous correctness guarantees on hardware and software systems. Thanks to the availability of mature tools, their use is well established in the industry, and in particular to check safety-critical applications as they undergo a stringent certification process. As machine learning is becoming more popular, machine-learned components are now considered for inclusion in critical systems. This raises the question of their safety and their verification. Yet, established formal methods are limited to classic, i.e. non machine-learned software. Applying formal methods to verify systems that include machine learning has only been considered recently and poses novel challenges in soundness, precision, and scalability. We first recall established formal methods and their current use in an exemplar safety-critical field, avionic software, with a focus on abstract interpretation based techniques as they provide a high level of scalability. This provides a golden standard and sets high expectations for machine learning verification. We then provide a comprehensive and detailed review of the formal methods developed so far for machine learning, highlighting their strengths and limitations. The large majority of them verify trained neural networks and employ either {SMT}, optimization, or abstract interpretation techniques. We also discuss methods for support vector machines and decision tree ensembles, as well as methods targeting training and data preparation, which are critical but often neglected aspects of machine learning. Finally, we offer perspectives for future research directions towards the formal verification of machine learning systems.},
	journaltitle = {{arXiv}:2104.02466 [cs]},
	author = {Urban, Caterina and Miné, Antoine},
	urldate = {2021-08-26},
	date = {2021-04-21},
	eprinttype = {arxiv},
	eprint = {2104.02466},
	keywords = {Computer Science - Programming Languages, Computer Science - Logic in Computer Science, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/5RDEWXMD/2104.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/HY8ZHNDE/Urban and Miné - 2021 - A Review of Formal Methods applied to Machine Lear.pdf:application/pdf},
}

@article{sabour_dynamic_2017,
	title = {Dynamic Routing Between Capsules},
	url = {http://arxiv.org/abs/1710.09829},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on {MNIST} and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	journaltitle = {{arXiv}:1710.09829 [cs]},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
	urldate = {2021-03-01},
	date = {2017-11-07},
	eprinttype = {arxiv},
	eprint = {1710.09829},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/UWI8UQRK/1710.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/II56WD84/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf:application/pdf},
}

@inproceedings{hinton_transforming_2011,
	location = {Berlin, Heidelberg},
	title = {Transforming Auto-Encoders},
	isbn = {978-3-642-21735-7},
	doi = {10.1007/978-3-642-21735-7_6},
	series = {Lecture Notes in Computer Science},
	abstract = {The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like {SIFT} [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.},
	pages = {44--51},
	booktitle = {Artificial Neural Networks and Machine Learning – {ICANN} 2011},
	publisher = {Springer},
	author = {Hinton, Geoffrey E. and Krizhevsky, Alex and Wang, Sida D.},
	editor = {Honkela, Timo and Duch, Włodzisław and Girolami, Mark and Kaski, Samuel},
	date = {2011},
	langid = {english},
	keywords = {auto-encoder, Invariance, shape representation},
	file = {Submitted Version:/home/fordrl/Zotero/storage/94MKG8HG/Hinton et al. - 2011 - Transforming Auto-Encoders.pdf:application/pdf},
}

@incollection{hinton_next_2020,
	location = {New York, {NY}, {USA}},
	title = {The Next Generation of Neural Networks},
	isbn = {978-1-4503-8016-4},
	url = {https://doi.org/10.1145/3397271.3402425},
	abstract = {The most important unsolved problem with artificial neural networks is how to do unsupervised learning as effectively as the brain. There are currently two main approaches to unsupervised learning. In the first approach, exemplified by {BERT} and Variational Autoencoders, a deep neural network is used to reconstruct its input. This is problematic for images because the deepest layers of the network need to encode the fine details of the image. An alternative approach, introduced by Becker and Hinton in 1992, is to train two copies of a deep neural network to produce output vectors that have high mutual information when given two different crops of the same image as their inputs. This approach was designed to allow the representations to be untethered from irrelevant details of the input. The method of optimizing mutual information used by Becker and Hinton was flawed (for a subtle reason that I will explain) so Pacannaro and Hinton (2001) replaced it by a discriminative objective in which one vector representation must select a corresponding vector representation from among many alternatives. With faster hardware, contrastive learning of representations has recently become very popular and is proving to be very effective, but it suffers from a major flaw: To learn pairs of representation vectors that have N bits of mutual information we need to contrast the correct corresponding vector with about 2N incorrect alternatives. I will describe a novel and effective way of dealing with this limitation. I will also show that this leads to a simple way of implementing perceptual learning in cortex.},
	pages = {1},
	booktitle = {Proceedings of the 43rd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Hinton, Geoffrey},
	urldate = {2021-02-22},
	date = {2020-07-25},
	keywords = {deep learning, neural networks, unsupervised learning},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/2XC3KVUD/Hinton - 2020 - The Next Generation of Neural Networks.pdf:application/pdf},
}

@inproceedings{suneja_towards_2021,
	location = {New York, {NY}, {USA}},
	title = {Towards Reliable {AI} for Source Code Understanding},
	isbn = {978-1-4503-8638-8},
	url = {https://doi.org/10.1145/3472883.3486995},
	doi = {10.1145/3472883.3486995},
	series = {{SoCC} '21},
	abstract = {Cloud maturity and popularity have resulted in Open source software ({OSS}) proliferation. And, in turn, managing {OSS} code quality has become critical in ensuring sustainable Cloud growth. On this front, {AI} modeling has gained popularity in source code understanding tasks, promoted by the ready availability of large open codebases. However, we have been observing certain peculiarities with these black-boxes, motivating a call for their reliability to be verified before offsetting traditional code analysis. In this work, we highlight and organize different reliability issues affecting {AI}-for-code into three stages of an {AI} pipeline- data collection, model training, and prediction analysis. We highlight the need for concerted efforts from the research community to ensure credibility, accountability, and traceability for {AI}-for-code. For each stage, we discuss unique opportunities afforded by the source code and software engineering setting to improve {AI} reliability.},
	pages = {403--411},
	booktitle = {Proceedings of the {ACM} Symposium on Cloud Computing},
	publisher = {Association for Computing Machinery},
	author = {Suneja, Sahil and Zheng, Yunhui and Zhuang, Yufan and Laredo, Jim A. and Morari, Alessandro},
	urldate = {2021-11-03},
	date = {2021-11-01},
	keywords = {explainability, machine learning, reliability, signal awareness},
	file = {Full Text PDF:/home/fordrl/Zotero/storage/5E63V8IX/Suneja et al. - 2021 - Towards Reliable AI for Source Code Understanding.pdf:application/pdf},
}

@inproceedings{torfah_formal_2021,
	location = {Cham},
	title = {Formal Analysis of {AI}-Based Autonomy: From Modeling to Runtime Assurance},
	isbn = {978-3-030-88494-9},
	url = {https://link.springer.com/chapter/10.1007/978-3-030-88494-9_19},
	doi = {10.1007/978-3-030-88494-9_19},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Formal Analysis of {AI}-Based Autonomy},
	abstract = {Autonomous systems are increasingly deployed in safety-critical applications and rely more on high-performance components based on artificial intelligence ({AI}) and machine learning ({ML}). Runtime monitors play an important role in raising the level of assurance in {AI}/{ML}-based autonomous systems by ensuring that the autonomous system stays safe within its operating environment. In this tutorial, we present {VerifAI}, an open-source toolkit for the formal design and analysis of systems that include {AI}/{ML} components. {VerifAI} provides features supporting a variety of use cases including formal modeling of the autonomous system and its environment, automatic falsification of system-level specifications as well as other simulation-based verification and testing methods, automated diagnosis of errors, and automatic specification-driven parameter and component synthesis. In particular, we describe the use of {VerifAI} for generating runtime monitors that capture the safe operational environment of systems with {AI}/{ML} components. We illustrate the advantages and applicability of {VerifAI} in real-life applications using a case study from the domain of autonomous aviation.},
	pages = {311--330},
	booktitle = {Runtime Verification},
	publisher = {Springer International Publishing},
	author = {Torfah, Hazem and Junges, Sebastian and Fremont, Daniel J. and Seshia, Sanjit A.},
	editor = {Feng, Lu and Fisman, Dana},
	date = {2021},
	langid = {english},
}

@article{mirman_provable_2020,
	title = {A Provable Defense for Deep Residual Networks},
	url = {http://arxiv.org/abs/1903.12519},
	abstract = {We present a training system, which can provably defend significantly larger neural networks than previously possible, including {ResNet}-34 and {DenseNet}-100. Our approach is based on differentiable abstract interpretation and introduces two novel concepts: (i) abstract layers for fine-tuning the precision and scalability of the abstraction, (ii) a flexible domain specific language ({DSL}) for describing training objectives that combine abstract and concrete losses with arbitrary specifications. Our training method is implemented in the {DiffAI} system.},
	journaltitle = {{arXiv}:1903.12519 [cs, stat]},
	author = {Mirman, Matthew and Singh, Gagandeep and Vechev, Martin},
	urldate = {2021-11-26},
	date = {2020-01-07},
	eprinttype = {arxiv},
	eprint = {1903.12519},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Programming Languages, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/3MX4LKPZ/1903.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/S7PSZWA4/Mirman et al. - 2020 - A Provable Defense for Deep Residual Networks.pdf:application/pdf},
}

@article{mirman_differentiable_nodate,
	title = {Differentiable Abstract Interpretation for Provably Robust Neural Networks},
	abstract = {We introduce a scalable method for training robust neural networks based on abstract interpretation. We present several abstract transformers which balance efﬁciency with precision and show these can be used to train large neural networks that are certiﬁably robust to adversarial perturbations.},
	pages = {13},
	author = {Mirman, Matthew and Gehr, Timon and Vechev, Martin},
	langid = {english},
	file = {Mirman et al. - Differentiable Abstract Interpretation for Provabl.pdf:/home/fordrl/Zotero/storage/FV7RWUIF/Mirman et al. - Differentiable Abstract Interpretation for Provabl.pdf:application/pdf},
}

@article{yang_accelerating_nodate,
	title = {Accelerating Symbolic Analysis for Android Apps},
	abstract = {While tools based on symbolic execution are commonly used to analyze mobile applications, these tools can suffer from path explosion when real-world applications have more paths than available computing resources can handle. However, many of the paths are unsatisﬁable, that is, no input exists that can satisfy all the path constraints and cause the path to execute. Unfortunately, analysis tools cannot determine this without constraint collection and constraint solving, which are expensive to perform. As a result, analysis tools waste valuable computational resources on unsatisﬁable paths.},
	pages = {6},
	author = {Yang, Mingyue and Lie, David and Papernot, Nicolas},
	langid = {english},
	file = {Yang et al. - Accelerating Symbolic Analysis for Android Apps.pdf:/home/fordrl/Zotero/storage/ZLVU7TIB/Yang et al. - Accelerating Symbolic Analysis for Android Apps.pdf:application/pdf},
}

@article{cito_counterfactual_2021,
	title = {Counterfactual Explanations for Models of Code},
	url = {http://arxiv.org/abs/2111.05711},
	abstract = {Machine learning ({ML}) models play an increasingly prevalent role in many software engineering tasks. However, because most models are now powered by opaque deep neural networks, it can be difficult for developers to understand why the model came to a certain conclusion and how to act upon the model's prediction. Motivated by this problem, this paper explores counterfactual explanations for models of source code. Such counterfactual explanations constitute minimal changes to the source code under which the model "changes its mind". We integrate counterfactual explanation generation to models of source code in a real-world setting. We describe considerations that impact both the ability to find realistic and plausible counterfactual explanations, as well as the usefulness of such explanation to the user of the model. In a series of experiments we investigate the efficacy of our approach on three different models, each based on a {BERT}-like architecture operating over source code.},
	journaltitle = {{arXiv}:2111.05711 [cs]},
	author = {Cito, Jürgen and Dillig, Isil and Murali, Vijayaraghavan and Chandra, Satish},
	urldate = {2021-11-26},
	date = {2021-11-10},
	eprinttype = {arxiv},
	eprint = {2111.05711},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/fordrl/Zotero/storage/7FZ8LQRA/2111.html:text/html;arXiv Fulltext PDF:/home/fordrl/Zotero/storage/6QEVDLID/Cito et al. - 2021 - Counterfactual Explanations for Models of Code.pdf:application/pdf},
}

@article{daggitt_vehicle_2022,
	title = {Vehicle: Interfacing Neural Network Verifiers with Interactive Theorem Provers},
	url = {http://arxiv.org/abs/2202.05207},
	shorttitle = {Vehicle},
	abstract = {Verification of neural networks is currently a hot topic in automated theorem proving. Progress has been rapid and there are now a wide range of tools available that can verify properties of networks with hundreds of thousands of nodes. In theory this opens the door to the verification of larger control systems that make use of neural network components. However, although work has managed to incorporate the results of these verifiers to prove larger properties of individual systems, there is currently no general methodology for bridging the gap between verifiers and interactive theorem provers ({ITPs}). In this paper we present Vehicle, our solution to this problem. Vehicle is equipped with an expressive domain specific language for stating neural network specifications which can be compiled to both verifiers and {ITPs}. It overcomes previous issues with maintainability and scalability in similar {ITP} formalisations by using a standard {ONNX} file as the single canonical representation of the network. We demonstrate its utility by using it to connect the neural network verifier Marabou to Agda and then formally verifying that a car steered by a neural network never leaves the road, even in the face of an unpredictable cross wind and imperfect sensors. The network has over 20,000 nodes, and therefore this proof represents an improvement of 3 orders of magnitude over prior proofs about neural network enhanced systems in {ITPs}.},
	journaltitle = {{arXiv}:2202.05207 [cs]},
	author = {Daggitt, Matthew L. and Kokke, Wen and Atkey, Robert and Arnaboldi, Luca and Komendantskya, Ekaterina},
	urldate = {2022-02-15},
	date = {2022-02-10},
	eprinttype = {arxiv},
	eprint = {2202.05207},
	keywords = {Computer Science - Programming Languages, Computer Science - Logic in Computer Science, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/fordrl/Zotero/storage/H9FMY6TN/Daggitt et al. - 2022 - Vehicle Interfacing Neural Network Verifiers with.pdf:application/pdf;arXiv.org Snapshot:/home/fordrl/Zotero/storage/BKQT6NJH/2202.html:text/html},
}

@article{schumi_exais_2022,
	title = {{ExAIS}: Executable {AI} Semantics},
	url = {http://arxiv.org/abs/2202.09868},
	doi = {10.1145/3510003.3510112},
	shorttitle = {{ExAIS}},
	abstract = {Neural networks can be regarded as a new programming paradigm, i.e., instead of building ever-more complex programs through (often informal) logical reasoning in the programmers' mind, complex '{AI}' systems are built by optimising generic neural network models with big data. In this new paradigm, {AI} frameworks such as {TensorFlow} and {PyTorch} play a key role, which is as essential as the compiler for traditional programs. It is known that the lack of a proper semantics for programming languages (such as C), i.e., a correctness specification for compilers, has contributed to many problematic program behaviours and security issues. While it is in general hard to have a correctness specification for compilers due to the high complexity of programming languages and their rapid evolution, we have a unique opportunity to do it right this time for neural networks (which have a limited set of functions, and most of them have stable semantics). In this work, we report our effort on providing a correctness specification of neural network frameworks such as {TensorFlow}. We specify the semantics of almost all {TensorFlow} layers in the logical programming language Prolog. We demonstrate the usefulness of the semantics through two applications. One is a fuzzing engine for {TensorFlow}, which features a strong oracle and a systematic way of generating valid neural networks. The other is a model validation approach which enables consistent bug reporting for {TensorFlow} models.},
	journaltitle = {{arXiv}:2202.09868 [cs]},
	author = {Schumi, Richard and Sun, Jun},
	urldate = {2022-03-03},
	date = {2022-02-20},
	eprinttype = {arxiv},
	eprint = {2202.09868},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/fordrl/Zotero/storage/6VC5KAQ5/Schumi and Sun - 2022 - ExAIS Executable AI Semantics.pdf:application/pdf;arXiv.org Snapshot:/home/fordrl/Zotero/storage/JGMZ2H9X/2202.html:text/html},
}